{
  "1611.04684v1_q0": {
    "paper_id": "1611.04684v1",
    "question_index": 0,
    "question": "What are the main differences between the educational philosophies of the Bonaparte and Voltaire schools?",
    "ground_truth": "The Bonaparte school focuses on outdoor physical activities, maneuvers, and strategies, with a specialization in horse riding, lances, and swords. They aim to develop students into good leaders. The Voltaire school, on the other hand, encourages independent thinking and focuses on indoor activities. They aim to instill good moral values and develop students into philosophical thinkers.",
    "predicted_answer": "The main differences are that Bonaparte focuses on outdoor physical activities, leadership, and strategies, while Voltaire emphasizes indoor activities, independent thinking, and moral development.",
    "reference": "1611.04684v1-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0547008547008547,
      "phrase_overlap": 0.0547008547008547,
      "is_correct": false
    }
  },
  "1611.04684v1_q1": {
    "paper_id": "1611.04684v1",
    "question_index": 1,
    "question": "Which model performs the best for response selection, and how can we tell?",
    "ground_truth": "The KEHNN model performs the best for response selection. This is evident because it achieves the highest scores across all metrics (R$2$@1, R${10}$@1, R${10}$@2, and R${10}$@5) compared to all other models in the table.",
    "predicted_answer": "KEHNN performs the best for response selection, as it achieves the highest scores across all metrics (R2@1: 0.786, R10@1: 0.460, R10@2: 0.591, R10@5: 0.819).",
    "reference": "1611.04684v1-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.27055702917771884,
      "phrase_overlap": 0.27055702917771884,
      "is_correct": true
    }
  },
  "1611.04684v1_q2": {
    "paper_id": "1611.04684v1",
    "question_index": 2,
    "question": "Which model performs best on the Ubuntu dataset for text lengths between 60 and 90 words?",
    "ground_truth": "KEHNN",
    "predicted_answer": "C",
    "reference": "1611.04684v1-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1611.04684v1_q3": {
    "paper_id": "1611.04684v1",
    "question_index": 3,
    "question": "What is the role of the knowledge gates in the KEHNN architecture?",
    "ground_truth": "The knowledge gates are responsible for selecting relevant information from the knowledge base and incorporating it into the model's hidden state.",
    "predicted_answer": "The role of the knowledge gates in the KEHNN architecture is to combine external knowledge embeddings (ek) with input embeddings (ew) using a gating mechanism, producing enhanced embeddings (ew') for further processing.",
    "reference": "1611.04684v1-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3452054794520548,
      "phrase_overlap": 0.3452054794520548,
      "is_correct": false
    }
  },
  "1611.04684v1_q4": {
    "paper_id": "1611.04684v1",
    "question_index": 4,
    "question": "How does the average number of answers per question differ between the training and development sets? What might be a possible explanation for this difference?",
    "ground_truth": "The training set has a higher average number of answers per question (6.36) compared to the development set (5.48). This suggests that questions in the training set tend to have more potential answers associated with them than those in the development set.",
    "predicted_answer": "The average number of answers per question is 6.36 for the Training set and 5.48 for the Development set, showing a difference of 0.88. A possible explanation for this difference could be that the Training set is designed to be more comprehensive and diverse, including more answers per question to better train the model, while the Development set might be smaller and less diverse.",
    "reference": "1611.04684v1-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.17214397496087636,
      "phrase_overlap": 0.17214397496087636,
      "is_correct": true
    }
  },
  "1611.05742v3_q0": {
    "paper_id": "1611.05742v3",
    "question_index": 0,
    "question": "What is the function of the ReOrth Layer in the Projection Block of the Grassmann Network architecture?",
    "ground_truth": "The ReOrth Layer re-orthogonalizes the output of the FRMap Layer.",
    "predicted_answer": "ReOrth Layer performs QR decomposition.",
    "reference": "1611.05742v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3269230769230769,
      "phrase_overlap": 0.3269230769230769,
      "is_correct": false
    }
  },
  "1611.05742v3_q1": {
    "paper_id": "1611.05742v3",
    "question_index": 1,
    "question": "Which pooling method is the most accurate for the AFEW database?",
    "ground_truth": "W-ProjPooling",
    "predicted_answer": "D",
    "reference": "1611.05742v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1611.05742v3_q2": {
    "paper_id": "1611.05742v3",
    "question_index": 2,
    "question": "Which method performs best on the PaSC dataset for the handheld testing scenario (PaSC2), and how does its performance compare to other methods?",
    "ground_truth": "The method that performs best on the PaSC dataset for the handheld testing scenario (PaSC2) is SPDNet, with an accuracy of 72.83%. This performance is slightly higher than GrNet-2Blocks (72.76%) and significantly higher than other methods like VGGDeepFace (68.24%) and DeepO2P (60.14%).",
    "predicted_answer": "SPDNet performs best on the PaSC2 dataset for the handheld testing scenario with a performance of 72.83%, slightly outperforming GrNet-2Blocks (72.76%) and GrNet-1Block (72.51%).",
    "reference": "1611.05742v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3577586206896552,
      "phrase_overlap": 0.3577586206896552,
      "is_correct": true
    }
  },
  "1611.07718v2_q0": {
    "paper_id": "1611.07718v2",
    "question_index": 0,
    "question": "Which network architecture has the highest accuracy on the CIFAR-10 dataset?",
    "ground_truth": "DMRNet-Wide",
    "predicted_answer": "C",
    "reference": "1611.07718v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1611.07718v2_q1": {
    "paper_id": "1611.07718v2",
    "question_index": 1,
    "question": "What is the difference between a deep residual network and a network built by stacking inception-like blocks?",
    "ground_truth": "Deep residual networks have skip connections that allow the gradient to flow directly from one layer to another, while networks built by stacking inception-like blocks do not.",
    "predicted_answer": "Residual networks use skip connections (a), while inception-like networks use parallel convolutional paths (b, c).",
    "reference": "1611.07718v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.47750865051903113,
      "phrase_overlap": 0.47750865051903113,
      "is_correct": false
    }
  },
  "1611.07718v2_q2": {
    "paper_id": "1611.07718v2",
    "question_index": 2,
    "question": "What is the difference between the residual block and the merge-and-run block?",
    "ground_truth": "The residual block assembles two residual branches sequentially, while the merge-and-run block assembles the same two residual branches in parallel.",
    "predicted_answer": "Residual block uses a single skip connection, while merge-and-run block uses multiple parallel skip connections.",
    "reference": "1611.07718v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5461538461538461,
      "phrase_overlap": 0.5461538461538461,
      "is_correct": false
    }
  },
  "1611.07718v2_q3": {
    "paper_id": "1611.07718v2",
    "question_index": 3,
    "question": "How does the training loss of DMRNet compare to that of ResNet on the CIFAR-10 dataset with L = 30?",
    "ground_truth": "The training loss of DMRNet is lower than that of ResNet on the CIFAR-10 dataset with L = 30.",
    "predicted_answer": "Lower.",
    "reference": "1611.07718v2-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.12121212121212122,
      "phrase_overlap": 0.12121212121212122,
      "is_correct": false
    }
  },
  "1611.07718v2_q4": {
    "paper_id": "1611.07718v2",
    "question_index": 4,
    "question": "Which network has the shortest average path length when L = 9?",
    "ground_truth": "DMRNet",
    "predicted_answer": "C",
    "reference": "1611.07718v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1611.07718v2_q5": {
    "paper_id": "1611.07718v2",
    "question_index": 5,
    "question": "Which model has the lowest Top-1 validation error on ImageNet?",
    "ground_truth": "ResNet-101 from the reference paper reports top-1 validation error of 23.60 which is lower than ResNet-101 reevaluated (26.41) and DMRNet (23.66)",
    "predicted_answer": "ResNet-101 [7] has the lowest Top-1 validation error on ImageNet with 23.60%.",
    "reference": "1611.07718v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.45045045045045046,
      "phrase_overlap": 0.45045045045045046,
      "is_correct": true
    }
  },
  "1611.07718v2_q6": {
    "paper_id": "1611.07718v2",
    "question_index": 6,
    "question": "Does sharing the first convolutional layer and the last fully connected layer improve the accuracy of the merge-and-run mapping?",
    "ground_truth": "Yes.",
    "predicted_answer": "Yes, sharing the first convolutional layer and the last fully connected layer improves the accuracy of the merge-and-run mapping for both CIFAR-10 and CIFAR-100 datasets.",
    "reference": "1611.07718v2-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.04597701149425287,
      "phrase_overlap": 0.04597701149425287,
      "is_correct": false
    }
  },
  "1611.07718v2_q7": {
    "paper_id": "1611.07718v2",
    "question_index": 7,
    "question": "How does the classification error of a residual network change as the average path length increases?",
    "ground_truth": "The classification error of a residual network generally increases as the average path length increases.",
    "predicted_answer": "The classification error of a residual network increases as the average path length increases.",
    "reference": "1611.07718v2-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9494949494949495,
      "phrase_overlap": 0.9494949494949495,
      "is_correct": true
    }
  },
  "1611.03780v2_q0": {
    "paper_id": "1611.03780v2",
    "question_index": 0,
    "question": "How does GeoCUTS perform compared to other clusterings for highly active users?",
    "ground_truth": "GeoCUTS performs comparably to other clusterings for highly active users.",
    "predicted_answer": "GeoCUTS performs equally well as DMA and Grid for US Highly Active users.",
    "reference": "1611.03780v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6986301369863014,
      "phrase_overlap": 0.6986301369863014,
      "is_correct": true
    }
  },
  "1611.03780v2_q1": {
    "paper_id": "1611.03780v2",
    "question_index": 1,
    "question": "How is the Hilbert space-filling curve constructed?",
    "ground_truth": "The Hilbert space-filling curve is constructed recursively. The curve starts with a simple square, and then at each subsequent iteration, the curve is subdivided into four smaller squares. The curve is then drawn through each of these squares in a specific order.",
    "predicted_answer": "The Hilbert space-filling curve is constructed through a recursive process where the space is subdivided into smaller regions, and the curve is adjusted at each step to fill the space more completely.",
    "reference": "1611.03780v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.23758099352051837,
      "phrase_overlap": 0.23758099352051837,
      "is_correct": false
    }
  },
  "1611.03780v2_q2": {
    "paper_id": "1611.03780v2",
    "question_index": 2,
    "question": "How does the performance of GeoCUTS compare to the Grid method in identifying highly mobile clusters when the number of clusters is increased from approximately 25 to 50?",
    "ground_truth": "GeoCUTS consistently outperforms the Grid method in identifying highly mobile clusters, regardless of the number of clusters. However, the performance of both methods decreases as the number of clusters increases.",
    "predicted_answer": "GeoCUTS outperforms the Grid method in identifying highly mobile clusters across all thresholds when the number of clusters increases from approximately 25 to 50.",
    "reference": "1611.03780v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4053333333333333,
      "phrase_overlap": 0.4053333333333333,
      "is_correct": false
    }
  },
  "1611.03780v2_q3": {
    "paper_id": "1611.03780v2",
    "question_index": 3,
    "question": "What is the relationship between the query graph and the interference graph?",
    "ground_truth": " The interference graph is a folded version of the query graph. The nodes in the interference graph represent regions, and the edges represent the interference between regions. The edge weights in the interference graph are calculated from the edge weights in the query graph.",
    "predicted_answer": "Fold.",
    "reference": "1611.03780v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.03571428571428571,
      "phrase_overlap": 0.03571428571428571,
      "is_correct": false
    }
  },
  "1611.03780v2_q4": {
    "paper_id": "1611.03780v2",
    "question_index": 4,
    "question": "Which metropolitan area is predicted by the GeoCUTS algorithm to include San Francisco, Berkeley, and Palo Alto, but not Sacramento?",
    "ground_truth": "The Bay Area.",
    "predicted_answer": "San Francisco Bay Area",
    "reference": "1611.03780v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5142857142857142,
      "phrase_overlap": 0.5142857142857142,
      "is_correct": false
    }
  },
  "1611.03780v2_q5": {
    "paper_id": "1611.03780v2",
    "question_index": 5,
    "question": "Which metropolitan areas are correctly identified by the GeoCUTS algorithm in France?",
    "ground_truth": "Paris, Bordeaux, and Lyon.",
    "predicted_answer": "A",
    "reference": "1611.03780v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.07407407407407407,
      "phrase_overlap": 0.07407407407407407,
      "is_correct": true
    }
  },
  "1611.03780v2_q6": {
    "paper_id": "1611.03780v2",
    "question_index": 6,
    "question": "Which clustering algorithm produced the smallest cut size for highly active users in the US dataset, and how much smaller was it compared to the cut size produced by grid partitioning?",
    "ground_truth": "Both GeoCUTS and Linear Embedding produced the smallest cut size for highly active users, with a cut size of 4%. This is 11% smaller than the cut size produced by grid partitioning, which had a cut size of 15%.",
    "predicted_answer": "GeoCUTS, 11%",
    "reference": "1611.03780v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.05405405405405406,
      "phrase_overlap": 0.05405405405405406,
      "is_correct": true
    }
  },
  "1611.03780v2_q7": {
    "paper_id": "1611.03780v2",
    "question_index": 7,
    "question": "Which method performs best for highly active users in the US?",
    "ground_truth": "Both GeoCUTS and DMA perform equally well for highly active users in the US.",
    "predicted_answer": "A",
    "reference": "1611.03780v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.025974025974025976,
      "phrase_overlap": 0.025974025974025976,
      "is_correct": true
    }
  },
  "1611.04363v2_q0": {
    "paper_id": "1611.04363v2",
    "question_index": 0,
    "question": "Which method performed best according to the P@1 metric for the QA-Expert task, and how much better did it perform compared to the average P@1 score of the D2V method? ",
    "ground_truth": "The WeakFG method achieved the highest P@1 score for the QA-Expert task with a score of 52.8. This is 23.2% higher than the average P@1 score of the D2V method, which was 29.6. ",
    "predicted_answer": "52.8, 23.2",
    "reference": "1611.04363v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0967741935483871,
      "phrase_overlap": 0.0967741935483871,
      "is_correct": true
    }
  },
  "1611.04363v2_q1": {
    "paper_id": "1611.04363v2",
    "question_index": 1,
    "question": "What is the relationship between the decline probability of an expert and whether or not they have a \"friend\" who has already declined?",
    "ground_truth": "The decline probability of an expert is higher if they have a \"friend\" who has already declined.",
    "predicted_answer": "The decline probability of an expert increases when they have a 'friend' who has already declined.",
    "reference": "1611.04363v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.8969072164948454,
      "phrase_overlap": 0.8969072164948454,
      "is_correct": true
    }
  },
  "1608.02784v2_q0": {
    "paper_id": "1608.02784v2",
    "question_index": 0,
    "question": "Which system from Ortiz et al. achieved the highest BLEU and METEOR scores, and how does it compare to the CCA inference algorithm in terms of performance?",
    "ground_truth": "The SMT system from Ortiz et al. achieved the highest BLEU score (43.7) and METEOR score (35.6) among their tested systems. While the SMT system outperforms the CCA inference algorithm in terms of BLEU score, the CCA algorithm achieves a slightly higher METEOR score (25.6 vs 25.5).",
    "predicted_answer": "The SMT system from Ortiz et al. achieved the highest BLEU (43.7) and METEOR (35.6) scores. Compared to the CCA inference algorithm, SMT significantly outperformed CCA in both BLEU and METEOR scores.",
    "reference": "1608.02784v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.36174636174636177,
      "phrase_overlap": 0.36174636174636177,
      "is_correct": true
    }
  },
  "1608.02784v2_q1": {
    "paper_id": "1608.02784v2",
    "question_index": 1,
    "question": "What is the difference between the outputs of the machine translation system (SMT) and the CCA inference?",
    "ground_truth": "The SMT outputs are literal translations of the images, while the CCA outputs take into account the context of the images and generate more natural-sounding descriptions.",
    "predicted_answer": "The difference between the outputs of the machine translation system (SMT) and the CCA inference is that SMT focuses on describing Jenny's actions, while CCA provides more context or alternative interpretations involving Mike and other elements in the scene.",
    "reference": "1608.02784v2-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3598130841121495,
      "phrase_overlap": 0.3598130841121495,
      "is_correct": false
    }
  },
  "1608.02784v2_q2": {
    "paper_id": "1608.02784v2",
    "question_index": 2,
    "question": "Which system, CCA or SMT, generally performs better when the caption is of low quality (average rank less than 3)?",
    "ground_truth": "It is difficult to definitively say which system performs better for low-quality captions based solely on the provided data. However, we can observe some trends:\n\n1. When the SMT average rank is below 3, the CCA average rank is also lower (1.64 vs. 1.77).\n2. When the SMT average rank is 3 or higher, the CCA average rank is significantly higher (3.54 vs. 3.46).\n\nThis suggests that CCA might perform relatively better for low-quality captions, but more data and analysis are needed for a conclusive answer.",
    "predicted_answer": "SMT generally performs better when the caption is of low quality (average rank < 3).",
    "reference": "1608.02784v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1460101867572156,
      "phrase_overlap": 0.1460101867572156,
      "is_correct": true
    }
  },
  "1608.02784v2_q3": {
    "paper_id": "1608.02784v2",
    "question_index": 3,
    "question": "What is the purpose of the singular value decomposition step in the CCA algorithm?",
    "ground_truth": "The singular value decomposition step is used to find the projection matrices U and V.",
    "predicted_answer": "To calculate projection matrices U and V.",
    "reference": "1608.02784v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5354330708661418,
      "phrase_overlap": 0.5354330708661418,
      "is_correct": false
    }
  },
  "1608.02784v2_q4": {
    "paper_id": "1608.02784v2",
    "question_index": 4,
    "question": "What is the relationship between the input space and the output space in CCA inference?",
    "ground_truth": "The input space and the output space are related by a cosine similarity measure.",
    "predicted_answer": "The relationship between the input space and the output space in CCA inference is a mapping from visual input (image) to textual output (description).",
    "reference": "1608.02784v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.46956521739130436,
      "phrase_overlap": 0.46956521739130436,
      "is_correct": false
    }
  },
  "1608.02784v2_q5": {
    "paper_id": "1608.02784v2",
    "question_index": 5,
    "question": "What is the role of the temperature variable t in the CCA decoding algorithm?",
    "ground_truth": "The temperature variable t controls the probability of accepting a new candidate solution y. As t decreases, the probability of accepting a worse solution decreases.",
    "predicted_answer": "t controls the annealing process in the algorithm.",
    "reference": "1608.02784v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.33488372093023255,
      "phrase_overlap": 0.33488372093023255,
      "is_correct": false
    }
  },
  "1608.02784v2_q6": {
    "paper_id": "1608.02784v2",
    "question_index": 6,
    "question": "What is the relationship between BLEU score and human ranking for CCA and SMT systems?",
    "ground_truth": "The correlation between BLEU scores and human ranking is not high for either CCA or SMT systems.",
    "predicted_answer": "Higher BLEU scores generally correspond to higher human rankings for both CCA and SMT systems.",
    "reference": "1608.02784v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6210526315789474,
      "phrase_overlap": 0.6210526315789474,
      "is_correct": true
    }
  },
  "1611.02654v2_q0": {
    "paper_id": "1611.02654v2",
    "question_index": 0,
    "question": "Which model performed the best on the SICK dataset according to the MSE metric?",
    "ground_truth": "The supervised model performed the best on the SICK dataset according to the MSE metric.",
    "predicted_answer": "A",
    "reference": "1611.02654v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.02247191011235955,
      "phrase_overlap": 0.02247191011235955,
      "is_correct": true
    }
  },
  "1611.02654v2_q1": {
    "paper_id": "1611.02654v2",
    "question_index": 1,
    "question": "What is the effect of pre-training with the ordering task on the ROUGE-L score for extractive summarization?",
    "ground_truth": "Pre-training with the ordering task increases the ROUGE-L score for extractive summarization.",
    "predicted_answer": "Pre-training with the ordering task improves the ROUGE-L score for extractive summarization by 0.46 for summary length 75b and by 0.58 for summary length 275b.",
    "reference": "1611.02654v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6984126984126984,
      "phrase_overlap": 0.6984126984126984,
      "is_correct": true
    }
  },
  "1611.02654v2_q2": {
    "paper_id": "1611.02654v2",
    "question_index": 2,
    "question": "Which model performs the best for the order discrimination task on the Accidents dataset and how does it compare to the other data-driven approaches?",
    "ground_truth": "The proposed model in this paper achieves the best performance for the order discrimination task on the Accidents dataset with an accuracy of 0.944. It outperforms the other data-driven approaches, namely Window (Recurrent) with 0.840, Window (Recursive) with 0.864, and Seq2seq with 0.930.",
    "predicted_answer": "Ours",
    "reference": "1611.02654v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1611.02654v2_q3": {
    "paper_id": "1611.02654v2",
    "question_index": 3,
    "question": "What can you say about the relationship between the sentences in a document based on the t-SNE embeddings?",
    "ground_truth": "Sentences that are closer together in the embedding space are more semantically similar than those that are farther apart.",
    "predicted_answer": "Sentences exhibit a progression in embedding space from first to last within documents.",
    "reference": "1611.02654v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4784688995215311,
      "phrase_overlap": 0.4784688995215311,
      "is_correct": false
    }
  },
  "1611.02654v2_q4": {
    "paper_id": "1611.02654v2",
    "question_index": 4,
    "question": "How does the proposed model compare to the other models in terms of accuracy on the NIPS Abstracts dataset?",
    "ground_truth": "The proposed model has the highest accuracy on the NIPS Abstracts dataset, with an accuracy of 51.55.",
    "predicted_answer": "The proposed model achieves the highest accuracy (51.55%) on the NIPS Abstracts dataset compared to other models.",
    "reference": "1611.02654v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7383177570093458,
      "phrase_overlap": 0.7383177570093458,
      "is_correct": true
    }
  },
  "1701.03077v10_q0": {
    "paper_id": "1701.03077v10",
    "question_index": 0,
    "question": "How does the shape of the IRLS weight function change as the shape parameter α increases?",
    "ground_truth": "The IRLS weight function becomes more peaked and concentrated around zero as the shape parameter α increases.",
    "predicted_answer": "As the shape parameter α increases, the IRLS weight function becomes smoother and more spread out, with less steepness in its curves.",
    "reference": "1701.03077v10-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2892561983471074,
      "phrase_overlap": 0.2892561983471074,
      "is_correct": false
    }
  },
  "1701.03077v10_q1": {
    "paper_id": "1701.03077v10",
    "question_index": 1,
    "question": "How do the reconstructed faces in the \"Mean Reconstruction\" differ from those in the \"Sampled Reconstruction\"?",
    "ground_truth": "The reconstructed faces in the \"Mean Reconstruction\" are smoother and less detailed than those in the \"Sampled Reconstruction\". This is because the mean reconstruction is based on the average of all the possible reconstructions, while the sampled reconstruction is based on a single sample from the distribution.",
    "predicted_answer": "Mean Reconstruction produces smooth and clear faces, while Sampled Reconstruction results in noisy and distorted faces with artifacts.",
    "reference": "1701.03077v10-Figure12-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.16591928251121077,
      "phrase_overlap": 0.16591928251121077,
      "is_correct": false
    }
  },
  "1701.03077v10_q2": {
    "paper_id": "1701.03077v10",
    "question_index": 2,
    "question": "How does the performance of the adaptive model compare to the fixed model with different values of α?",
    "ground_truth": "The adaptive model consistently outperforms the fixed model for all values of α.",
    "predicted_answer": "Adaptive model consistently outperforms fixed model.",
    "reference": "1701.03077v10-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7878787878787878,
      "phrase_overlap": 0.7878787878787878,
      "is_correct": true
    }
  },
  "1701.03077v10_q3": {
    "paper_id": "1701.03077v10",
    "question_index": 3,
    "question": "On which dataset did the gRCC* algorithm achieve the largest relative improvement over the RCC algorithm, and by approximately how much did it improve?",
    "ground_truth": "The gRCC* algorithm achieved the largest relative improvement over the RCC algorithm on the YTF dataset, with a relative improvement of approximately 31.9%.",
    "predicted_answer": "The gRCC* algorithm achieved the largest relative improvement over the RCC algorithm on the YTF dataset, with an improvement of approximately 31.9%.",
    "reference": "1701.03077v10-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9671052631578947,
      "phrase_overlap": 0.9671052631578947,
      "is_correct": true
    }
  },
  "1701.03077v10_q4": {
    "paper_id": "1701.03077v10",
    "question_index": 4,
    "question": "What is the relationship between the shape parameter α and the shape of the loss function?",
    "ground_truth": "The shape parameter α controls the shape of the loss function. As α increases, the loss function becomes more peaked, and as α decreases, the loss function becomes more flat.",
    "predicted_answer": "α determines the steepness and curvature of the loss function.",
    "reference": "1701.03077v10-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3135593220338983,
      "phrase_overlap": 0.3135593220338983,
      "is_correct": false
    }
  },
  "1701.03077v10_q5": {
    "paper_id": "1701.03077v10",
    "question_index": 5,
    "question": "What is the effect of replacing the loss function in the \"Baseline\" network with the \"adaptive\" loss over wavelet coefficients?",
    "ground_truth": "Replacing the loss function in the \"Baseline\" network with the \"adaptive\" loss over wavelet coefficients results in significantly improved depth estimates.",
    "predicted_answer": "Replacing the loss function in the 'Baseline' network with the 'adaptive' loss over wavelet coefficients improves the depth estimation quality, making it closer to the ground truth.",
    "reference": "1701.03077v10-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7380952380952381,
      "phrase_overlap": 0.7380952380952381,
      "is_correct": true
    }
  },
  "1701.03077v10_q6": {
    "paper_id": "1701.03077v10",
    "question_index": 6,
    "question": "Which method for setting the shape parameter of the proposed loss function achieved the best performance in terms of average error? How much improvement did it offer compared to the reproduced baseline?",
    "ground_truth": "The \"adaptive $\\power \\in (0, 2)$\" strategy, where each wavelet coefficient has its own shape parameter that is optimized during training, achieved the best performance in terms of average error. It reduced the average error by approximately 17% compared to the reproduced baseline.",
    "predicted_answer": "Ours, adaptive α ∈ (0, 2); 0.066 improvement.",
    "reference": "1701.03077v10-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.05504587155963303,
      "phrase_overlap": 0.05504587155963303,
      "is_correct": true
    }
  },
  "1701.03077v10_q7": {
    "paper_id": "1701.03077v10",
    "question_index": 7,
    "question": "Why did the authors choose to use a nonlinearity to curve α before fitting the cubic hermite spline?",
    "ground_truth": "The authors chose to use a nonlinearity to curve α before fitting the cubic hermite spline because it allows for increased knot density near α = 2 and decreased knot density when α > 4. This helps to better approximate the log partition function, which is difficult to evaluate for arbitrary inputs.",
    "predicted_answer": "To improve interpolation accuracy.",
    "reference": "1701.03077v10-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.024024024024024024,
      "phrase_overlap": 0.024024024024024024,
      "is_correct": false
    }
  },
  "1701.03077v10_q8": {
    "paper_id": "1701.03077v10",
    "question_index": 8,
    "question": "What is the range of values for the shape parameter α?",
    "ground_truth": "The range of values for the shape parameter α is from 0 to 2.",
    "predicted_answer": "无法从图表中确定答案。",
    "reference": "1701.03077v10-Figure13-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1701.03077v10_q9": {
    "paper_id": "1701.03077v10",
    "question_index": 9,
    "question": "How does the performance of gFGR change as the shape parameter α increases?",
    "ground_truth": "The performance of gFGR generally improves as the shape parameter α increases.",
    "predicted_answer": "Mean RMSE and Max RMSE increase as α increases.",
    "reference": "1701.03077v10-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.432,
      "phrase_overlap": 0.432,
      "is_correct": false
    }
  },
  "1701.03077v10_q10": {
    "paper_id": "1701.03077v10",
    "question_index": 10,
    "question": "Which image representation results in the sharpest and highest-quality samples?",
    "ground_truth": "DCT and wavelet representations.",
    "predicted_answer": "Ours",
    "reference": "1701.03077v10-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1111111111111111,
      "phrase_overlap": 0.1111111111111111,
      "is_correct": false
    }
  },
  "1701.03077v10_q11": {
    "paper_id": "1701.03077v10",
    "question_index": 11,
    "question": "Which dataset shows the greatest sensitivity to the choice of $\\power$?",
    "ground_truth": "RCV1",
    "predicted_answer": "A",
    "reference": "1701.03077v10-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1701.03077v10_q12": {
    "paper_id": "1701.03077v10",
    "question_index": 12,
    "question": "How does the shape of the negative log-likelihood (NLL) and probability density functions change as the value of α increases?",
    "ground_truth": " As the value of α increases, the NLL functions become more peaked and the probability density functions become more concentrated around the mean.",
    "predicted_answer": "As α increases, the negative log-likelihood (NLL) becomes sharper and narrower, while the probability density function becomes more peaked and concentrated around the mean.",
    "reference": "1701.03077v10-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7255520504731862,
      "phrase_overlap": 0.7255520504731862,
      "is_correct": true
    }
  },
  "1701.03077v10_q13": {
    "paper_id": "1701.03077v10",
    "question_index": 13,
    "question": "How do the results of the baseline and the proposed method compare in terms of accuracy?",
    "ground_truth": "The proposed method appears to be more accurate than the baseline method. The depth maps generated by the proposed method are more detailed and realistic than those generated by the baseline method.",
    "predicted_answer": "The proposed method (Ours) demonstrates better accuracy in depth estimation compared to the baseline, as evidenced by its closer resemblance to the ground truth in all scenes.",
    "reference": "1701.03077v10-Figure17-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.46648793565683644,
      "phrase_overlap": 0.46648793565683644,
      "is_correct": false
    }
  },
  "1701.03077v10_q14": {
    "paper_id": "1701.03077v10",
    "question_index": 14,
    "question": "How does the choice of distribution affect the quality of the reconstructions?",
    "ground_truth": "Reconstructions from models using general distributions tend to be sharper and more detailed than reconstructions from the corresponding model that uses normal distributions.",
    "predicted_answer": "The choice of distribution significantly affects the quality of reconstructions, with \"Ours\" consistently producing the most realistic and detailed results across all methods (Pixels + RGB, DCT + YUV, Wavelets + YUV).",
    "reference": "1701.03077v10-Figure15-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3273657289002558,
      "phrase_overlap": 0.3273657289002558,
      "is_correct": false
    }
  },
  "1802.07222v1_q0": {
    "paper_id": "1802.07222v1",
    "question_index": 0,
    "question": "During which hours of the day did the most network-related reboots occur?",
    "ground_truth": "The most network-related reboots occurred between 18:00 and 20:00.",
    "predicted_answer": "The most network-related reboots occurred during hours 6 and 8.",
    "reference": "1802.07222v1-Figure14-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7751937984496124,
      "phrase_overlap": 0.7751937984496124,
      "is_correct": true
    }
  },
  "1603.00286v5_q0": {
    "paper_id": "1603.00286v5",
    "question_index": 0,
    "question": "Which agent values the entire share $Z_j$?",
    "ground_truth": "Agent $j$.",
    "predicted_answer": "No agent values the entire share Z_j; each agent values only a portion of it.",
    "reference": "1603.00286v5-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1839080459770115,
      "phrase_overlap": 0.1839080459770115,
      "is_correct": false
    }
  },
  "1603.00286v5_q1": {
    "paper_id": "1603.00286v5",
    "question_index": 1,
    "question": "What is the purpose of the blank space labeled Z'5?",
    "ground_truth": "The blank space labeled Z'5 is used to complete the allocation of the original pieces.",
    "predicted_answer": "The blank space labeled Z'5 is introduced to fill the remaining area in the red box after rearranging the original spaces Z1, Z2, Z3, and Z4.",
    "reference": "1603.00286v5-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6079295154185022,
      "phrase_overlap": 0.6079295154185022,
      "is_correct": true
    }
  },
  "1603.00286v5_q2": {
    "paper_id": "1603.00286v5",
    "question_index": 2,
    "question": "What can you say about the relationship between the complexity of a cake shape and the minimum number of blanks required for a complete partition into smaller pieces?",
    "ground_truth": "The complexity of the cake shape generally leads to a higher minimum number of blanks required for a complete partition.",
    "predicted_answer": "The complexity of the cake shape increases the minimum number of blanks required for a complete partition into smaller pieces.",
    "reference": "1603.00286v5-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.8292682926829268,
      "phrase_overlap": 0.8292682926829268,
      "is_correct": true
    }
  },
  "1603.00286v5_q3": {
    "paper_id": "1603.00286v5",
    "question_index": 3,
    "question": " \n\nWhy does the author state that there is a qualitative difference between 2-D and 1-D division?",
    "ground_truth": " \n\nIn 2-D division, there may be unallocated cake even when there are geometric constraints on the pieces, as shown in Figure 1. This is not the case in 1-D division, where the entire cake can always be allocated.",
    "predicted_answer": "The author states there is a qualitative difference between 2-D and 1-D division because 2-D division involves partitioning a space into multiple distinct regions, as illustrated by the division into Z1, Z2, Z3, and Z4, whereas 1-D division would involve simpler linear partitions.",
    "reference": "1603.00286v5-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.09368635437881874,
      "phrase_overlap": 0.09368635437881874,
      "is_correct": true
    }
  },
  "1603.00286v5_q4": {
    "paper_id": "1603.00286v5",
    "question_index": 4,
    "question": "What is the minimum number of sides that a rectilinear polygon with four reflex vertices must have?",
    "ground_truth": "Six.",
    "predicted_answer": "8",
    "reference": "1603.00286v5-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1603.03833v4_q0": {
    "paper_id": "1603.03833v4",
    "question_index": 0,
    "question": "What is the role of the LSTM-MDN network in the training phase?",
    "ground_truth": "The LSTM-MDN network is used to learn the relationship between the gripper pose and status, the pose of relevant objects, and the joint angles of the robot arm.",
    "predicted_answer": "Negative log likelihood cost.",
    "reference": "1603.03833v4-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.19047619047619047,
      "phrase_overlap": 0.19047619047619047,
      "is_correct": false
    }
  },
  "1603.03833v4_q1": {
    "paper_id": "1603.03833v4",
    "question_index": 1,
    "question": "Why is the number of demonstrations after the shift not available for the \"Push to Pose\" task?",
    "ground_truth": "The passage mentions that additional trajectories were generated for the \"Pick and Place\" task by reducing the frequency of the recorded demonstrations. This process was not applied to the \"Push to Pose\" task, therefore no \"Demonstrations after shift\" are listed for it.",
    "predicted_answer": "The number of demonstrations after the shift is not available for the 'Push to pose' task, as indicated by the '-' symbol in the table.",
    "reference": "1603.03833v4-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.13333333333333333,
      "phrase_overlap": 0.13333333333333333,
      "is_correct": false
    }
  },
  "1603.03833v4_q2": {
    "paper_id": "1603.03833v4",
    "question_index": 2,
    "question": "How does the frequency reduction process create multiple trajectories from a single demonstration?",
    "ground_truth": " The frequency reduction process takes a high-frequency trajectory and samples it at a lower frequency, resulting in multiple trajectories with different starting and ending points. ",
    "predicted_answer": "By selecting different subsets of waypoints.",
    "reference": "1603.03833v4-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2857142857142857,
      "phrase_overlap": 0.2857142857142857,
      "is_correct": false
    }
  },
  "1603.03833v4_q3": {
    "paper_id": "1603.03833v4",
    "question_index": 3,
    "question": "What is the role of the virtual environment in the proposed approach?",
    "ground_truth": "The virtual environment is used to collect demonstrations of the task from the user. This allows for safe and efficient data collection.",
    "predicted_answer": "The virtual environment is used for training the LSTM network by providing demonstrations and a dataset of trajectories, enabling the network to learn and predict gripper states for task execution in the physical world.",
    "reference": "1603.03833v4-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4676056338028169,
      "phrase_overlap": 0.4676056338028169,
      "is_correct": false
    }
  },
  "1603.03833v4_q4": {
    "paper_id": "1603.03833v4",
    "question_index": 4,
    "question": "What is the difference between the pick and place task in simulation and the real world?",
    "ground_truth": "In the simulation, the robot is able to pick up the object and place it in the desired location without any errors. However, in the real world, the robot makes some errors, such as dropping the object or placing it in the wrong location.",
    "predicted_answer": "The difference between the pick and place task in simulation and the real world lies in the complexity and realism of the objects and surfaces. The simulation uses simplified objects and textures, while the real world involves more detailed and realistic objects and environments.",
    "reference": "1603.03833v4-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.07736943907156674,
      "phrase_overlap": 0.07736943907156674,
      "is_correct": false
    }
  },
  "1603.03833v4_q5": {
    "paper_id": "1603.03833v4",
    "question_index": 5,
    "question": "What are the three different network architectures used in the comparison study?",
    "ground_truth": "Feedforward-MSE, LSTM-MSE, and Feedforward-MDN.",
    "predicted_answer": "Feedforward-MSE, LSTM-MSE, Feedforward-MDN",
    "reference": "1603.03833v4-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9438202247191011,
      "phrase_overlap": 0.9438202247191011,
      "is_correct": true
    }
  },
  "1605.07496v3_q0": {
    "paper_id": "1605.07496v3",
    "question_index": 0,
    "question": "Which policy resulted in the highest average cost?",
    "ground_truth": "MAP Policy",
    "predicted_answer": "B",
    "reference": "1605.07496v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1605.07496v3_q1": {
    "paper_id": "1605.07496v3",
    "question_index": 1,
    "question": "Which of the algorithms performs the best on the robotic arm joint breakage task?",
    "ground_truth": "ALOQ.",
    "predicted_answer": "A",
    "reference": "1605.07496v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3333333333333333,
      "phrase_overlap": 0.3333333333333333,
      "is_correct": true
    }
  },
  "1605.07496v3_q2": {
    "paper_id": "1605.07496v3",
    "question_index": 2,
    "question": "Which algorithm performs the best in the Joint Breakage experiment?",
    "ground_truth": "ALOQ",
    "predicted_answer": "D",
    "reference": "1605.07496v3-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1605.07496v3_q3": {
    "paper_id": "1605.07496v3",
    "question_index": 3,
    "question": "Which algorithm performed better on the arm breakage task?",
    "ground_truth": "Both TRPO and Reinforce performed similarly on the arm breakage task.",
    "predicted_answer": "No Break TRPO performed better on the arm breakage task.",
    "reference": "1605.07496v3-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.704,
      "phrase_overlap": 0.704,
      "is_correct": true
    }
  },
  "1605.07496v3_q4": {
    "paper_id": "1605.07496v3",
    "question_index": 4,
    "question": "Between WSN and ALOQ, which method is the most efficient in terms of runtime for both F-SRE1 and F-SRE2?",
    "ground_truth": "ALOQ is significantly more efficient than WSN.",
    "predicted_answer": "B",
    "reference": "1605.07496v3-Figure11-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1605.07496v3_q5": {
    "paper_id": "1605.07496v3",
    "question_index": 5,
    "question": "Which method performs the best on the F-SRE1 test function?",
    "ground_truth": "ALOQ",
    "predicted_answer": "D",
    "reference": "1605.07496v3-Figure10-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1605.07496v3_q6": {
    "paper_id": "1605.07496v3",
    "question_index": 6,
    "question": "How does the predicted return change as a function of θ for a fixed value of π = 1.5?",
    "ground_truth": "The predicted return decreases as θ increases, with a minimum at around θ = 0.5.",
    "predicted_answer": "The predicted return changes non-linearly as a function of θ for a fixed value of π = 1.5, with a peak near θ = 0.5 and a trough near θ = -0.5.",
    "reference": "1605.07496v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5112107623318386,
      "phrase_overlap": 0.5112107623318386,
      "is_correct": true
    }
  },
  "1605.07496v3_q7": {
    "paper_id": "1605.07496v3",
    "question_index": 7,
    "question": "What is the difference between the \"True max\" and the \"ALOQ\" curves?",
    "ground_truth": "The \"True max\" curve is the true maximum of the function, while the \"ALOQ\" curve is an approximation of the maximum. The \"ALOQ\" curve is lower than the \"True max\" curve, indicating that it underestimates the maximum value of the function.",
    "predicted_answer": "2.",
    "reference": "1605.07496v3-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.008333333333333333,
      "phrase_overlap": 0.008333333333333333,
      "is_correct": false
    }
  },
  "1605.07496v3_q8": {
    "paper_id": "1605.07496v3",
    "question_index": 8,
    "question": "Which method performs the best on the Branin function?",
    "ground_truth": "One Step ALOQ",
    "predicted_answer": "D",
    "reference": "1605.07496v3-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1612.02803v5_q0": {
    "paper_id": "1612.02803v5",
    "question_index": 0,
    "question": "What is the equation that describes the motion of a mass attached to a spring?",
    "ground_truth": "The equation that describes the motion of a mass attached to a spring is:\n```\nm d^2 X / dt^2 + kX = 0\n```\nwhere:\n* m is the mass of the object\n* X is the displacement of the object from its equilibrium position\n* k is the spring constant\n* t is time",
    "predicted_answer": "mẌ + KX = 0",
    "reference": "1612.02803v5-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.06923076923076923,
      "phrase_overlap": 0.06923076923076923,
      "is_correct": true
    }
  },
  "1606.07384v2_q0": {
    "paper_id": "1606.07384v2",
    "question_index": 0,
    "question": "How does the performance of the Filtering algorithm compare to the performance of MLE with noise?",
    "ground_truth": "The Filtering algorithm performs better than MLE with noise in both the random tree and random graph settings.",
    "predicted_answer": "The performance of the Filtering algorithm is comparable to the performance of MLE with noise, as both show similar trends and values in Estimated d_{TV} across varying numbers of parameters.",
    "reference": "1606.07384v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5049833887043189,
      "phrase_overlap": 0.5049833887043189,
      "is_correct": false
    }
  },
  "1606.07384v2_q1": {
    "paper_id": "1606.07384v2",
    "question_index": 1,
    "question": "Which method performs the best when there is a high fraction of corrupted samples?",
    "ground_truth": "RANSAC",
    "predicted_answer": "MLE w/ noise and RANSAC perform the best when there is a high fraction of corrupted samples.",
    "reference": "1606.07384v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.12244897959183673,
      "phrase_overlap": 0.12244897959183673,
      "is_correct": true
    }
  },
  "1803.01128v3_q0": {
    "paper_id": "1803.01128v3",
    "question_index": 0,
    "question": "What are the key differences between Seq2Sick and existing attack methods on RNN-based models?",
    "ground_truth": "Seq2Sick differs from existing attack methods in two key aspects:\n\n1. Search Strategy: While previous methods primarily rely on greedy search, which becomes increasingly inefficient for longer sequences, Seq2Sick employs group lasso regularization and projected gradient descent with gradient regularization. This allows for simultaneous searching of all replacement positions, leading to improved efficiency.\n\n2. Targeted Attack Type: Existing methods focus on targeting specific classes or binary classifications, while Seq2Sick introduces a novel \"keyword\" target type, allowing attacks to be directed towards specific keywords within the generated sequence.",
    "predicted_answer": "Gradient Based, Sequential Output, Keyword Targeted Attack.",
    "reference": "1803.01128v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.027855153203342618,
      "phrase_overlap": 0.027855153203342618,
      "is_correct": false
    }
  },
  "1803.01128v3_q1": {
    "paper_id": "1803.01128v3",
    "question_index": 1,
    "question": "What is the relationship between the success rate of the non-overlapping attack and the number of words changed in the input sentence?",
    "ground_truth": "There is a negative correlation between the success rate of the non-overlapping attack and the number of words changed in the input sentence. In other words, the fewer words that are changed, the higher the success rate of the attack.",
    "predicted_answer": "The success rate of the non-overlapping attack decreases slightly as the number of words changed in the input sentence increases.",
    "reference": "1803.01128v3-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5399449035812672,
      "phrase_overlap": 0.5399449035812672,
      "is_correct": false
    }
  },
  "1803.01128v3_q2": {
    "paper_id": "1803.01128v3",
    "question_index": 2,
    "question": "How does the difficulty of performing a successful targeted keywords attack change as the number of targeted keywords increases?",
    "ground_truth": "The difficulty of performing a successful targeted keywords attack increases as the number of targeted keywords increases.",
    "predicted_answer": "The difficulty of performing a successful targeted keywords attack increases as the number of targeted keywords increases, evidenced by a significant drop in success rate, lower BLEU scores, and a higher number of changed words across all datasets.",
    "reference": "1803.01128v3-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6594594594594595,
      "phrase_overlap": 0.6594594594594595,
      "is_correct": true
    }
  },
  "1803.01128v3_q3": {
    "paper_id": "1803.01128v3",
    "question_index": 3,
    "question": "Which method resulted in the highest BLEU score?",
    "ground_truth": "The 1-keyword method resulted in the highest BLEU score of 0.705.",
    "predicted_answer": "1-keyword",
    "reference": "1803.01128v3-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.24324324324324326,
      "phrase_overlap": 0.24324324324324326,
      "is_correct": true
    }
  },
  "1803.01128v3_q4": {
    "paper_id": "1803.01128v3",
    "question_index": 4,
    "question": "Do adversarial examples generated with the 2-keyword constraint maintain a similar syntactic structure to the original sentences?",
    "ground_truth": "No, adversarial examples generated with the 2-keyword constraint deviate significantly from the original syntactic structure.",
    "predicted_answer": "No, adversarial examples generated with the 2-keyword constraint do not maintain a similar syntactic structure to the original sentences, as the similarity scores are significantly higher under the 2-keyword constraint compared to the original sentences.",
    "reference": "1803.01128v3-Table6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.49604221635883905,
      "phrase_overlap": 0.49604221635883905,
      "is_correct": true
    }
  },
  "1703.00899v2_q0": {
    "paper_id": "1703.00899v2",
    "question_index": 0,
    "question": "What is the relationship between the true market state qt and the noisy version q̂t at time t?",
    "ground_truth": "The noisy version q̂t at time t is equal to the true market state qt plus a sum of Laplace noise vectors obtained by following the arrows all the way back to 0.",
    "predicted_answer": "q̂t is a noisy observation of qt.",
    "reference": "1703.00899v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.19689119170984457,
      "phrase_overlap": 0.19689119170984457,
      "is_correct": false
    }
  },
  "1703.02507v3_q0": {
    "paper_id": "1703.02507v3",
    "question_index": 0,
    "question": "Which model performed the best on the MSRP task for the Ordered Sentences dataset?",
    "ground_truth": "SkipThought",
    "predicted_answer": "C",
    "reference": "1703.02507v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1703.02507v3_q1": {
    "paper_id": "1703.02507v3",
    "question_index": 1,
    "question": "Which model performs best on the SICK 2014 dataset in terms of average Spearman and Pearson correlation?",
    "ground_truth": "C-PHRASE",
    "predicted_answer": "Sent2Vec uni. + bi. performs best on the SICK 2014 dataset with an average correlation of .65/.67.",
    "reference": "1703.02507v3-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.11320754716981132,
      "phrase_overlap": 0.11320754716981132,
      "is_correct": false
    }
  },
  "1703.02507v3_q2": {
    "paper_id": "1703.02507v3",
    "question_index": 2,
    "question": "What is the relationship between the $L_2$-norm of a word vector and its frequency?",
    "ground_truth": "The $L_2$-norm of a word vector is inversely proportional to its frequency.",
    "predicted_answer": "The $L_2$-norm of a word vector increases with frequency up to a certain point and then decreases.",
    "reference": "1703.02507v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5895953757225434,
      "phrase_overlap": 0.5895953757225434,
      "is_correct": true
    }
  },
  "1703.02507v3_q3": {
    "paper_id": "1703.02507v3",
    "question_index": 3,
    "question": "Which dataset has the shortest average sentence length?",
    "ground_truth": "Headlines.",
    "predicted_answer": "Headlines",
    "reference": "1703.02507v3-Table8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9473684210526315,
      "phrase_overlap": 0.9473684210526315,
      "is_correct": true
    }
  },
  "1703.02507v3_q4": {
    "paper_id": "1703.02507v3",
    "question_index": 4,
    "question": "Which model performed the best on the MSRP task for the Twitter dataset?",
    "ground_truth": "The Sent2Vec uni. + bi. model performed the best on the MSRP task for the Twitter dataset.",
    "predicted_answer": "C",
    "reference": "1703.02507v3-Table6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.02197802197802198,
      "phrase_overlap": 0.02197802197802198,
      "is_correct": true
    }
  },
  "1802.07351v2_q0": {
    "paper_id": "1802.07351v2",
    "question_index": 0,
    "question": "What is the purpose of the residual connection in the decoding module?",
    "ground_truth": "The residual connection allows the output of a layer to be added to the output of another layer, which helps to improve the flow of information through the network.",
    "predicted_answer": "To preserve spatial information.",
    "reference": "1802.07351v2-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.23469387755102042,
      "phrase_overlap": 0.23469387755102042,
      "is_correct": false
    }
  },
  "1802.07351v2_q1": {
    "paper_id": "1802.07351v2",
    "question_index": 1,
    "question": "What are the differences between the results of the three methods, LiteFlowNet, PWC-Net, and Devon, compared to the ground truth?",
    "ground_truth": "LiteFlowNet, PWC-Net, and Devon all produce results that are similar to the ground truth, but there are some subtle differences. For example, LiteFlowNet tends to overestimate the motion of the small object, while PWC-Net and Devon tend to underestimate it. Additionally, all three methods produce some artifacts around the edges of the moving object.",
    "predicted_answer": "The differences between the results of LiteFlowNet, PWC-Net, and Devon compared to the ground truth are primarily in the accuracy of flow representation, with PWC-Net showing sharper transitions, LiteFlowNet being smoother but less precise, and Devon balancing smoothness and sharpness but still deviating in certain areas.",
    "reference": "1802.07351v2-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0830860534124629,
      "phrase_overlap": 0.0830860534124629,
      "is_correct": false
    }
  },
  "1802.07351v2_q2": {
    "paper_id": "1802.07351v2",
    "question_index": 2,
    "question": "What is the difference between a standard cost volume and a deformable cost volume?",
    "ground_truth": "A standard cost volume computes the matching costs for a neighborhood of the same location on the feature maps of the first and second images. A deformable cost volume computes the matching costs for a dilated neighborhood of the same location on the feature maps of the first and second images, offset by a flow vector.",
    "predicted_answer": "Offset by a flow vector.",
    "reference": "1802.07351v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.13953488372093023,
      "phrase_overlap": 0.13953488372093023,
      "is_correct": true
    }
  },
  "1802.07351v2_q3": {
    "paper_id": "1802.07351v2",
    "question_index": 3,
    "question": "Based on the ablation study, which modification to the Devon model architecture had the most significant negative impact on performance for the KITTI 2015 dataset?",
    "ground_truth": "Removing the normalization in the relation modules had the most significant negative impact on performance for the KITTI 2015 dataset.",
    "predicted_answer": "The modification 'Without norm' had the most significant negative impact on performance for the KITTI 2015 dataset, increasing the error from 13.25 to 15.64.",
    "reference": "1802.07351v2-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6941580756013745,
      "phrase_overlap": 0.6941580756013745,
      "is_correct": true
    }
  },
  "1802.07351v2_q4": {
    "paper_id": "1802.07351v2",
    "question_index": 4,
    "question": "Which modification to the Devon model resulted in the fastest processing time for both forward and backward passes, and how much faster was it compared to the full model in terms of the backward pass? ",
    "ground_truth": "The \"Without dilation\" configuration resulted in the fastest processing time for both forward and backward passes. It was approximately 29.43 ms faster than the full model in terms of the backward pass (147.74 ms vs. 177.17 ms).",
    "predicted_answer": "The 'Without dilation' modification resulted in the fastest processing time for both forward (49.07 ms) and backward (147.74 ms) passes. Compared to the full model, the backward pass was faster by 29.43 ms.",
    "reference": "1802.07351v2-Table6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2304147465437788,
      "phrase_overlap": 0.2304147465437788,
      "is_correct": true
    }
  },
  "1802.07351v2_q5": {
    "paper_id": "1802.07351v2",
    "question_index": 5,
    "question": "What is the purpose of the residual connection in the encoding module?",
    "ground_truth": "The residual connection adds the output of a layer to the output of another layer, which helps to prevent the vanishing gradient problem.",
    "predicted_answer": "The purpose of the residual connection in the encoding module is to preserve information from earlier layers and improve feature propagation.",
    "reference": "1802.07351v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4244604316546763,
      "phrase_overlap": 0.4244604316546763,
      "is_correct": false
    }
  },
  "1802.07351v2_q6": {
    "paper_id": "1802.07351v2",
    "question_index": 6,
    "question": "What is the role of the relation module (Rt) in the Deformable Volume Network (Devon)?",
    "ground_truth": "The relation module (Rt) is responsible for capturing the spatial relationships between the features extracted from the first and second images.",
    "predicted_answer": "To model relations between features.",
    "reference": "1802.07351v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4,
      "phrase_overlap": 0.4,
      "is_correct": false
    }
  },
  "1802.07351v2_q7": {
    "paper_id": "1802.07351v2",
    "question_index": 7,
    "question": "Explain the rationale behind using five deformable cost volumes with different hyperparameter settings in Devon's relation module.",
    "ground_truth": "The five deformable cost volumes in Devon's relation module are designed to capture multi-scale motion by combining dense correspondences near the image center with sparser correspondences in the periphery. This is achieved by using different neighborhood sizes (k) and dilation rates (r) for each cost volume, as shown in Table 1. Smaller neighborhood sizes and dilation rates result in denser correspondences, focusing on finer details and small displacements, while larger values capture broader context and larger motions.",
    "predicted_answer": "To capture multi-scale and multi-range spatial relationships.",
    "reference": "1802.07351v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0817717206132879,
      "phrase_overlap": 0.0817717206132879,
      "is_correct": false
    }
  },
  "1802.07351v2_q8": {
    "paper_id": "1802.07351v2",
    "question_index": 8,
    "question": "Based on the table, which model achieved the best performance on the KITTI 2015 test set in terms of F1-all score, and how does its performance compare to Devon (ft) on the same dataset?",
    "ground_truth": "PWC-Net (ft) achieved the best performance on the KITTI 2015 test set with an F1-all score of 9.16%. This is significantly better than Devon (ft), which achieved an F1-all score of 14.31% on the same dataset. ",
    "predicted_answer": "SpyNet (ft), 35.07% compared to 14.31%.",
    "reference": "1802.07351v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.10526315789473684,
      "phrase_overlap": 0.10526315789473684,
      "is_correct": true
    }
  },
  "1802.07351v2_q9": {
    "paper_id": "1802.07351v2",
    "question_index": 9,
    "question": "Based on the table, which method performs best on the Sintel \"Final\" test set, and how does its performance compare to Devon (ft) on the same set? ",
    "ground_truth": "PWC-Net (ft) performs best on the Sintel \"Final\" test set with an error of 5.04. Devon (ft) has a higher error of 6.35 on the same set. ",
    "predicted_answer": "C",
    "reference": "1802.07351v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.014705882352941176,
      "phrase_overlap": 0.014705882352941176,
      "is_correct": true
    }
  },
  "1802.07351v2_q10": {
    "paper_id": "1802.07351v2",
    "question_index": 10,
    "question": "Which of the three methods, LiteFlowNet, PWC-Net, or Devon, most accurately predicts the motion of the small object in the scene?",
    "ground_truth": "Devon.",
    "predicted_answer": "Devon",
    "reference": "1802.07351v2-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9090909090909091,
      "phrase_overlap": 0.9090909090909091,
      "is_correct": true
    }
  },
  "1702.03584v3_q0": {
    "paper_id": "1702.03584v3",
    "question_index": 0,
    "question": "How does the observed error compare to the underlying true error as CPU time increases?",
    "ground_truth": "The observed error is initially higher than the underlying true error, but it quickly decreases and converges to the true error as CPU time increases.",
    "predicted_answer": "The observed error is consistently lower than the underlying true error as CPU time increases.",
    "reference": "1702.03584v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6885245901639344,
      "phrase_overlap": 0.6885245901639344,
      "is_correct": true
    }
  },
  "1702.03584v3_q1": {
    "paper_id": "1702.03584v3",
    "question_index": 1,
    "question": "How does the performance of SPIRAL-DTW-kMeans compare to k-Shape and CLDS?",
    "ground_truth": "SPIRAL-DTW-kMeans performs better than k-Shape and CLDS on most datasets.",
    "predicted_answer": "SPIRAL-DTW-kMeans performs better than both k-Shape and CLDS.",
    "reference": "1702.03584v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.835820895522388,
      "phrase_overlap": 0.835820895522388,
      "is_correct": true
    }
  },
  "1702.03584v3_q2": {
    "paper_id": "1702.03584v3",
    "question_index": 2,
    "question": "Which method performs the best in terms of NMI and what percentage of datasets does it outperform the other methods on?",
    "ground_truth": "The SPIRAL-MSM-kMeans method performs the best in terms of NMI with a score of 0.365. It outperforms the other methods on 89.4% of the datasets.",
    "predicted_answer": "SPIRAL-MSM-kMeans, N/A",
    "reference": "1702.03584v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.24096385542168675,
      "phrase_overlap": 0.24096385542168675,
      "is_correct": false
    }
  },
  "1802.07459v2_q0": {
    "paper_id": "1802.07459v2",
    "question_index": 0,
    "question": " How many negative samples are there in the training set of the CNSE dataset?",
    "ground_truth": "There are approximately 9,719 negative samples in the training set of the CNSE dataset.",
    "predicted_answer": "16198",
    "reference": "1802.07459v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.043478260869565216,
      "phrase_overlap": 0.043478260869565216,
      "is_correct": false
    }
  },
  "1802.07459v2_q1": {
    "paper_id": "1802.07459v2",
    "question_index": 1,
    "question": "Which model variant achieves the best performance on the CNSS dataset in terms of F1-score, and what are its key components?",
    "ground_truth": "Model XVIII, CIG-Sim&Siam-GCN-Sim$^{g}$, achieves the best performance on the CNSS dataset with an F1-score of 90.29%. This model utilizes the following key components:\n\n1. CIG: It directly uses keywords as concepts without community detection.\n2. Sim & Siam: It employs both term-based similarity encoder (\"Sim\") and Siamese encoder (\"Siam\") for generating matching vectors on vertices.\n3. GCN: It performs convolution on local matching vectors through GCN layers.\n4. Sim$^{g}$: It incorporates additional global features based on the five term-based similarity metrics.",
    "predicted_answer": "E",
    "reference": "1802.07459v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1802.07459v2_q2": {
    "paper_id": "1802.07459v2",
    "question_index": 2,
    "question": "What are the different stages involved in constructing the Concept Interaction Graph (CIG) from a pair of documents?",
    "ground_truth": "The different stages involved in constructing the Concept Interaction Graph (CIG) from a pair of documents are: (a) Representation, (b) Encoding, (c) Transformation, and (d) Aggregation.",
    "predicted_answer": "1. Construct KeyGraph by Word Co-occurrence  \n2. Detect Concepts by Community Detection  \n3. Assign Sentences by Similarities  \n4. Get Edge Weights by Vertex Similarities",
    "reference": "1802.07459v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2914285714285714,
      "phrase_overlap": 0.2914285714285714,
      "is_correct": false
    }
  },
  "1702.08694v3_q0": {
    "paper_id": "1702.08694v3",
    "question_index": 0,
    "question": "Which method, C-Tarone or Binarization, achieves higher precision when the number of features is small and the number of data points is large?",
    "ground_truth": "C-Tarone.",
    "predicted_answer": "C-Tarone achieves higher precision when the number of features is small and the number of data points is large.",
    "reference": "1702.08694v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.15,
      "phrase_overlap": 0.15,
      "is_correct": false
    }
  },
  "1702.08694v3_q1": {
    "paper_id": "1702.08694v3",
    "question_index": 1,
    "question": "How does the C-Tarone method compare to the binarization method in terms of precision, recall, F-measure, and running time?",
    "ground_truth": "The C-Tarone method has higher precision and F-measure than the binarization method in all datasets. The C-Tarone method has better or competitive recall than the binarization method. The running time of the C-Tarone method is competitive with the binarization method.",
    "predicted_answer": "C-Tarone method generally has higher precision, recall, and F-measure, but longer running time compared to the binarization method.",
    "reference": "1702.08694v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.14035087719298245,
      "phrase_overlap": 0.14035087719298245,
      "is_correct": false
    }
  },
  "1702.08694v3_q2": {
    "paper_id": "1702.08694v3",
    "question_index": 2,
    "question": "For a fixed value of $b$, how does the maximum achievable KL divergence and the corresponding minimum achievable p-value change with increasing values of $a$?",
    "ground_truth": "The maximum achievable KL divergence initially increases with increasing values of $a$ until it reaches a peak. Then, it decreases with increasing values of $a$. The minimum achievable p-value initially decreases with increasing values of $a$ until it reaches a minimum. Then, it increases with increasing values of $a$.",
    "predicted_answer": "Maximum KL divergence: Peaks at intermediate values of \\(a\\).  \nMinimum p-value: Decreases significantly at intermediate values of \\(a\\).",
    "reference": "1702.08694v3-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2857142857142857,
      "phrase_overlap": 0.2857142857142857,
      "is_correct": false
    }
  },
  "1702.08694v3_q3": {
    "paper_id": "1702.08694v3",
    "question_index": 3,
    "question": "Which dataset would likely require the most computational resources for C-Tarone to analyze?",
    "ground_truth": "The \"wdbc\" dataset would likely require the most computational resources for C-Tarone to analyze.",
    "predicted_answer": "wdbc",
    "reference": "1702.08694v3-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.07920792079207921,
      "phrase_overlap": 0.07920792079207921,
      "is_correct": true
    }
  },
  "1803.02750v3_q0": {
    "paper_id": "1803.02750v3",
    "question_index": 0,
    "question": "Which synchronization method is the most efficient in terms of CPU processing time?",
    "ground_truth": "The proposed method compared to state-based and delta-based methods.",
    "predicted_answer": "The 'This paper' synchronization method is the most efficient in terms of CPU processing time.",
    "reference": "1803.02750v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2962962962962963,
      "phrase_overlap": 0.2962962962962963,
      "is_correct": false
    }
  },
  "1803.02750v3_q1": {
    "paper_id": "1803.02750v3",
    "question_index": 1,
    "question": "What is the difference between GCounter and GSet?",
    "ground_truth": "GCounter measures the number of times an event has occurred, while GSet measures the number of unique elements in a set.",
    "predicted_answer": "The difference between GCounter and GSet is that GCounter involves a single increment and measures the number of entries in a map, while GSet involves the addition of unique elements and measures the number of elements in a set.",
    "reference": "1803.02750v3-TableI-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5229885057471264,
      "phrase_overlap": 0.5229885057471264,
      "is_correct": false
    }
  },
  "1803.02750v3_q2": {
    "paper_id": "1803.02750v3",
    "question_index": 2,
    "question": "Which of the algorithms is most efficient in terms of transmission in a tree topology?",
    "ground_truth": "Op-based GSet.",
    "predicted_answer": "Delta-based BP+RR is the most efficient algorithm in terms of transmission in a tree topology.",
    "reference": "1803.02750v3-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.18518518518518517,
      "phrase_overlap": 0.18518518518518517,
      "is_correct": false
    }
  },
  "1803.02750v3_q3": {
    "paper_id": "1803.02750v3",
    "question_index": 3,
    "question": "What is the role of the RR optimization in the delta-based synchronization of a GSet?",
    "ground_truth": "The RR optimization helps to reduce the number of messages that need to be exchanged between replicas.",
    "predicted_answer": "The RR optimization in delta-based synchronization of a GSet ensures efficient propagation and synchronization of elements, reducing redundant transmissions and improving performance.",
    "reference": "1803.02750v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2807017543859649,
      "phrase_overlap": 0.2807017543859649,
      "is_correct": false
    }
  },
  "1803.02750v3_q4": {
    "paper_id": "1803.02750v3",
    "question_index": 4,
    "question": "How does the average metadata required per node for the Op-based BP+RR approach change as the number of nodes in the network increases?",
    "ground_truth": "The average metadata required per node for the Op-based BP+RR approach increases as the number of nodes in the network increases.",
    "predicted_answer": "The average metadata required per node for the Op-based BP+RR approach increases significantly as the number of nodes in the network increases.",
    "reference": "1803.02750v3-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9485294117647058,
      "phrase_overlap": 0.9485294117647058,
      "is_correct": true
    }
  },
  "1803.02750v3_q5": {
    "paper_id": "1803.02750v3",
    "question_index": 5,
    "question": "Which algorithm performs the best in terms of average memory ratio with respect to BP+RR for GMap 10%?",
    "ground_truth": "Delta-based BP+RR",
    "predicted_answer": "Op-based algorithm performs the best in terms of average memory ratio with respect to BP+RR for GMap 10%, with a value of 4.6.",
    "reference": "1803.02750v3-Figure10-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.16783216783216784,
      "phrase_overlap": 0.16783216783216784,
      "is_correct": false
    }
  },
  "1803.02750v3_q6": {
    "paper_id": "1803.02750v3",
    "question_index": 6,
    "question": "What is the difference between the `inc_i(p)` and `inc_i'(p)` operations in the Grow-only Counter data type?",
    "ground_truth": "The `inc_i(p)` operation increments the value associated with the key `i` in the counter `p`, while the `inc_i'(p)` operation increments the value associated with the key `i` in the counter `p` only if the key `i` is not already present in the counter.",
    "predicted_answer": "`inc_i(p)` modifies the existing map `p` by incrementing the value at key `i`, while `inc_i'(p)` creates a new map with only the incremented value at key `i`.",
    "reference": "1803.02750v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2146341463414634,
      "phrase_overlap": 0.2146341463414634,
      "is_correct": false
    }
  },
  "1803.02750v3_q7": {
    "paper_id": "1803.02750v3",
    "question_index": 7,
    "question": "Which topology has the highest transmission rate for GMap 100%?",
    "ground_truth": "Mesh",
    "predicted_answer": "Mesh topology has the highest transmission rate for GMap 100%.",
    "reference": "1803.02750v3-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.12121212121212122,
      "phrase_overlap": 0.12121212121212122,
      "is_correct": true
    }
  },
  "1803.02750v3_q8": {
    "paper_id": "1803.02750v3",
    "question_index": 8,
    "question": "If a user with 100 followers posts a tweet, how many CRDT updates will be performed in total, and what percentage of the overall workload does this represent?",
    "ground_truth": "Posting a tweet will result in 1 + 100 = 101 CRDT updates. This represents 35% of the overall workload.",
    "predicted_answer": "If a user with 100 followers posts a tweet, the total CRDT updates performed will be 101 (1 + 100 followers), representing 35% of the overall workload.",
    "reference": "1803.02750v3-TableII-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5511811023622047,
      "phrase_overlap": 0.5511811023622047,
      "is_correct": true
    }
  },
  "1803.02750v3_q9": {
    "paper_id": "1803.02750v3",
    "question_index": 9,
    "question": "If we use the lexicographic product with a chain as the first component and a distributive lattice as the second component to design a CRDT, will the resulting CRDT lattice be guaranteed to be distributive and satisfy the descending chain condition (DCC)?",
    "ground_truth": "Yes, the resulting CRDT lattice will be guaranteed to be both distributive and satisfy the DCC.",
    "predicted_answer": "No.",
    "reference": "1803.02750v3-TableIII-1.png",
    "question_type": "multiple_choice",
    "evaluation": {
      "similarity_score": 0.061224489795918366,
      "phrase_overlap": 0.061224489795918366,
      "is_correct": false
    }
  },
  "1803.02750v3_q10": {
    "paper_id": "1803.02750v3",
    "question_index": 10,
    "question": "How does the CPU overhead of classic delta-based compare to delta-based BP+RR as the Zipf coefficient increases?",
    "ground_truth": "The CPU overhead of classic delta-based is consistently higher than that of delta-based BP+RR as the Zipf coefficient increases.",
    "predicted_answer": "As the Zipf coefficient increases, the CPU overhead of classic delta-based compared to delta-based BP+RR remains constant at 4% for lower coefficients (0.50, 0.75, 1.00) but increases sharply to 548% at 1.25 and 789% at 1.50.",
    "reference": "1803.02750v3-Figure12-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5325779036827195,
      "phrase_overlap": 0.5325779036827195,
      "is_correct": false
    }
  },
  "1701.06171v4_q0": {
    "paper_id": "1701.06171v4",
    "question_index": 0,
    "question": "How many iterations did the greedy EM-type learning process take to learn the part models for the watch image?",
    "ground_truth": "22 iterations",
    "predicted_answer": "22",
    "reference": "1701.06171v4-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.26666666666666666,
      "phrase_overlap": 0.26666666666666666,
      "is_correct": true
    }
  },
  "1701.06171v4_q1": {
    "paper_id": "1701.06171v4",
    "question_index": 1,
    "question": "What is the relationship between the variables in the Compositional Active Basis Model?",
    "ground_truth": "The variables in the Compositional Active Basis Model are hierarchically dependent. The variables at each layer are dependent on the variables at the layer above it.",
    "predicted_answer": "The relationship between the variables in the Compositional Active Basis Model is hierarchical, where higher-level shape components (β^2, β^1) influence lower-level appearance components (β^0, c).",
    "reference": "1701.06171v4-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5152354570637119,
      "phrase_overlap": 0.5152354570637119,
      "is_correct": false
    }
  },
  "1701.06171v4_q2": {
    "paper_id": "1701.06171v4",
    "question_index": 2,
    "question": "What is the difference between the hierarchical part dictionary learned with the bottom-up process and the holistic object model learned with the top-down process?",
    "ground_truth": "The hierarchical part dictionary learned with the bottom-up process is a set of parts that can be combined to create objects. The holistic object model learned with the top-down process is a single model that represents the entire object.",
    "predicted_answer": "Hierarchical part dictionary focuses on decomposing objects into parts, while holistic object model represents the entire object as a whole.",
    "reference": "1701.06171v4-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2222222222222222,
      "phrase_overlap": 0.2222222222222222,
      "is_correct": false
    }
  },
  "1701.06171v4_q3": {
    "paper_id": "1701.06171v4",
    "question_index": 3,
    "question": "What is the difference between the top-down and bottom-up compositional learning schemes?",
    "ground_truth": "The top-down compositional learning scheme starts with a holistic object model and decomposes it into smaller parts, while the bottom-up compositional learning scheme starts with basic parts and composes them into a holistic object model.",
    "predicted_answer": "The difference is that the top-down scheme starts with a high-level concept and breaks it into simpler components, while the bottom-up scheme starts with simple components and builds them up to form a high-level concept.",
    "reference": "1701.06171v4-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.17467248908296942,
      "phrase_overlap": 0.17467248908296942,
      "is_correct": false
    }
  },
  "1703.00060v2_q0": {
    "paper_id": "1703.00060v2",
    "question_index": 0,
    "question": "How does the discrimination in the prediction of the two-phase framework (MSG) compare to that of DI, both with and without classifier tweaking, when the sample size is 2000?",
    "ground_truth": "When the sample size is 2000, the two-phase framework (MSG) achieves lower discrimination in prediction compared to DI, both with and without classifier tweaking.\n\nWith classifier tweaking: MSG achieves a discrimination level of 0.016 ± 5.3E-4, while DI shows a significantly higher level of 0.095 ± 1.6E-3.\nWithout classifier tweaking: MSG still demonstrates lower discrimination with 0.067 ± 4.3E-3 compared to DI's 0.095 ± 1.6E-3.\n\nThis indicates that the two-phase framework is more effective in removing discrimination from predictions than DI, regardless of whether classifier tweaking is applied.",
    "predicted_answer": "For a sample size of 2000, DI has higher discrimination (DEM*) at 0.095 ± 1.6E−3 compared to MSG, which is 0.016 ± 5.3E−4 with classifier tweaking and 0.067 ± 4.3E−3 without classifier tweaking.",
    "reference": "1703.00060v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.18867924528301888,
      "phrase_overlap": 0.18867924528301888,
      "is_correct": true
    }
  },
  "1703.07015v3_q0": {
    "paper_id": "1703.07015v3",
    "question_index": 0,
    "question": "Which dataset exhibits the strongest seasonality?",
    "ground_truth": "The Traffic dataset.",
    "predicted_answer": "Traffic dataset exhibits the strongest seasonality.",
    "reference": "1703.07015v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4507042253521127,
      "phrase_overlap": 0.4507042253521127,
      "is_correct": false
    }
  },
  "1703.07015v3_q1": {
    "paper_id": "1703.07015v3",
    "question_index": 1,
    "question": "What component of LSTNet is most important for its performance?",
    "ground_truth": "The AR component.",
    "predicted_answer": "The AR component is the most important for LSTNet's performance.",
    "reference": "1703.07015v3-Figure56-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.41975308641975306,
      "phrase_overlap": 0.41975308641975306,
      "is_correct": false
    }
  },
  "1703.07015v3_q2": {
    "paper_id": "1703.07015v3",
    "question_index": 2,
    "question": "Which of the two models, LSTw/oAR or LST-Skip, seems to perform better in predicting electricity consumption?",
    "ground_truth": "LST-Skip seems to perform better in predicting electricity consumption.",
    "predicted_answer": "LST-Skip",
    "reference": "1703.07015v3-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.20253164556962025,
      "phrase_overlap": 0.20253164556962025,
      "is_correct": true
    }
  },
  "1703.07015v3_q3": {
    "paper_id": "1703.07015v3",
    "question_index": 3,
    "question": "Which model, VAR or LSTNet, is better at capturing both daily and weekly repeating patterns in the data?",
    "ground_truth": "LSTNet",
    "predicted_answer": "LSTNet",
    "reference": "1703.07015v3-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1703.07015v3_q4": {
    "paper_id": "1703.07015v3",
    "question_index": 4,
    "question": "Which dataset has the highest temporal resolution, meaning it provides data points at the most frequent intervals?",
    "ground_truth": "The solar dataset has the highest temporal resolution.",
    "predicted_answer": "B",
    "reference": "1703.07015v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1703.07015v3_q5": {
    "paper_id": "1703.07015v3",
    "question_index": 5,
    "question": "What are the different types of layers in the LSTNet model and how are they connected?",
    "ground_truth": "The LSTNet model has four main types of layers:\n\n1. Convolutional layer: This layer extracts local dependency patterns from the input data. \n2. Recurrent and recurrent-skip layer: These layers capture long-term dependencies in the data. \n3. Fully connected and element-wise sum output layer: This layer combines the outputs from the convolutional and recurrent layers to produce the final prediction.\n4. Autoregressive layer: This layer provides a linear bypass to the non-linear neural network part of the model. \n\nThe convolutional layer receives the input data and passes its output to the recurrent and recurrent-skip layers. These layers then pass their output to the fully connected and element-wise sum output layer. The autoregressive layer receives the input data directly and its output is also fed into the fully connected and element-wise sum output layer.",
    "predicted_answer": "Convolutional Layer → Recurrent and Recurrent-skip Layer → Fully Connected Layer → Autoregressive Component.",
    "reference": "1703.07015v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.11534500514933059,
      "phrase_overlap": 0.11534500514933059,
      "is_correct": false
    }
  },
  "1703.07015v3_q6": {
    "paper_id": "1703.07015v3",
    "question_index": 6,
    "question": "How does the performance of LSTNet-attn vary with the horizon on the Solar-Energy dataset?",
    "ground_truth": "The performance of LSTNet-attn generally improves as the horizon increases on the Solar-Energy dataset. This is evident from the fact that both the RMSE and correlation values improve with increasing horizon.",
    "predicted_answer": "RSE increases and Correlation decreases as the horizon increases.",
    "reference": "1703.07015v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.08791208791208792,
      "phrase_overlap": 0.08791208791208792,
      "is_correct": false
    }
  },
  "1703.04887v4_q0": {
    "paper_id": "1703.04887v4",
    "question_index": 0,
    "question": "What is the relationship between the number of Monte Carlo samples (N) and the translation performance of the BR-CSGAN model? Why is there a trade-off when choosing the value of N?",
    "ground_truth": "The table and passage show that the translation performance of the BR-CSGAN model generally improves as the number of Monte Carlo samples (N) increases. However, this improvement plateaus after N reaches a certain point (around 20 in this case).\n\nThere is a trade-off when choosing the value of N because increasing N also increases the computational complexity and training time. While a higher N leads to more accurate reward estimations and better performance, it also requires more computational resources and longer training times. Therefore, choosing the optimal N involves balancing the desired performance with the available computational resources and time constraints.",
    "predicted_answer": "The relationship between the number of Monte Carlo samples (N) and the translation performance of the BR-CSGAN model is that increasing N generally improves translation performance (NIST scores). However, there is a trade-off because the improvement diminishes as N becomes larger, suggesting diminishing returns with higher values of N.",
    "reference": "1703.04887v4-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.10453648915187377,
      "phrase_overlap": 0.10453648915187377,
      "is_correct": false
    }
  },
  "1703.04887v4_q1": {
    "paper_id": "1703.04887v4",
    "question_index": 1,
    "question": "How does the initial accuracy of the discriminator affect the BLEU score?",
    "ground_truth": "The BLEU score decreases as the initial accuracy of the discriminator increases.",
    "predicted_answer": "Higher initial accuracy of the discriminator results in more stable BLEU scores over test steps, while lower initial accuracy leads to a rapid decline in BLEU scores.",
    "reference": "1703.04887v4-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3902439024390244,
      "phrase_overlap": 0.3902439024390244,
      "is_correct": false
    }
  },
  "1703.04887v4_q2": {
    "paper_id": "1703.04887v4",
    "question_index": 2,
    "question": "How does BR-CSGAN compare to MRT in terms of translation performance and what is the likely reason for this difference?",
    "ground_truth": "BR-CSGAN consistently outperforms MRT on both Chinese-English and English-German translation tasks, achieving higher BLEU scores.\n\nWhile both methods optimize similar objectives, BR-CSGAN uses a reinforcement learning procedure with a dynamic discriminator to maximize rewards for the generator. This dynamic feedback seems to be more effective than the static objective and random sampling approach used by MRT, leading to better translation performance.",
    "predicted_answer": "BR-CSGAN outperforms MRT in translation performance, likely due to its better optimization and adversarial training approach.",
    "reference": "1703.04887v4-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1070811744386874,
      "phrase_overlap": 0.1070811744386874,
      "is_correct": false
    }
  },
  "1703.04887v4_q3": {
    "paper_id": "1703.04887v4",
    "question_index": 3,
    "question": "Which model and configuration achieves the best performance on the Chinese-English translation task, and how much improvement does it offer compared to the baseline RNNSearch model?",
    "ground_truth": "The Transformer+BR-CSGAN model with λ=0.8 achieves the best performance on the Chinese-English translation task with an average BLEU score of 42.61. This represents an improvement of 0.81 BLEU points compared to the baseline RNNSearch model.",
    "predicted_answer": "The Transformer+BR-CSGAN(λ = 0.8) model achieves the best performance on the Chinese-English translation task with an average BLEU score of 42.61, offering an improvement of 8.67 BLEU points compared to the baseline RNNSearch model (average BLEU score of 33.94).",
    "reference": "1703.04887v4-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.827037773359841,
      "phrase_overlap": 0.827037773359841,
      "is_correct": true
    }
  },
  "1703.04887v4_q4": {
    "paper_id": "1703.04887v4",
    "question_index": 4,
    "question": "What is the role of the discriminator (D) in the proposed BR-CSGAN model?",
    "ground_truth": " The discriminator (D) is responsible for distinguishing between real sentence pairs translated by humans and generated sentence pairs produced by the generator (G). It provides feedback to G in the form of rewards, helping G improve its ability to generate realistic sentence pairs.",
    "predicted_answer": "The role of the discriminator (D) in the proposed BR-CSGAN model is to evaluate the generated data against human data and provide rewards based on the quality of the generated data.",
    "reference": "1703.04887v4-Figure1-1.png",
    "question_type": "multiple_choice",
    "evaluation": {
      "similarity_score": 0.1468682505399568,
      "phrase_overlap": 0.1468682505399568,
      "is_correct": true
    }
  },
  "1703.10730v2_q0": {
    "paper_id": "1703.10730v2",
    "question_index": 0,
    "question": "What are the three goals that the proposed algorithm must achieve simultaneously?",
    "ground_truth": "The three goals that the proposed algorithm must achieve simultaneously are: \n1. To predict the locations of the input patches. \n2. To generate the entire image based on the predicted locations of the input patches. \n3. To do so without any geometric priors.",
    "predicted_answer": "1. Reconstruct the object shape.  \n2. Preserve the texture details.  \n3. Ensure semantic consistency.",
    "reference": "1703.10730v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0625,
      "phrase_overlap": 0.0625,
      "is_correct": true
    }
  },
  "1703.10730v2_q1": {
    "paper_id": "1703.10730v2",
    "question_index": 1,
    "question": "How does the network's focus change as the training epoch increases?",
    "ground_truth": "The network initially focuses on predicting a good mask. As the epoch increases, the input parts become sharper. Finally, the network concentrates on generating realistic images.",
    "predicted_answer": "The network's focus becomes progressively more refined and accurate as the training epoch increases, with masks aligning better with the relevant regions of the images.",
    "reference": "1703.10730v2-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.49710982658959535,
      "phrase_overlap": 0.49710982658959535,
      "is_correct": false
    }
  },
  "1703.10730v2_q2": {
    "paper_id": "1703.10730v2",
    "question_index": 2,
    "question": "What is the relationship between the input patches and the generated images?",
    "ground_truth": "The input patches are used to generate the images. The generator network takes the input patches as input and generates new images that are similar to the input patches.",
    "predicted_answer": "The input patches (cropped facial features) are used to generate full-face images that closely resemble real images, as demonstrated by the generated images and their corresponding masks.",
    "reference": "1703.10730v2-Figure13-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.42134831460674155,
      "phrase_overlap": 0.42134831460674155,
      "is_correct": false
    }
  },
  "1703.10730v2_q3": {
    "paper_id": "1703.10730v2",
    "question_index": 3,
    "question": "What are the inputs to the image generation network?",
    "ground_truth": "The inputs to the image generation network are the observed images (x) and a random noise vector (z).",
    "predicted_answer": "E, z",
    "reference": "1703.10730v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.05714285714285714,
      "phrase_overlap": 0.05714285714285714,
      "is_correct": false
    }
  },
  "1703.10730v2_q4": {
    "paper_id": "1703.10730v2",
    "question_index": 4,
    "question": "How does the presence of noise in the input image affect the quality of the generated images?",
    "ground_truth": "The presence of noise in the input image can degrade the quality of the generated images, but the proposed algorithm is still able to generate realistic images even with a certain amount of noise.",
    "predicted_answer": "The presence of noise in the input image negatively affects the quality of the generated images, leading to less accurate representations and masks.",
    "reference": "1703.10730v2-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6511627906976745,
      "phrase_overlap": 0.6511627906976745,
      "is_correct": true
    }
  },
  "1803.03467v4_q0": {
    "paper_id": "1803.03467v4",
    "question_index": 0,
    "question": "Which dataset has the most 4-hop triples?",
    "ground_truth": "Bing-News.",
    "predicted_answer": "Bing-News",
    "reference": "1803.03467v4-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9473684210526315,
      "phrase_overlap": 0.9473684210526315,
      "is_correct": true
    }
  },
  "1803.03467v4_q1": {
    "paper_id": "1803.03467v4",
    "question_index": 1,
    "question": "How does the number of common k-hop neighbors change as the hop distance increases for items with and without common raters?",
    "ground_truth": "The number of common k-hop neighbors generally decreases as the hop distance increases for both items with and without common raters. However, the number of common k-hop neighbors is consistently higher for items with common raters than for items without common raters.",
    "predicted_answer": "The number of common k-hop neighbors decreases as the hop distance increases for items with and without common raters.",
    "reference": "1803.03467v4-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6098191214470284,
      "phrase_overlap": 0.6098191214470284,
      "is_correct": true
    }
  },
  "1803.03467v4_q2": {
    "paper_id": "1803.03467v4",
    "question_index": 2,
    "question": "Which model performs the best in terms of AUC on the MovieLens-1M dataset?",
    "ground_truth": "RippleNet*",
    "predicted_answer": "A",
    "reference": "1803.03467v4-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1803.03467v4_q3": {
    "paper_id": "1803.03467v4",
    "question_index": 3,
    "question": "What is the role of the ripple sets in the RippleNet framework?",
    "ground_truth": "The ripple sets are used to propagate a user's preferences from his or her click history to his or her relevant entities.",
    "predicted_answer": "To propagate user preferences through the knowledge graph.",
    "reference": "1803.03467v4-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4692737430167598,
      "phrase_overlap": 0.4692737430167598,
      "is_correct": false
    }
  },
  "1803.03467v4_q4": {
    "paper_id": "1803.03467v4",
    "question_index": 4,
    "question": "What is the relationship between the movies \"Forrest Gump\" and \"Cast Away\"?",
    "ground_truth": "The movies \"Forrest Gump\" and \"Cast Away\" are connected by the actor Tom Hanks.",
    "predicted_answer": "The relationship between 'Forrest Gump' and 'Cast Away' is that both movies feature Tom Hanks as the lead actor.",
    "reference": "1803.03467v4-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5445026178010471,
      "phrase_overlap": 0.5445026178010471,
      "is_correct": false
    }
  },
  "1803.03467v4_q5": {
    "paper_id": "1803.03467v4",
    "question_index": 5,
    "question": "Which dataset has the highest AUC for all ripple set sizes?",
    "ground_truth": "MovieLens-1M",
    "predicted_answer": "MovieLens-1M",
    "reference": "1803.03467v4-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1803.03467v4_q6": {
    "paper_id": "1803.03467v4",
    "question_index": 6,
    "question": "How does the dimension of embedding affect the AUC of RippleNet on MovieLens-1M?",
    "ground_truth": "The AUC of RippleNet first increases and then decreases with the increase of the dimension of embedding.",
    "predicted_answer": "The AUC of RippleNet on MovieLens-1M peaks at a dimension of embedding (d) equal to 8, with a value of 0.93.",
    "reference": "1803.03467v4-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5094339622641509,
      "phrase_overlap": 0.5094339622641509,
      "is_correct": false
    }
  },
  "1803.04572v2_q0": {
    "paper_id": "1803.04572v2",
    "question_index": 0,
    "question": "Which constraint has the most significant impact on the FIT values for the CMS data set when the target rank is 15?",
    "ground_truth": "The smoothness constraint on $\\M{U_k}$ has the most significant impact on the FIT values for the CMS data set when the target rank is 15.",
    "predicted_answer": "COPA(Non-neg on H, Sk, V) and COPA(Smoothness & l0 norm) have the most significant impact on the FIT values for the CMS data set when the target rank is 15.",
    "reference": "1803.04572v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7781569965870307,
      "phrase_overlap": 0.7781569965870307,
      "is_correct": true
    }
  },
  "1803.04572v2_q1": {
    "paper_id": "1803.04572v2",
    "question_index": 1,
    "question": "What are some common medications used to treat Sickle Cell Anemia?",
    "ground_truth": "According to the table, some common medications used to treat Sickle Cell Anemia include:\n\nBeta-adrenergic agents\nAnalgesics (narcotics and non-narcotics)\nNSAIDs (cyclooxygenase inhibitor - type)\nPotassium replacement\nSodium/saline preparations\nGeneral inhalation agents\nLaxatives and cathartics\nIV solutions (dextrose-saline)\nAntiemetic/antivertigo agents\nSedative-hypnotics (non-barbiturate)\nGlucocorticoids (orally inhaled)\nFolic acid preparations\nAnalgesic narcotic anesthetic adjunct agents",
    "predicted_answer": "BETA-ADRENERGIC AGENTS, ANALGESIC NARCOTICS, NSAIDS, CYCLOOXYGENASE INHIBITOR - TYPE, ANALGESIC/ANTIPYRETICS NON-SALICYLATE, POTASSIUM REPLACEMENT, SODIUM/SALINE PREPARATIONS, GENERAL INHALATION AGENTS, LAXATIVES AND CATHARTICS, IV SOLUTIONS: DEXTROSE-SALINE, ANTIEMETIC/ANTIVERTIGO AGENTS, SEDATIVE-HYPNOTICS NON-BARBITURATE, GLUCOCORTICOIDS, ORALLY INHALED, FOLIC ACID PREPARATIONS, ANALGESICS NARCOTIC ANESTHETIC ADJUNCT AGENTS.",
    "reference": "1803.04572v2-Table7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.20756756756756756,
      "phrase_overlap": 0.20756756756756756,
      "is_correct": false
    }
  },
  "1803.04572v2_q2": {
    "paper_id": "1803.04572v2",
    "question_index": 2,
    "question": "What are the three constraints imposed by COPA on PARAFAC2 model factors?",
    "ground_truth": "Non-negativity, smoothness, and sparsity.",
    "predicted_answer": "Smoothness, Non-negativity, Sparsity",
    "reference": "1803.04572v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6233766233766234,
      "phrase_overlap": 0.6233766233766234,
      "is_correct": true
    }
  },
  "1803.04572v2_q3": {
    "paper_id": "1803.04572v2",
    "question_index": 3,
    "question": "How do the temporal patterns of phenotype magnitude differ between sickle cell anemia and leukemia patients?",
    "ground_truth": "The temporal patterns of phenotype magnitude differ between sickle cell anemia and leukemia patients in terms of both shape and magnitude. For sickle cell anemia patients, the patterns are generally smoother and more periodic, with lower overall magnitude. For leukemia patients, the patterns are more erratic and have higher overall magnitude.",
    "predicted_answer": "Sickle cell anemia shows periodic fluctuations, while leukemia exhibits a more abrupt and irregular pattern.",
    "reference": "1803.04572v2-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.10619469026548672,
      "phrase_overlap": 0.10619469026548672,
      "is_correct": false
    }
  },
  "1803.04572v2_q4": {
    "paper_id": "1803.04572v2",
    "question_index": 4,
    "question": "Which algorithm converged faster in both cases of target rank?",
    "ground_truth": "SPARTan converged faster in both cases of target rank.",
    "predicted_answer": "COPA",
    "reference": "1803.04572v2-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.10344827586206896,
      "phrase_overlap": 0.10344827586206896,
      "is_correct": false
    }
  },
  "1803.04572v2_q5": {
    "paper_id": "1803.04572v2",
    "question_index": 5,
    "question": "Which method is faster, COPA or Helwig?",
    "ground_truth": "COPA is faster than Helwig.",
    "predicted_answer": "COPA is faster than Helwig.",
    "reference": "1803.04572v2-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1803.04572v2_q6": {
    "paper_id": "1803.04572v2",
    "question_index": 6,
    "question": "Which dataset has the largest number of clinical visits per patient?",
    "ground_truth": "CMS",
    "predicted_answer": "C",
    "reference": "1803.04572v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5,
      "phrase_overlap": 0.5,
      "is_correct": true
    }
  },
  "1704.05426v4_q0": {
    "paper_id": "1704.05426v4",
    "question_index": 0,
    "question": "What are the three types of sentences that the annotators are asked to write?",
    "ground_truth": "The three types of sentences are: \n1. A sentence that is definitely correct about the situation or event in the line.\n2. A sentence that might be correct about the situation or event in the line.\n3. A sentence that is definitely incorrect about the situation or event in the line.",
    "predicted_answer": "The three types of sentences are: 1) Definitely correct about the situation or event, 2) Might be correct about the situation or event, 3) Definitely incorrect about the situation or event.",
    "reference": "1704.05426v4-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.782051282051282,
      "phrase_overlap": 0.782051282051282,
      "is_correct": true
    }
  },
  "1704.05426v4_q1": {
    "paper_id": "1704.05426v4",
    "question_index": 1,
    "question": "Which type of word has the greatest difference in frequency of occurrence between MultiNLI and SNLI?",
    "ground_truth": "Negation (PTB)",
    "predicted_answer": "Pronouns (PTB)",
    "reference": "1704.05426v4-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5714285714285714,
      "phrase_overlap": 0.5714285714285714,
      "is_correct": false
    }
  },
  "1704.05426v4_q2": {
    "paper_id": "1704.05426v4",
    "question_index": 2,
    "question": "Which model performs better on the MultiNLI dataset when considering the percentage of individual labels that match the author's label?",
    "ground_truth": "SNLI performs better than MultiNLI when considering the percentage of individual labels that match the author's label. SNLI has a score of 85.8%, while MultiNLI has a score of 85.2%.",
    "predicted_answer": "SNLI performs slightly better on the MultiNLI dataset when considering the percentage of individual labels that match the author's label, with 85.8% compared to MultiNLI's 85.2%.",
    "reference": "1704.05426v4-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7944444444444444,
      "phrase_overlap": 0.7944444444444444,
      "is_correct": true
    }
  },
  "1704.05426v4_q3": {
    "paper_id": "1704.05426v4",
    "question_index": 3,
    "question": "Which genre in the MultiNLI corpus has the highest percentage of sentences where the Stanford Parser produced a parse rooted with an 'S' (sentence) node, and how does this compare to the overall average for the corpus?",
    "ground_truth": "The genre with the highest percentage of 'S' parses is **9/11**, with **99%** of its sentences receiving this parse. This is higher than the overall average for the MultiNLI corpus, which sits at **91%**.",
    "predicted_answer": "C",
    "reference": "1704.05426v4-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1704.05426v4_q4": {
    "paper_id": "1704.05426v4",
    "question_index": 4,
    "question": "How does the performance of the ESIM model differ when trained on MNLI alone versus trained on both MNLI and SNLI combined?",
    "ground_truth": "When trained on MNLI alone, the ESIM model achieves an accuracy of 60.7% on SNLI, 72.3% on matched genres in MNLI, and 72.1% on mismatched genres in MNLI. However, when trained on both MNLI and SNLI combined, the ESIM model's performance improves across all tasks, reaching 79.7% accuracy on SNLI, 72.4% on matched MNLI genres, and 71.9% on mismatched MNLI genres.",
    "predicted_answer": "When trained on MNLI alone, ESIM achieves 72.3 (Match) and 72.1 (Mis.). When trained on both MNLI and SNLI combined, ESIM achieves 72.4 (Match) and 71.9 (Mis.).",
    "reference": "1704.05426v4-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4312977099236641,
      "phrase_overlap": 0.4312977099236641,
      "is_correct": true
    }
  },
  "1704.05958v2_q0": {
    "paper_id": "1704.05958v2",
    "question_index": 0,
    "question": "Can you estimate the percentage of entity pairs in the NYT training set that have a corresponding relational fact in the Knowledge Base (KB)?",
    "ground_truth": "Approximately 6.66%.",
    "predicted_answer": "Approximately 6.66% of entity pairs in the NYT training set have a corresponding relational fact in the Knowledge Base (KB).",
    "reference": "1704.05958v2-Table1-1.png",
    "question_type": "multiple_choice",
    "evaluation": {
      "similarity_score": 0.2777777777777778,
      "phrase_overlap": 0.2777777777777778,
      "is_correct": true
    }
  },
  "1704.05958v2_q1": {
    "paper_id": "1704.05958v2",
    "question_index": 1,
    "question": "Why is conventional distant supervision problematic?",
    "ground_truth": "Conventional distant supervision can lead to wrong labeling of textual relations with KB relations.",
    "predicted_answer": "Conventional distant supervision is problematic because it can lead to incorrect label assignments due to mismatches between the text corpus and the knowledge base.",
    "reference": "1704.05958v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.532319391634981,
      "phrase_overlap": 0.532319391634981,
      "is_correct": false
    }
  },
  "1704.05958v2_q2": {
    "paper_id": "1704.05958v2",
    "question_index": 2,
    "question": "What is the role of the GRU cell in the embedding model?",
    "ground_truth": "The GRU cell is used to map a textual relation embedding to a probability distribution over KB relations.",
    "predicted_answer": "The GRU cell processes sequential input tokens to generate embeddings that contribute to predicting the target 'place_of_birth'.",
    "reference": "1704.05958v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.463519313304721,
      "phrase_overlap": 0.463519313304721,
      "is_correct": false
    }
  },
  "1803.05776v2_q0": {
    "paper_id": "1803.05776v2",
    "question_index": 0,
    "question": "Which model performs the best at SNR=0dB?",
    "ground_truth": "GPG-K",
    "predicted_answer": "D",
    "reference": "1803.05776v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1803.06506v3_q0": {
    "paper_id": "1803.06506v3",
    "question_index": 0,
    "question": "What is the role of the Joint Attention Module in the model?",
    "ground_truth": "The Joint Attention Module takes the embedded image and phrase features as input and uses them to induce a parameterization for spatial attention. This spatial attention map is then used by the decoder to predict the common concept.",
    "predicted_answer": "The Joint Attention Module combines visual and textual features to produce localized attention maps (7x7), which are used for refining visual features in subsequent embedding and prediction tasks.",
    "reference": "1803.06506v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.17757009345794392,
      "phrase_overlap": 0.17757009345794392,
      "is_correct": false
    }
  },
  "1803.06506v3_q1": {
    "paper_id": "1803.06506v3",
    "question_index": 1,
    "question": "How does the quality of the output heatmap change when the selected concept, predicted concept, and the real entity to be grounded are all aligned?",
    "ground_truth": "When the selected concept, predicted concept, and the real entity to be grounded are all aligned, the generated heatmap produces a good localization of the phrase.",
    "predicted_answer": "The quality of the output heatmap improves significantly when the selected concept, predicted concept, and real entity are all aligned, resulting in precise focus on the correct region.",
    "reference": "1803.06506v3-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5747126436781609,
      "phrase_overlap": 0.5747126436781609,
      "is_correct": false
    }
  },
  "1803.06506v3_q2": {
    "paper_id": "1803.06506v3",
    "question_index": 2,
    "question": "Why does the Semantic self-supervision model perform better on ReferIt (mask) compared to ReferIt (bbox), and how does this relate to the difference in performance between Visual Genome and Flickr30k datasets? ",
    "ground_truth": "The Semantic self-supervision model performs better on ReferIt (mask) compared to ReferIt (bbox) because mask-based annotations are considered more precise and accurate for measuring localization performance than bounding boxes. Masks tightly encompass the specific region referred to by the phrase, whereas bounding boxes can include extraneous areas. \n\nThis relates to the difference in performance between Visual Genome and Flickr30k because both Visual Genome and ReferIt (mask) contain phrases that refer to very specific regions or non-salient objects. Precise localization is crucial for achieving high accuracy on these datasets. Flickr30k, on the other hand, annotates all bounding boxes referring to a phrase, leading to potentially less precise localization requirements and generally higher performance across methods. ",
    "predicted_answer": "Semantic self-supervision model performs better on ReferIt (mask) compared to ReferIt (bbox) because mask-based annotations provide more detailed spatial information, aiding semantic understanding. The difference in performance between Visual Genome and Flickr30k datasets is due to Flickr30k's richer linguistic annotations and simpler visual scenes, which enhance model accuracy.",
    "reference": "1803.06506v3-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.34408602150537637,
      "phrase_overlap": 0.34408602150537637,
      "is_correct": true
    }
  },
  "1803.06506v3_q3": {
    "paper_id": "1803.06506v3",
    "question_index": 3,
    "question": "How does the proposed method's attention map differ from the VGG16 feature map?",
    "ground_truth": "The attention map generated by proposed method is able to localize regions that were weak or non-existent in the activations of the input maps, while the VGG16 feature map simply amplifies the activations present in VGG16 channels.",
    "predicted_answer": "The proposed method's attention map focuses on specific semantic details (e.g., \"boy in purple and white,\" \"its head is black in color,\" \"foot dragging on the ground,\" \"male surfer making a face\"), while the VGG16 feature map highlights broader regions without detailed semantic emphasis.",
    "reference": "1803.06506v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.16184971098265896,
      "phrase_overlap": 0.16184971098265896,
      "is_correct": true
    }
  },
  "1803.06506v3_q4": {
    "paper_id": "1803.06506v3",
    "question_index": 4,
    "question": "Which loss type performs best when the concept batch size is 5k?",
    "ground_truth": "Independent and common concept",
    "predicted_answer": "Independent and common concept performs best when the concept batch size is 5k, with a score of 28.91.",
    "reference": "1803.06506v3-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.45454545454545453,
      "phrase_overlap": 0.45454545454545453,
      "is_correct": true
    }
  },
  "1803.06506v3_q5": {
    "paper_id": "1803.06506v3",
    "question_index": 5,
    "question": "How does the performance of the model vary with respect to the bounding box area and the similarity of the concept with ImageNet classes?",
    "ground_truth": "The performance of the model increases with increasing bounding box area and decreases with increasing similarity of the concept with ImageNet classes.",
    "predicted_answer": "Bounding box area: Positive correlation (0.85).  \nSimilarity with ImageNet classes: No correlation (-0.02).",
    "reference": "1803.06506v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4296875,
      "phrase_overlap": 0.4296875,
      "is_correct": false
    }
  },
  "1803.06506v3_q6": {
    "paper_id": "1803.06506v3",
    "question_index": 6,
    "question": "Which dataset would you expect to be the easiest for a model to localize phrases in, and why?",
    "ground_truth": "Flickr30k is likely the easiest dataset for a model to localize phrases in. \nFlickr30k has the shortest average phrase length (2.3 words) and the lowest average noun count (1.2) per phrase. This suggests that the phrases in this dataset are simpler and often directly refer to single objects present in the image. This makes the localization task easier, almost approaching a weakly supervised setting where the object to be localized is explicitly named in the phrase.",
    "predicted_answer": "B",
    "reference": "1803.06506v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0042643923240938165,
      "phrase_overlap": 0.0042643923240938165,
      "is_correct": true
    }
  },
  "1704.04539v2_q0": {
    "paper_id": "1704.04539v2",
    "question_index": 0,
    "question": "What is the relationship between the Silver Snatch and the Gold Snatch?",
    "ground_truth": "The Silver Snatch and the Gold Snatch are positively correlated. As the Gold Snatch increases, the Silver Snatch also increases.",
    "predicted_answer": "There is a positive correlation between Silver Snatch and Gold Snatch, but the relationship is weaker compared to the stronger positive correlation between Full-cycle Snatch and Gold Snatch.",
    "reference": "1704.04539v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2389937106918239,
      "phrase_overlap": 0.2389937106918239,
      "is_correct": false
    }
  },
  "1704.04539v2_q1": {
    "paper_id": "1704.04539v2",
    "question_index": 1,
    "question": "Which system performs best on the IT domain in terms of full-cycle Smatch score, and how does its performance compare to the projection-based system in the same domain?",
    "ground_truth": "The GT system achieves the highest full-cycle Smatch score in the IT domain with a score of 59. This is 14 points higher than the projection-based system in the same domain, which scored 45.",
    "predicted_answer": "GT, 14 points higher.",
    "reference": "1704.04539v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1895734597156398,
      "phrase_overlap": 0.1895734597156398,
      "is_correct": true
    }
  },
  "1704.04539v2_q2": {
    "paper_id": "1704.04539v2",
    "question_index": 2,
    "question": "Which translation system performs the best and how does its performance compare to Google Translate (GT)? Is the comparison with GT completely fair? Explain your answer.",
    "ground_truth": "According to Table 2, Moses achieves the highest BLEU scores among the listed translation systems (Moses, Nematus) across all language pairs. However, its performance still falls behind Google Translate (GT) in every case.\n\nThe comparison with GT might not be entirely fair because, as GT has the advantage of being trained on a significantly larger dataset. This suggests that GT's performance advantage might be partially due to its training data rather than solely its inherent capabilities.",
    "predicted_answer": "GT  \n不完全公平。",
    "reference": "1704.04539v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1704.04539v2_q3": {
    "paper_id": "1704.04539v2",
    "question_index": 3,
    "question": "What is the difference between the parsing trees for \"I like eating\" and \"I like grapes\"?",
    "ground_truth": "The parsing tree for \"I like eating\" has only one argument, while the parsing tree for \"I like grapes\" has two arguments.",
    "predicted_answer": "The difference is that 'I like eating' uses 'eat-01' with one argument (ARG0: 'I'), while 'I like grapes' uses 'like-01' with two arguments (ARG0: 'I' and ARG1: 'grape').",
    "reference": "1704.04539v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5292096219931272,
      "phrase_overlap": 0.5292096219931272,
      "is_correct": false
    }
  },
  "1803.04383v2_q0": {
    "paper_id": "1803.04383v2",
    "question_index": 0,
    "question": "How do the outcome curves for the black and white groups differ?",
    "ground_truth": "The outcome curve for the black group is generally lower than the outcome curve for the white group. This indicates that, for a given selection rate, the black group experiences a smaller change in credit score than the white group.",
    "predicted_answer": "Black group shows a decrease in score change (Δμ) and utility (U) at higher selection rates, while the White group shows an increase in score change and utility at higher selection rates.",
    "reference": "1803.04383v2-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.10023866348448687,
      "phrase_overlap": 0.10023866348448687,
      "is_correct": false
    }
  },
  "1803.04383v2_q1": {
    "paper_id": "1803.04383v2",
    "question_index": 1,
    "question": "How does the selection rate affect the expected outcome and institution utilities for different decision rules?",
    "ground_truth": "The selection rate has a different effect on the expected outcome and institution utilities for different decision rules. For example, the maximum expected outcome is achieved at a higher selection rate for the MaxUtil rule than for the EqOpt rule.",
    "predicted_answer": "Higher selection rates generally decrease institution utilities (U) for all decision rules (MU, DP, EO), while the expected outcome (Δμ) peaks at βMaxUtil and decreases thereafter.",
    "reference": "1803.04383v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.102803738317757,
      "phrase_overlap": 0.102803738317757,
      "is_correct": false
    }
  },
  "1803.04383v2_q2": {
    "paper_id": "1803.04383v2",
    "question_index": 2,
    "question": "Does the probability of repaying a debt increase or decrease with credit score?",
    "ground_truth": "The probability of repaying a debt increases with credit score.",
    "predicted_answer": "Increase.",
    "reference": "1803.04383v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.25,
      "phrase_overlap": 0.25,
      "is_correct": false
    }
  },
  "1803.04383v2_q3": {
    "paper_id": "1803.04383v2",
    "question_index": 3,
    "question": "Which fairness criteria results in the highest loan approval rate for the Black group when the loss/profit ratio is -4?",
    "ground_truth": "The maximum profit criteria ($\\maxprof$) results in the highest loan approval rate for the Black group when the loss/profit ratio is -4.",
    "predicted_answer": "harm",
    "reference": "1803.04383v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.05714285714285714,
      "phrase_overlap": 0.05714285714285714,
      "is_correct": false
    }
  },
  "1803.04383v2_q4": {
    "paper_id": "1803.04383v2",
    "question_index": 4,
    "question": "What does the outcome curve tell us about the relationship between selection rate and mean change in score?",
    "ground_truth": "The outcome curve shows that the relationship between selection rate and mean change in score is complex and depends on the specific group being considered. For groups with high potential for gain, increasing the selection rate can lead to large increases in mean score. However, for groups with low potential for gain, increasing the selection rate can actually lead to decreases in mean score.",
    "predicted_answer": "The outcome curve shows that the relationship between selection rate and mean change in score is non-linear: increasing selection rate initially leads to improvement in scores, but beyond a certain point (β*), further increases in selection rate result in harm, with active harm occurring at the highest selection rates.",
    "reference": "1803.04383v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2993006993006993,
      "phrase_overlap": 0.2993006993006993,
      "is_correct": false
    }
  },
  "1704.07121v2_q0": {
    "paper_id": "1704.07121v2",
    "question_index": 0,
    "question": "Which model performed best on the \"All\" category of Visual7W, and how did its performance compare to human performance?",
    "ground_truth": "The MLP-IQA model achieved the highest accuracy in the \"All\" category of Visual7W, with a score of 45.1%. However, this performance still falls significantly short of human performance, which stands at 84.1% for the same category.",
    "predicted_answer": "MLP-IQA performed best on the 'All' category with an accuracy of 45.1%, which is significantly lower than human performance at 84.1%.",
    "reference": "1704.07121v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3085399449035813,
      "phrase_overlap": 0.3085399449035813,
      "is_correct": true
    }
  },
  "1704.07121v2_q1": {
    "paper_id": "1704.07121v2",
    "question_index": 1,
    "question": "Which method performs best overall on VQA-2014val, and how does its performance compare to human performance on the same dataset?",
    "ground_truth": "MLP-IQA achieves the highest overall accuracy (46.5%) among the machine learning methods tested on VQA-2014val. However, this performance still falls short of human performance, which reaches an accuracy of 85.5% on the same dataset.",
    "predicted_answer": "Human-IQA performs best overall on VQA-2014val with a score of 85.5, significantly outperforming the second-best method, Attention*, which has a score of 58.6.",
    "reference": "1704.07121v2-Table7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1326530612244898,
      "phrase_overlap": 0.1326530612244898,
      "is_correct": true
    }
  },
  "1704.07121v2_q2": {
    "paper_id": "1704.07121v2",
    "question_index": 2,
    "question": "Which model performs the best on qaVG when considering both image understanding (IU) and question understanding (QU) individually, and how does its performance compare to humans?",
    "ground_truth": "MLP-QA performs the best when considering both IU and QU individually, achieving 89.3% accuracy on IU and 45.6% accuracy on QU. However, its combined performance (IU+QU) of 43.9% is still significantly lower than human performance, which stands at 82.5%.",
    "predicted_answer": "MLP-IQA",
    "reference": "1704.07121v2-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.03065134099616858,
      "phrase_overlap": 0.03065134099616858,
      "is_correct": false
    }
  },
  "1704.07121v2_q3": {
    "paper_id": "1704.07121v2",
    "question_index": 3,
    "question": "How can the shortcuts in the Visual7W dataset be remedied?",
    "ground_truth": "The shortcuts in the Visual7W dataset can be remedied by creating alternative decoys that are more likely to be correct, based on either the image or the question alone. This forces the machine to consider all of the information together in order to select the correct answer.",
    "predicted_answer": "The shortcuts in the Visual7W dataset can be remedied by ensuring that both image and question contexts are equally considered during training and evaluation, reducing reliance on either context alone.",
    "reference": "1704.07121v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.33542976939203356,
      "phrase_overlap": 0.33542976939203356,
      "is_correct": true
    }
  },
  "1704.07121v2_q4": {
    "paper_id": "1704.07121v2",
    "question_index": 4,
    "question": "Which dataset presents the biggest challenge for a model trying to distinguish true triplets from decoys, and why?",
    "ground_truth": "The VQA dataset presents the biggest challenge.",
    "predicted_answer": "The VQA dataset presents the biggest challenge for a model trying to distinguish true triplets from decoys because it has the highest number of decoys per triplet (17).",
    "reference": "1704.07121v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4372093023255814,
      "phrase_overlap": 0.4372093023255814,
      "is_correct": false
    }
  },
  "1704.07121v2_q5": {
    "paper_id": "1704.07121v2",
    "question_index": 5,
    "question": "Which embedding method performed the best overall across all model architectures on the Visual7W dataset with IoU + QoU decoys? Was there a significant difference in performance compared to the other methods?",
    "ground_truth": "The MLP-IQA model achieved the highest accuracy across all embedding methods, reaching 52.5% with GloVe, 51.4% with Translation embeddings, and 52.0% with word2vec. However, the passage notes that there was no significant difference in performance between the different embedding methods.",
    "predicted_answer": "GloVE.",
    "reference": "1704.07121v2-Table11-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1704.07121v2_q6": {
    "paper_id": "1704.07121v2",
    "question_index": 6,
    "question": "Which model performs the best on COCOQA dataset when considering the combined accuracy of identifying irrelevant image-question pairs (IU) and irrelevant question-answer pairs (QU)? How does this compare to the performance of the model that only observes answers?",
    "ground_truth": "The MLP-IQA model performs the best when considering the combined accuracy of identifying irrelevant image-question and question-answer pairs, achieving an accuracy of 75.9%. This is significantly higher than the MLP-A model, which only observes answers and achieves a combined accuracy of 26.6%, close to random performance.",
    "predicted_answer": "MLP-IQA",
    "reference": "1704.07121v2-Table8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.04216867469879518,
      "phrase_overlap": 0.04216867469879518,
      "is_correct": true
    }
  },
  "1704.07121v2_q7": {
    "paper_id": "1704.07121v2",
    "question_index": 7,
    "question": "On the VQAv2-2017 validation set, which model performs best when considering all three sources of information (images, questions, and answers) and how does its performance compare to the model that only uses answers?",
    "ground_truth": "The model that performs best on VQAv2-2017 val when considering all three sources of information is MLP-IQA. It achieves an accuracy of 61.1% on the \\IU+\\QU -decoys metric, significantly outperforming the model that only uses answers (MLP-A) which achieves only 27.7% on the same metric. This demonstrates the importance of incorporating all available information for accurate prediction.",
    "predicted_answer": "MLP-IQA, 61.1 vs 27.7",
    "reference": "1704.07121v2-Table9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.06845965770171149,
      "phrase_overlap": 0.06845965770171149,
      "is_correct": true
    }
  },
  "1704.00774v3_q0": {
    "paper_id": "1704.00774v3",
    "question_index": 0,
    "question": "What is the effect of increasing K on the test PPL of the different models?",
    "ground_truth": "The test PPL of all the models decreases as K increases.",
    "predicted_answer": "Test PPL decreases.",
    "reference": "1704.00774v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5066666666666667,
      "phrase_overlap": 0.5066666666666667,
      "is_correct": false
    }
  },
  "1804.00863v3_q0": {
    "paper_id": "1804.00863v3",
    "question_index": 0,
    "question": "How does the appearance of the sphere differ between the re-synthesis using DAMs and the reference image?",
    "ground_truth": "The sphere in the re-synthesis using DAMs appears to have a more even and consistent surface texture than the reference image.",
    "predicted_answer": "The sphere appearance in 'Ours' is smoother and more uniform, while the reference images ('Ref. maps Reference') show detailed imperfections and realistic lighting.",
    "reference": "1804.00863v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3448275862068966,
      "phrase_overlap": 0.3448275862068966,
      "is_correct": false
    }
  },
  "1804.00863v3_q1": {
    "paper_id": "1804.00863v3",
    "question_index": 1,
    "question": "Which method performs best for the \"Representation\" task when the view is \"Novel\"?",
    "ground_truth": "The \"OUR\" method performs best for the \"Representation\" task when the view is \"Novel\".",
    "predicted_answer": "A",
    "reference": "1804.00863v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.022988505747126436,
      "phrase_overlap": 0.022988505747126436,
      "is_correct": true
    }
  },
  "1804.00863v3_q2": {
    "paper_id": "1804.00863v3",
    "question_index": 2,
    "question": "How do the reconstructions compare to the original samples?",
    "ground_truth": "The reconstructions are very similar to the original samples.",
    "predicted_answer": "The reconstructions closely resemble the original samples in terms of color and lighting conditions, maintaining the visual characteristics of the spheres.",
    "reference": "1804.00863v3-Figure11-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.42592592592592593,
      "phrase_overlap": 0.42592592592592593,
      "is_correct": false
    }
  },
  "1804.00863v3_q3": {
    "paper_id": "1804.00863v3",
    "question_index": 3,
    "question": "What is the difference between the representation module and the learning-to-learn module?",
    "ground_truth": "The representation module takes an input image and outputs a feature representation. The learning-to-learn module takes a set of features and learns how to segment the image.",
    "predicted_answer": "The Representation module processes a smaller input (9) and outputs a smaller feature space (3) with fewer layers, while the Learning-to-learn module starts with a larger input (256) and outputs a significantly larger feature space (3360) with more layers.",
    "reference": "1804.00863v3-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5488372093023256,
      "phrase_overlap": 0.5488372093023256,
      "is_correct": false
    }
  },
  "1804.00863v3_q4": {
    "paper_id": "1804.00863v3",
    "question_index": 4,
    "question": "What is the difference between the representation task and the learning-to-learn task?",
    "ground_truth": "The representation task takes an appearance as input and outputs an RGB value, while the learning-to-learn task takes an image as input and outputs a DAM representation.",
    "predicted_answer": "The representation task focuses on extracting features from input data, while the learning-to-learn task involves adapting the model to learn new tasks efficiently.",
    "reference": "1804.00863v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5645645645645646,
      "phrase_overlap": 0.5645645645645646,
      "is_correct": false
    }
  },
  "1804.00863v3_q5": {
    "paper_id": "1804.00863v3",
    "question_index": 5,
    "question": "What is the relationship between gloss and representation error?",
    "ground_truth": "The representation error decreases as the gloss decreases.",
    "predicted_answer": "Representation error decreases as gloss decreases.",
    "reference": "1804.00863v3-Figure10-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9259259259259259,
      "phrase_overlap": 0.9259259259259259,
      "is_correct": true
    }
  },
  "1804.01429v3_q0": {
    "paper_id": "1804.01429v3",
    "question_index": 0,
    "question": "What is the difference between an \"agent-in-place\" action and a generic action category?",
    "ground_truth": "An agent-in-place action is an action that is performed by an agent in a specific place, while a generic action category is a more general category of action that does not specify the place where the action is performed.",
    "predicted_answer": "The difference is that 'agent-in-place' actions focus on localized context where the agent remains in a specific location, while generic action categories involve broader movements or interactions across different contexts.",
    "reference": "1804.01429v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.07674943566591422,
      "phrase_overlap": 0.07674943566591422,
      "is_correct": false
    }
  },
  "1804.01429v3_q1": {
    "paper_id": "1804.01429v3",
    "question_index": 1,
    "question": "What are the different types of features used by the Layout-induced Video Representation Network?",
    "ground_truth": "The Layout-induced Video Representation Network uses three types of features: place-based features, distance-based features, and topological features.",
    "predicted_answer": "Place-based features, distance-based features, and topological features.",
    "reference": "1804.01429v3-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6486486486486487,
      "phrase_overlap": 0.6486486486486487,
      "is_correct": true
    }
  },
  "1804.01429v3_q2": {
    "paper_id": "1804.01429v3",
    "question_index": 2,
    "question": "What is the role of the 3D ConvNet in the distance-based place discretization process?",
    "ground_truth": "The 3D ConvNet is used to extract features from the input images. These features are then used to generate place-based feature descriptions.",
    "predicted_answer": "The 3D ConvNet processes spatial and temporal data from features and DT results to generate place-based feature descriptions.",
    "reference": "1804.01429v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6943396226415094,
      "phrase_overlap": 0.6943396226415094,
      "is_correct": true
    }
  },
  "1804.01429v3_q3": {
    "paper_id": "1804.01429v3",
    "question_index": 3,
    "question": "How does the LIVR framework decompose semantic features into different places?",
    "ground_truth": "The LIVR framework decomposes semantic features into different places by utilizing bitmaps encoded with the semantic labels of places. This decomposition encourages the network to learn features of generic place-based motion patterns that are independent of scene layouts.",
    "predicted_answer": "The LIVR framework decomposes semantic features into different places such as street, sidewalk, lawn, and walkway through Semantic Feature Decomposition, aggregates them using Topological Feature Aggregation, and discretizes them based on distance.",
    "reference": "1804.01429v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.28846153846153844,
      "phrase_overlap": 0.28846153846153844,
      "is_correct": false
    }
  },
  "1804.01429v3_q4": {
    "paper_id": "1804.01429v3",
    "question_index": 4,
    "question": "Why does the proposed method outperform the baselines for the action \"<person, move toward (home), walkway>\"?",
    "ground_truth": "The proposed method outperforms the baselines for the action \"<person, move toward (home), walkway>\" because it is better at modeling moving directions.",
    "predicted_answer": "PD+DD+Topo-Agg: 0.89",
    "reference": "1804.01429v3-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.10465116279069768,
      "phrase_overlap": 0.10465116279069768,
      "is_correct": false
    }
  },
  "1804.01429v3_q5": {
    "paper_id": "1804.01429v3",
    "question_index": 5,
    "question": "Which actions are most challenging for the network to recognize, and how do the proposed methods improve the performance on these actions?",
    "ground_truth": "The actions that are most challenging for the network to recognize are those that include moving directions, such as \"person, move toward (home)\", \"person, move away (home)\", and \"vehicle, move toward (person)\". The proposed methods, distance-based place discretization (DD) and topological feature aggregation (Topo-Agg), significantly improve the average precision on almost all action categories, especially those that are more challenging and are associated with moving directions.",
    "predicted_answer": "<person, move away (home), driveway>",
    "reference": "1804.01429v3-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.04990403071017274,
      "phrase_overlap": 0.04990403071017274,
      "is_correct": false
    }
  },
  "1811.02721v3_q0": {
    "paper_id": "1811.02721v3",
    "question_index": 0,
    "question": "How does batching affect the radio duty cycle and CPU duty cycle?",
    "ground_truth": "Batching reduces both the radio duty cycle and CPU duty cycle.",
    "predicted_answer": "Batching reduces both the radio duty cycle and CPU duty cycle.",
    "reference": "1811.02721v3-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1811.02721v3_q1": {
    "paper_id": "1811.02721v3",
    "question_index": 1,
    "question": "Which technique was most effective at reducing memory consumption in both send and receive buffers?",
    "ground_truth": "The \"Resource Constraints\" challenge was addressed with two techniques: \"Zero-Copy Send\" and \"In-Place Reass.\" The first led to a 50% reduction in send buffer memory usage, while the second achieved a 38% reduction in receive buffer memory. Therefore, Zero-Copy Send was slightly more effective in reducing overall memory consumption.",
    "predicted_answer": "A",
    "reference": "1811.02721v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1811.02721v3_q2": {
    "paper_id": "1811.02721v3",
    "question_index": 2,
    "question": "Based on Table 1 and the passage, why does TCP perform poorly on IEEE 802.15.4 networks compared to other network types listed? ",
    "ground_truth": "TCP performs poorly on IEEE 802.15.4 networks because the Maximum Transmission Unit (MTU) for these networks is significantly smaller than other network types. This small MTU size results in a high percentage of overhead due to the TCP/IP headers, exceeding 50%. ",
    "predicted_answer": "TCP performs poorly on IEEE 802.15.4 networks due to its significantly lower capacity (250 kb/s), smaller MTU (104–116 B), and higher transmission time (4.1 ms), which result in reduced efficiency and increased overhead compared to other network types.",
    "reference": "1811.02721v3-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2607003891050584,
      "phrase_overlap": 0.2607003891050584,
      "is_correct": true
    }
  },
  "1811.02721v3_q3": {
    "paper_id": "1811.02721v3",
    "question_index": 3,
    "question": "Why is relying on fragmentation effective for reducing header overhead?",
    "ground_truth": "Relying on fragmentation is effective because the TCP/IP headers are only included in the first fragment, not in subsequent fragments. This significantly reduces the overhead in later fragments.",
    "predicted_answer": "Relying on fragmentation is effective for reducing header overhead because subsequent frames (nth Frame) do not include IPv6 and TCP headers, significantly decreasing the total header size.",
    "reference": "1811.02721v3-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5848563968668408,
      "phrase_overlap": 0.5848563968668408,
      "is_correct": false
    }
  },
  "1811.02721v3_q4": {
    "paper_id": "1811.02721v3",
    "question_index": 4,
    "question": "How does varying the buffer size affect TCP goodput?",
    "ground_truth": "Increasing the buffer size generally leads to increased TCP goodput, but only up to a certain point.",
    "predicted_answer": "Increasing buffer size improves TCP goodput for uplink significantly, peaking at buffer size 6-7 segments, while downlink shows minor improvement.",
    "reference": "1811.02721v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.42276422764227645,
      "phrase_overlap": 0.42276422764227645,
      "is_correct": false
    }
  },
  "1811.02721v3_q5": {
    "paper_id": "1811.02721v3",
    "question_index": 5,
    "question": "What is the function of the Hamilton-based PCB in the ultrasonic anemometer?",
    "ground_truth": "The Hamilton-based PCB is the electronic control board of the anemometer. It houses the microcontroller, sensors, and other electronic components that are necessary for the anemometer to function.",
    "predicted_answer": "The Hamilton-based PCB in the ultrasonic anemometer functions as the signal processing and control unit for the device.",
    "reference": "1811.02721v3-Figure13-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3619047619047619,
      "phrase_overlap": 0.3619047619047619,
      "is_correct": false
    }
  },
  "1811.02721v3_q6": {
    "paper_id": "1811.02721v3",
    "question_index": 6,
    "question": "What is the difference in response time between CoAP and HTTP for a response size of 50 KiB?",
    "ground_truth": "The difference in response time between CoAP and HTTP for a response size of 50 KiB is approximately 20 seconds.",
    "predicted_answer": "The difference in response time between CoAP and HTTP for a response size of 50 KiB is approximately 20 seconds.",
    "reference": "1811.02721v3-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1811.02721v3_q7": {
    "paper_id": "1811.02721v3",
    "question_index": 7,
    "question": "How does the maximum link delay affect the segment loss rate and goodput in a TCP connection with one hop?",
    "ground_truth": "As the maximum link delay increases, the segment loss rate increases and the goodput decreases.",
    "predicted_answer": "Segment loss rate increases, goodput decreases.",
    "reference": "1811.02721v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.647887323943662,
      "phrase_overlap": 0.647887323943662,
      "is_correct": true
    }
  },
  "1811.02721v3_q8": {
    "paper_id": "1811.02721v3",
    "question_index": 8,
    "question": "What protocol has a higher radio duty cycle in the first 7 hours of the trial?",
    "ground_truth": "TCP",
    "predicted_answer": "CoAP",
    "reference": "1811.02721v3-Figure11-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5714285714285714,
      "phrase_overlap": 0.5714285714285714,
      "is_correct": false
    }
  },
  "1811.02721v3_q9": {
    "paper_id": "1811.02721v3",
    "question_index": 9,
    "question": "How does the maximum link delay affect the number of TCP timeouts and fast retransmissions?",
    "ground_truth": "The number of TCP timeouts and fast retransmissions decreases as the maximum link delay increases.",
    "predicted_answer": "Maximum link delay increase reduces fast retransmissions and slightly increases timeouts.",
    "reference": "1811.02721v3-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.31016042780748665,
      "phrase_overlap": 0.31016042780748665,
      "is_correct": false
    }
  },
  "1811.02721v3_q10": {
    "paper_id": "1811.02721v3",
    "question_index": 10,
    "question": "Which module of TCPlp consumes the most memory in the active RAM on TinyOS, and how much memory does it utilize?",
    "ground_truth": "The protocol implementation module consumes the most memory in the active RAM on TinyOS, utilizing 488 bytes.",
    "predicted_answer": "The Protocol module consumes the most memory in the active RAM on TinyOS, utilizing 488 B.",
    "reference": "1811.02721v3-Table8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9045226130653267,
      "phrase_overlap": 0.9045226130653267,
      "is_correct": true
    }
  },
  "1811.02721v3_q11": {
    "paper_id": "1811.02721v3",
    "question_index": 11,
    "question": "How does the reliability of CoAP compare to TCPlp and what potential factors contribute to this difference?",
    "ground_truth": "Table 1 shows that CoAP has slightly higher reliability (99.5%) compared to TCPlp (99.3%). While both protocols perform well, this difference could be attributed to several factors, including:\n\nRetransmission mechanisms: CoAP employs a built-in retransmission mechanism for lost packets, while TCPlp relies on the underlying network layer for retransmissions. This could give CoAP an edge in recovering lost packets and achieving higher reliability.\nCongestion control: CoAP includes mechanisms to adapt to network congestion, potentially reducing packet loss and improving reliability.\nPacket size: CoAP typically uses smaller packets compared to TCPlp. Smaller packets are less prone to loss in wireless networks, potentially contributing to CoAP's slightly higher reliability.",
    "predicted_answer": "CoAP is more reliable than TCPlp (99.5% vs. 99.3%). Factors contributing to this difference may include CoAP's lower Radio DC (1.84% vs. 2.29%) and CPU DC (0.834% vs. 0.973%), indicating more efficient resource usage.",
    "reference": "1811.02721v3-Table7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.08442211055276382,
      "phrase_overlap": 0.08442211055276382,
      "is_correct": true
    }
  },
  "1811.02721v3_q12": {
    "paper_id": "1811.02721v3",
    "question_index": 12,
    "question": "How does the memory usage of the RIOT OS posix_sockets module compare to the memory used by the protocol and socket layer combined, for both active and passive connections?",
    "ground_truth": "The posix_sockets module consistently uses less memory than the combined usage of the protocol and socket layer. For an active connection, it requires about 5468 B compared to 19972 B + 6216 B = 26188 B for the other layers. Similarly, for a passive connection, it uses 5468 B compared to 19972 B + 6216 B = 26188 B for the other layers.",
    "predicted_answer": "RAM (Active): Less  \nRAM (Passive): Less",
    "reference": "1811.02721v3-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.06933333333333333,
      "phrase_overlap": 0.06933333333333333,
      "is_correct": false
    }
  },
  "1811.02721v3_q13": {
    "paper_id": "1811.02721v3",
    "question_index": 13,
    "question": "Which TCP stack provides the most complete implementation of core TCP features, and which stack lacks the most features?",
    "ground_truth": "The TCP stack presented in this paper (TCPlp) provides the most complete implementation of core TCP features, including flow control, congestion control, RTT estimation, MSS option, OOO reassembly, and various advanced features like timestamps and selective ACKs. In contrast, BLIP lacks the most features, as it does not implement congestion control, RTT estimation, or several other functionalities present in other stacks.",
    "predicted_answer": "TCPlp provides the most complete implementation of core TCP features, while uIP lacks the most features.",
    "reference": "1811.02721v3-Table9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2646502835538752,
      "phrase_overlap": 0.2646502835538752,
      "is_correct": false
    }
  },
  "1811.02721v3_q14": {
    "paper_id": "1811.02721v3",
    "question_index": 14,
    "question": "How many hops are there between the Hamilton and the Internet?",
    "ground_truth": "5 hops",
    "predicted_answer": "3",
    "reference": "1811.02721v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1704.07854v4_q0": {
    "paper_id": "1704.07854v4",
    "question_index": 0,
    "question": "What does the parameter network do to the initial surface?",
    "ground_truth": "The parameter network weights the initial surface, causing it to deform.",
    "predicted_answer": "The parameter network β modifies the initial surface ψ₀ to produce the reference surface φₐₓ.",
    "reference": "1704.07854v4-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6545454545454545,
      "phrase_overlap": 0.6545454545454545,
      "is_correct": true
    }
  },
  "1704.07854v4_q1": {
    "paper_id": "1704.07854v4",
    "question_index": 1,
    "question": "How do the parameter network and the deformation network differ in terms of complexity and function?",
    "ground_truth": "The parameter network is a simple structure with two fully connected layers, while the deformation network is more complex and contains two fully connected layers followed by two or more four-dimensional de-convolution layers. The parameter network learns how to apply multiple long-range, non-linear deformation fields, while the deformation network learns to generate dense deformation fields to refine the final surface.",
    "predicted_answer": "The parameter network is simpler with fewer layers and smaller dimensions, focusing on parameter mapping. The deformation network is more complex with additional layers and larger dimensions, designed for spatial deformation tasks.",
    "reference": "1704.07854v4-Figure12-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.10091743119266056,
      "phrase_overlap": 0.10091743119266056,
      "is_correct": false
    }
  },
  "1704.07854v4_q2": {
    "paper_id": "1704.07854v4",
    "question_index": 2,
    "question": "Which gradient approximation method leads to a more stable and lower loss value during training?",
    "ground_truth": "The corrected gradient method leads to a more stable and lower loss value during training.",
    "predicted_answer": "corrected gradient",
    "reference": "1704.07854v4-Figure15-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3333333333333333,
      "phrase_overlap": 0.3333333333333333,
      "is_correct": true
    }
  },
  "1704.07854v4_q3": {
    "paper_id": "1704.07854v4",
    "question_index": 3,
    "question": "How does the flow of water change as the central wall obstacle is shifted to the right?",
    "ground_truth": "The flow of water increases as the central wall obstacle is shifted to the right.",
    "predicted_answer": "The flow of water becomes more concentrated on the left side.",
    "reference": "1704.07854v4-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5915492957746479,
      "phrase_overlap": 0.5915492957746479,
      "is_correct": false
    }
  },
  "1704.07854v4_q4": {
    "paper_id": "1704.07854v4",
    "question_index": 4,
    "question": "Which of the two scenes, Drop or Staris, requires more computation time for rendering?",
    "ground_truth": "Staris",
    "predicted_answer": "Stairs requires more computation time for rendering (35ms vs. 21ms for Drop).",
    "reference": "1704.07854v4-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.14457831325301204,
      "phrase_overlap": 0.14457831325301204,
      "is_correct": false
    }
  },
  "1704.07854v4_q5": {
    "paper_id": "1704.07854v4",
    "question_index": 5,
    "question": "What is the relationship between the resolution of the simulation and the training time?",
    "ground_truth": " The higher the resolution of the simulation, the longer the training time. ",
    "predicted_answer": "Higher resolution leads to longer training time.",
    "reference": "1704.07854v4-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7049180327868853,
      "phrase_overlap": 0.7049180327868853,
      "is_correct": true
    }
  },
  "1704.07854v4_q6": {
    "paper_id": "1704.07854v4",
    "question_index": 6,
    "question": "What is the role of the parameter network in the weighting and refinement stage?",
    "ground_truth": "The parameter network is used to infer a weighting function.",
    "predicted_answer": "Generate weights β for pre-computed deformations.",
    "reference": "1704.07854v4-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3669724770642202,
      "phrase_overlap": 0.3669724770642202,
      "is_correct": false
    }
  },
  "1704.07854v4_q7": {
    "paper_id": "1704.07854v4",
    "question_index": 7,
    "question": "Which of the methods is able to reconstruct the shape of the liquid properly?",
    "ground_truth": "Only the full method with a deformation network is able to produce a perfect reconstruction.",
    "predicted_answer": "With NN deformation",
    "reference": "1704.07854v4-Figure16-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3063063063063063,
      "phrase_overlap": 0.3063063063063063,
      "is_correct": false
    }
  },
  "1704.07854v4_q8": {
    "paper_id": "1704.07854v4",
    "question_index": 8,
    "question": "How do the initial conditions of the simulations vary?",
    "ground_truth": "The initial conditions of the simulations vary in two dimensions: the position of the liquid drop along the x-axis (α1) and the size of the drop (α2).",
    "predicted_answer": "The initial conditions vary systematically across two dimensions: Drop Position X and Drop Size, with each combination resulting in a distinct initial state.",
    "reference": "1704.07854v4-Figure13-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.495114006514658,
      "phrase_overlap": 0.495114006514658,
      "is_correct": false
    }
  },
  "1804.04410v2_q0": {
    "paper_id": "1804.04410v2",
    "question_index": 0,
    "question": "How are documents ranked and pruned in the telescoping architecture?",
    "ground_truth": "Documents are first matched using a pre-defined match plan. Then, they are passed through additional rank-and-prune stages, which are implemented as a cascade of machine learning models.",
    "predicted_answer": "Documents are ranked and pruned progressively across multiple levels (L0, L1, L2) in the telescoping architecture.",
    "reference": "1804.04410v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.31333333333333335,
      "phrase_overlap": 0.31333333333333335,
      "is_correct": false
    }
  },
  "1804.04410v2_q1": {
    "paper_id": "1804.04410v2",
    "question_index": 1,
    "question": "How does the RL policy compare to the baseline in terms of index blocks accessed?",
    "ground_truth": "The RL policy accesses fewer index blocks than the baseline.",
    "predicted_answer": "The RL policy accesses fewer index blocks compared to the baseline.",
    "reference": "1804.04410v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.8976377952755905,
      "phrase_overlap": 0.8976377952755905,
      "is_correct": true
    }
  },
  "1804.04410v2_q2": {
    "paper_id": "1804.04410v2",
    "question_index": 2,
    "question": "How does the performance of the learned policy compare to the production baseline for CAT2 queries in terms of relevance and efficiency?",
    "ground_truth": "For CAT2 queries, the learned policy shows a slight improvement in relevance (NCG) for the weighted set and a significant reduction in index blocks accessed for both weighted and unweighted sets.",
    "predicted_answer": "For CAT2 queries, the learned policy (weighted set) improves relevance slightly (+0.2% NCG@100) while significantly reducing index block access (-22.7%), indicating better efficiency and maintained or slightly improved relevance compared to the production baseline.",
    "reference": "1804.04410v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5043478260869565,
      "phrase_overlap": 0.5043478260869565,
      "is_correct": true
    }
  },
  "1704.08615v2_q0": {
    "paper_id": "1704.08615v2",
    "question_index": 0,
    "question": "How does the performance of the SIM saliency map change as the number of fixations increases?",
    "ground_truth": "The performance of the SIM saliency map increases as the number of fixations increases.",
    "predicted_answer": "Performance improves as the number of fixations increases.",
    "reference": "1704.08615v2-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7724137931034483,
      "phrase_overlap": 0.7724137931034483,
      "is_correct": true
    }
  },
  "1704.08615v2_q1": {
    "paper_id": "1704.08615v2",
    "question_index": 1,
    "question": "Which saliency map method achieved the highest score for the sAUC metric, and how does its performance compare to other methods based on this metric? ",
    "ground_truth": "The saliency map method with the highest sAUC score is **SIM**. Its sAUC score appears to be significantly higher than all other methods listed in the table.",
    "predicted_answer": "DeepGaze II achieved the highest score for the sAUC metric with a value of 0.778316, outperforming all other methods in this metric.",
    "reference": "1704.08615v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4913494809688581,
      "phrase_overlap": 0.4913494809688581,
      "is_correct": false
    }
  },
  "1704.08615v2_q2": {
    "paper_id": "1704.08615v2",
    "question_index": 2,
    "question": "What is the relationship between the ground truth fixation density and the saliency maps?",
    "ground_truth": "The ground truth fixation density predicts different saliency maps depending on the intended metric.",
    "predicted_answer": "sAUC(x, y | I)",
    "reference": "1704.08615v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.12280701754385964,
      "phrase_overlap": 0.12280701754385964,
      "is_correct": false
    }
  },
  "1704.08615v2_q3": {
    "paper_id": "1704.08615v2",
    "question_index": 3,
    "question": "What is the relationship between the number of fixations and the CC score?",
    "ground_truth": "The CC score increases as the number of fixations increases.",
    "predicted_answer": "Positive correlation.",
    "reference": "1704.08615v2-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.345679012345679,
      "phrase_overlap": 0.345679012345679,
      "is_correct": false
    }
  },
  "1704.08615v2_q4": {
    "paper_id": "1704.08615v2",
    "question_index": 4,
    "question": "What is the relationship between the fixation density map and the ground truth fixations?",
    "ground_truth": "The fixation density map predicts the probability of a person fixating on a particular location in the image. The ground truth fixations are the actual locations where people fixated on the image.",
    "predicted_answer": "The fixation density map visually and quantitatively correlates with the ground truth fixations, as areas of higher density align with clusters of fixations.",
    "reference": "1704.08615v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5495750708215298,
      "phrase_overlap": 0.5495750708215298,
      "is_correct": false
    }
  },
  "1811.02553v4_q0": {
    "paper_id": "1811.02553v4",
    "question_index": 0,
    "question": "How does the number of state-action pairs affect the reward landscape for the surrogate and true reward functions?",
    "ground_truth": "As the number of state-action pairs increases, the reward landscape for both the surrogate and true reward functions becomes smoother and more accurate.",
    "predicted_answer": "More state-action pairs (10^6) result in a closer match between the surrogate and true reward landscapes.",
    "reference": "1811.02553v4-Figure17-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5447470817120622,
      "phrase_overlap": 0.5447470817120622,
      "is_correct": false
    }
  },
  "1811.02553v4_q1": {
    "paper_id": "1811.02553v4",
    "question_index": 1,
    "question": "How does the number of state-action pairs affect the optimization landscape for the PPO algorithm?",
    "ground_truth": "As the number of state-action pairs increases, the optimization landscape becomes more complex and has more local optima. This makes it more difficult for the PPO algorithm to find the global optimum.",
    "predicted_answer": "More state-action pairs (10^6) result in a more complex and noisy optimization landscape compared to fewer state-action pairs (2,000).",
    "reference": "1811.02553v4-Figure13-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1497005988023952,
      "phrase_overlap": 0.1497005988023952,
      "is_correct": false
    }
  },
  "1811.02553v4_q2": {
    "paper_id": "1811.02553v4",
    "question_index": 2,
    "question": "How do TRPO and PPO compare in terms of convergence to the true gradient?",
    "ground_truth": "TRPO generally converges faster to the true gradient than PPO.",
    "predicted_answer": "TRPO converges to the true gradient faster than PPO.",
    "reference": "1811.02553v4-Figure10-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7894736842105263,
      "phrase_overlap": 0.7894736842105263,
      "is_correct": true
    }
  },
  "1811.02553v4_q3": {
    "paper_id": "1811.02553v4",
    "question_index": 3,
    "question": "How does the landscape concentration of the humanoid-v2 PPO policy change with respect to the number of state-action pairs?",
    "ground_truth": "The landscape concentration increases with the number of state-action pairs.",
    "predicted_answer": "The landscape concentration increases with the number of state-action pairs.",
    "reference": "1811.02553v4-Figure20-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1811.02553v4_q4": {
    "paper_id": "1811.02553v4",
    "question_index": 4,
    "question": "How does the quality of gradient estimation change as the number of state-action pairs used in estimation increases?",
    "ground_truth": "The quality of gradient estimation increases as the number of state-action pairs used in estimation increases.",
    "predicted_answer": "The quality of gradient estimation improves as the number of state-action pairs increases.",
    "reference": "1811.02553v4-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.86,
      "phrase_overlap": 0.86,
      "is_correct": true
    }
  },
  "1804.04786v3_q0": {
    "paper_id": "1804.04786v3",
    "question_index": 0,
    "question": "What is the purpose of the audio encoder in the proposed conditional recurrent adversarial video generation network structure?",
    "ground_truth": "The audio encoder extracts audio features from the MFCC features of each audio segment.",
    "predicted_answer": "To encode audio features for conditional video generation.",
    "reference": "1804.04786v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5379310344827586,
      "phrase_overlap": 0.5379310344827586,
      "is_correct": false
    }
  },
  "1804.04786v3_q1": {
    "paper_id": "1804.04786v3",
    "question_index": 1,
    "question": "Which loss function combination is most important for generating realistic mouth movements?",
    "ground_truth": "The combination of Lrec, LI, and LV is most important for generating realistic mouth movements.",
    "predicted_answer": "L_r,l,v,l",
    "reference": "1804.04786v3-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1346153846153846,
      "phrase_overlap": 0.1346153846153846,
      "is_correct": false
    }
  },
  "1804.04786v3_q2": {
    "paper_id": "1804.04786v3",
    "question_index": 2,
    "question": "What is the difference between the sequential and recurrent generation schemes?",
    "ground_truth": "The sequential generation scheme generates each frame of the video independently, while the recurrent generation scheme uses the previous frame to generate the next frame. This can be seen in the optical flow images, which show the motion of pixels between frames. The recurrent scheme has a smoother flow of motion, while the sequential scheme has more abrupt changes.",
    "predicted_answer": "The difference is that sequential generation produces frames independently, while recurrent generation incorporates temporal dependencies from previous frames.",
    "reference": "1804.04786v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.16287878787878787,
      "phrase_overlap": 0.16287878787878787,
      "is_correct": false
    }
  },
  "1705.02798v6_q0": {
    "paper_id": "1705.02798v6",
    "question_index": 0,
    "question": "Which component of the model seems to have the biggest impact on the F1 score on SQuAD dataset, and how much does removing it affect the score?",
    "ground_truth": "The DCRL training method appears to have the biggest impact on the F1 score. Removing it leads to a drop of 0.9 points in F1, which is the largest decrease observed for any single component in the ablation study.",
    "predicted_answer": "C",
    "reference": "1705.02798v6-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1705.02798v6_q1": {
    "paper_id": "1705.02798v6",
    "question_index": 1,
    "question": "Which model performs the best on the AddOneSent dataset in terms of F1 score?",
    "ground_truth": "R.M.-Reader.",
    "predicted_answer": "R.M-Reader performs the best on the AddOneSent dataset in terms of F1 score with a value of 67.0.",
    "reference": "1705.02798v6-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2018348623853211,
      "phrase_overlap": 0.2018348623853211,
      "is_correct": false
    }
  },
  "1705.02798v6_q2": {
    "paper_id": "1705.02798v6",
    "question_index": 2,
    "question": "How does the performance of the single R.M-Reader model compare to the best single models of other approaches on the SQuAD test set?",
    "ground_truth": "The single R.M-Reader model achieves an EM score of 79.5% and an F1 score of 86.6% on the SQuAD test set. This performance is better than all other single models listed in the table, except for SLQA and Hybrid AoA Reader, which achieve slightly higher F1 scores of 87.0% and 87.3%, respectively.",
    "predicted_answer": "C",
    "reference": "1705.02798v6-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1705.02798v6_q3": {
    "paper_id": "1705.02798v6",
    "question_index": 3,
    "question": "What are the two types of attention mechanisms used in the Reinforced Mnemonic Reader architecture?",
    "ground_truth": "The two types of attention mechanisms are reattention and self-attention.",
    "predicted_answer": "Reattention, Self Reattention",
    "reference": "1705.02798v6-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.49019607843137253,
      "phrase_overlap": 0.49019607843137253,
      "is_correct": false
    }
  },
  "1705.02798v6_q4": {
    "paper_id": "1705.02798v6",
    "question_index": 4,
    "question": "What is the purpose of the fusion modules in the interactive alignment and self-alignment modules?",
    "ground_truth": "The fusion modules are used to combine the outputs of the interactive alignment and self-alignment modules.",
    "predicted_answer": "The purpose of the fusion modules in the interactive alignment and self-alignment modules is to combine aligned features from different inputs (u and v in Interactive Alignment, h in Self Alignment) to enhance feature representation.",
    "reference": "1705.02798v6-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4411764705882353,
      "phrase_overlap": 0.4411764705882353,
      "is_correct": false
    }
  },
  "1705.02798v6_q5": {
    "paper_id": "1705.02798v6",
    "question_index": 5,
    "question": "How does reattention affect the redundancy and deficiency of attention distributions? Can you explain the observed differences in the impact of reattention on different blocks?",
    "ground_truth": "This paper shows that reattention helps alleviate both redundancy and deficiency in attention distributions.\n\nRedundancy: Reattention increases the KL divergence between adjacent attention blocks, indicating that the attention distributions across blocks become more distinct and less redundant.\nDeficiency: Reattention reduces the KL divergence between the normalized attention distribution ($E^t$) and the ideal uniform distribution (${E^t}^*$), suggesting that the attention becomes more balanced and closer to the desired distribution.\nHowever, the improvement in redundancy is more pronounced between the first two blocks ($E^1$ to $E^2$) than the last two blocks ($B^2$ to $B^3$). This suggests that the first reattention is more effective in capturing word pair similarities using the original word representations. In contrast, the later reattention might be negatively impacted by the highly non-linear word representations generated in the previous layers.",
    "predicted_answer": "Reattention increases redundancy and decreases deficiency in attention distributions. The impact varies across blocks, with a more significant increase in redundancy observed in block transitions (e.g., B¹ to B²) compared to element transitions (e.g., E¹ to E²).",
    "reference": "1705.02798v6-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.16299918500407498,
      "phrase_overlap": 0.16299918500407498,
      "is_correct": false
    }
  },
  "1804.05936v2_q0": {
    "paper_id": "1804.05936v2",
    "question_index": 0,
    "question": "Which relevance label category of documents received the most significant rank promotion according to the NegPair reduction metric?",
    "ground_truth": "The perfect results received the largest promotions in rank.",
    "predicted_answer": "perfect",
    "reference": "1804.05936v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.208955223880597,
      "phrase_overlap": 0.208955223880597,
      "is_correct": true
    }
  },
  "1804.05936v2_q1": {
    "paper_id": "1804.05936v2",
    "question_index": 1,
    "question": "What is the role of the GRU in the Deep Listwise Context Model (DLCM)?",
    "ground_truth": "The GRU is used to process the ranked list of documents provided by a global ranking function.",
    "predicted_answer": "To model the contextual dependencies among documents.",
    "reference": "1804.05936v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.29931972789115646,
      "phrase_overlap": 0.29931972789115646,
      "is_correct": false
    }
  },
  "1804.05936v2_q2": {
    "paper_id": "1804.05936v2",
    "question_index": 2,
    "question": "How does the NegPair reduction vary with the number of perfect results in a query?",
    "ground_truth": "The NegPair reduction generally increases as the number of perfect results in a query increases.",
    "predicted_answer": "NegPair reduction increases initially and then decreases as the number of perfect results in a query increases.",
    "reference": "1804.05936v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.8502415458937198,
      "phrase_overlap": 0.8502415458937198,
      "is_correct": true
    }
  },
  "1804.05936v2_q3": {
    "paper_id": "1804.05936v2",
    "question_index": 3,
    "question": "Which dataset would be most suitable for training a learning-to-rank model with limited computational resources, and why?",
    "ground_truth": "Microsoft 10k would be the most suitable dataset for training with limited computational resources.",
    "predicted_answer": "Microsoft 10K",
    "reference": "1804.05936v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.23214285714285715,
      "phrase_overlap": 0.23214285714285715,
      "is_correct": true
    }
  },
  "1804.05936v2_q4": {
    "paper_id": "1804.05936v2",
    "question_index": 4,
    "question": "Which of the following methods has the best performance?",
    "ground_truth": "LambdaMART",
    "predicted_answer": "A",
    "reference": "1804.05936v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.18181818181818182,
      "phrase_overlap": 0.18181818181818182,
      "is_correct": true
    }
  },
  "1804.05936v2_q5": {
    "paper_id": "1804.05936v2",
    "question_index": 5,
    "question": "Which combination of initial list, model, and loss function achieved the best overall performance on the Yahoo! set 1, as measured by nDCG@10 and ERR@10?",
    "ground_truth": "LambdaMART initial list, DLCM model, and AttRank loss function achieved the best overall performance on the Yahoo! set 1, with an nDCG@10 of 0.743 and an ERR@10 of 0.453.",
    "predicted_answer": "The combination of LambdaMART initial list, DLCM model, and SoftRank loss function achieved the best overall performance on the Yahoo! set 1, with nDCG@10 = 0.743 and ERR@10 = 0.453.",
    "reference": "1804.05936v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.8977272727272727,
      "phrase_overlap": 0.8977272727272727,
      "is_correct": true
    }
  },
  "1804.07849v4_q0": {
    "paper_id": "1804.07849v4",
    "question_index": 0,
    "question": "Which method achieved the highest accuracy on the 45-tag Penn WSJ dataset, and how does its performance compare to the other methods?",
    "ground_truth": "The Variational $\\wh{J}^{\\mathrm{var}}$ method achieved the highest accuracy of 78.1% on the 45-tag Penn WSJ dataset. This is significantly higher than all other methods listed in the table, with the next best performing method (Berg-Kirkpatrick et al., 2010) achieving an accuracy of 74.9%.",
    "predicted_answer": "Variational \\( \\hat{J}_{var} \\) achieved the highest accuracy of 78.1%, outperforming the second-best method (Berg-Kirkpatrick et al. (2010)) by 3.2%.",
    "reference": "1804.07849v4-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4580498866213152,
      "phrase_overlap": 0.4580498866213152,
      "is_correct": true
    }
  },
  "1804.07849v4_q1": {
    "paper_id": "1804.07849v4",
    "question_index": 1,
    "question": "According to the ablation experiments, which factor contributes the most to the best model's performance compared to the baseline model?",
    "ground_truth": "Morphological modeling with LSTMs contributes the most to the best model's performance compared to the baseline model.",
    "predicted_answer": "The factor contributing the most to the best model's performance compared to the baseline ('No character encoding') is the inclusion of character encoding, which improves accuracy from 65.6 to 80.1.",
    "reference": "1804.07849v4-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5443037974683544,
      "phrase_overlap": 0.5443037974683544,
      "is_correct": false
    }
  },
  "1804.07849v4_q2": {
    "paper_id": "1804.07849v4",
    "question_index": 2,
    "question": "Which method achieved the highest average V-measure (VM) across all languages, and how much higher was its average compared to the Baum-Welch method?",
    "ground_truth": "The Variational $\\wh{J}^{\\mathrm{var}}$ method achieved the highest average VM score (50.4). Its average score is 39.6 points higher than the Baum-Welch method, which achieved an average VM score of 10.8.",
    "predicted_answer": "The method that achieved the highest average V-measure (VM) across all languages is Variational J^var (7), with an average VM of 50.4. Its average VM is 39.6 higher compared to the Baum-Welch method, which has an average VM of 10.8.",
    "reference": "1804.07849v4-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2798165137614679,
      "phrase_overlap": 0.2798165137614679,
      "is_correct": true
    }
  },
  "1804.07849v4_q3": {
    "paper_id": "1804.07849v4",
    "question_index": 3,
    "question": "Which method achieved the highest accuracy on the Italian language data set?",
    "ground_truth": "Variational J^var (7)",
    "predicted_answer": "Variational Ĵvar achieved the highest accuracy on the Italian language dataset with 67.4%.",
    "reference": "1804.07849v4-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.32142857142857145,
      "phrase_overlap": 0.32142857142857145,
      "is_correct": false
    }
  },
  "1804.07849v4_q4": {
    "paper_id": "1804.07849v4",
    "question_index": 4,
    "question": "What is the role of the BiLSTM in the architecture?",
    "ground_truth": "The BiLSTM takes as input the character-level representations of the words and outputs a word-level representation for each word.",
    "predicted_answer": "The role of the BiLSTM in the architecture is to process character embeddings and generate contextualized representations.",
    "reference": "1804.07849v4-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3745019920318725,
      "phrase_overlap": 0.3745019920318725,
      "is_correct": false
    }
  },
  "1811.07073v3_q0": {
    "paper_id": "1811.07073v3",
    "question_index": 0,
    "question": "How does the performance of the model with convolutional self-correction compare to the model with no self-correction as the number of images in set $\\mathcal{F}$ increases?",
    "ground_truth": "The model with convolutional self-correction consistently outperforms the model with no self-correction as the number of images in set $\\mathcal{F}$ increases.",
    "predicted_answer": "As the number of images in set $\\mathcal{F}$ increases, the performance of the model with convolutional self-correction surpasses the model with no self-correction.",
    "reference": "1811.07073v3-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.34055727554179566,
      "phrase_overlap": 0.34055727554179566,
      "is_correct": false
    }
  },
  "1811.07073v3_q1": {
    "paper_id": "1811.07073v3",
    "question_index": 1,
    "question": "Which method performed the best on the PASCAL VOC 2012 test set and how does it compare to the baseline model without self-correction?",
    "ground_truth": "The Conv. Self-Corr. method achieved the highest performance on the PASCAL VOC 2012 test set with a score of 82.72. This is approximately 1.11 points higher than the baseline model (\"No Self-Corr.\") which achieved a score of 81.61.",
    "predicted_answer": "Conv. Self-Corr. performed the best with a test score of 82.72, which is 1.11 higher than the baseline model without self-correction.",
    "reference": "1811.07073v3-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1978021978021978,
      "phrase_overlap": 0.1978021978021978,
      "is_correct": true
    }
  },
  "1811.07073v3_q2": {
    "paper_id": "1811.07073v3",
    "question_index": 2,
    "question": "What is the input to the convolutional self-correction model?",
    "ground_truth": "The input to the convolutional self-correction model is the logits generated by the primary and ancillary models.",
    "predicted_answer": "The input to the convolutional self-correction model is Primary Logits and Ancillary Logits.",
    "reference": "1811.07073v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7902439024390244,
      "phrase_overlap": 0.7902439024390244,
      "is_correct": true
    }
  },
  "1811.07073v3_q3": {
    "paper_id": "1811.07073v3",
    "question_index": 3,
    "question": "How does the performance of the \"Conv. Self-Correction\" method compare to other methods when using 30% of the training examples as $\\F$ and the remaining as $\\W$ on the Cityscapes validation set?",
    "ground_truth": "The \"Conv. Self-Correction\" method achieves the highest mIOU score of 79.46 compared to other methods listed in the table under the same data split condition.",
    "predicted_answer": "The Conv. Self-Correction method achieves the highest mIOU (79.46) compared to other methods when using 30% of the training examples as F and the remaining as W on the Cityscapes validation set.",
    "reference": "1811.07073v3-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7045454545454546,
      "phrase_overlap": 0.7045454545454546,
      "is_correct": true
    }
  },
  "1811.07073v3_q4": {
    "paper_id": "1811.07073v3",
    "question_index": 4,
    "question": "How does the bounding box encoder network influence the segmentation process?",
    "ground_truth": "The bounding box encoder network embeds bounding box information at different scales and outputs attention maps that are used to fuse with feature maps from the encoder before being passed to the decoder.",
    "predicted_answer": "It refines segmentation by incorporating bounding box information at multiple scales.",
    "reference": "1811.07073v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.08304498269896193,
      "phrase_overlap": 0.08304498269896193,
      "is_correct": false
    }
  },
  "1811.07073v3_q5": {
    "paper_id": "1811.07073v3",
    "question_index": 5,
    "question": "What is the role of the self-correction module in the segmentation framework?",
    "ground_truth": "The self-correction module refines the segmentations generated by the ancillary and current primary model for the weak set.",
    "predicted_answer": "The self-correction module refines the soft labels generated by the ancillary segmentation model to improve the segmentation framework's accuracy.",
    "reference": "1811.07073v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6840148698884758,
      "phrase_overlap": 0.6840148698884758,
      "is_correct": true
    }
  },
  "1811.07073v3_q6": {
    "paper_id": "1811.07073v3",
    "question_index": 6,
    "question": "What is the purpose of the ancillary heatmap shown in this paper?",
    "ground_truth": "The ancillary heatmap is used to correct the labels for missing or oversegmented objects in the images.",
    "predicted_answer": "The purpose of the ancillary heatmap is to highlight regions of interest or uncertainty within the segmentation masks, aiding in refining segmentation accuracy.",
    "reference": "1811.07073v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.44866920152091255,
      "phrase_overlap": 0.44866920152091255,
      "is_correct": false
    }
  },
  "1705.02946v3_q0": {
    "paper_id": "1705.02946v3",
    "question_index": 0,
    "question": "What is the initial value of V2 in the interval [0.35, 0.67]?",
    "ground_truth": "0.35",
    "predicted_answer": "0.35",
    "reference": "1705.02946v3-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1705.02946v3_q1": {
    "paper_id": "1705.02946v3",
    "question_index": 1,
    "question": "What is the distance from equitability for the allocation that can be obtained by cutting at $x$ with the player order $(1,2)$?",
    "ground_truth": "The distance from equitability is $b-a$.",
    "predicted_answer": "The distance from equitability for the allocation obtained by cutting at $x$ with the player order $(1,2)$ is |a + b|.",
    "reference": "1705.02946v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.45569620253164556,
      "phrase_overlap": 0.45569620253164556,
      "is_correct": false
    }
  },
  "1705.02946v3_q2": {
    "paper_id": "1705.02946v3",
    "question_index": 2,
    "question": "What is the upper bound on the query complexity for finding an ε-perfect allocation with minimum cuts for 3 or more players?",
    "ground_truth": "O(n^3 / ε)",
    "predicted_answer": "O(n³/ε²)",
    "reference": "1705.02946v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6666666666666666,
      "phrase_overlap": 0.6666666666666666,
      "is_correct": true
    }
  },
  "1705.07164v8_q0": {
    "paper_id": "1705.07164v8",
    "question_index": 0,
    "question": "What can you infer from the training curves for the ACGAN?",
    "ground_truth": "The training curves for the ACGAN show that the generator and discriminator losses both decrease over time. This indicates that the ACGAN is able to learn to generate realistic images.",
    "predicted_answer": "The ACGAN training curves indicate that the generator is improving its ability to produce realistic samples, as evidenced by the increasing generator loss. The discriminator's loss remains stable, suggesting balanced training dynamics.",
    "reference": "1705.07164v8-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.38663484486873506,
      "phrase_overlap": 0.38663484486873506,
      "is_correct": false
    }
  },
  "1705.07164v8_q1": {
    "paper_id": "1705.07164v8",
    "question_index": 1,
    "question": "Which model performs better based on the training curves?",
    "ground_truth": "It is difficult to say definitively which model performs better based on the training curves alone. However, it appears that the WGAN(g) model may be performing better than the other models, as its generator and discriminator losses are both lower than the other models.",
    "predicted_answer": "The image does not provide a direct comparison with another model, so performance cannot be determined solely from this data.",
    "reference": "1705.07164v8-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.030379746835443037,
      "phrase_overlap": 0.030379746835443037,
      "is_correct": false
    }
  },
  "1705.07164v8_q2": {
    "paper_id": "1705.07164v8",
    "question_index": 2,
    "question": "Which method achieved the highest Inception Score (IS) at the end of training for both CIFAR10 and ImageNet datasets? Did this method also achieve the highest initial IS score?",
    "ground_truth": "For CIFAR10, WGAN achieved the highest IS at the end of training (2.42). However, RWGAN had a higher initial IS score (1.86) compared to WGAN's 1.63. \n\nFor ImageNet, WGAN again achieved the highest final IS (2.80), while RWGAN had a slightly higher initial IS (2.04 vs. 2.00). ",
    "predicted_answer": "B",
    "reference": "1705.07164v8-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1705.07164v8_q3": {
    "paper_id": "1705.07164v8",
    "question_index": 3,
    "question": "What is the difference between the Euclidean and Mahalanobis Bregman cost functions?",
    "ground_truth": "The Euclidean Bregman cost function is simply the squared difference between two points, while the Mahalanobis Bregman cost function takes into account the covariance of the data.",
    "predicted_answer": "Matrix A is introduced in the Mahalanobis cost function.",
    "reference": "1705.07164v8-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3659574468085106,
      "phrase_overlap": 0.3659574468085106,
      "is_correct": false
    }
  },
  "1804.05995v2_q0": {
    "paper_id": "1804.05995v2",
    "question_index": 0,
    "question": "What are the top 5 section recommendations for the Wikipedia article on Lausanne according to the category-section counts method?",
    "ground_truth": "The top 5 section recommendations for the Wikipedia article on Lausanne according to the category-section counts method are HISTORY, DEMOGRAPHICS, ECONOMY, EDUCATION, and POLITICS.",
    "predicted_answer": "HISTORY, DEMOGRAPHICS, ECONOMY, EDUCATION, POLITICS",
    "reference": "1804.05995v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.44155844155844154,
      "phrase_overlap": 0.44155844155844154,
      "is_correct": false
    }
  },
  "1804.05995v2_q1": {
    "paper_id": "1804.05995v2",
    "question_index": 1,
    "question": "What is the trend in precision and recall as the number of recommended sections k increases?",
    "ground_truth": "Precision generally decreases and recall generally increases as k increases.",
    "predicted_answer": "Precision decreases, recall increases.",
    "reference": "1804.05995v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6491228070175439,
      "phrase_overlap": 0.6491228070175439,
      "is_correct": true
    }
  },
  "1804.05995v2_q2": {
    "paper_id": "1804.05995v2",
    "question_index": 2,
    "question": "What is the percentage of categories that can generate at least 10 recommendations using the section-count-based method?",
    "ground_truth": "Around 68%.",
    "predicted_answer": "55%",
    "reference": "1804.05995v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.14285714285714285,
      "phrase_overlap": 0.14285714285714285,
      "is_correct": false
    }
  },
  "1811.06635v1_q0": {
    "paper_id": "1811.06635v1",
    "question_index": 0,
    "question": "What is the sample complexity lower bound for recovering a tree-structured sparse signal using standard compressed sensing?",
    "ground_truth": "Ω(s)",
    "predicted_answer": "Ω̂(s)",
    "reference": "1811.06635v1-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.8888888888888888,
      "phrase_overlap": 0.8888888888888888,
      "is_correct": true
    }
  },
  "1804.07707v2_q0": {
    "paper_id": "1804.07707v2",
    "question_index": 0,
    "question": "How does the performance of the proposed model compare to other models when trained on the LDC2017T10 dataset, and what does this suggest about the effectiveness of incorporating syntax into the model?",
    "ground_truth": "When trained on the LDC2017T10 dataset, the proposed model achieves the highest BLEU scores on both Dev and Test sets compared to other models listed in the table. This suggests that incorporating syntax into the model significantly improves its performance in generating text from AMR representations.",
    "predicted_answer": "The proposed model achieves higher BLEU scores (Dev: 26.1, Test: 26.8) compared to other models trained on LDC2017T10, suggesting that incorporating syntax improves model effectiveness.",
    "reference": "1804.07707v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.22176591375770022,
      "phrase_overlap": 0.22176591375770022,
      "is_correct": true
    }
  },
  "1804.07707v2_q1": {
    "paper_id": "1804.07707v2",
    "question_index": 1,
    "question": "How does explicitly modeling meaning-preserving invariances impact the generation of paraphrases?",
    "ground_truth": "Explicitly modeling meaning-preserving invariances leads to the generation of better paraphrases.",
    "predicted_answer": "Explicitly modeling meaning-preserving invariances improves the generation of paraphrases, as the Syntax-aware model achieves a higher number of good realisations (1.52) compared to the Baseline s2s model (1.19).",
    "reference": "1804.07707v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5372168284789643,
      "phrase_overlap": 0.5372168284789643,
      "is_correct": false
    }
  },
  "1804.07707v2_q2": {
    "paper_id": "1804.07707v2",
    "question_index": 2,
    "question": "Which model performs the best at predicting the delexicalised constituency tree of an example, and how much better does it perform compared to the baseline model in terms of unlabelled F1 score?",
    "ground_truth": "The Text-to-parse model performs the best at predicting the delexicalised constituency tree, achieving an unlabelled F1 score of 87.5. This is significantly higher than the baseline Unconditional model, which achieves an unlabelled F1 score of 38.5. The Text-to-parse model therefore performs approximately 49 points better than the baseline.",
    "predicted_answer": "The Text-to-parse model performs the best at predicting the delexicalised constituency tree with an Unlabelled F1 score of 87.5, which is 49.0 points better than the baseline Unconditional model.",
    "reference": "1804.07707v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.49534450651769085,
      "phrase_overlap": 0.49534450651769085,
      "is_correct": true
    }
  },
  "1804.07931v2_q0": {
    "paper_id": "1804.07931v2",
    "question_index": 0,
    "question": "What are the two auxiliary tasks that are used in the ESMM architecture for CVR modeling?",
    "ground_truth": "The two auxiliary tasks are CTR and CTCVR.",
    "predicted_answer": "pCTCVR and pCTR",
    "reference": "1804.07931v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.38596491228070173,
      "phrase_overlap": 0.38596491228070173,
      "is_correct": false
    }
  },
  "1804.07931v2_q1": {
    "paper_id": "1804.07931v2",
    "question_index": 1,
    "question": "What is the relationship between clicks and impressions?",
    "ground_truth": "Clicks are a subset of impressions.",
    "predicted_answer": "Clicks are a subset of impressions, meaning clicks occur only after an impression is made.",
    "reference": "1804.07931v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.56,
      "phrase_overlap": 0.56,
      "is_correct": false
    }
  },
  "1804.07931v2_q2": {
    "paper_id": "1804.07931v2",
    "question_index": 2,
    "question": "How much more prevalent are click events compared to conversion events in the Product Dataset?",
    "ground_truth": "Click events are roughly 184 times more prevalent than conversion events in the Product Dataset.",
    "predicted_answer": "322.26M",
    "reference": "1804.07931v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.019417475728155338,
      "phrase_overlap": 0.019417475728155338,
      "is_correct": false
    }
  },
  "1804.07931v2_q3": {
    "paper_id": "1804.07931v2",
    "question_index": 3,
    "question": "Which model performs best overall on both the CVR and CTCVR tasks?",
    "ground_truth": "The ESMM model performs the best overall on both the CVR and CTCVR tasks.",
    "predicted_answer": "C",
    "reference": "1804.07931v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.02702702702702703,
      "phrase_overlap": 0.02702702702702703,
      "is_correct": true
    }
  },
  "1804.07931v2_q4": {
    "paper_id": "1804.07931v2",
    "question_index": 4,
    "question": "How does the performance of ESMM compare to other models on the CVR task and CTCVR task with different training set sizes?",
    "ground_truth": "ESMM-NS and ESMM outperform all other models consistently across different training set sizes on both the CVR and CTCVR tasks.",
    "predicted_answer": "ESMM outperforms all other models on both CVR and CTCVR tasks across all training set sizes, achieving the highest AUC values consistently.",
    "reference": "1804.07931v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.46037735849056605,
      "phrase_overlap": 0.46037735849056605,
      "is_correct": false
    }
  },
  "1705.08016v3_q0": {
    "paper_id": "1705.08016v3",
    "question_index": 0,
    "question": "How does the test accuracy of the different models vary with the hyperparameter λ?",
    "ground_truth": "The test accuracy of all models decreases as λ increases.",
    "predicted_answer": "Test accuracy decreases significantly when λ exceeds \\(10^3\\).",
    "reference": "1705.08016v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5378151260504201,
      "phrase_overlap": 0.5378151260504201,
      "is_correct": false
    }
  },
  "1705.08016v3_q1": {
    "paper_id": "1705.08016v3",
    "question_index": 1,
    "question": "Which method achieves the highest Top-1 accuracy on the CUB-200-2011 dataset?",
    "ground_truth": "PC-DenseNet-161",
    "predicted_answer": "PC-DenseNet-161 achieves the highest Top-1 accuracy of 86.87% on the CUB-200-2011 dataset.",
    "reference": "1705.08016v3-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2857142857142857,
      "phrase_overlap": 0.2857142857142857,
      "is_correct": true
    }
  },
  "1705.08016v3_q2": {
    "paper_id": "1705.08016v3",
    "question_index": 2,
    "question": "How does Pairwise Confusion (PC) affect the localization ability of a CNN?",
    "ground_truth": "PC improves the localization ability of a CNN.",
    "predicted_answer": "Improves localization ability.",
    "reference": "1705.08016v3-Figure3-1.png",
    "question_type": "multiple_choice",
    "evaluation": {
      "similarity_score": 0.7894736842105263,
      "phrase_overlap": 0.7894736842105263,
      "is_correct": true
    }
  },
  "1705.08016v3_q3": {
    "paper_id": "1705.08016v3",
    "question_index": 3,
    "question": "Which dataset has the highest number of samples per class?",
    "ground_truth": "SVHN",
    "predicted_answer": "D",
    "reference": "1705.08016v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1705.08016v3_q4": {
    "paper_id": "1705.08016v3",
    "question_index": 4,
    "question": "Which dataset benefited more from the Pairwise Confusion (PC) optimization method: ImageNet-Dogs or ImageNet-Random?",
    "ground_truth": "ImageNet-Dogs benefited more from the PC optimization method compared to ImageNet-Random.",
    "predicted_answer": "ImageNet-Dogs",
    "reference": "1705.08016v3-Table3-1.png",
    "question_type": "multiple_choice",
    "evaluation": {
      "similarity_score": 0.2549019607843137,
      "phrase_overlap": 0.2549019607843137,
      "is_correct": true
    }
  },
  "1705.08016v3_q5": {
    "paper_id": "1705.08016v3",
    "question_index": 5,
    "question": "What is the role of the shared weights in the Siamese-like architecture shown in the first figure?",
    "ground_truth": "The shared weights allow the two branches of the network to learn similar representations of the input images. This helps to improve the performance of the Euclidean Confusion loss, which measures the distance between the conditional probability distributions of the two branches.",
    "predicted_answer": "To ensure both input branches are processed identically.",
    "reference": "1705.08016v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.017857142857142856,
      "phrase_overlap": 0.017857142857142856,
      "is_correct": false
    }
  },
  "1811.08257v1_q0": {
    "paper_id": "1811.08257v1",
    "question_index": 0,
    "question": "How is convolution represented in the frequency domain?",
    "ground_truth": "In the frequency domain, convolution is represented by element-wise multiplication.",
    "predicted_answer": "Convolution in the frequency domain is represented as elementwise multiplication of the frequency representations of filters and input data, followed by summation and inverse FFT.",
    "reference": "1811.08257v1-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5114503816793893,
      "phrase_overlap": 0.5114503816793893,
      "is_correct": false
    }
  },
  "1811.08257v1_q1": {
    "paper_id": "1811.08257v1",
    "question_index": 1,
    "question": "How does the number of classes affect the setup and online time for the Softmax?",
    "ground_truth": "The setup and online time for the Softmax increases as the number of classes increases.",
    "predicted_answer": "The setup and online time for the Softmax increase as the number of classes increases.",
    "reference": "1811.08257v1-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9942196531791907,
      "phrase_overlap": 0.9942196531791907,
      "is_correct": true
    }
  },
  "1811.08257v1_q2": {
    "paper_id": "1811.08257v1",
    "question_index": 2,
    "question": "Which framework has the lowest total communication cost for MNIST?",
    "ground_truth": "FALCON",
    "predicted_answer": "GAZELLE",
    "reference": "1811.08257v1-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3076923076923077,
      "phrase_overlap": 0.3076923076923077,
      "is_correct": false
    }
  },
  "1811.08257v1_q3": {
    "paper_id": "1811.08257v1",
    "question_index": 3,
    "question": "What is the function of the DataPreprocessing function?",
    "ground_truth": "The DataPreprocessing function performs Yao Sharing, which is a cryptographic technique for securely sharing data between multiple parties.",
    "predicted_answer": "The function DataPreprocessing performs data preprocessing by combining inputs (x_C and x_S) using ADDGate, comparing the result with p using GTGate, calculating the difference using SUBGate, and selecting the final result using MUXGate.",
    "reference": "1811.08257v1-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.32978723404255317,
      "phrase_overlap": 0.32978723404255317,
      "is_correct": false
    }
  },
  "1811.08257v1_q4": {
    "paper_id": "1811.08257v1",
    "question_index": 4,
    "question": "Which framework is faster for setting up and running the FC layer?",
    "ground_truth": "FALCON is faster for both setting up and running the FC layer.",
    "predicted_answer": "FALCON is faster for setting up and running the FC layer.",
    "reference": "1811.08257v1-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.957983193277311,
      "phrase_overlap": 0.957983193277311,
      "is_correct": true
    }
  },
  "1811.08257v1_q5": {
    "paper_id": "1811.08257v1",
    "question_index": 5,
    "question": "What is the purpose of the `SubsetGate` function in the MaxPooling function?",
    "ground_truth": "The `SubsetGate` function is used to split the input `x` into `k` pieces.",
    "predicted_answer": "Split `x` into `k` pieces.",
    "reference": "1811.08257v1-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5252525252525253,
      "phrase_overlap": 0.5252525252525253,
      "is_correct": false
    }
  },
  "1811.08257v1_q6": {
    "paper_id": "1811.08257v1",
    "question_index": 6,
    "question": "What is the purpose of the activation layer in a convolutional neural network?",
    "ground_truth": "The activation layer applies a non-linear function to the output of the convolution layer. This allows the network to learn more complex features from the data.",
    "predicted_answer": "To introduce non-linearity.",
    "reference": "1811.08257v1-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.21390374331550802,
      "phrase_overlap": 0.21390374331550802,
      "is_correct": false
    }
  },
  "1811.08257v1_q7": {
    "paper_id": "1811.08257v1",
    "question_index": 7,
    "question": "What is the role of the filter in the convolution operation?",
    "ground_truth": "The filter is used to extract features from the input image. It is a small matrix that is applied to each pixel in the image, and the result is a new pixel value.",
    "predicted_answer": "Extracts features.",
    "reference": "1811.08257v1-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.18888888888888888,
      "phrase_overlap": 0.18888888888888888,
      "is_correct": false
    }
  },
  "1811.08257v1_q8": {
    "paper_id": "1811.08257v1",
    "question_index": 8,
    "question": "Which operation has the lowest online time?",
    "ground_truth": "ReLU",
    "predicted_answer": "C",
    "reference": "1811.08257v1-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1805.00912v4_q0": {
    "paper_id": "1805.00912v4",
    "question_index": 0,
    "question": "Which model has the lowest memory consumption and time cost on synthetic data?",
    "ground_truth": "MTSA",
    "predicted_answer": "MTSA",
    "reference": "1805.00912v4-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1805.00912v4_q1": {
    "paper_id": "1805.00912v4",
    "question_index": 1,
    "question": "Which model has the highest test accuracy on the SNLI dataset, and how does its training time per epoch compare to the MTSA model?",
    "ground_truth": "The model with the highest test accuracy is MTSA, with an accuracy of 86.3%. Its training time per epoch is 180s, which is faster than the training time of several other models with lower accuracy, such as Bi-LSTM (854s), Bi-GRU (850s), and DiSA (390s).",
    "predicted_answer": "C",
    "reference": "1805.00912v4-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1805.00912v4_q2": {
    "paper_id": "1805.00912v4",
    "question_index": 2,
    "question": "Which model performed best on the SNLI test set?",
    "ground_truth": "The Transfer + MTSA model performed best on the SNLI test set with an accuracy of 86.9%.",
    "predicted_answer": "D",
    "reference": "1805.00912v4-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.02247191011235955,
      "phrase_overlap": 0.02247191011235955,
      "is_correct": true
    }
  },
  "1805.00912v4_q3": {
    "paper_id": "1805.00912v4",
    "question_index": 3,
    "question": "Based on the table, how does MTSA compare to the Bi-LSTM and Multi-CNN baselines in terms of performance and training time?",
    "ground_truth": "MTSA outperforms both Bi-LSTM and Multi-CNN baselines across all evaluation metrics (P, R, F1, and Comp.) on all three test sets (Development, WSJ Test, and Brown Test). While MTSA achieves the highest scores, its training time is comparable to Multi-head and Multi-CNN, and significantly faster than Bi-LSTM.",
    "predicted_answer": "MTSA outperforms Multi-CNN in all F1 scores and matches or slightly exceeds Bi-LSTM in F1 scores across datasets, while having a significantly shorter training time than Bi-LSTM and similar training time to Multi-CNN.",
    "reference": "1805.00912v4-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.11026615969581749,
      "phrase_overlap": 0.11026615969581749,
      "is_correct": true
    }
  },
  "1805.00912v4_q4": {
    "paper_id": "1805.00912v4",
    "question_index": 4,
    "question": "What is the purpose of the positional mask in the TSA mechanism?",
    "ground_truth": "The positional mask is used to provide information about the relative position of tokens in the input sequence. This information is used to compute the attention weights, which determine how much each token attends to each other token.",
    "predicted_answer": "A",
    "reference": "1805.00912v4-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1705.07384v2_q0": {
    "paper_id": "1705.07384v2",
    "question_index": 0,
    "question": "Which policy learning method achieved the lowest regret in Ex. 2?",
    "ground_truth": "The DR-SVM method achieved the lowest regret in Ex. 2, with a regret of 0.18.",
    "predicted_answer": "Balanced policy learner",
    "reference": "1705.07384v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.24,
      "phrase_overlap": 0.24,
      "is_correct": false
    }
  },
  "1705.09296v2_q0": {
    "paper_id": "1705.09296v2",
    "question_index": 0,
    "question": "Which year is most associated with the terms \"sept\", \"hijackers\", and \"attacks\"?",
    "ground_truth": "2001",
    "predicted_answer": "2001",
    "reference": "1705.09296v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1705.09296v2_q1": {
    "paper_id": "1705.09296v2",
    "question_index": 1,
    "question": "Which node in the generative model represents the latent variable?",
    "ground_truth": "The node labeled η represents the latent variable.",
    "predicted_answer": "θ",
    "reference": "1705.09296v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1705.09296v2_q2": {
    "paper_id": "1705.09296v2",
    "question_index": 2,
    "question": "Which model achieves the best NPMI scores (both internal and external) in the unsupervised setting, and what trade-off does this model exhibit compared to other models?",
    "ground_truth": "The Scholar + w.v. model achieves the best NPMI scores (both internal and external) in the unsupervised setting. However, this model also has the highest number of people parameters, indicating a trade-off between topic coherence and model complexity.",
    "predicted_answer": "SCHOLAR + W.V.; trade-off: higher perplexity.",
    "reference": "1705.09296v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.21621621621621623,
      "phrase_overlap": 0.21621621621621623,
      "is_correct": false
    }
  },
  "1705.09296v2_q3": {
    "paper_id": "1705.09296v2",
    "question_index": 3,
    "question": "Which topic has the highest internal coherence value?",
    "ground_truth": "The topic with the highest internal coherence value is \"turks armenian armenia turkish roads escape soviet muslim mountain soul\".",
    "predicted_answer": "turks armenian armenia turkish roads escape soviet muslim mountain soul",
    "reference": "1705.09296v2-Table6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.71,
      "phrase_overlap": 0.71,
      "is_correct": true
    }
  },
  "1705.09296v2_q4": {
    "paper_id": "1705.09296v2",
    "question_index": 4,
    "question": "Which model performed best on the IMDB dataset for classifying documents with categorical labels, and how much better did it perform compared to the SLDA model?",
    "ground_truth": "Both the SCHOLAR (covariates) and Logistic Regression models achieved the highest accuracy of 0.87 on the IMDB dataset. This represents a 0.23 improvement over the SLDA model, which achieved an accuracy of 0.64.",
    "predicted_answer": "SCHOLAR (covariates), 0.23",
    "reference": "1705.09296v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2109704641350211,
      "phrase_overlap": 0.2109704641350211,
      "is_correct": true
    }
  },
  "1705.09296v2_q5": {
    "paper_id": "1705.09296v2",
    "question_index": 5,
    "question": "Which topic is most likely to be associated with a pro-immigration stance?",
    "ground_truth": "\"english language city spanish community\"",
    "predicted_answer": "visas visa applications students citizenship",
    "reference": "1705.09296v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3058823529411765,
      "phrase_overlap": 0.3058823529411765,
      "is_correct": false
    }
  },
  "1705.09296v2_q6": {
    "paper_id": "1705.09296v2",
    "question_index": 6,
    "question": "How does the model capture different perspectives on immigration when considering tone as a covariate?",
    "ground_truth": "The model captures different perspectives on immigration by highlighting contrasting words associated with the same topic, depending on whether the tone is anti-immigration or pro-immigration.",
    "predicted_answer": "The model captures different perspectives on immigration by associating anti-immigration interactions with terms like \"criminal\" and \"illegals,\" and pro-immigration interactions with terms like \"detainees\" and \"newcomers.\"",
    "reference": "1705.09296v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5169082125603864,
      "phrase_overlap": 0.5169082125603864,
      "is_correct": false
    }
  },
  "1805.01216v3_q0": {
    "paper_id": "1805.01216v3",
    "question_index": 0,
    "question": "Which task-oriented dialog system performs the best when the percentage of unseen information in the KB is high?",
    "ground_truth": "BoSsNet",
    "predicted_answer": "BoSsNet performs the best when the percentage of unseen information in the KB is high.",
    "reference": "1805.01216v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.15053763440860216,
      "phrase_overlap": 0.15053763440860216,
      "is_correct": true
    }
  },
  "1805.01216v3_q1": {
    "paper_id": "1805.01216v3",
    "question_index": 1,
    "question": "Why did Seq2Seq and Mem2Seq models perform poorly when the percentage of unseen entities in the knowledge base (KB) increased?",
    "ground_truth": "Seq2Seq and Mem2Seq models performed poorly because they struggled to capture the semantic representations of unseen entities. This means they couldn't understand the meaning and relationships of new restaurants introduced in the KB. As a result, they were unable to accurately identify the correct restaurant and provide its address when faced with unseen entities.",
    "predicted_answer": "Seq2Seq and Mem2Seq models performed poorly because they struggled to generalize and retrieve correct entities when faced with a higher percentage of unseen entities in the knowledge base.",
    "reference": "1805.01216v3-Table4-1.png",
    "question_type": "multiple_choice",
    "evaluation": {
      "similarity_score": 0.3285198555956679,
      "phrase_overlap": 0.3285198555956679,
      "is_correct": true
    }
  },
  "1805.01216v3_q2": {
    "paper_id": "1805.01216v3",
    "question_index": 2,
    "question": "Which model achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset?",
    "ground_truth": "The BOSSNET model achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset.",
    "predicted_answer": "BoSSNet achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset.",
    "reference": "1805.01216v3-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9583333333333334,
      "phrase_overlap": 0.9583333333333334,
      "is_correct": true
    }
  },
  "1805.01216v3_q3": {
    "paper_id": "1805.01216v3",
    "question_index": 3,
    "question": "Can you explain why the BOSSNET with multi-hop encoder performs better on bAbI tasks 3 and 5 compared to the 1-hop encoder, and how this relates to the tasks themselves?",
    "ground_truth": "The multi-hop encoder performs better on bAbI tasks 3 and 5 because these tasks specifically require inferencing over multiple KB tuples. In other words, the model needs to \"hop\" between different pieces of information in the knowledge base to make the correct inferences and recommendations.\n\nTask 3 involves sorting restaurants by rating, and task 5 requires recommending a restaurant based on user preferences. Both tasks necessitate the model to consider various restaurant attributes and their relationships, which the multi-hop encoder facilitates by capturing longer-range dependencies within the knowledge base.",
    "predicted_answer": "Multi-hop encoder performs better because tasks 3 and 5 require reasoning over multiple pieces of information, which is better handled by multi-hop mechanisms.",
    "reference": "1805.01216v3-Table14-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.23938223938223938,
      "phrase_overlap": 0.23938223938223938,
      "is_correct": true
    }
  },
  "1805.01216v3_q4": {
    "paper_id": "1805.01216v3",
    "question_index": 4,
    "question": "How does the encoder understand the last user utterance?",
    "ground_truth": "The encoder understands the last user utterance by using the memory cell representations of the dialog history and KB tuples.",
    "predicted_answer": "Bag-of-Sequences (BoSs) Memory.",
    "reference": "1805.01216v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.21794871794871795,
      "phrase_overlap": 0.21794871794871795,
      "is_correct": false
    }
  },
  "1805.01216v3_q5": {
    "paper_id": "1805.01216v3",
    "question_index": 5,
    "question": "Which model performs best on tasks T3 and T3-OOV in terms of per-dialog accuracy, and how does its performance differ between the two test sets?",
    "ground_truth": "The proposed system model (BOSSNET) performs best on both tasks T3 and T3-OOV in terms of per-dialog accuracy. However, its performance is significantly higher on the T3-OOV test set (95.7%) compared to the non-OOV T3 test set (95.2%).",
    "predicted_answer": "BoSSNet; 95.2% on T3 and 95.7% on T3-OOV.",
    "reference": "1805.01216v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.08695652173913043,
      "phrase_overlap": 0.08695652173913043,
      "is_correct": true
    }
  },
  "1805.01216v3_q6": {
    "paper_id": "1805.01216v3",
    "question_index": 6,
    "question": "Why might the authors claim that although BOSSNET achieves a lower BLEU score than Mem2Seq on the SMD dataset, it still performs better in conveying necessary entity information?",
    "ground_truth": "While BOSSNET has a lower BLEU score than Mem2Seq on SMD, it achieves the highest Entity F1 score on that dataset. This suggests that BOSSNET is better at capturing and including the relevant entities in its responses, even though it may not have as much lexical overlap with the gold responses as Mem2Seq.",
    "predicted_answer": "Ent. F1: 35.9",
    "reference": "1805.01216v3-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.025078369905956112,
      "phrase_overlap": 0.025078369905956112,
      "is_correct": true
    }
  },
  "1805.01216v3_q7": {
    "paper_id": "1805.01216v3",
    "question_index": 7,
    "question": "Which model performs the best in terms of Entity F1 score when the percentage of unseen entities in the response is low?",
    "ground_truth": "BoSsNet",
    "predicted_answer": "A",
    "reference": "1805.01216v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1805.01216v3_q8": {
    "paper_id": "1805.01216v3",
    "question_index": 8,
    "question": " \n\nWhat is the difference between the original and pre-processed SMD Navigate data? ",
    "ground_truth": " \n\nThe pre-processed SMD Navigate data combines all the properties (such as distance, address) of a point of interest (POI) into a single subject with the object being \"poi\". The original data had separate entries for each property. ",
    "predicted_answer": "The difference is that in the pre-processed data, new subjects are created by combining distance, traffic_info, and poi_type, and these are mapped to their respective POIs, while the original structure is retained alongside the new mappings.",
    "reference": "1805.01216v3-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.059574468085106386,
      "phrase_overlap": 0.059574468085106386,
      "is_correct": false
    }
  },
  "1805.01216v3_q9": {
    "paper_id": "1805.01216v3",
    "question_index": 9,
    "question": "Which model performs best when the percentage of unseen entities in the response is low?",
    "ground_truth": "BoSsNet",
    "predicted_answer": "BoSsNet performs best when the percentage of unseen entities in the response is low.",
    "reference": "1805.01216v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.15384615384615385,
      "phrase_overlap": 0.15384615384615385,
      "is_correct": true
    }
  },
  "1805.01216v3_q10": {
    "paper_id": "1805.01216v3",
    "question_index": 10,
    "question": "Which task required the highest learning rate and how does this compare to the learning rate used for CamRest?",
    "ground_truth": "Task T1 and T2 required the highest learning rate of 0.001. This is twice the learning rate used for CamRest, which was trained with a learning rate of 0.0005.",
    "predicted_answer": "The tasks T1, T2, and T4 required the highest learning rate of 0.001, which is higher than the learning rate used for CamRest (0.0005).",
    "reference": "1805.01216v3-Table9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7414965986394558,
      "phrase_overlap": 0.7414965986394558,
      "is_correct": true
    }
  },
  "1805.01216v3_q11": {
    "paper_id": "1805.01216v3",
    "question_index": 11,
    "question": "What is the difference between the attention weights in the two-level attention model and the one-level attention model?",
    "ground_truth": " The two-level attention model has higher attention weights on the relevant information in the memory, while the one-level attention model has more uniform attention weights.",
    "predicted_answer": "The difference is that the Two-Level Attention model distributes attention weights across multiple memory cells, while the One-Level Attention model focuses on a single memory cell for each query.",
    "reference": "1805.01216v3-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5962059620596206,
      "phrase_overlap": 0.5962059620596206,
      "is_correct": false
    }
  },
  "1804.05938v2_q0": {
    "paper_id": "1804.05938v2",
    "question_index": 0,
    "question": "Which correction method resulted in the best performance in terms of nDCG@10 and ERR@10, and how does it compare to not using any correction method?",
    "ground_truth": "The DNN trained with DLA achieved the best performance in terms of both nDCG@10 (0.421) and ERR@10 (0.582). Compared to not using any correction method (NoCorrect), DLA shows a significant improvement in both metrics, with nDCG@10 being higher by 0.063 and ERR@10 being higher by 0.082.",
    "predicted_answer": "DLA",
    "reference": "1804.05938v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.020761245674740483,
      "phrase_overlap": 0.020761245674740483,
      "is_correct": true
    }
  },
  "1804.05938v2_q1": {
    "paper_id": "1804.05938v2",
    "question_index": 1,
    "question": "Which method performs better in terms of mean square error?",
    "ground_truth": "DLA performs better than RandList in terms of mean square error.",
    "predicted_answer": "DLA",
    "reference": "1804.05938v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.08955223880597014,
      "phrase_overlap": 0.08955223880597014,
      "is_correct": true
    }
  },
  "1804.05938v2_q2": {
    "paper_id": "1804.05938v2",
    "question_index": 2,
    "question": "Explain the difference between the features \"TF-IDF\" and \"BM25\".",
    "ground_truth": "Both TF-IDF and BM25 are features used to estimate the relevance of a document to a query. However, they differ in their underlying calculations.\n\nTF-IDF: This feature represents the average product of term frequency (TF) and inverse document frequency (IDF) for each query term within different document sections (URL, title, content, and whole document). TF measures how often a term appears in a specific document section, while IDF measures how important that term is across the entire document collection.\n\nBM25: This feature utilizes the BM25 ranking function, which is a probabilistic model that considers term frequency, document length, and average document length to estimate relevance. While it also considers term frequency like TF-IDF, it incorporates additional factors to improve the weighting scheme.",
    "predicted_answer": "TF-IDF calculates the average value of term frequency multiplied by inverse document frequency, while BM25 provides a scoring mechanism considering term frequency, document length, and saturation effects.",
    "reference": "1804.05938v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.11787819253438114,
      "phrase_overlap": 0.11787819253438114,
      "is_correct": true
    }
  },
  "1705.09882v2_q0": {
    "paper_id": "1705.09882v2",
    "question_index": 0,
    "question": "Which method achieves the highest Top-1 Accuracy for multi-shot person re-identification on the BIWI dataset, and how does it compare to the best single-shot method on the same dataset?",
    "ground_truth": "The proposed method with RTA attention achieves the highest Top-1 Accuracy for multi-shot person re-identification on the BIWI dataset with a score of 50.0%. This is significantly higher than the best single-shot method on the same dataset, which is our method (CNN) with a score of 25.4%.",
    "predicted_answer": "The method 'Our method with RTA attention' achieves the highest Top-1 Accuracy for multi-shot person re-identification on the BIWI dataset with 50.0%. It outperforms the best single-shot method ('3D RAM') on the same dataset, which achieves 30.1%.",
    "reference": "1705.09882v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.48134328358208955,
      "phrase_overlap": 0.48134328358208955,
      "is_correct": true
    }
  },
  "1705.09882v2_q1": {
    "paper_id": "1705.09882v2",
    "question_index": 1,
    "question": "How does the proposed split-rate RGB-to-Depth transfer scheme differ from the R3D [90] method of Yosinski et al.?",
    "ground_truth": "The proposed split-rate RGB-to-Depth transfer scheme differs from the R3D [90] method in two ways. First, the proposed method uses a different learning rate for the bottom three layers of the network. Second, the proposed method uses a different initialization for the weights of the bottom three layers of the network.",
    "predicted_answer": "Our transfer (R3D) uses split-rate scaling (x0.1 for RGB weights and x10 for Depth weights), while R3D [90] does not.",
    "reference": "1705.09882v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.12385321100917432,
      "phrase_overlap": 0.12385321100917432,
      "is_correct": true
    }
  },
  "1705.09882v2_q2": {
    "paper_id": "1705.09882v2",
    "question_index": 2,
    "question": "What is the difference between the grayscale depth representation and the result after background subtraction?",
    "ground_truth": " The grayscale depth representation shows the depth of each pixel in the image, with darker pixels representing closer objects and lighter pixels representing further objects. The result after background subtraction shows only the foreground object, with the background removed.",
    "predicted_answer": "The grayscale depth representation provides depth information using varying shades, while the result after background subtraction isolates the foreground object (person) in a binary format by removing the background.",
    "reference": "1705.09882v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.15821501014198783,
      "phrase_overlap": 0.15821501014198783,
      "is_correct": false
    }
  },
  "1705.09882v2_q3": {
    "paper_id": "1705.09882v2",
    "question_index": 3,
    "question": "Which modality achieved the highest top-1 accuracy in the multi-shot evaluation on TUM-GAID?",
    "ground_truth": "Body Depth & Head RGB (ms: LSTM & RTA)",
    "predicted_answer": "Body Depth & Head RGB (ms: LSTM & RTA)",
    "reference": "1705.09882v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1705.09882v2_q4": {
    "paper_id": "1705.09882v2",
    "question_index": 4,
    "question": "What is the relationship between the Bernoulli parameter and the image?",
    "ground_truth": "The Bernoulli parameter is a measure of the probability of a pixel being foreground or background. The higher the Bernoulli parameter, the more likely the pixel is to be foreground. This is reflected in the images, where the pixels with higher Bernoulli parameters are more likely to be part of the person's silhouette.",
    "predicted_answer": "The relationship between the Bernoulli parameter and the image is that higher Bernoulli parameters result in clearer silhouettes with less noise, while lower parameters lead to more noise and distortion.",
    "reference": "1705.09882v2-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.04980842911877394,
      "phrase_overlap": 0.04980842911877394,
      "is_correct": false
    }
  },
  "1705.09882v2_q5": {
    "paper_id": "1705.09882v2",
    "question_index": 5,
    "question": "Which part of the model is responsible for deciding which frames are most important for the re-identification task?",
    "ground_truth": "The Reinforced Temporal Attention (RTA) unit.",
    "predicted_answer": "fw(θw) is responsible for deciding which frames are most important for the re-identification task.",
    "reference": "1705.09882v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2517482517482518,
      "phrase_overlap": 0.2517482517482518,
      "is_correct": false
    }
  },
  "1705.09882v2_q6": {
    "paper_id": "1705.09882v2",
    "question_index": 6,
    "question": "How does the performance of our RGB-to-Depth transfer compare to Yosinski et al. [90] in terms of top-1 accuracy on DPI-T when all layers are fine-tuned?",
    "ground_truth": "The proposed RGB-to-Depth transfer performs slightly better than Yosinski et al. [90] in terms of top-1 accuracy on DPI-T when all layers are fine-tuned.",
    "predicted_answer": "Our transfer achieves higher top-1 accuracy than Yosinski et al. [90] when all layers are fine-tuned.",
    "reference": "1705.09882v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6141732283464567,
      "phrase_overlap": 0.6141732283464567,
      "is_correct": true
    }
  },
  "1705.09882v2_q7": {
    "paper_id": "1705.09882v2",
    "question_index": 7,
    "question": "What is the difference between the filter responses from the “conv1”, “conv2” and “conv3” layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB and the feature embedding fCNN of a framework that utilizes depth data?",
    "ground_truth": "The filter responses from the “conv1”, “conv2” and “conv3” layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB are more detailed and contain more information than the filter responses from the fCNN of a framework that utilizes depth data. This is because RGB images contain more information than depth images.",
    "predicted_answer": "Filter responses from \"conv1\", \"conv2\", and \"conv3\" layers differ in texture and feature abstraction, with RGB-based responses showing more detailed textures and depth-based responses emphasizing shape and silhouette.",
    "reference": "1705.09882v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.11418685121107267,
      "phrase_overlap": 0.11418685121107267,
      "is_correct": true
    }
  },
  "1811.08481v2_q0": {
    "paper_id": "1811.08481v2",
    "question_index": 0,
    "question": "Which method has the highest overall accuracy for answering questions about images on the 100 questions sampled from VQA v2 dataset?",
    "ground_truth": "UnCoRd-VG-E",
    "predicted_answer": "UnCoRd-VG-E has the highest overall accuracy of 92.0.",
    "reference": "1811.08481v2-Table8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.34375,
      "phrase_overlap": 0.34375,
      "is_correct": true
    }
  },
  "1811.08481v2_q1": {
    "paper_id": "1811.08481v2",
    "question_index": 1,
    "question": "Explain how the answering procedure works.",
    "ground_truth": "The answering procedure follows the question graph, searching for a valid assignment in the image. At each step, the handled node is set and objects (extracted using Mask R-CNN) are examined according to the node’s requirements (utilizing corresponding visual estimators). If successful, a new node is set (according to a DFS traversal) and the function is called again to handle the unassigned subgraph.",
    "predicted_answer": "brown",
    "reference": "1811.08481v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.009779951100244499,
      "phrase_overlap": 0.009779951100244499,
      "is_correct": false
    }
  },
  "1811.08481v2_q2": {
    "paper_id": "1811.08481v2",
    "question_index": 2,
    "question": "Which method has the highest overall accuracy?",
    "ground_truth": "UnCoRd-VG-E",
    "predicted_answer": "UnCoRd-VG-E",
    "reference": "1811.08481v2-Table7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1811.08481v2_q3": {
    "paper_id": "1811.08481v2",
    "question_index": 3,
    "question": "What is the effect of training on more diverse data on the accuracy of graph representation for VQA?",
    "ground_truth": "Training on more diverse data improves the accuracy of graph representation for VQA.",
    "predicted_answer": "Training on more diverse data improves the accuracy of graph representation for VQA, with VG achieving the highest accuracy (B: 34, E: 50).",
    "reference": "1811.08481v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7533632286995515,
      "phrase_overlap": 0.7533632286995515,
      "is_correct": true
    }
  },
  "1811.08481v2_q4": {
    "paper_id": "1811.08481v2",
    "question_index": 4,
    "question": "Which estimator achieves the highest accuracy on the CLEVR validation set?",
    "ground_truth": "Size estimator.",
    "predicted_answer": "Size",
    "reference": "1811.08481v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.42105263157894735,
      "phrase_overlap": 0.42105263157894735,
      "is_correct": true
    }
  },
  "1811.08481v2_q5": {
    "paper_id": "1811.08481v2",
    "question_index": 5,
    "question": "Which method achieves the highest overall accuracy on the validation set?",
    "ground_truth": "UnCoRd-None-B.",
    "predicted_answer": "NS-VQA and UnCoRdNone-B achieve the highest overall accuracy on the validation set, both scoring 99.8.",
    "reference": "1811.08481v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.22413793103448276,
      "phrase_overlap": 0.22413793103448276,
      "is_correct": false
    }
  },
  "1705.09966v2_q0": {
    "paper_id": "1705.09966v2",
    "question_index": 0,
    "question": "What is the role of the auxiliary discriminator $D_{X_{\\textit{aux}}}$ in the Conditional CycleGAN for identity-guided face generation?",
    "ground_truth": " The auxiliary discriminator $D_{X_{\\textit{aux}}}$ helps to enforce the identity constraint in the generated image. It takes the generated image or the ground truth image as input and outputs a feature embedding. This embedding is then used to compute the identity loss, which encourages the generated image to have the same identity as the input image.",
    "predicted_answer": "To ensure identity preservation during face generation.",
    "reference": "1705.09966v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.024509803921568627,
      "phrase_overlap": 0.024509803921568627,
      "is_correct": false
    }
  },
  "1705.09966v2_q1": {
    "paper_id": "1705.09966v2",
    "question_index": 1,
    "question": "How does the proposed method compare to the method in~\\cite{kim2017learning}?",
    "ground_truth": "The proposed method produces more realistic and natural-looking images than the method in~\\cite{kim2017learning}.",
    "predicted_answer": "The proposed method performs better than the method in~\\cite{kim2017learning} in terms of accuracy and visual quality for attribute manipulation tasks such as changing hair color and facial expressions.",
    "reference": "1705.09966v2-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4380952380952381,
      "phrase_overlap": 0.4380952380952381,
      "is_correct": true
    }
  },
  "1705.09966v2_q2": {
    "paper_id": "1705.09966v2",
    "question_index": 2,
    "question": "Which of the methods among Conditional GAN, Unsupervised GAN and Consitional CycleGAN would you expect to produce images that are most visually similar to the real images in the CelebA dataset?",
    "ground_truth": "The Conditional CycleGAN method is expected to produce images most visually similar to the real images.",
    "predicted_answer": "C",
    "reference": "1705.09966v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.019230769230769232,
      "phrase_overlap": 0.019230769230769232,
      "is_correct": true
    }
  },
  "1705.09966v2_q3": {
    "paper_id": "1705.09966v2",
    "question_index": 3,
    "question": "How does the proposed attribute-guided face generation method compare to conventional face super-resolution methods in terms of identity preservation?",
    "ground_truth": "The proposed attribute-guided face generation method preserves the identity of the person in the high-resolution result, while conventional face super-resolution methods do not necessarily guarantee this.",
    "predicted_answer": "The proposed attribute-guided face generation method demonstrates superior identity preservation compared to conventional face super-resolution methods.",
    "reference": "1705.09966v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3258426966292135,
      "phrase_overlap": 0.3258426966292135,
      "is_correct": false
    }
  },
  "1705.09966v2_q4": {
    "paper_id": "1705.09966v2",
    "question_index": 4,
    "question": "What role does the low-resolution input play in the identity-guided face generation process?",
    "ground_truth": "The low-resolution input provides an overall shape constraint for the generated high-resolution image. The head pose and facial expression of the generated high-res images adopt those in the low-res inputs.",
    "predicted_answer": "The low-resolution input plays the role of providing identity features that guide the generation of the face in the identity-guided face generation process.",
    "reference": "1705.09966v2-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.23204419889502761,
      "phrase_overlap": 0.23204419889502761,
      "is_correct": false
    }
  },
  "1705.09966v2_q5": {
    "paper_id": "1705.09966v2",
    "question_index": 5,
    "question": "What happens when there is a conflict between the low-res image and the feature vector?",
    "ground_truth": "The generated high-res digit follows the given class label.",
    "predicted_answer": "B",
    "reference": "1705.09966v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.03333333333333333,
      "phrase_overlap": 0.03333333333333333,
      "is_correct": true
    }
  },
  "1705.09966v2_q6": {
    "paper_id": "1705.09966v2",
    "question_index": 6,
    "question": "What is the difference between the input and output of the frontal face generation process?",
    "ground_truth": "The input is a low-resolution frontal face image and a high-resolution side-face image. The output is a high-resolution frontal face image.",
    "predicted_answer": "The input is a non-frontal face, and the output is a frontal face.",
    "reference": "1705.09966v2-Figure13-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6146341463414634,
      "phrase_overlap": 0.6146341463414634,
      "is_correct": true
    }
  },
  "1705.09966v2_q7": {
    "paper_id": "1705.09966v2",
    "question_index": 7,
    "question": "How does the proposed method preserve facial details and expression during face swapping?",
    "ground_truth": "The proposed method utilizes Light-CNN as both the source of the identity features and face verification loss. This allows the method to transfer the appearance of eyes, eyebrows, hairs, etc., while keeping other factors intact, e.g., head pose, shape of face, and facial expression.",
    "predicted_answer": "The proposed method preserves facial details and expressions better than the baseline method, as shown in the comparison between (c) and (d).",
    "reference": "1705.09966v2-Figure11-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.15566037735849056,
      "phrase_overlap": 0.15566037735849056,
      "is_correct": false
    }
  },
  "1705.09966v2_q8": {
    "paper_id": "1705.09966v2",
    "question_index": 8,
    "question": "How does the proposed method compare to icGAN in terms of generating images with different hair colors?",
    "ground_truth": "The proposed method is able to generate images with different hair colors more accurately than icGAN.",
    "predicted_answer": "The proposed method generates images with more realistic and distinct hair colors (Blond Hair and Gray Hair) compared to icGAN.",
    "reference": "1705.09966v2-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6140350877192983,
      "phrase_overlap": 0.6140350877192983,
      "is_correct": true
    }
  },
  "1705.09966v2_q9": {
    "paper_id": "1705.09966v2",
    "question_index": 9,
    "question": "What is the role of the attribute vector $z$ in the Conditional CycleGAN network?",
    "ground_truth": "The attribute vector $z$ provides additional information about the desired attributes of the generated high-resolution face image $\\hat{X}$. This information is used by the generator networks $G_{X \\to Y}$ and $G_{Y \\to X}$ to generate images that are more consistent with the desired attributes.",
    "predicted_answer": "The attribute vector $z$ specifies the desired attributes (e.g., black hair or blonde hair) for the generated images.",
    "reference": "1705.09966v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.14043583535108958,
      "phrase_overlap": 0.14043583535108958,
      "is_correct": false
    }
  },
  "1805.02349v2_q0": {
    "paper_id": "1805.02349v2",
    "question_index": 0,
    "question": "Which algorithm has the fastest runtime?",
    "ground_truth": "The algorithm proposed in this paper has the fastest runtime.",
    "predicted_answer": "D",
    "reference": "1805.02349v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.03225806451612903,
      "phrase_overlap": 0.03225806451612903,
      "is_correct": true
    }
  },
  "1811.09393v4_q0": {
    "paper_id": "1811.09393v4",
    "question_index": 0,
    "question": "Which method has the highest T-Diff on average for the Vid4 dataset?",
    "ground_truth": "TecoGAN.",
    "predicted_answer": "B",
    "reference": "1811.09393v4-Figure15-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1811.09393v4_q1": {
    "paper_id": "1811.09393v4",
    "question_index": 1,
    "question": "Which GAN model is able to generate the most realistic blinking motions?",
    "ground_truth": "TecoGAN",
    "predicted_answer": "Teco-GAN",
    "reference": "1811.09393v4-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9333333333333333,
      "phrase_overlap": 0.9333333333333333,
      "is_correct": true
    }
  },
  "1811.09393v4_q2": {
    "paper_id": "1811.09393v4",
    "question_index": 2,
    "question": "Which of the VSR models in the figure achieves the best balance of spatial detail and temporal coherence?",
    "ground_truth": "TecoGAN",
    "predicted_answer": "DsDt",
    "reference": "1811.09393v4-Figure14-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.18181818181818182,
      "phrase_overlap": 0.18181818181818182,
      "is_correct": false
    }
  },
  "1811.09393v4_q3": {
    "paper_id": "1811.09393v4",
    "question_index": 3,
    "question": "Which method achieves the highest PSNR on the Vid4 data set?",
    "ground_truth": "DUF",
    "predicted_answer": "E",
    "reference": "1811.09393v4-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1811.09393v4_q4": {
    "paper_id": "1811.09393v4",
    "question_index": 4,
    "question": " What is the role of the warped triplets in the conditional VSR Ds,t?",
    "ground_truth": " The warped triplets provide additional information about the motion and appearance of the scene, which helps the VSR Ds,t to generate more accurate and realistic results.",
    "predicted_answer": "The role of the warped triplets in the conditional VSR Ds,t is to provide additional temporal alignment information by warping the original triplets, which are then combined with the original triplets to enhance the discriminator's ability to distinguish between real and generated sequences.",
    "reference": "1811.09393v4-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.43722943722943725,
      "phrase_overlap": 0.43722943722943725,
      "is_correct": false
    }
  },
  "1811.09393v4_q5": {
    "paper_id": "1811.09393v4",
    "question_index": 5,
    "question": "How does the PP loss improve the temporal coherence of the video sequence?",
    "ground_truth": "The PP loss constrains the output sequence to be symmetric by reducing the L2 distance between corresponding frames in the forward and backward passes. This helps to reduce drifting artifacts and improve temporal coherence.",
    "predicted_answer": "The PP loss improves temporal coherence by enforcing consistency in video frames when processed in both forward and backward directions, ensuring smooth transitions and reducing artifacts.",
    "reference": "1811.09393v4-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.22384428223844283,
      "phrase_overlap": 0.22384428223844283,
      "is_correct": false
    }
  },
  "1811.09393v4_q6": {
    "paper_id": "1811.09393v4",
    "question_index": 6,
    "question": "What is the role of the Motion Compensation block in the Frame-Recurrent Generator?",
    "ground_truth": "The Motion Compensation block estimates the motion between the previous frame and the current frame, and uses this information to warp the previous frame to the current frame. This helps the generator to produce more realistic images by taking into account the temporal information in the video sequence.",
    "predicted_answer": "Motion Compensation block adjusts the previous frame \\( g_{t-1} \\) using motion \\( w \\) to align it with the current frame \\( g_t \\).",
    "reference": "1811.09393v4-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2425629290617849,
      "phrase_overlap": 0.2425629290617849,
      "is_correct": false
    }
  },
  "1811.09393v4_q7": {
    "paper_id": "1811.09393v4",
    "question_index": 7,
    "question": "Which method produces the least amount of artifacts?",
    "ground_truth": "TecoGAN⊖.",
    "predicted_answer": "TecoGAN",
    "reference": "1811.09393v4-Figure23-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.875,
      "phrase_overlap": 0.875,
      "is_correct": true
    }
  },
  "1811.09393v4_q8": {
    "paper_id": "1811.09393v4",
    "question_index": 8,
    "question": "Which method produces the most realistic results for the Vid4 scenes?",
    "ground_truth": "TecoGAN.",
    "predicted_answer": "GT produces the most realistic results, followed closely by EDVR and TecoGAN.",
    "reference": "1811.09393v4-Figure12-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.18823529411764706,
      "phrase_overlap": 0.18823529411764706,
      "is_correct": true
    }
  },
  "1811.09393v4_q9": {
    "paper_id": "1811.09393v4",
    "question_index": 9,
    "question": "What is the learning rate for the generator in the DsOnly model?",
    "ground_truth": "5.00E-05",
    "predicted_answer": "5e-5",
    "reference": "1811.09393v4-Table6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6666666666666666,
      "phrase_overlap": 0.6666666666666666,
      "is_correct": true
    }
  },
  "1811.09393v4_q10": {
    "paper_id": "1811.09393v4",
    "question_index": 10,
    "question": "Why does flow estimation become less accurate near image boundaries?",
    "ground_truth": "Flow estimation becomes less accurate near image boundaries because there is less information available to estimate the flow. This is because the pixels at the boundaries are only surrounded by pixels on one side, whereas pixels in the interior of the image are surrounded by pixels on all sides.",
    "predicted_answer": "Flow estimation becomes less accurate near image boundaries due to limited information and lack of context in these regions.",
    "reference": "1811.09393v4-Figure22-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3,
      "phrase_overlap": 0.3,
      "is_correct": false
    }
  },
  "1811.09393v4_q11": {
    "paper_id": "1811.09393v4",
    "question_index": 11,
    "question": "Which method has the best perceptual performance according to the tOF score?",
    "ground_truth": "TecoGAN.",
    "predicted_answer": "C",
    "reference": "1811.09393v4-Figure18-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2222222222222222,
      "phrase_overlap": 0.2222222222222222,
      "is_correct": true
    }
  },
  "1811.09393v4_q12": {
    "paper_id": "1811.09393v4",
    "question_index": 12,
    "question": "Which of the methods generated the sharpest details?",
    "ground_truth": "TecoGAN",
    "predicted_answer": "GT",
    "reference": "1811.09393v4-Figure11-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2222222222222222,
      "phrase_overlap": 0.2222222222222222,
      "is_correct": false
    }
  },
  "1811.09393v4_q13": {
    "paper_id": "1811.09393v4",
    "question_index": 13,
    "question": "What is the purpose of the UVT cycle link?",
    "ground_truth": "The UVT cycle link is used to transfer knowledge between two recurrent generators.",
    "predicted_answer": "The purpose of the UVT cycle link is to evaluate consistency between Domain A and Domain B using static, original, and warped triplets, resulting in a 0/1 decision.",
    "reference": "1811.09393v4-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.36585365853658536,
      "phrase_overlap": 0.36585365853658536,
      "is_correct": false
    }
  },
  "1811.09393v4_q14": {
    "paper_id": "1811.09393v4",
    "question_index": 14,
    "question": "What is the purpose of the user study?",
    "ground_truth": "The user study is designed to test which of two images is closer to a reference video.",
    "predicted_answer": "A",
    "reference": "1811.09393v4-Figure19-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.022988505747126436,
      "phrase_overlap": 0.022988505747126436,
      "is_correct": true
    }
  },
  "1805.04609v3_q0": {
    "paper_id": "1805.04609v3",
    "question_index": 0,
    "question": "Which algorithm has the highest percentage of switched instances?",
    "ground_truth": "US-HC-MQ",
    "predicted_answer": "US-HC-MQ algorithm in the KS dataset with 0.32.",
    "reference": "1805.04609v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2909090909090909,
      "phrase_overlap": 0.2909090909090909,
      "is_correct": true
    }
  },
  "1805.04609v3_q1": {
    "paper_id": "1805.04609v3",
    "question_index": 1,
    "question": "How does the accuracy of the US-BS-MQ method compare to that of the S-MQ method when adding SST examples?",
    "ground_truth": "The US-BS-MQ method achieves higher accuracy than the S-MQ method when adding SST examples.",
    "predicted_answer": "A",
    "reference": "1805.04609v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.021739130434782608,
      "phrase_overlap": 0.021739130434782608,
      "is_correct": true
    }
  },
  "1705.10667v4_q0": {
    "paper_id": "1705.10667v4",
    "question_index": 0,
    "question": "What is the main difference between the Multilinear Conditioning architecture and the Randomized Multilinear Conditioning architecture?",
    "ground_truth": "The main difference is that the Multilinear Conditioning architecture uses a multilinear map to condition the domain discriminator on the classifier prediction, while the Randomized Multilinear Conditioning architecture uses a randomized multilinear map.",
    "predicted_answer": "E",
    "reference": "1705.10667v4-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1705.10667v4_q1": {
    "paper_id": "1705.10667v4",
    "question_index": 1,
    "question": "Which of the four methods (ResNet, DANN, CDAN-f, CDAN-fg) is most effective at separating the two classes of data points?",
    "ground_truth": "CDAN-fg",
    "predicted_answer": "E",
    "reference": "1705.10667v4-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1705.10667v4_q2": {
    "paper_id": "1705.10667v4",
    "question_index": 2,
    "question": "Why might CDAN+E be considered a more versatile method for unsupervised domain adaptation compared to UNIT, CyCADA, and GTA?",
    "ground_truth": "CDAN+E performs well across all five datasets listed in the table, including both digit and synthetic-to-real datasets, while UNIT, CyCADA, and GTA show strong results only on the digits and synthetic-to-real datasets.",
    "predicted_answer": "E",
    "reference": "1705.10667v4-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1705.10667v4_q3": {
    "paper_id": "1705.10667v4",
    "question_index": 3,
    "question": "Which sampling strategy for random matrices in CDAN+E leads to the highest average accuracy across all domain adaptation tasks on Office-31? How does this compare to the performance of CDAN+E variants that use random sampling?",
    "ground_truth": "The table shows that CDAN+E (w/o random sampling) achieves the highest average accuracy of 87.7% across all domain adaptation tasks. This is slightly higher than the performance of CDAN+E with uniform sampling (87.0%) and Gaussian sampling (86.4%).",
    "predicted_answer": "C",
    "reference": "1705.10667v4-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1705.10667v4_q4": {
    "paper_id": "1705.10667v4",
    "question_index": 4,
    "question": "Which model performs the best in terms of test error?",
    "ground_truth": "CDAN (M)",
    "predicted_answer": "C",
    "reference": "1705.10667v4-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2222222222222222,
      "phrase_overlap": 0.2222222222222222,
      "is_correct": true
    }
  },
  "1805.04687v2_q0": {
    "paper_id": "1805.04687v2",
    "question_index": 0,
    "question": "How does increasing the training set size affect the performance of the lane marking and drivable area segmentation tasks?",
    "ground_truth": "Increasing the training set size generally leads to improved performance for both lane marking and drivable area segmentation tasks.",
    "predicted_answer": "Increasing the training set size improves the performance of both lane marking and drivable area segmentation tasks, with diminishing returns beyond 20K samples.\n\n根据题目要求，只输出选项的字母：\n\nB",
    "reference": "1805.04687v2-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7051282051282052,
      "phrase_overlap": 0.7051282051282052,
      "is_correct": true
    }
  },
  "1805.04687v2_q1": {
    "paper_id": "1805.04687v2",
    "question_index": 1,
    "question": "How does joint training with the object detection set affect instance segmentation performance, and what is the likely reason for this effect?",
    "ground_truth": "Joint training with the object detection set improves instance segmentation performance significantly. This is evidenced by the increase in AP, AP50, and AP75 metrics in Table 1 when comparing \"Inst-Seg\" and \"Inst-Seg + Det\" rows.\n\nThe passage explains that this improvement is likely due to the richer diversity of images and object examples in the detection set. This allows the instance segmentation model to learn better object appearance features and localization.",
    "predicted_answer": "Joint training with the object detection set improves instance segmentation performance, likely because the additional object detection data provides more diverse and complementary information for training.",
    "reference": "1805.04687v2-Table6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.31750741839762614,
      "phrase_overlap": 0.31750741839762614,
      "is_correct": false
    }
  },
  "1805.04687v2_q2": {
    "paper_id": "1805.04687v2",
    "question_index": 2,
    "question": "Based on Table 1, which category of objects has the largest total number of annotations in the BDD100K MOT dataset, considering both bounding boxes and instance tracks? ",
    "ground_truth": "Cars have the largest total number of annotations.",
    "predicted_answer": "The 'car' category has the largest total number of annotations in the BDD100K MOT dataset, considering both bounding boxes and instance tracks.",
    "reference": "1805.04687v2-Table11-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.48704663212435234,
      "phrase_overlap": 0.48704663212435234,
      "is_correct": false
    }
  },
  "1805.04687v2_q3": {
    "paper_id": "1805.04687v2",
    "question_index": 3,
    "question": "How does the segmentation model perform in areas with no lane markings?",
    "ground_truth": "The segmentation model learns to interpolate in areas that have no lane markings.",
    "predicted_answer": "B",
    "reference": "1805.04687v2-Figure11-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1805.04687v2_q4": {
    "paper_id": "1805.04687v2",
    "question_index": 4,
    "question": "What is the most common type of scene in the dataset?",
    "ground_truth": "City Street",
    "predicted_answer": "C",
    "reference": "1805.04687v2-Figure10-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.16666666666666666,
      "phrase_overlap": 0.16666666666666666,
      "is_correct": true
    }
  },
  "1805.04687v2_q5": {
    "paper_id": "1805.04687v2",
    "question_index": 5,
    "question": "What are the different types of annotations that are included in the dataset?",
    "ground_truth": "The dataset includes a rich set of annotations: scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation.",
    "predicted_answer": "A",
    "reference": "1805.04687v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1805.04687v2_q6": {
    "paper_id": "1805.04687v2",
    "question_index": 6,
    "question": "Explain why the model trained on both MOT and detection sets has a higher number of identity switches (IDS) compared to the model trained only on the MOT set, even though it achieves better performance in terms of AP, MOTA, and MOTP?",
    "ground_truth": "While the model trained on both MOT and detection sets shows improved performance in detection and tracking metrics (AP, MOTA, MOTP), it also exhibits a higher number of identity switches (IDS). This can be attributed to the increased diversity of instances introduced by the detection set. Although the MOT set provides a larger number of bounding boxes for training, the detection set adds varied examples that may lead to more frequent identity switches during tracking, even as it improves the model's overall performance.",
    "predicted_answer": "The model trained on both MOT and detection sets has a higher number of identity switches (IDS) because incorporating detection data likely improves object detection accuracy (reflected in higher AP, MOTA, and MOTP), but it may introduce more false positives or mismatches in identity tracking, leading to increased IDS.",
    "reference": "1805.04687v2-Table7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.08983451536643026,
      "phrase_overlap": 0.08983451536643026,
      "is_correct": false
    }
  },
  "1805.04687v2_q7": {
    "paper_id": "1805.04687v2",
    "question_index": 7,
    "question": "Which domain discrepancy has a larger impact on object detection performance: city vs. non-city or daytime vs. nighttime?",
    "ground_truth": "Daytime vs. nighttime has a larger impact on object detection performance.",
    "predicted_answer": "Daytime_vs_NonDaytime",
    "reference": "1805.04687v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.29473684210526313,
      "phrase_overlap": 0.29473684210526313,
      "is_correct": false
    }
  },
  "1805.04687v2_q8": {
    "paper_id": "1805.04687v2",
    "question_index": 8,
    "question": "Why does the proposed dataset have a lower number of persons per image compared to the Cityscapes dataset, even though it has a much higher total number of pedestrians?",
    "ground_truth": "The proposed dataset contains non-city scenes like highways, which typically have fewer pedestrians per image compared to cityscapes.",
    "predicted_answer": "The proposed dataset has a lower number of persons per image compared to Cityscapes because it likely contains a larger number of images with fewer pedestrians per image, despite having a much higher total number of pedestrians overall.",
    "reference": "1805.04687v2-Table10-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.36314363143631434,
      "phrase_overlap": 0.36314363143631434,
      "is_correct": false
    }
  },
  "1805.04687v2_q9": {
    "paper_id": "1805.04687v2",
    "question_index": 9,
    "question": "How does the performance of lane marking detection change with different thresholds (τ) for direction, continuity, and category?",
    "ground_truth": "As the threshold (τ) increases, the ODS-F scores for direction, continuity, and category generally increase as well. This indicates that the model performs better in detecting lane markings with higher thresholds, meaning it can tolerate larger deviations from the ground truth annotations.",
    "predicted_answer": "B",
    "reference": "1805.04687v2-Table14-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.006872852233676976,
      "phrase_overlap": 0.006872852233676976,
      "is_correct": true
    }
  },
  "1805.04687v2_q10": {
    "paper_id": "1805.04687v2",
    "question_index": 10,
    "question": "Which weather condition has the highest classification accuracy?",
    "ground_truth": "Clear weather.",
    "predicted_answer": "A",
    "reference": "1805.04687v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.13333333333333333,
      "phrase_overlap": 0.13333333333333333,
      "is_correct": true
    }
  },
  "1805.04687v2_q11": {
    "paper_id": "1805.04687v2",
    "question_index": 11,
    "question": "Which type of object is the most common in the dataset?",
    "ground_truth": "Cars are the most common object in the dataset.",
    "predicted_answer": "car",
    "reference": "1805.04687v2-Figure14-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.12,
      "phrase_overlap": 0.12,
      "is_correct": true
    }
  },
  "1805.04687v2_q12": {
    "paper_id": "1805.04687v2",
    "question_index": 12,
    "question": "Based on the table, which approach achieved the highest mean IoU for semantic segmentation, and how did it perform compared to the baseline Sem-Seg model?",
    "ground_truth": "The **Sem-Seg + Det** approach achieved the highest mean IoU of 58.3, which is an improvement of 1.4 points compared to the baseline Sem-Seg model with a mean IoU of 56.9.",
    "predicted_answer": "The Sem-Seg + Det approach achieved the highest mean IoU for semantic segmentation with a value of 58.3, outperforming the baseline Sem-Seg model by 1.4 (56.9).",
    "reference": "1805.04687v2-Table8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6465256797583081,
      "phrase_overlap": 0.6465256797583081,
      "is_correct": true
    }
  },
  "1805.04687v2_q13": {
    "paper_id": "1805.04687v2",
    "question_index": 13,
    "question": "Which training approach achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score?",
    "ground_truth": "The training approach \"Det + T + I + S\" achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score.",
    "predicted_answer": "The 'Det + T + I + S' training approach achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score.",
    "reference": "1805.04687v2-Table9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9072164948453608,
      "phrase_overlap": 0.9072164948453608,
      "is_correct": true
    }
  },
  "1805.04687v2_q14": {
    "paper_id": "1805.04687v2",
    "question_index": 14,
    "question": "Which category of objects has the highest total number of annotations, and is there evidence that this category might be more challenging to annotate accurately?",
    "ground_truth": "The category with the highest total number of annotations is \"Masks,\" with 129K annotations. There is evidence that this category might be more challenging to annotate accurately because it also has the highest number of annotations in the \"Occluded\" subcategory, indicating that a large portion of these objects are partially hidden in the images.",
    "predicted_answer": "C",
    "reference": "1805.04687v2-Table12-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1805.04687v2_q15": {
    "paper_id": "1805.04687v2",
    "question_index": 15,
    "question": "What percentage of occlusions last for more than 10 frames?",
    "ground_truth": "Approximately 80%",
    "predicted_answer": "20%",
    "reference": "1805.04687v2-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2,
      "phrase_overlap": 0.2,
      "is_correct": false
    }
  },
  "1805.04687v2_q16": {
    "paper_id": "1805.04687v2",
    "question_index": 16,
    "question": "Why are MOTS datasets like KITTI MOTS and MOTS Challenge smaller in size compared to VOS datasets like YouTube VOS, even though BDD100K MOTS has a comparable number of annotations?",
    "ground_truth": "MOTS datasets require denser annotations per frame because they involve both segmentation and tracking of multiple objects in crowded scenes. This results in smaller dataset sizes compared to VOS datasets, which typically focus on segmenting a single or fewer objects. ",
    "predicted_answer": "MOTS datasets like KITTI MOTS and MOTS Challenge are smaller in size compared to VOS datasets like YouTube VOS because they focus on fewer frames and sequences but have higher annotations per frame. BDD100K MOTS has a comparable number of annotations to YouTube VOS but fewer frames, indicating a focus on detailed annotation density rather than dataset size。因此，答案是：B。",
    "reference": "1805.04687v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.20125786163522014,
      "phrase_overlap": 0.20125786163522014,
      "is_correct": false
    }
  },
  "1805.04687v2_q17": {
    "paper_id": "1805.04687v2",
    "question_index": 17,
    "question": "What are the three main geographical regions where the data for this study was collected?",
    "ground_truth": "New York, San Francisco Bay Area, and Berkeley.",
    "predicted_answer": "New York, Berkeley, San Francisco",
    "reference": "1805.04687v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.575,
      "phrase_overlap": 0.575,
      "is_correct": false
    }
  },
  "1805.04687v2_q18": {
    "paper_id": "1805.04687v2",
    "question_index": 18,
    "question": "Which category of object is the least common in the dataset?",
    "ground_truth": "Train",
    "predicted_answer": "Train",
    "reference": "1805.04687v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1805.04687v2_q19": {
    "paper_id": "1805.04687v2",
    "question_index": 19,
    "question": "How does the BDD100K dataset compare to the KITTI and MOT17 datasets in terms of size and complexity?",
    "ground_truth": "The BDD100K dataset is significantly larger and more complex than both the KITTI and MOT17 datasets. It contains roughly 40 times more frames, 16 times more sequences, and 13 times more identities than KITTI. Compared to MOT17, BDD100K has about 10 times more frames, 80 times more sequences, and 8 times more identities. This increase in size and complexity makes BDD100K a more challenging and comprehensive benchmark for multiple object tracking algorithms. ",
    "predicted_answer": "BDD100K",
    "reference": "1805.04687v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1805.04687v2_q20": {
    "paper_id": "1805.04687v2",
    "question_index": 20,
    "question": "Which dataset has the most lane marking annotations?",
    "ground_truth": "BDD100K",
    "predicted_answer": "BDD100K",
    "reference": "1805.04687v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1706.00633v4_q0": {
    "paper_id": "1706.00633v4",
    "question_index": 0,
    "question": "Which method performed better on the MNIST dataset, ResNet-32 (CE) or ResNet-56 (RCE)?",
    "ground_truth": "ResNet-56 (RCE) performed better on the MNIST dataset with a classification error rate of 0.32% compared to ResNet-32 (CE) which had a classification error rate of 0.38%.",
    "predicted_answer": "ResNet-32 (RCE) performed better on the MNIST dataset with an error rate of 0.29 compared to ResNet-32 (CE) with an error rate of 0.38.",
    "reference": "1706.00633v4-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.8590163934426229,
      "phrase_overlap": 0.8590163934426229,
      "is_correct": true
    }
  },
  "1706.00633v4_q1": {
    "paper_id": "1706.00633v4",
    "question_index": 1,
    "question": "Which combination of training procedure and thresholding metric consistently performs the best across both MNIST and CIFAR-10 datasets for all attack types?",
    "ground_truth": "RCE training combined with the K-density metric consistently performs the best across both MNIST and CIFAR-10 datasets for all attack types.",
    "predicted_answer": "RCE training procedure combined with the K-density thresholding metric consistently performs the best across both MNIST and CIFAR-10 datasets for all attack types.",
    "reference": "1706.00633v4-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9240924092409241,
      "phrase_overlap": 0.9240924092409241,
      "is_correct": true
    }
  },
  "1706.00633v4_q2": {
    "paper_id": "1706.00633v4",
    "question_index": 2,
    "question": "Which attack method is the most effective at reducing the accuracy of the Resnet-32 model on the MNIST dataset?",
    "ground_truth": "The most effective attack method at reducing the accuracy of the Resnet-32 model on the MNIST dataset is BIM/CE.",
    "predicted_answer": "B",
    "reference": "1706.00633v4-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.017699115044247787,
      "phrase_overlap": 0.017699115044247787,
      "is_correct": true
    }
  },
  "1706.00633v4_q3": {
    "paper_id": "1706.00633v4",
    "question_index": 3,
    "question": "Which of the following algorithms performs the best when trained via the CE?",
    "ground_truth": "C&W-hc",
    "predicted_answer": "A",
    "reference": "1706.00633v4-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1706.00633v4_q4": {
    "paper_id": "1706.00633v4",
    "question_index": 4,
    "question": "How does the accuracy of the model change as the value of c increases?",
    "ground_truth": "The accuracy of the model decreases as the value of c increases.",
    "predicted_answer": "The accuracy of the model decreases as the value of c increases, with a steep drop occurring around log10(c) = -3 to -2. \n\n选项：B",
    "reference": "1706.00633v4-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6772486772486772,
      "phrase_overlap": 0.6772486772486772,
      "is_correct": true
    }
  },
  "1706.00633v4_q5": {
    "paper_id": "1706.00633v4",
    "question_index": 5,
    "question": "Which objective function resulted in a higher ratio of f2(x∗) > 0 for the MNIST dataset?",
    "ground_truth": "RCE",
    "predicted_answer": "RCE resulted in a higher ratio of f2(x∗) > 0 for the MNIST dataset.",
    "reference": "1706.00633v4-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.08571428571428572,
      "phrase_overlap": 0.08571428571428572,
      "is_correct": true
    }
  },
  "1706.00633v4_q6": {
    "paper_id": "1706.00633v4",
    "question_index": 6,
    "question": "Which type of adversarial example generation method results in images that are visually more similar to the original images?",
    "ground_truth": "The CE method results in images that are visually more similar to the original images than the RCE method.",
    "predicted_answer": "B",
    "reference": "1706.00633v4-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1706.00633v4_q7": {
    "paper_id": "1706.00633v4",
    "question_index": 7,
    "question": "How does the proposed metric of non-ME help detect adversarial examples?",
    "ground_truth": " The non-ME metric measures the entropy of the normalized non-maximal elements in the final hidden vector of the classifier. Adversarial examples often have low non-ME values, indicating that they are close to the decision boundary and have high confidence in the incorrect class.",
    "predicted_answer": "B",
    "reference": "1706.00633v4-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.007142857142857143,
      "phrase_overlap": 0.007142857142857143,
      "is_correct": true
    }
  },
  "1706.00633v4_q8": {
    "paper_id": "1706.00633v4",
    "question_index": 8,
    "question": "Based on the table, which attack method is the most efficient in terms of time taken to craft an adversarial example, and how much faster is it compared to the slowest method for the same objective function?",
    "ground_truth": "The FGSM attack is the most efficient, requiring approximately 1.9 milliseconds to craft an adversarial example with the CE objective function. This is roughly **55,000 times faster** than the slowest method, C&W-wb, which takes about 700 seconds for the same objective function.",
    "predicted_answer": "The most efficient attack method in terms of time taken is FGSM for both CE (~1.9 × 10⁻³ seconds) and RCE (~2.4 × 10⁻³ seconds) objective functions. Compared to the slowest method (C&W-wb), FGSM is faster by approximately 7.0 × 10² seconds for CE and 1.3 × 10³ seconds for RCE.",
    "reference": "1706.00633v4-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1366906474820144,
      "phrase_overlap": 0.1366906474820144,
      "is_correct": true
    }
  },
  "1811.10673v1_q0": {
    "paper_id": "1811.10673v1",
    "question_index": 0,
    "question": "What is the effect of downsampling on the quality of reconstructed frames?",
    "ground_truth": "Downsampling reduces the quality of reconstructed frames.",
    "predicted_answer": "Downsampling reduces the quality of reconstructed frames, leading to loss of detail and clarity.",
    "reference": "1811.10673v1-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7450980392156863,
      "phrase_overlap": 0.7450980392156863,
      "is_correct": true
    }
  },
  "1811.10673v1_q1": {
    "paper_id": "1811.10673v1",
    "question_index": 1,
    "question": "How does the proposed method compare to H.264 in terms of MS-SSIM score at low bitrates?",
    "ground_truth": "The proposed method achieves significantly higher MS-SSIM scores than H.264 at bitrates below 10 Kbps.",
    "predicted_answer": "The proposed method achieves higher MS-SSIM scores compared to H.264 at low bitrates across all video types.",
    "reference": "1811.10673v1-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7333333333333333,
      "phrase_overlap": 0.7333333333333333,
      "is_correct": true
    }
  },
  "1811.10673v1_q2": {
    "paper_id": "1811.10673v1",
    "question_index": 2,
    "question": "What is the purpose of the second-stage decoder $D_2$?",
    "ground_truth": "The second-stage decoder $D_2$ takes soft edges $x_G$ as input and produces reconstructed frames.",
    "predicted_answer": "E",
    "reference": "1811.10673v1-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.02040816326530612,
      "phrase_overlap": 0.02040816326530612,
      "is_correct": true
    }
  },
  "1811.10673v1_q3": {
    "paper_id": "1811.10673v1",
    "question_index": 3,
    "question": "What are the steps involved in the second encoding stage ($E_2$)?",
    "ground_truth": "The second encoding stage involves three steps: down-sampling, soft edge detection, and spatio-temporal edge map compression.",
    "predicted_answer": "The steps involved in the second encoding stage ($E_2$) are: Down sampler, Soft Edge Detector, and Spatio-temporal Lossless compression.",
    "reference": "1811.10673v1-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.735632183908046,
      "phrase_overlap": 0.735632183908046,
      "is_correct": true
    }
  },
  "1811.10673v1_q4": {
    "paper_id": "1811.10673v1",
    "question_index": 4,
    "question": "Which lossless compression scheme achieved the highest compression gain in the example shown in Figure 1?",
    "ground_truth": "The proposed scheme achieved the highest compression gain.",
    "predicted_answer": "D",
    "reference": "1811.10673v1-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.03389830508474576,
      "phrase_overlap": 0.03389830508474576,
      "is_correct": true
    }
  },
  "1811.10673v1_q5": {
    "paper_id": "1811.10673v1",
    "question_index": 5,
    "question": "How does the proposed model compare to H.264 in terms of visual quality at low bitrates?",
    "ground_truth": "The proposed model delivers significantly better visual quality at low bitrates than H.264.",
    "predicted_answer": "The proposed model provides better visual quality at lower bitrates compared to H.264, as demonstrated by higher PSNR, SSIM, MS-SSIM, and VMAF values.",
    "reference": "1811.10673v1-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5726141078838174,
      "phrase_overlap": 0.5726141078838174,
      "is_correct": true
    }
  },
  "1811.10673v1_q6": {
    "paper_id": "1811.10673v1",
    "question_index": 6,
    "question": "How does the level of quantization affect the output of the soft edge detector?",
    "ground_truth": "As the quantization level $k$ is decreased, the cardinality of colors co-located with edges decreases.",
    "predicted_answer": "The level of quantization significantly affects the output of the soft edge detector. Higher quantization levels produce detailed edges, wide gray-scale distributions, and diverse clusters, while lower levels result in highly simplified edges, binary gray-scale distributions, and single dominant clusters.\n\nB",
    "reference": "1811.10673v1-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.22926829268292684,
      "phrase_overlap": 0.22926829268292684,
      "is_correct": false
    }
  },
  "1811.10673v1_q7": {
    "paper_id": "1811.10673v1",
    "question_index": 7,
    "question": "How does the quality of the reconstructed frames change as the resolution increases?",
    "ground_truth": "The quality of the reconstructed frames increases monotonically as the resolution increases.",
    "predicted_answer": "The quality of the reconstructed frames improves as the resolution increases, as evidenced by higher PSNR, SSIM, and MS-SSIM values.",
    "reference": "1811.10673v1-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6517857142857143,
      "phrase_overlap": 0.6517857142857143,
      "is_correct": true
    }
  },
  "1811.10673v1_q8": {
    "paper_id": "1811.10673v1",
    "question_index": 8,
    "question": "How does the quality of the reconstructed frames change as the quantization level of the soft edge detector increases?",
    "ground_truth": "The quality of the reconstructed frames increases as the quantization level of the soft edge detector increases.",
    "predicted_answer": "The quality of the reconstructed frames decreases as the quantization level of the soft edge detector increases.",
    "reference": "1811.10673v1-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9821428571428571,
      "phrase_overlap": 0.9821428571428571,
      "is_correct": true
    }
  },
  "1811.10673v1_q9": {
    "paper_id": "1811.10673v1",
    "question_index": 9,
    "question": "Which quality factor improves the most as k is increased?",
    "ground_truth": "MS-SSIM",
    "predicted_answer": "E",
    "reference": "1811.10673v1-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1706.00827v2_q0": {
    "paper_id": "1706.00827v2",
    "question_index": 0,
    "question": "Which method achieved the most accurate results for simultaneous line and circle fitting?",
    "ground_truth": "Multi-X achieved the most accurate results for simultaneous line and circle fitting.",
    "predicted_answer": "Multi-X achieved the most accurate results for simultaneous line and circle fitting.",
    "reference": "1706.00827v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1706.00827v2_q1": {
    "paper_id": "1706.00827v2",
    "question_index": 1,
    "question": "Which method has the lowest average misclassification error for the cubechips image pair?",
    "ground_truth": "Multi-X",
    "predicted_answer": "Multi-X",
    "reference": "1706.00827v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1706.00827v2_q2": {
    "paper_id": "1706.00827v2",
    "question_index": 2,
    "question": "What is the difference between AdelaideRMF and Multi-H?",
    "ground_truth": "AdelaideRMF tends to assign points to more planes than Multi-H.",
    "predicted_answer": "The difference between AdelaideRMF and Multi-H lies in the color schemes used for visualizing feature correspondences: AdelaideRMF uses red and green markers, while Multi-H uses blue and green markers.",
    "reference": "1706.00827v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.17424242424242425,
      "phrase_overlap": 0.17424242424242425,
      "is_correct": false
    }
  },
  "1706.00827v2_q3": {
    "paper_id": "1706.00827v2",
    "question_index": 3,
    "question": "What is the relationship between the color of the points and the motion of the object?",
    "ground_truth": "The color of the points indicates the motion that the Multi-X algorithm assigned to each point.",
    "predicted_answer": "The relationship between the color of the points and the motion of the object is that green points represent stationary or less motion, blue points represent moderate motion, and red points represent significant motion.",
    "reference": "1706.00827v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3503184713375796,
      "phrase_overlap": 0.3503184713375796,
      "is_correct": false
    }
  },
  "1706.00827v2_q4": {
    "paper_id": "1706.00827v2",
    "question_index": 4,
    "question": "Which of the five methods tested had the lowest average misclassification error?",
    "ground_truth": "Multi-X",
    "predicted_answer": "Multi-X",
    "reference": "1706.00827v2-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1706.00827v2_q5": {
    "paper_id": "1706.00827v2",
    "question_index": 5,
    "question": "In which scenario did Multi-X perform worse than another method in terms of misclassification error for simultaneous plane and cylinder fitting?",
    "ground_truth": "Multi-X performed worse than PEARL in test case (6), with a misclassification error of 21.72% compared to PEARL's 17.35%. ",
    "predicted_answer": "Scenario (6)",
    "reference": "1706.00827v2-Table6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.15037593984962405,
      "phrase_overlap": 0.15037593984962405,
      "is_correct": true
    }
  },
  "1706.00827v2_q6": {
    "paper_id": "1706.00827v2",
    "question_index": 6,
    "question": "How does the Mean-Shift algorithm perform in the presence of outliers?",
    "ground_truth": "The Mean-Shift algorithm is robust to outliers.",
    "predicted_answer": "A",
    "reference": "1706.00827v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.041666666666666664,
      "phrase_overlap": 0.041666666666666664,
      "is_correct": true
    }
  },
  "1706.00827v2_q7": {
    "paper_id": "1706.00827v2",
    "question_index": 7,
    "question": "Which method has the lowest average misclassification error?",
    "ground_truth": "Multi-X",
    "predicted_answer": "Multi-X",
    "reference": "1706.00827v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1706.00827v2_q8": {
    "paper_id": "1706.00827v2",
    "question_index": 8,
    "question": "Which algorithm is generally faster for fitting planes and cylinders: Multi-X or T-Linkage?",
    "ground_truth": "Multi-X is generally faster for fitting planes and cylinders compared to T-Linkage.",
    "predicted_answer": "Multi-X is generally faster for fitting planes and cylinders compared to T-Linkage.",
    "reference": "1706.00827v2-Table8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1805.06431v4_q0": {
    "paper_id": "1805.06431v4",
    "question_index": 0,
    "question": "Which method appears to be the safest for autonomous driving on straight lanes with different levels of outlier vehicles?",
    "ground_truth": "ChoiceNet appears to be the safest method for autonomous driving on straight lanes, regardless of the percentage of outlier vehicles present.",
    "predicted_answer": "ChoiceNet appears to be the safest method for autonomous driving on straight lanes with different levels of outlier vehicles.",
    "reference": "1805.06431v4-Table7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.849624060150376,
      "phrase_overlap": 0.849624060150376,
      "is_correct": true
    }
  },
  "1805.06431v4_q1": {
    "paper_id": "1805.06431v4",
    "question_index": 1,
    "question": "Which of the compared methods is most likely to be the safest?",
    "ground_truth": "ChoiceNet",
    "predicted_answer": "ChoiceNet",
    "reference": "1805.06431v4-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1805.06431v4_q2": {
    "paper_id": "1805.06431v4",
    "question_index": 2,
    "question": "How does the ChoiceNet model perform on datasets with uniform corruptions?",
    "ground_truth": "The ChoiceNet model performs poorly on datasets with uniform corruptions.",
    "predicted_answer": "E",
    "reference": "1805.06431v4-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.02702702702702703,
      "phrase_overlap": 0.02702702702702703,
      "is_correct": true
    }
  },
  "1805.06431v4_q3": {
    "paper_id": "1805.06431v4",
    "question_index": 3,
    "question": "Which method performs the best when there are a lot of outliers in the data?",
    "ground_truth": "ChoiceNet.",
    "predicted_answer": "ChoiceNet performs the best when there are a lot of outliers in the data.",
    "reference": "1805.06431v4-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.24096385542168675,
      "phrase_overlap": 0.24096385542168675,
      "is_correct": false
    }
  },
  "1805.06431v4_q4": {
    "paper_id": "1805.06431v4",
    "question_index": 4,
    "question": "Which of the two approaches, density estimation or mixture of classifiers, is more robust to outliers?",
    "ground_truth": "Mixture of classifiers.",
    "predicted_answer": "Density Estimation",
    "reference": "1805.06431v4-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.24390243902439024,
      "phrase_overlap": 0.24390243902439024,
      "is_correct": false
    }
  },
  "1805.06431v4_q5": {
    "paper_id": "1805.06431v4",
    "question_index": 5,
    "question": "How does the performance of the different models change as the corruption level increases? Which model appears to be the most robust to label corruption?",
    "ground_truth": "As the corruption level increases, the performance of all models decreases. However, ChoiceNet consistently outperforms both ConvNet and ConvNet+Mixup across all corruption levels, maintaining high accuracy even when almost half of the labels are incorrect. This suggests that ChoiceNet is significantly more robust to label corruption compared to the other models.",
    "predicted_answer": "The performance of all models decreases as the corruption level increases. ChoiceNet appears to be the most robust to label corruption, consistently achieving the highest performance across all corruption levels.",
    "reference": "1805.06431v4-Table12-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.058925476603119586,
      "phrase_overlap": 0.058925476603119586,
      "is_correct": false
    }
  },
  "1805.06431v4_q6": {
    "paper_id": "1805.06431v4",
    "question_index": 6,
    "question": "Which method appears to be most robust to the presence of outliers in the training data?",
    "ground_truth": "ChoiceNet appears to be the most robust to outliers in the training data.",
    "predicted_answer": "ChoiceNet appears to be the most robust method to the presence of outliers in the training data.",
    "reference": "1805.06431v4-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.863905325443787,
      "phrase_overlap": 0.863905325443787,
      "is_correct": true
    }
  },
  "1805.06431v4_q7": {
    "paper_id": "1805.06431v4",
    "question_index": 7,
    "question": "Which method performs best when there is no label corruption (p = 0%) on the Large Movie Review dataset and how does its performance change as the corruption level increases?",
    "ground_truth": "When there is no label corruption (p = 0%), Mixup achieves the highest test accuracy of 79.77%. However, as the corruption level increases, Mixup's performance deteriorates more rapidly compared to other methods. ChoiceNet, on the other hand, demonstrates a more stable performance across different corruption levels, maintaining the highest accuracy when p is 10%, 20%, 30%, and 40%.",
    "predicted_answer": "Mixup performs best when there is no label corruption (p = 0%) with an accuracy of 79.77%. As the corruption level increases, its performance decreases steadily, reaching 69.63% at 40% corruption.",
    "reference": "1805.06431v4-Table15-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.12758620689655173,
      "phrase_overlap": 0.12758620689655173,
      "is_correct": true
    }
  },
  "1805.06431v4_q8": {
    "paper_id": "1805.06431v4",
    "question_index": 8,
    "question": "How does the accuracy of the WideResNet model compare to the ChoiceNet model on the CIFAR-10 dataset with 50% random shuffle?",
    "ground_truth": "The WideResNet model has higher accuracy than the ChoiceNet model on the CIFAR-10 dataset with 50% random shuffle.",
    "predicted_answer": "A",
    "reference": "1805.06431v4-Figure14-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.017391304347826087,
      "phrase_overlap": 0.017391304347826087,
      "is_correct": true
    }
  },
  "1805.06431v4_q9": {
    "paper_id": "1805.06431v4",
    "question_index": 9,
    "question": "Which of the four methods has the best performance in terms of average error for the step function?",
    "ground_truth": "The proposed method.",
    "predicted_answer": "Proposed",
    "reference": "1805.06431v4-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5714285714285714,
      "phrase_overlap": 0.5714285714285714,
      "is_correct": true
    }
  },
  "1805.06431v4_q10": {
    "paper_id": "1805.06431v4",
    "question_index": 10,
    "question": " How does the performance of ChoiceNet compare to other methods under different noise settings? Briefly explain the strengths and weaknesses of ChoiceNet.",
    "ground_truth": "ChoiceNet generally performs well compared to other methods, achieving the highest accuracy on both symmetric noise settings (sym-50% and sym-20%). However, it falls to second place under the Pair-45% asymmetric noise setting, indicating a weakness in handling this specific type of noise.",
    "predicted_answer": "ChoiceNet consistently outperforms other methods across all noise settings, achieving the highest accuracy in Pair-45%, Sym-50%, and Sym-20%. Strengths: High robustness to noise and consistent accuracy improvement. Weaknesses: Not explicitly mentioned in the table.",
    "reference": "1805.06431v4-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.09386281588447654,
      "phrase_overlap": 0.09386281588447654,
      "is_correct": true
    }
  },
  "1805.06431v4_q11": {
    "paper_id": "1805.06431v4",
    "question_index": 11,
    "question": "Which method generally performed better in the HalfCheetah task, ChoiceNet or MDN? How does the performance gap between these two methods change as the percentage of outliers increases?",
    "ground_truth": "ChoiceNet generally performed better than MDN in the HalfCheetah task. This is evident from the higher average returns of ChoiceNet across all outlier percentages (10%, 20%, and 30%).\n\nThe performance gap between ChoiceNet and MDN appears to decrease as the percentage of outliers increases. At 10% outliers, ChoiceNet has a significantly higher average return than MDN (2068.14 vs. 192.53). However, at 30% outliers, the difference in average return is smaller (2035.91 vs. 363.08).",
    "predicted_answer": "ChoiceNet\nA",
    "reference": "1805.06431v4-Table6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.04056795131845842,
      "phrase_overlap": 0.04056795131845842,
      "is_correct": true
    }
  },
  "1805.06431v4_q12": {
    "paper_id": "1805.06431v4",
    "question_index": 12,
    "question": "How does the performance of ChoiceNet compare to other methods under different noise settings on the CIFAR-10 dataset? Briefly explain the strengths and weaknesses of ChoiceNet. ",
    "ground_truth": "ChoiceNet generally performs well compared to other methods, achieving the highest accuracy on both symmetric noise settings (sym-50% and sym-20%). However, it falls to second place under the Pair-45% asymmetric noise setting, indicating a weakness in handling this specific type of noise.",
    "predicted_answer": "ChoiceNet在对称噪声设置（sym-50%和sym-20%）下表现出色，准确率最高。然而，在配对噪声（Pair-45%）下，Co-teaching略优于ChoiceNet。ChoiceNet的优势在于其对对称噪声的鲁棒性和高准确性，但其弱点是在处理配对噪声时效果稍逊于Co-teaching。",
    "reference": "1805.06431v4-Table14-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1415525114155251,
      "phrase_overlap": 0.1415525114155251,
      "is_correct": true
    }
  },
  "1805.06431v4_q13": {
    "paper_id": "1805.06431v4",
    "question_index": 13,
    "question": "Which method performs best at all noise levels?",
    "ground_truth": "ChoiceNet.",
    "predicted_answer": "A",
    "reference": "1805.06431v4-Figure13-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1805.06431v4_q14": {
    "paper_id": "1805.06431v4",
    "question_index": 14,
    "question": "What is the role of the Cholesky block in the ChoiceNet architecture?",
    "ground_truth": "The Cholesky block is used to decompose the covariance matrix Σk into a lower triangular matrix and its transpose. This decomposition is used to ensure that the covariance matrix is positive definite, which is a requirement for the Gaussian distribution.",
    "predicted_answer": "The Cholesky block in the ChoiceNet architecture processes intermediate computations (ρk) to generate parameters for the mixture components, including the mean (μk), variance (Σk), and mixture prior (πk).",
    "reference": "1805.06431v4-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.11790393013100436,
      "phrase_overlap": 0.11790393013100436,
      "is_correct": false
    }
  },
  "1805.06431v4_q15": {
    "paper_id": "1805.06431v4",
    "question_index": 15,
    "question": "What is the purpose of the Cholesky Block in this figure?",
    "ground_truth": "The Cholesky Block is used to distinguish abnormal patterns from normal patterns.",
    "predicted_answer": "The purpose of the Cholesky Block is to transform intermediate variables (W*, Z) and their statistical properties (mean and covariance) into a correlated weight matrix (W̃k) using the Cholesky Transform.",
    "reference": "1805.06431v4-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.29577464788732394,
      "phrase_overlap": 0.29577464788732394,
      "is_correct": false
    }
  },
  "1805.06431v4_q16": {
    "paper_id": "1805.06431v4",
    "question_index": 16,
    "question": "Is it more beneficial to use ConvNet+CN with or without Mixup when the corruption probability is 80%? Explain your reasoning.",
    "ground_truth": "ConvNet+CN with Mixup achieves a higher accuracy (75.4%) than ConvNet+CN without Mixup (65.2%) when the corruption probability is 80%.",
    "predicted_answer": "without",
    "reference": "1805.06431v4-Table13-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.09929078014184398,
      "phrase_overlap": 0.09929078014184398,
      "is_correct": true
    }
  },
  "1805.06431v4_q17": {
    "paper_id": "1805.06431v4",
    "question_index": 17,
    "question": "How does the accuracy of the Mixup method change as the level of random shuffle increases?",
    "ground_truth": "The accuracy of the Mixup method decreases as the level of random shuffle increases.",
    "predicted_answer": "The accuracy of the Mixup method decreases in test performance as the level of random shuffle increases, with significant drops at 90% and 95% shuffle levels.",
    "reference": "1805.06431v4-Figure12-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6942148760330579,
      "phrase_overlap": 0.6942148760330579,
      "is_correct": true
    }
  },
  "1812.00108v4_q0": {
    "paper_id": "1812.00108v4",
    "question_index": 0,
    "question": "How does the Multi-DPP module increase diversity within the selected time-steps?",
    "ground_truth": "The Multi-DPP module increases diversity within the selected time-steps by using a determinantal point process (DPP) to select a subset of diverse time-steps from the input sequence.",
    "predicted_answer": "B",
    "reference": "1812.00108v4-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.01092896174863388,
      "phrase_overlap": 0.01092896174863388,
      "is_correct": true
    }
  },
  "1812.00108v4_q1": {
    "paper_id": "1812.00108v4",
    "question_index": 1,
    "question": "How does the performance of the model change as the number of views increases?",
    "ground_truth": "The performance of the model generally improves as the number of views increases. For example, when the model is trained and tested on two-view data, the F1-score is 29.67. However, when the model is trained and tested on three-view data, the F1-score increases to 30.2. This suggests that the model is able to learn more effectively from data with more views.",
    "predicted_answer": "B",
    "reference": "1812.00108v4-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.00554016620498615,
      "phrase_overlap": 0.00554016620498615,
      "is_correct": true
    }
  },
  "1812.00108v4_q2": {
    "paper_id": "1812.00108v4",
    "question_index": 2,
    "question": "Which method achieved the highest F1 score on the Lobby dataset?",
    "ground_truth": "Ours-supervised achieved the highest F1 score on the Lobby dataset with a score of 93.4.",
    "predicted_answer": "Ours-supervised",
    "reference": "1812.00108v4-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2912621359223301,
      "phrase_overlap": 0.2912621359223301,
      "is_correct": true
    }
  },
  "1812.00281v3_q0": {
    "paper_id": "1812.00281v3",
    "question_index": 0,
    "question": "What is the relationship between the camera yaw angle and the silhouette distance?",
    "ground_truth": "The silhouette distance generally increases as the camera yaw angle increases.",
    "predicted_answer": "The silhouette distance is highest at -120 degrees and 60 degrees, and lowest at 0 degrees for both 'Bottom' and 'Top' categories.",
    "reference": "1812.00281v3-Figure16-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4326923076923077,
      "phrase_overlap": 0.4326923076923077,
      "is_correct": false
    }
  },
  "1812.00281v3_q1": {
    "paper_id": "1812.00281v3",
    "question_index": 1,
    "question": "How does HUMBI compare to other datasets in terms of the number of subjects?",
    "ground_truth": "HUMBI has the highest number of subjects compared to the other datasets.",
    "predicted_answer": "A",
    "reference": "1812.00281v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0273972602739726,
      "phrase_overlap": 0.0273972602739726,
      "is_correct": true
    }
  },
  "1812.00281v3_q2": {
    "paper_id": "1812.00281v3",
    "question_index": 2,
    "question": "Which dataset has the most concentrated distribution of gaze and head pose?",
    "ground_truth": "MPII-Gaze",
    "predicted_answer": "MPII dataset has the most concentrated distribution of gaze and head pose.",
    "reference": "1812.00281v3-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1927710843373494,
      "phrase_overlap": 0.1927710843373494,
      "is_correct": false
    }
  },
  "1812.00281v3_q3": {
    "paper_id": "1812.00281v3",
    "question_index": 3,
    "question": "What are the differences between the results of the monocular 3D body prediction network trained with different dataset combinations?",
    "ground_truth": "The results of the monocular 3D body prediction network trained with different dataset combinations show that the Up3d+HUMBI dataset combination produces the most accurate results. This is evident in the images where the predicted 3D body poses are closer to the ground-truth poses than the other dataset combinations.",
    "predicted_answer": "E",
    "reference": "1812.00281v3-Figure15-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1812.00281v3_q4": {
    "paper_id": "1812.00281v3",
    "question_index": 4,
    "question": "What are the different stages of HUMBI body and cloth reconstruction?",
    "ground_truth": "The different stages of HUMBI body and cloth reconstruction are: \n1. Input image of the person (Ibody)\n2. Keypoint estimation (Kbody)\n3. Occupancy map generation (Obody)\n4. Body model fitting (Mbody)\n5. Cloth model fitting (Mcloth)",
    "predicted_answer": "E",
    "reference": "1812.00281v3-Figure11-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1812.00281v3_q5": {
    "paper_id": "1812.00281v3",
    "question_index": 5,
    "question": "Which dataset performs best when used alone for training a 3D body keypoint prediction model, and how does its performance compare to models trained on combined datasets?",
    "ground_truth": "HUMBI performs best when used alone for training, with an average AUC of 0.399. While this is lower than the average AUC of models trained on combined datasets (0.433 for H36M+HUMBI and 0.413 for MI3D+HUMBI), HUMBI still achieves the highest score among the individual datasets.",
    "predicted_answer": "The HUMBI dataset performs best when used alone for training a 3D body keypoint prediction model, with an average performance of 0.399. However, models trained on combined datasets, such as H36M + HUMBI, achieve better performance, with an average of 0.433.",
    "reference": "1812.00281v3-Table7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2766355140186916,
      "phrase_overlap": 0.2766355140186916,
      "is_correct": true
    }
  },
  "1812.00281v3_q6": {
    "paper_id": "1812.00281v3",
    "question_index": 6,
    "question": "Which training data configuration resulted in the lowest prediction error for both UP-3D and HUMBI test sets?",
    "ground_truth": "Training with UP-3D + HUMBI resulted in the lowest prediction error for both UP-3D and HUMBI test sets.",
    "predicted_answer": "UP-3D+HUMBI training resulted in the lowest prediction error for both UP-3D (18.4±13.8) and HUMBI (12.5±8.4) test sets.",
    "reference": "1812.00281v3-Table8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7567567567567568,
      "phrase_overlap": 0.7567567567567568,
      "is_correct": true
    }
  },
  "1812.00281v3_q7": {
    "paper_id": "1812.00281v3",
    "question_index": 7,
    "question": "How does the number of cameras used affect the accuracy of the garment reconstruction?",
    "ground_truth": "The accuracy of the garment reconstruction increases as the number of cameras used increases.",
    "predicted_answer": "Increasing the number of cameras improves the accuracy of garment reconstruction by increasing density and reducing error.",
    "reference": "1812.00281v3-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4744186046511628,
      "phrase_overlap": 0.4744186046511628,
      "is_correct": false
    }
  },
  "1812.00281v3_q8": {
    "paper_id": "1812.00281v3",
    "question_index": 8,
    "question": "How does HUMBI capture diverse appearance of human expressions?",
    "ground_truth": "HUMBI includes 772 distinctive subjects across gender, ethnicity, age, clothing style, and physical condition, which generates diverse appearance of human expressions.",
    "predicted_answer": "HUMBI captures diverse human expressions by including a balanced gender distribution, a wide range of age groups, varied skin colors, and diverse clothing styles and hair types.",
    "reference": "1812.00281v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.22674418604651161,
      "phrase_overlap": 0.22674418604651161,
      "is_correct": false
    }
  },
  "1812.00281v3_q9": {
    "paper_id": "1812.00281v3",
    "question_index": 9,
    "question": "What is the difference between the \"median appearance\" and the \"view-specific appearance\"?",
    "ground_truth": "The median appearance is the average of all the multiview images, while the view-specific appearance is a single image that is rendered from a specific viewpoint.",
    "predicted_answer": "E",
    "reference": "1812.00281v3-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.012269938650306749,
      "phrase_overlap": 0.012269938650306749,
      "is_correct": true
    }
  },
  "1812.00281v3_q10": {
    "paper_id": "1812.00281v3",
    "question_index": 10,
    "question": "What is the purpose of the decoder in the 3D mesh prediction pipeline?",
    "ground_truth": "The decoder is responsible for generating the final 3D mesh from the intermediate representations produced by the regression network.",
    "predicted_answer": "The purpose of the decoder in the 3D mesh prediction pipeline is to convert the predicted outputs (3D mesh and viewpoint) into specific models for face (3DMM), body (SMPL), and hand (MANO).",
    "reference": "1812.00281v3-Figure12-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.32919254658385094,
      "phrase_overlap": 0.32919254658385094,
      "is_correct": true
    }
  },
  "1812.00281v3_q11": {
    "paper_id": "1812.00281v3",
    "question_index": 11,
    "question": "Which dataset provides data for **both** facial expressions and full-body motion capture, including clothing, in a natural setting (not synthesized)? ",
    "ground_truth": "HUMBI is the only dataset that provides data for both facial expressions and full-body motion capture, including clothing, in a natural setting. ",
    "predicted_answer": "HUMBI",
    "reference": "1812.00281v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.06711409395973154,
      "phrase_overlap": 0.06711409395973154,
      "is_correct": true
    }
  },
  "1706.03847v3_q0": {
    "paper_id": "1706.03847v3",
    "question_index": 0,
    "question": "How does the alpha parameter affect recommendation accuracy for different sample sizes on the CLASS dataset?",
    "ground_truth": "The alpha parameter generally increases recommendation accuracy as the sample size increases. However, the effect of alpha varies depending on the loss function used. For cross-entropy loss, alpha has a relatively small effect on accuracy. For TOP1-max loss, alpha has a larger effect on accuracy, especially for smaller sample sizes. For BPR-max loss, alpha has a very large effect on accuracy, with higher values of alpha leading to much higher accuracy.",
    "predicted_answer": "The alpha parameter affects recommendation accuracy by influencing Recall values. Lower alpha values (0.25 and 0.5) consistently achieve higher Recall across all sample sizes, while higher alpha values (0.75 and 1.0) perform worse.",
    "reference": "1706.03847v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.14556040756914118,
      "phrase_overlap": 0.14556040756914118,
      "is_correct": true
    }
  },
  "1706.03847v3_q1": {
    "paper_id": "1706.03847v3",
    "question_index": 1,
    "question": "Which combination of method and dataset achieved the highest Recall@20 score, and how much higher was it compared to the original GRU4Rec model?",
    "ground_truth": "The highest Recall@20 score was achieved by the GRU4Rec with additional samples and BPR-max loss function on the RSC15 dataset. This score was 42.37% higher than the Recall@20 score of the original GRU4Rec model on the same dataset.",
    "predicted_answer": "E",
    "reference": "1706.03847v3-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1706.03847v3_q2": {
    "paper_id": "1706.03847v3",
    "question_index": 2,
    "question": "What is the performance of GRU4Rec relative to the baseline in terms of watch time?",
    "ground_truth": "GRU4Rec has a slightly higher performance than the baseline in terms of watch time.",
    "predicted_answer": "The performance of GRU4Rec in 'next best' mode is 105.00% relative to the baseline in terms of watch time, which is higher than the 'sequence' mode at 100.00%.",
    "reference": "1706.03847v3-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4462809917355372,
      "phrase_overlap": 0.4462809917355372,
      "is_correct": true
    }
  },
  "1706.03847v3_q3": {
    "paper_id": "1706.03847v3",
    "question_index": 3,
    "question": "Which dataset contains the most interactions (events) in the training set and how much larger is it compared to the dataset with the least interactions in the training set?",
    "ground_truth": "The VIDXL dataset contains the most interactions (events) in the training set with 69,312,698 events. This is roughly 7.7 times larger than the RSC15 dataset, which has the least interactions (9,011,321) in the training set. ",
    "predicted_answer": "VIDXL\n\n60301377",
    "reference": "1706.03847v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.07563025210084033,
      "phrase_overlap": 0.07563025210084033,
      "is_correct": false
    }
  },
  "1706.03847v3_q4": {
    "paper_id": "1706.03847v3",
    "question_index": 4,
    "question": "What is the purpose of negative sampling?",
    "ground_truth": "Negative sampling is a technique used to reduce the number of negative examples that need to be considered during training.",
    "predicted_answer": "The purpose of negative sampling is to reduce computation by focusing on a subset of negative items instead of all possible outputs, enabling efficient training of models.",
    "reference": "1706.03847v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.47619047619047616,
      "phrase_overlap": 0.47619047619047616,
      "is_correct": false
    }
  },
  "1706.03847v3_q5": {
    "paper_id": "1706.03847v3",
    "question_index": 5,
    "question": "How does the training time of the different losses change as the number of additional samples increases?",
    "ground_truth": "The training time of all losses increases as the number of additional samples increases.",
    "predicted_answer": "The training time for all losses increases as the number of additional samples increases, with a significant rise for larger sample sizes, especially for the 'ALL' case.",
    "reference": "1706.03847v3-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6770428015564203,
      "phrase_overlap": 0.6770428015564203,
      "is_correct": true
    }
  },
  "1706.03847v3_q6": {
    "paper_id": "1706.03847v3",
    "question_index": 6,
    "question": "Which dataset has the highest Recall@20 and MRR@20?",
    "ground_truth": "VIDXL has the highest Recall@20 and MRR@20.",
    "predicted_answer": "VIDXL",
    "reference": "1706.03847v3-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.20833333333333334,
      "phrase_overlap": 0.20833333333333334,
      "is_correct": true
    }
  },
  "1706.03847v3_q7": {
    "paper_id": "1706.03847v3",
    "question_index": 7,
    "question": "How does the addition of negative samples affect the gradient of BPR and BPR-max with respect to the target score?",
    "ground_truth": "The addition of negative samples increases the gradient of BPR and BPR-max with respect to the target score.",
    "predicted_answer": "E",
    "reference": "1706.03847v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.01834862385321101,
      "phrase_overlap": 0.01834862385321101,
      "is_correct": true
    }
  },
  "1706.04269v2_q0": {
    "paper_id": "1706.04269v2",
    "question_index": 0,
    "question": "What are the three main components of the Action Search model architecture?",
    "ground_truth": "The three main components of the Action Search model architecture are the visual encoder, the LSTM, and the spotting target.",
    "predicted_answer": "LSTM, Visual Encoder, Spotting Target",
    "reference": "1706.04269v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.422360248447205,
      "phrase_overlap": 0.422360248447205,
      "is_correct": false
    }
  },
  "1706.04269v2_q1": {
    "paper_id": "1706.04269v2",
    "question_index": 1,
    "question": "Which method requires the fewest observations to spot an action in a video with 2.5% action coverage?",
    "ground_truth": "Action Search",
    "predicted_answer": "Action Search",
    "reference": "1706.04269v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1706.04269v2_q2": {
    "paper_id": "1706.04269v2",
    "question_index": 2,
    "question": "How does Action Search use temporal context to reason about where to search next?",
    "ground_truth": "Action Search uses temporal context to reason about where to search next by looking at the frames before and after the current frame. This allows the model to learn the temporal patterns of actions and to predict where the action is most likely to occur in the next frame.",
    "predicted_answer": "A",
    "reference": "1706.04269v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.007326007326007326,
      "phrase_overlap": 0.007326007326007326,
      "is_correct": true
    }
  },
  "1706.04269v2_q3": {
    "paper_id": "1706.04269v2",
    "question_index": 3,
    "question": "How does the training size of the Action Search model affect its performance?",
    "ground_truth": "As the training size increases, the mAP and S score of the Action Search model also improve.",
    "predicted_answer": "B",
    "reference": "1706.04269v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1706.04284v3_q0": {
    "paper_id": "1706.04284v3",
    "question_index": 0,
    "question": "What is the difference between the conventional semantic segmentation pipeline and the proposed framework for joint image denoising and semantic segmentation?",
    "ground_truth": "The conventional semantic segmentation pipeline performs semantic segmentation directly on the noisy input image, while the proposed framework first denoises the image before performing semantic segmentation.",
    "predicted_answer": "B",
    "reference": "1706.04284v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.009569377990430622,
      "phrase_overlap": 0.009569377990430622,
      "is_correct": true
    }
  },
  "1706.04284v3_q1": {
    "paper_id": "1706.04284v3",
    "question_index": 1,
    "question": " Which denoiser performs the best on the sheep image, and how can you tell?",
    "ground_truth": " The denoiser trained with the classification network and evaluated for semantic segmentation performs the best on the sheep image. This is because the segmentation label map for this denoiser is the most accurate, and it correctly identifies the sheep's body and legs. ",
    "predicted_answer": "D",
    "reference": "1706.04284v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1706.04284v3_q2": {
    "paper_id": "1706.04284v3",
    "question_index": 2,
    "question": "What is the purpose of the skip connections in the proposed denoising network?",
    "ground_truth": "The skip connections are used to combine the features from the encoding and decoding modules at each scale. This helps to preserve the spatial information that is lost during the downsampling and upsampling operations.",
    "predicted_answer": "E",
    "reference": "1706.04284v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1706.04284v3_q3": {
    "paper_id": "1706.04284v3",
    "question_index": 3,
    "question": "Which denoising method performs the best on average across all noise levels tested on the Kodak dataset?",
    "ground_truth": "The proposed method performs the best on average across all noise levels tested on the Kodak dataset.",
    "predicted_answer": "Proposed",
    "reference": "1706.04284v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.14678899082568808,
      "phrase_overlap": 0.14678899082568808,
      "is_correct": true
    }
  },
  "1812.06589v2_q0": {
    "paper_id": "1812.06589v2",
    "question_index": 0,
    "question": "How do the different methods compare in terms of their ability to generate realistic faces?",
    "ground_truth": "The baseline method generates faces that are blurry and unrealistic, while the other methods generate faces that are more realistic.",
    "predicted_answer": "The proposed method (Ours +AMIE+DA) generates the most realistic faces with enhanced detail and minimal artifacts compared to other methods.",
    "reference": "1812.06589v2-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4338235294117647,
      "phrase_overlap": 0.4338235294117647,
      "is_correct": false
    }
  },
  "1812.06589v2_q1": {
    "paper_id": "1812.06589v2",
    "question_index": 1,
    "question": "What is the effect of adding DA to the baseline method?",
    "ground_truth": "Adding DA to the baseline method improves the PSNR and SSIM values, while slightly decreasing the LMD value.",
    "predicted_answer": "Adding DA to the baseline method improves PSNR from 28.88 to 29.19 and SSIM from 0.89 to 0.90, but increases LMD from 1.36 to 1.37.",
    "reference": "1812.06589v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.602510460251046,
      "phrase_overlap": 0.602510460251046,
      "is_correct": true
    }
  },
  "1812.06589v2_q2": {
    "paper_id": "1812.06589v2",
    "question_index": 2,
    "question": "Which method performed the best on the GRID dataset?",
    "ground_truth": "AMIE (Ours)",
    "predicted_answer": "AMIE (Ours) performed the best on the GRID dataset.",
    "reference": "1812.06589v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3548387096774194,
      "phrase_overlap": 0.3548387096774194,
      "is_correct": true
    }
  },
  "1812.06589v2_q3": {
    "paper_id": "1812.06589v2",
    "question_index": 3,
    "question": "What is the role of the frame discriminator in the proposed method?",
    "ground_truth": "The frame discriminator is used to detect whether the generated frame and audio are matched or not.",
    "predicted_answer": "D",
    "reference": "1812.06589v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.02,
      "phrase_overlap": 0.02,
      "is_correct": true
    }
  },
  "1812.06589v2_q4": {
    "paper_id": "1812.06589v2",
    "question_index": 4,
    "question": "How does the dynamic attention block improve the transition of generated video for arbitrary identities?",
    "ground_truth": "The dynamic attention block decouples the lip-related and identity-related information, allowing the network to focus on the most important area for generating realistic talking faces.",
    "predicted_answer": "D",
    "reference": "1812.06589v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.010810810810810811,
      "phrase_overlap": 0.010810810810810811,
      "is_correct": true
    }
  },
  "1812.06589v2_q5": {
    "paper_id": "1812.06589v2",
    "question_index": 5,
    "question": "Which method, AMIE or MINE, produces generated frames that are closer in distribution to the real frames?",
    "ground_truth": "MINE",
    "predicted_answer": "MINE produces generated frames that are closer in distribution to the real frames.",
    "reference": "1812.06589v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.09302325581395349,
      "phrase_overlap": 0.09302325581395349,
      "is_correct": true
    }
  },
  "1812.06589v2_q6": {
    "paper_id": "1812.06589v2",
    "question_index": 6,
    "question": " What are the limitations of the Zhou \\textit{et al.} and Chen \\textit{et al.} methods for generating talking-face videos, as compared to the method proposed in the paper?",
    "ground_truth": " The Zhou \\textit{et al.} method suffers from a \"zoom-in-and-out\" effect, while the Chen \\textit{et al.} method produces lip shapes that differ from the real ones.",
    "predicted_answer": "The Zhou et al. and Chen et al. methods have limitations in visual quality and lip synchronization compared to the proposed method, which achieves better facial detail preservation and accuracy.",
    "reference": "1812.06589v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3707865168539326,
      "phrase_overlap": 0.3707865168539326,
      "is_correct": false
    }
  },
  "1812.06589v2_q7": {
    "paper_id": "1812.06589v2",
    "question_index": 7,
    "question": "Which method performed the best according to the LMD metric?",
    "ground_truth": "AMIE (Ours)",
    "predicted_answer": "AMIE (Ours) performed the best according to the LMD metric with a value of 1.21.",
    "reference": "1812.06589v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.24175824175824176,
      "phrase_overlap": 0.24175824175824176,
      "is_correct": true
    }
  },
  "1805.06447v3_q0": {
    "paper_id": "1805.06447v3",
    "question_index": 0,
    "question": "How does the performance of ITN-V2 compare to other methods when both DDT and ST transformations are applied to the CIFAR-10 dataset?",
    "ground_truth": "ITN-V2 achieves the lowest testing error (56.95%) among all methods listed when both DDT and ST transformations are applied to the CIFAR-10 dataset.",
    "predicted_answer": "E",
    "reference": "1805.06447v3-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.013422818791946308,
      "phrase_overlap": 0.013422818791946308,
      "is_correct": true
    }
  },
  "1805.06447v3_q1": {
    "paper_id": "1805.06447v3",
    "question_index": 1,
    "question": "Which method performs the best when trained with only 1% of the MNIST training data, and how much does data augmentation improve its performance in this scenario?",
    "ground_truth": "When trained with only 1% of the MNIST training data, ITN (B-CNN) (w/ DA) performs the best with a testing error of 2.78%. Data augmentation further improves its performance by 0.4%, bringing the testing error down to 2.78% from 3.18% achieved by ITN (B-CNN) without data augmentation.",
    "predicted_answer": "B/C -0.40%",
    "reference": "1805.06447v3-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.013559322033898305,
      "phrase_overlap": 0.013559322033898305,
      "is_correct": false
    }
  },
  "1805.06447v3_q2": {
    "paper_id": "1805.06447v3",
    "question_index": 2,
    "question": "How does the ITN framework generate pseudo-negative samples?",
    "ground_truth": "The ITN framework generates pseudo-negative samples by applying learned transformations to positive samples.",
    "predicted_answer": "D",
    "reference": "1805.06447v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.01834862385321101,
      "phrase_overlap": 0.01834862385321101,
      "is_correct": true
    }
  },
  "1805.06447v3_q3": {
    "paper_id": "1805.06447v3",
    "question_index": 3,
    "question": "Which method performs best on the CIFAR-10 dataset?",
    "ground_truth": "ITN (ResNet-32) with data augmentation performs best on the CIFAR-10 dataset with a testing error of 5.82%.",
    "predicted_answer": "ITN (ResNet-32) (w/DA)",
    "reference": "1805.06447v3-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.29457364341085274,
      "phrase_overlap": 0.29457364341085274,
      "is_correct": true
    }
  },
  "1805.06447v3_q4": {
    "paper_id": "1805.06447v3",
    "question_index": 4,
    "question": "Which method achieved the lowest testing error on the miniImageNet dataset?",
    "ground_truth": "ITTN (ResNet-32) (w/ DA) achieved the lowest testing error on the miniImageNet dataset with an error rate of 29.65%.",
    "predicted_answer": "ITN (ResNet-32) (w/DA)",
    "reference": "1805.06447v3-Table6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3188405797101449,
      "phrase_overlap": 0.3188405797101449,
      "is_correct": true
    }
  },
  "1805.06447v3_q5": {
    "paper_id": "1805.06447v3",
    "question_index": 5,
    "question": "Which generative model generates more accurate and realistic samples on the MNIST dataset, AC-GATN or ITN?",
    "ground_truth": "ITN generates more accurate and realistic samples on the MNIST dataset compared to AC-GATN.",
    "predicted_answer": "AC-GATN generates more accurate and realistic samples on the MNIST dataset compared to ITN.",
    "reference": "1805.06447v3-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9340659340659341,
      "phrase_overlap": 0.9340659340659341,
      "is_correct": true
    }
  },
  "1805.06447v3_q6": {
    "paper_id": "1805.06447v3",
    "question_index": 6,
    "question": "How does the quality of the generated samples change as the update threshold increases?",
    "ground_truth": "The quality of the generated samples decreases as the update threshold increases.",
    "predicted_answer": "The quality of the generated samples decreases as the update threshold increases.",
    "reference": "1805.06447v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1805.06447v3_q7": {
    "paper_id": "1805.06447v3",
    "question_index": 7,
    "question": "Describe the relationship between the update threshold (Tu) and the performance of ITN (B-CNN) on the MNIST dataset.",
    "ground_truth": "The performance of ITN (B-CNN) on the MNIST dataset decreases as the update threshold (Tu) increases. This is evident from the increasing ITN error percentages as Tu goes from 1e-3 to 1e-1.",
    "predicted_answer": "As the update threshold (Tu) increases, the ITN error on the MNIST dataset increases.",
    "reference": "1805.06447v3-Table7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.43795620437956206,
      "phrase_overlap": 0.43795620437956206,
      "is_correct": false
    }
  },
  "1805.06447v3_q8": {
    "paper_id": "1805.06447v3",
    "question_index": 8,
    "question": "Which method has the lower testing error on the MNIST task?",
    "ground_truth": "ITN",
    "predicted_answer": "ITN has the lower testing error on the MNIST task with 0.47%.",
    "reference": "1805.06447v3-Table8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.09375,
      "phrase_overlap": 0.09375,
      "is_correct": true
    }
  },
  "1805.06447v3_q9": {
    "paper_id": "1805.06447v3",
    "question_index": 9,
    "question": "Which generative model generates more accurate and realistic samples on the MNIST dataset, AC-GATN or ITN?",
    "ground_truth": "ITN generates more accurate and realistic samples on the MNIST dataset compared to AC-GATN.",
    "predicted_answer": "ITN generates more accurate and realistic samples on the MNIST dataset as it achieves lower testing error percentages across all epochs.",
    "reference": "1805.06447v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7048458149779736,
      "phrase_overlap": 0.7048458149779736,
      "is_correct": true
    }
  },
  "1805.06447v3_q10": {
    "paper_id": "1805.06447v3",
    "question_index": 10,
    "question": "Which method performs best on the TMTA task and how much does data augmentation contribute to its performance?",
    "ground_truth": "The ITN (B-CNN) method with data augmentation (DA) performs best on the TMTA task, achieving a testing error of 21.31%. Data augmentation contributes significantly to its performance, as the ITN (B-CNN) method without DA has a higher testing error of 31.67%.",
    "predicted_answer": "The ITN (B-CNN) method performs best on the TMTA task with an error rate of 21.31% when using data augmentation, which contributes to a performance improvement of 10.36% compared to ITN (B-CNN) without data augmentation.",
    "reference": "1805.06447v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.28451882845188287,
      "phrase_overlap": 0.28451882845188287,
      "is_correct": true
    }
  },
  "1805.07567v2_q0": {
    "paper_id": "1805.07567v2",
    "question_index": 0,
    "question": "Which method has the fastest convergence and highest converged performance?",
    "ground_truth": "F-DSS",
    "predicted_answer": "DSS has the fastest convergence and highest converged performance.",
    "reference": "1805.07567v2-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.08450704225352113,
      "phrase_overlap": 0.08450704225352113,
      "is_correct": false
    }
  },
  "1805.07567v2_q1": {
    "paper_id": "1805.07567v2",
    "question_index": 1,
    "question": "Which dataset would you expect to be the most challenging for a model trained on MSRA-B to perform well on, and why?",
    "ground_truth": "The DUT-OMRON dataset is likely the most challenging for a model trained on MSRA-B.",
    "predicted_answer": "The DUT-OMRON dataset would likely be the most challenging for a model trained on MSRA-B to perform well on because it has low contrast, which differs significantly from the high contrast of MSRA-B.",
    "reference": "1805.07567v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5765124555160143,
      "phrase_overlap": 0.5765124555160143,
      "is_correct": false
    }
  },
  "1805.07567v2_q2": {
    "paper_id": "1805.07567v2",
    "question_index": 2,
    "question": "Which model performs best in terms of Mean F-measure on the DUT-OMRON dataset when trained on the MB dataset? How does the FLoss variant of this model compare to its base version?",
    "ground_truth": "The DSS+FLoss model performs best in terms of Mean F-measure on the DUT-OMRON dataset when trained on the MB dataset, achieving a score of 0.755.\n\nThe FLoss variant of the DSS model shows a clear improvement over the base DSS model, with a Mean F-measure increase from 0.738 to 0.755.",
    "predicted_answer": "The DSS+FLoss model performs best in terms of Mean F-measure on the DUT-OMRON dataset when trained on the MB dataset, achieving a Mean F-measure of 0.777. The FLoss variant of DSS improves upon its base version (DSS), which has a Mean F-measure of 0.755.",
    "reference": "1805.07567v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5586592178770949,
      "phrase_overlap": 0.5586592178770949,
      "is_correct": true
    }
  },
  "1805.07567v2_q3": {
    "paper_id": "1805.07567v2",
    "question_index": 3,
    "question": "Which algorithm achieves a better balance between precision and recall?",
    "ground_truth": "F-DSS.",
    "predicted_answer": "F-DSS",
    "reference": "1805.07567v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9090909090909091,
      "phrase_overlap": 0.9090909090909091,
      "is_correct": true
    }
  },
  "1805.07567v2_q4": {
    "paper_id": "1805.07567v2",
    "question_index": 4,
    "question": "Why does the proposed FLoss method perform better than the balanced cross-entropy loss, even though both methods aim to address the data imbalance problem in salient object detection?",
    "ground_truth": "The FLoss method performs better than the balanced cross-entropy loss because it can automatically adjust to data imbalance using the F-measure criterion, while the balanced cross-entropy loss relies on pre-defined weights for positive and negative samples.",
    "predicted_answer": "The proposed FLoss method performs better than the balanced cross-entropy loss because it achieves consistently higher MaxF and MeanF scores and lower MAE values across all datasets, indicating better handling of data imbalance and improved salient object detection performance. \n\n选项字母：C",
    "reference": "1805.07567v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3173431734317343,
      "phrase_overlap": 0.3173431734317343,
      "is_correct": false
    }
  },
  "1805.07567v2_q5": {
    "paper_id": "1805.07567v2",
    "question_index": 5,
    "question": " \n\nWhat is the effect of increasing the value of β2 on the precision and recall of the model? ",
    "ground_truth": " \n\nIncreasing the value of β2 decreases the precision and increases the recall of the model.",
    "predicted_answer": "Increasing β² decreases Precision slightly, increases Recall significantly, and keeps F-measure relatively stable.",
    "reference": "1805.07567v2-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5714285714285714,
      "phrase_overlap": 0.5714285714285714,
      "is_correct": false
    }
  },
  "1805.07567v2_q6": {
    "paper_id": "1805.07567v2",
    "question_index": 6,
    "question": "Which method produces higher contrast saliency maps, FLoss or Log-FLoss?",
    "ground_truth": "FLoss",
    "predicted_answer": "Log-FLoss",
    "reference": "1805.07567v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7142857142857143,
      "phrase_overlap": 0.7142857142857143,
      "is_correct": true
    }
  },
  "1707.00524v2_q0": {
    "paper_id": "1707.00524v2",
    "question_index": 0,
    "question": "Which model achieved the highest performance score on the Breakout game?",
    "ground_truth": "A3C-CTS",
    "predicted_answer": "A3C-CTS",
    "reference": "1707.00524v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1707.00524v2_q1": {
    "paper_id": "1707.00524v2",
    "question_index": 1,
    "question": "Which game has the highest code loss in phase 2?",
    "ground_truth": "Pacman",
    "predicted_answer": "Frostbite",
    "reference": "1707.00524v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1707.00524v2_q2": {
    "paper_id": "1707.00524v2",
    "question_index": 2,
    "question": "What is the difference between the predicted frame and the reconstructed frame for each task domain?",
    "ground_truth": "The predicted frame is generated by the prediction model, while the reconstructed frame is generated by the autoencoder. The predicted frame is typically more accurate than the reconstructed frame, as the prediction model is trained to predict the future state of the environment, while the autoencoder is only trained to reconstruct the input image.",
    "predicted_answer": "The difference between the predicted frame and the reconstructed frame for each task domain lies in the level of detail and clarity, with reconstructed frames generally showing more distortion or blurring compared to predicted frames.",
    "reference": "1707.00524v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.04794520547945205,
      "phrase_overlap": 0.04794520547945205,
      "is_correct": false
    }
  },
  "1707.00524v2_q3": {
    "paper_id": "1707.00524v2",
    "question_index": 3,
    "question": "What is the difference between the encoder and decoder networks in the action-conditional prediction model?",
    "ground_truth": "The encoder network takes a one-hot action and the current state as input and outputs a latent representation of the state. The decoder network takes the latent representation and outputs a prediction of the next state.",
    "predicted_answer": "A",
    "reference": "1707.00524v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1812.10735v2_q0": {
    "paper_id": "1812.10735v2",
    "question_index": 0,
    "question": "What is the difference between an overlapping case and an error case?",
    "ground_truth": "An overlapping case is when multiple aspects share the same opinion snippet, while an error case is when the model incorrectly identifies an aspect or opinion.",
    "predicted_answer": "The difference is that overlapping cases correctly identify multiple aspects within the same sentence, while error cases fail to accurately associate the sentiment or aspect with the correct context.",
    "reference": "1812.10735v2-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5586592178770949,
      "phrase_overlap": 0.5586592178770949,
      "is_correct": false
    }
  },
  "1812.10735v2_q1": {
    "paper_id": "1812.10735v2",
    "question_index": 1,
    "question": "Which dataset has a higher proportion of sentences containing multiple aspects: Rest14 or Rest15?",
    "ground_truth": "Rest14 has a higher proportion of sentences containing multiple aspects compared to Rest15.",
    "predicted_answer": "Rest15",
    "reference": "1812.10735v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.12371134020618557,
      "phrase_overlap": 0.12371134020618557,
      "is_correct": true
    }
  },
  "1812.10735v2_q2": {
    "paper_id": "1812.10735v2",
    "question_index": 2,
    "question": "Why do you think the performance of all models is generally lower on Rest15 compared to Rest14?",
    "ground_truth": "The performance of all models is generally lower on Rest15 because it has a larger number of aspect categories (13) compared to Rest14 (5). This increased complexity makes it more challenging for the models to accurately identify and classify the aspects.",
    "predicted_answer": "The performance of all models is generally lower on Rest15 compared to Rest14 likely due to differences in dataset characteristics, such as data quality, annotation consistency, or complexity of the tasks in Rest15.",
    "reference": "1812.10735v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.34893617021276596,
      "phrase_overlap": 0.34893617021276596,
      "is_correct": true
    }
  },
  "1812.10735v2_q3": {
    "paper_id": "1812.10735v2",
    "question_index": 3,
    "question": "Which model performed best on the Rest15 dataset for binary classification, and how does its performance compare to the best model for 3-way classification on the same dataset?",
    "ground_truth": "For binary classification on the Rest15 dataset, M-CAN-2$R_o$ achieved the highest performance with an accuracy of 82.14% and Macro-F1 of 81.58%. In comparison, the best performing model for 3-way classification on Rest15 was M-CAN-2$R_s$, achieving an accuracy of 78.22% and Macro-F1 of 55.80%. This indicates that M-CAN-2$R_o$ performed better in both accuracy and Macro-F1 for binary classification compared to the best model for 3-way classification on the same dataset.",
    "predicted_answer": "The model M-CAN-2Ro performed best on the Rest15 dataset for binary classification with an accuracy of 82.14 and F1 score of 81.58. Compared to the best model for 3-way classification (M-CAN-2Rs), which achieved an accuracy of 78.22 and F1 score of 55.80, M-CAN-2Ro shows significantly better performance.",
    "reference": "1812.10735v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.31065468549422337,
      "phrase_overlap": 0.31065468549422337,
      "is_correct": true
    }
  },
  "1812.10735v2_q4": {
    "paper_id": "1812.10735v2",
    "question_index": 4,
    "question": "What are the two main tasks that the CAN network is designed to perform?",
    "ground_truth": "Aspect-level sentiment classification (ALSC) and aspect category detection (ACD).",
    "predicted_answer": "A",
    "reference": "1812.10735v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.024390243902439025,
      "phrase_overlap": 0.024390243902439025,
      "is_correct": true
    }
  },
  "1707.01917v2_q0": {
    "paper_id": "1707.01917v2",
    "question_index": 0,
    "question": "What does the induced schema Win <A4, B3, C2> represent?",
    "ground_truth": "The induced schema Win <A4, B3, C2> represents the fact that player A4 won tournament C2, defeating player B3.",
    "predicted_answer": "The induced schema Win <A4, B3, C2> represents a relationship where player A4 (e.g., Federer, Djokovic, Murray, ...) wins against opponent B3 (e.g., Nadal, Roddick, Nalbandian, ...) in tournament C2 (e.g., Wimbledon, Rogers Cup, Roland Garros, ...).",
    "reference": "1707.01917v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.479108635097493,
      "phrase_overlap": 0.479108635097493,
      "is_correct": true
    }
  },
  "1707.01917v2_q1": {
    "paper_id": "1707.01917v2",
    "question_index": 1,
    "question": "Which dataset has the highest value for the hyperparameter  λa?",
    "ground_truth": "The NYT Sports dataset has the highest value for λa (0.9).",
    "predicted_answer": "NYT Sports",
    "reference": "1707.01917v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.29411764705882354,
      "phrase_overlap": 0.29411764705882354,
      "is_correct": true
    }
  },
  "1707.01917v2_q2": {
    "paper_id": "1707.01917v2",
    "question_index": 2,
    "question": "What is the shape of the tensor $x^1$ for the Shootings dataset?",
    "ground_truth": "The shape of the tensor $x^1$ for the Shootings dataset is 3365 x 1295 x 50.",
    "predicted_answer": "A",
    "reference": "1707.01917v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.025974025974025976,
      "phrase_overlap": 0.025974025974025976,
      "is_correct": true
    }
  },
  "1707.01917v2_q3": {
    "paper_id": "1707.01917v2",
    "question_index": 3,
    "question": "What is the definition of a non-negative tensor?",
    "ground_truth": "A non-negative tensor is a tensor whose elements are all non-negative real numbers.",
    "predicted_answer": "A",
    "reference": "1707.01917v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.023809523809523808,
      "phrase_overlap": 0.023809523809523808,
      "is_correct": true
    }
  },
  "1707.01917v2_q4": {
    "paper_id": "1707.01917v2",
    "question_index": 4,
    "question": "What is the role of OpenIE in Step 1 of TFBA?",
    "ground_truth": "OpenIE is used to extract tuples from the unlabeled text corpus. These tuples are then used to create the 3-mode tensors X1, X2, and X3.",
    "predicted_answer": "The role of OpenIE in Step 1 of TFBA is to extract structured tuples from an unlabeled text corpus, which are then used as input for tensor-based analysis.",
    "reference": "1707.01917v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6254295532646048,
      "phrase_overlap": 0.6254295532646048,
      "is_correct": true
    }
  },
  "1707.01917v2_q5": {
    "paper_id": "1707.01917v2",
    "question_index": 5,
    "question": "Which method achieves the highest accuracy on the Shootings dataset?",
    "ground_truth": "TFBA",
    "predicted_answer": "TFBA",
    "reference": "1707.01917v2-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1707.00189v3_q0": {
    "paper_id": "1707.00189v3",
    "question_index": 0,
    "question": "Which model performs best when trained on the NYT dataset and evaluated on the WT14 dataset, and how does its performance compare to the baselines?",
    "ground_truth": "The Conv-KNRM model performs best when trained on the NYT dataset and evaluated on the WT14 dataset, achieving an nDCG@20 score of 0.3215. This performance is significantly better than all the baselines: BM25 (B), WT10 (W), and AOL (A).",
    "predicted_answer": "Conv-KNRM_NYT",
    "reference": "1707.00189v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0963855421686747,
      "phrase_overlap": 0.0963855421686747,
      "is_correct": false
    }
  },
  "1706.08146v3_q0": {
    "paper_id": "1706.08146v3",
    "question_index": 0,
    "question": "What is the relationship between projection sparsity and normalized reconstruction error?",
    "ground_truth": "The normalized reconstruction error decreases as the projection sparsity increases, up to a certain point. After that, the error starts to increase again.",
    "predicted_answer": "The normalized reconstruction error decreases as projection sparsity increases up to p=6, after which it stabilizes and slightly increases.",
    "reference": "1706.08146v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7713310580204779,
      "phrase_overlap": 0.7713310580204779,
      "is_correct": true
    }
  },
  "1706.08146v3_q1": {
    "paper_id": "1706.08146v3",
    "question_index": 1,
    "question": "What is the relationship between compression factor and reconstruction error?",
    "ground_truth": "The reconstruction error increases as the compression factor increases.",
    "predicted_answer": "A",
    "reference": "1706.08146v3-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.027777777777777776,
      "phrase_overlap": 0.027777777777777776,
      "is_correct": true
    }
  },
  "1706.08146v3_q2": {
    "paper_id": "1706.08146v3",
    "question_index": 2,
    "question": "What are the three steps involved in compressed matrix factorization?",
    "ground_truth": "The three steps involved in compressed matrix factorization are: \n\n1. Compress the full data matrix M to obtain a compressed matrix M̃. \n2. Factorize M̃ to obtain matrices W̃ and H̃. \n3. Approximate the left factor of M via sparse recovery on each column of W̃.",
    "predicted_answer": "The three steps involved in compressed matrix factorization are: (i) Compress M into M_tilde, (ii) Factorize M_tilde into W_tilde and H_tilde, (iii) Reconstruct W_hat from W_tilde.",
    "reference": "1706.08146v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.37528604118993136,
      "phrase_overlap": 0.37528604118993136,
      "is_correct": false
    }
  },
  "1706.08146v3_q3": {
    "paper_id": "1706.08146v3",
    "question_index": 3,
    "question": "Which dataset would likely benefit the most from using the Fac.-Recover approach instead of Recover-Fac. in terms of computational efficiency?",
    "ground_truth": "The Leukemia dataset would likely benefit the most from using the Fac.-Recover approach.",
    "predicted_answer": "Leukemia",
    "reference": "1706.08146v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.16666666666666666,
      "phrase_overlap": 0.16666666666666666,
      "is_correct": true
    }
  },
  "1706.08146v3_q4": {
    "paper_id": "1706.08146v3",
    "question_index": 4,
    "question": "What is the effect of increasing the projection dimension d on the approximation error for sparse PCA and NMF?",
    "ground_truth": "Increasing the projection dimension d decreases the approximation error for both sparse PCA and NMF.",
    "predicted_answer": "E",
    "reference": "1706.08146v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.019801980198019802,
      "phrase_overlap": 0.019801980198019802,
      "is_correct": true
    }
  },
  "1706.08146v3_q5": {
    "paper_id": "1706.08146v3",
    "question_index": 5,
    "question": "Which method achieves lower approximation error when the compression factor is greater than 3?",
    "ground_truth": "Factorize-Recover",
    "predicted_answer": "FR",
    "reference": "1706.08146v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.21052631578947367,
      "phrase_overlap": 0.21052631578947367,
      "is_correct": false
    }
  },
  "1901.00056v2_q0": {
    "paper_id": "1901.00056v2",
    "question_index": 0,
    "question": "What is the range of values for the context number hyperparameter?",
    "ground_truth": "The range of values for the context number hyperparameter is from 1 to 20.",
    "predicted_answer": "The range of values for the context number hyperparameter (P) is {1, 3, 5, 10, 15, 20}.",
    "reference": "1901.00056v2-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.84472049689441,
      "phrase_overlap": 0.84472049689441,
      "is_correct": true
    }
  },
  "1901.00056v2_q1": {
    "paper_id": "1901.00056v2",
    "question_index": 1,
    "question": "What is the effect of increasing the margin on the AUC and MAP values?",
    "ground_truth": "The AUC and MAP values initially increase with increasing margin, but then decrease after a certain point.",
    "predicted_answer": "E",
    "reference": "1901.00056v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.018691588785046728,
      "phrase_overlap": 0.018691588785046728,
      "is_correct": true
    }
  },
  "1901.00056v2_q2": {
    "paper_id": "1901.00056v2",
    "question_index": 2,
    "question": "Which model and training objective combination performs best on the PubMed + UMLS dataset, and how does it compare to the DPE baseline?",
    "ground_truth": "The SYNONYMNET(Pairwise) model with Leaky Unit performs best on the PubMed + UMLS dataset, achieving an AUC of 0.9838 and a MAP of 0.9872. This is a statistically significant improvement over the DPE baseline, which achieved an AUC of 0.9513 and a MAP of 0.9623.",
    "predicted_answer": "SYNONYMNET (Triplet)",
    "reference": "1901.00056v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.014184397163120567,
      "phrase_overlap": 0.014184397163120567,
      "is_correct": false
    }
  },
  "1901.00056v2_q3": {
    "paper_id": "1901.00056v2",
    "question_index": 3,
    "question": "Which dataset has the most entities?",
    "ground_truth": "MedBook + MKG",
    "predicted_answer": "MedBook + MKG",
    "reference": "1901.00056v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1901.00056v2_q4": {
    "paper_id": "1901.00056v2",
    "question_index": 4,
    "question": "What are the four steps involved in the synonym discovery process using {\\modelname}?",
    "ground_truth": " The four steps involved in the synonym discovery process are: \n\n1. **Entity representation learning:** Learn entity representations from the corpus using WEMBED.\n2. **NN search:** Perform a nearest neighbor search to find candidate entities for the query entity.\n3. **Synonym score calculation:** Calculate the synonym score between the query entity and each candidate entity using SYNONYM NET.\n4. **Synonym entity discovery:** Select the candidate entities with the highest synonym scores as the discovered synonym entities.",
    "predicted_answer": "A",
    "reference": "1901.00056v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1901.00056v2_q5": {
    "paper_id": "1901.00056v2",
    "question_index": 5,
    "question": "What is the role of the Leaky Unit in the SYNONYMNET model?",
    "ground_truth": "The Leaky Unit helps to aggregate the context information from different sources and allows the model to learn the relationships between entities and their contexts.",
    "predicted_answer": "The Leaky Unit in the SYNONYMNET model plays a role in bilateral matching by enabling information flow between different contexts, improving the model's understanding of synonyms.",
    "reference": "1901.00056v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.45348837209302323,
      "phrase_overlap": 0.45348837209302323,
      "is_correct": false
    }
  },
  "1901.00056v2_q6": {
    "paper_id": "1901.00056v2",
    "question_index": 6,
    "question": "Which dataset and evaluation metric combination shows the largest performance gap between the best performing model and the baseline model DPE?",
    "ground_truth": "The largest performance gap is observed in the PubMed + UMLS dataset using the F1@K metric with K=1.",
    "predicted_answer": "PubMed + UMLS",
    "reference": "1901.00056v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.23008849557522124,
      "phrase_overlap": 0.23008849557522124,
      "is_correct": true
    }
  },
  "1805.08465v3_q0": {
    "paper_id": "1805.08465v3",
    "question_index": 0,
    "question": "Which steganography method achieves the best performance in terms of distortion for both cover and secret images when embedding 2 bits per channel?",
    "ground_truth": "The proposed method achieves the best performance for both cover and secret images when embedding 2 bits per channel.",
    "predicted_answer": "Ours",
    "reference": "1805.08465v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.049586776859504134,
      "phrase_overlap": 0.049586776859504134,
      "is_correct": false
    }
  },
  "1707.01922v5_q0": {
    "paper_id": "1707.01922v5",
    "question_index": 0,
    "question": "What is the difference between testing domain adaptation and testing sensor fusion?",
    "ground_truth": "In testing domain adaptation, the source and target CNNs are trained on different domains, and the joint classifier is used to predict the class of the target data. In testing sensor fusion, the source and target CNNs are trained on the same domain, and the joint classifier is used to predict the class of the target data using both the source and target data.",
    "predicted_answer": "The difference is that testing domain adaptation adapts classifiers to different domains (source or target), while testing sensor fusion combines information from multiple domains into a joint representation for prediction.",
    "reference": "1707.01922v5-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.13356164383561644,
      "phrase_overlap": 0.13356164383561644,
      "is_correct": false
    }
  },
  "1707.01922v5_q1": {
    "paper_id": "1707.01922v5",
    "question_index": 1,
    "question": "Which method performs better in terms of classification accuracy?",
    "ground_truth": "ZDDA3",
    "predicted_answer": "ZDDA3",
    "reference": "1707.01922v5-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1707.01922v5_q2": {
    "paper_id": "1707.01922v5",
    "question_index": 2,
    "question": "What is the role of the task-irrelevant data in ZDDA?",
    "ground_truth": "The task-irrelevant data is used to simulate the RGB representation using the gray scale image. This allows ZDDA to learn a joint network that can be used to classify digits in both the gray scale and RGB domains.",
    "predicted_answer": "E",
    "reference": "1707.01922v5-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1707.01922v5_q3": {
    "paper_id": "1707.01922v5",
    "question_index": 3,
    "question": "What is the key difference between ZDDA and UDA/MVL in terms of the available training data?",
    "ground_truth": "The key difference lies in the availability of target-domain training data. While UDA and MVL methods require T-R training data from the target domain, ZDDA does not. ZDDA only requires T-R training data from a single source domain.",
    "predicted_answer": "The key difference between ZDDA and UDA/MVL is that ZDDA does not use target-domain training data, while UDA and MVL do.",
    "reference": "1707.01922v5-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.20454545454545456,
      "phrase_overlap": 0.20454545454545456,
      "is_correct": false
    }
  },
  "1707.01922v5_q4": {
    "paper_id": "1707.01922v5",
    "question_index": 4,
    "question": "If you are performing domain adaptation with ZDDA using AlexNet as the base network architecture and $D_F$ as the target domain, which layers of the network would be considered part of the source CNN and which would be part of the source classifier?",
    "ground_truth": "In this scenario, the source CNN would consist of the AlexNet architecture up to and including the \"fc7\" layer. The remaining layers of AlexNet would then be used as the source classifier.",
    "predicted_answer": "For AlexNet, layers up to 26 are part of the source CNN, and fc7 is part of the source classifier.",
    "reference": "1707.01922v5-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4125874125874126,
      "phrase_overlap": 0.4125874125874126,
      "is_correct": true
    }
  },
  "1707.01922v5_q5": {
    "paper_id": "1707.01922v5",
    "question_index": 5,
    "question": "Which of the following statements about the training procedure of ZDDA is true?\n\n(a) ZDDA simulates the target-domain representation using the source-domain data. (b) ZDDA builds a joint network with the supervision from the target domain. (c) ZDDA trains a sensor fusion network in step 1. (d) ZDDA trains a sensor fusion network in step 2.",
    "ground_truth": "(a) ZDDA simulates the target-domain representation using the source-domain data.",
    "predicted_answer": "A",
    "reference": "1707.01922v5-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.024390243902439025,
      "phrase_overlap": 0.024390243902439025,
      "is_correct": true
    }
  },
  "1901.00398v2_q0": {
    "paper_id": "1901.00398v2",
    "question_index": 0,
    "question": " Which type of review was more accurately identified by the human evaluators, human-written or machine-generated? ",
    "ground_truth": "The human evaluators were more accurate at identifying human-written reviews than machine-generated reviews.",
    "predicted_answer": "Human-written",
    "reference": "1901.00398v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.21487603305785125,
      "phrase_overlap": 0.21487603305785125,
      "is_correct": true
    }
  },
  "1901.00398v2_q1": {
    "paper_id": "1901.00398v2",
    "question_index": 1,
    "question": "Which type of generator generally produced reviews that were most easily identified as machine-generated by the meta-adversarial evaluators?",
    "ground_truth": "MLE SeqGAN and Word LSTM with temperature 1.0.",
    "predicted_answer": "MLE SeqGAN",
    "reference": "1901.00398v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.35714285714285715,
      "phrase_overlap": 0.35714285714285715,
      "is_correct": true
    }
  },
  "1901.00398v2_q2": {
    "paper_id": "1901.00398v2",
    "question_index": 2,
    "question": "What is the task that the AMT workers are being asked to do?",
    "ground_truth": "The AMT workers are being asked to decide whether each of twenty one paragraphs extracted from product reviews is real (written by a person) or fake (written by a computer algorithm).",
    "predicted_answer": "The task is to decide if reviews are real (written by a person) or fake (written by a computer algorithm).",
    "reference": "1901.00398v2-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6851211072664359,
      "phrase_overlap": 0.6851211072664359,
      "is_correct": true
    }
  },
  "1805.08751v2_q0": {
    "paper_id": "1805.08751v2",
    "question_index": 0,
    "question": "What is the role of the GDU and HFLU modules in the FAKEDETECTOR framework?",
    "ground_truth": "The GDU (Gated Dilated Unit) modules are responsible for extracting features from the input data, while the HFLU (Hybrid Feature Learning Unit) modules are responsible for fusing the features extracted by the GDU modules.",
    "predicted_answer": "GDU负责决策，HFLU处理输入数据，在FAKEDETECTOR框架中。根据格式要求，输出选项字母：\n\nB",
    "reference": "1805.08751v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.014652014652014652,
      "phrase_overlap": 0.014652014652014652,
      "is_correct": false
    }
  },
  "1707.06320v2_q0": {
    "paper_id": "1707.06320v2",
    "question_index": 0,
    "question": "Which model performs the best for caption retrieval in terms of R@1 and MEDR? Briefly explain why the performance might be better than other models.",
    "ground_truth": "Cap2Img performs the best for caption retrieval in terms of both R@1 (27.1) and MEDR (4.0). This suggests that the model is more successful at retrieving the most relevant caption for a given image compared to other models.",
    "predicted_answer": "Cap2Img",
    "reference": "1707.06320v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.06086956521739131,
      "phrase_overlap": 0.06086956521739131,
      "is_correct": true
    }
  },
  "1707.06320v2_q1": {
    "paper_id": "1707.06320v2",
    "question_index": 1,
    "question": "Which type of grounding appears to be most beneficial for the MRPC task, and how does its performance compare to the baseline model (ST-LN)?",
    "ground_truth": "GroundSent-Cap appears to be most beneficial for the MRPC task, achieving an accuracy of 72.9/82.2 compared to the baseline model ST-LN's 69.6/81.2.",
    "predicted_answer": "GroundSent-Cap",
    "reference": "1707.06320v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1728395061728395,
      "phrase_overlap": 0.1728395061728395,
      "is_correct": true
    }
  },
  "1707.06320v2_q2": {
    "paper_id": "1707.06320v2",
    "question_index": 2,
    "question": "Which model performs best on the SNLI dataset, and how much does grounding contribute to its performance compared to the baseline STb-1024 model?",
    "ground_truth": "The GroundSent-Both model performs best on the SNLI dataset, achieving an accuracy of 72.0%. Grounding contributes to an improvement of 4.7% compared to the baseline STb-1024 model, which achieves 67.3%.",
    "predicted_answer": "A",
    "reference": "1707.06320v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1707.06320v2_q3": {
    "paper_id": "1707.06320v2",
    "question_index": 3,
    "question": "How do the word embeddings learned by the Cap2Img model compare to the original GloVe embeddings in terms of semantic similarity?",
    "ground_truth": "The word embeddings learned by the Cap2Img model outperform the original GloVe embeddings in terms of semantic similarity.",
    "predicted_answer": "The word embeddings learned by the Cap2Img model show higher semantic similarity scores compared to the original GloVe embeddings across all metrics (MEN, SimLex, RW, W353).",
    "reference": "1707.06320v2-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6779661016949152,
      "phrase_overlap": 0.6779661016949152,
      "is_correct": true
    }
  },
  "1707.06320v2_q4": {
    "paper_id": "1707.06320v2",
    "question_index": 4,
    "question": "What is the role of the \"max\" function in the model architecture?",
    "ground_truth": "The \"max\" function is used to select the most probable word at each time step in the decoding process.",
    "predicted_answer": "The 'max' function in the model architecture is used to combine features from different components or sequences to create an aggregated feature representation.",
    "reference": "1707.06320v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3065134099616858,
      "phrase_overlap": 0.3065134099616858,
      "is_correct": false
    }
  },
  "1707.08608v3_q0": {
    "paper_id": "1707.08608v3",
    "question_index": 0,
    "question": "Which genre in the SRL-NW network has the lowest failure rate and how does its inference time compare to other genres within the same network?",
    "ground_truth": "The PT genre within the SRL-NW network has the lowest failure rate at 10.01%. Its inference time is also the lowest across all genres in the SRL-NW network for all three inference procedures (Viterbi, GBI, and A*).",
    "predicted_answer": "PT\n\nB",
    "reference": "1707.08608v3-Table11-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.03669724770642202,
      "phrase_overlap": 0.03669724770642202,
      "is_correct": false
    }
  },
  "1707.08608v3_q1": {
    "paper_id": "1707.08608v3",
    "question_index": 1,
    "question": "Which genre shows the **largest absolute improvement** in F1 score on the failure set after applying GBI for **both** syntactic parsing and SRL?",
    "ground_truth": "Pivot Corpus (PT) shows the largest absolute improvement in F1 score on the failure set after applying GBI for both syntactic parsing and SRL.",
    "predicted_answer": "Pivot Corpus (PT)",
    "reference": "1707.08608v3-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2138364779874214,
      "phrase_overlap": 0.2138364779874214,
      "is_correct": true
    }
  },
  "1707.08608v3_q2": {
    "paper_id": "1707.08608v3",
    "question_index": 2,
    "question": "How did the accuracy of the model change as the iterations progressed?",
    "ground_truth": "The accuracy of the model increased from 66.7% to 100% as the iterations progressed.",
    "predicted_answer": "The accuracy remained constant at 66.7% for the first three iterations and then increased to 100% in the final iteration.",
    "reference": "1707.08608v3-Table9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4878048780487805,
      "phrase_overlap": 0.4878048780487805,
      "is_correct": true
    }
  },
  "1707.08608v3_q3": {
    "paper_id": "1707.08608v3",
    "question_index": 3,
    "question": "What is the effect of enforcing syntactic constraints on the semantic role labeling output?",
    "ground_truth": "Enforcing syntactic constraints can correct the number of agreeing spans, and also change the semantic roles assigned to tokens.",
    "predicted_answer": "Enforcing syntactic constraints improves semantic role labeling accuracy from 50% to 100% and reduces loss to 0.",
    "reference": "1707.08608v3-Table7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.49166666666666664,
      "phrase_overlap": 0.49166666666666664,
      "is_correct": false
    }
  },
  "1707.08608v3_q4": {
    "paper_id": "1707.08608v3",
    "question_index": 4,
    "question": "What is the relationship between the number of shifts and the accuracy of the output?",
    "ground_truth": "The accuracy of the output increases as the number of shifts increases.",
    "predicted_answer": "The accuracy of the output increases with the number of iterations, reaching 100% at iteration 12, while the loss decreases to 0.",
    "reference": "1707.08608v3-Table8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.67,
      "phrase_overlap": 0.67,
      "is_correct": true
    }
  },
  "1707.08608v3_q5": {
    "paper_id": "1707.08608v3",
    "question_index": 5,
    "question": "Which inference method consistently leads to the highest F1 score on the failure set across all three networks (Net3, Net4, and Net5)?",
    "ground_truth": "Beam search with a width of 9 consistently leads to the highest F1 score on the failure set across all three networks.",
    "predicted_answer": "Beam 9",
    "reference": "1707.08608v3-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0967741935483871,
      "phrase_overlap": 0.0967741935483871,
      "is_correct": true
    }
  },
  "1707.08608v3_q6": {
    "paper_id": "1707.08608v3",
    "question_index": 6,
    "question": "How does GBI compare to A* in terms of reducing disagreement rate on the SRL-100 network's failure set?",
    "ground_truth": "GBI is more effective than A* in reducing the disagreement rate on the SRL-100 network's failure set. After applying GBI, the average disagreement rate drops to 24.92%, while A* only reduces it to 33.91%. This represents an 19.93% greater reduction in disagreement rate when using GBI compared to A*.",
    "predicted_answer": "GBI reduces the disagreement rate on the SRL-100 network's failure set from 44.85% to 24.92%, while A* reduces it to 33.91%. Thus, GBI is more effective in reducing disagreement rate compared to A*.",
    "reference": "1707.08608v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5180722891566265,
      "phrase_overlap": 0.5180722891566265,
      "is_correct": true
    }
  },
  "1809.00263v5_q0": {
    "paper_id": "1809.00263v5",
    "question_index": 0,
    "question": "What is the value of the learning rate α for the BAIR dataset?",
    "ground_truth": "0.0002",
    "predicted_answer": "0.0002",
    "reference": "1809.00263v5-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1809.00263v5_q1": {
    "paper_id": "1809.00263v5",
    "question_index": 1,
    "question": "Which feature has the highest dimensionality in the first two dimensions?",
    "ground_truth": "All features have the same dimensionality in the first two dimensions.",
    "predicted_answer": "All features have the same dimensionality in the first two dimensions (value 4).",
    "reference": "1809.00263v5-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9333333333333333,
      "phrase_overlap": 0.9333333333333333,
      "is_correct": true
    }
  },
  "1809.00263v5_q2": {
    "paper_id": "1809.00263v5",
    "question_index": 2,
    "question": "What is the relationship between the length of the interval and the uncertainty in the generated frames?",
    "ground_truth": "The uncertainty in the generated frames increases with the length of the interval.",
    "predicted_answer": "The relationship is that as the length of the interval increases, the uncertainty in the generated frames also increases.",
    "reference": "1809.00263v5-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.49261083743842365,
      "phrase_overlap": 0.49261083743842365,
      "is_correct": false
    }
  },
  "1809.00263v5_q3": {
    "paper_id": "1809.00263v5",
    "question_index": 3,
    "question": "Explain the likely reason why the \"SDVI loss term 1&3\" model performs worse than the full SDVI model in terms of PSNR and SSIM across all datasets.",
    "ground_truth": "The \"SDVI loss term 1&3\" model only uses the pixel reconstruction loss and the inclusive KL divergence loss, while the full SDVI model additionally incorporates the pixel prediction loss and the exclusive KL divergence loss. According to the passage, the exclusive KL divergence term encourages the inference distribution to be more accurate, while the pixel prediction loss further improves video quality during inference. Therefore, the absence of these terms in the \"SDVI loss term 1&3\" model likely explains its inferior performance compared to the full SDVI model.",
    "predicted_answer": "E",
    "reference": "1809.00263v5-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1809.00263v5_q4": {
    "paper_id": "1809.00263v5",
    "question_index": 4,
    "question": "What is the difference between the Inference module and the Posterior module?",
    "ground_truth": "The Inference module takes the previous frame (Xt-1) and the dynamic constraint (ĥt) as input, while the Posterior module takes the current frame (Xt) as input. This means that the Inference module is trying to predict the next frame based on the previous frame and the dynamic constraint, while the Posterior module is trying to reconstruct the current frame.",
    "predicted_answer": "The Inference Module uses the previous time step data (Xt-1) and hidden state (ht-1) to generate Pinfr, while the Posterior Module uses the current time step data (Xt) and hidden state (ht) to generate Ppst.",
    "reference": "1809.00263v5-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.323943661971831,
      "phrase_overlap": 0.323943661971831,
      "is_correct": true
    }
  },
  "1809.00263v5_q5": {
    "paper_id": "1809.00263v5",
    "question_index": 5,
    "question": "Which method generates the best moving objects?",
    "ground_truth": "SDVI",
    "predicted_answer": "SDVI best SSIM",
    "reference": "1809.00263v5-Figure16-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4444444444444444,
      "phrase_overlap": 0.4444444444444444,
      "is_correct": true
    }
  },
  "1809.00263v5_q6": {
    "paper_id": "1809.00263v5",
    "question_index": 6,
    "question": "What is the effect of the sliding tendency of SepConv on the generated images?",
    "ground_truth": "The sliding tendency of SepConv will cause motion errors and high LMS.",
    "predicted_answer": "The sliding tendency of SepConv causes misalignment and distortion in the generated images compared to the ground truth. \n\n选项字母：N/A（此题不是选择题，而是解释题） \n\n根据题目要求只输出最终选项字母的情况下，本题答案为：\nN/A",
    "reference": "1809.00263v5-Figure13-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4,
      "phrase_overlap": 0.4,
      "is_correct": false
    }
  },
  "1809.00263v5_q7": {
    "paper_id": "1809.00263v5",
    "question_index": 7,
    "question": "What is the role of the residual connections in the RBConvLSTM network?",
    "ground_truth": "The residual connections add the output of the previous layer to the input of the next layer. This helps to improve the flow of information through the network and can help to prevent vanishing gradients.",
    "predicted_answer": "enhance feature propagation and mitigate vanishing gradient issues",
    "reference": "1809.00263v5-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.02962962962962963,
      "phrase_overlap": 0.02962962962962963,
      "is_correct": false
    }
  },
  "1809.00263v5_q8": {
    "paper_id": "1809.00263v5",
    "question_index": 8,
    "question": "What is the relationship between the feature maps of $\\sigma$ and $\\mu$ and the sampled vector?",
    "ground_truth": "The sampled vector is element-wise multiplied by the feature map of $\\sigma$ and added to the feature map of $\\mu$.",
    "predicted_answer": "The sampled vector is scaled by the feature map of σ and then shifted by the feature map of μ.",
    "reference": "1809.00263v5-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7464114832535885,
      "phrase_overlap": 0.7464114832535885,
      "is_correct": true
    }
  },
  "1708.00160v2_q0": {
    "paper_id": "1708.00160v2",
    "question_index": 0,
    "question": "What is the probability of finding the pattern {head=F, ant=NAM} in the data?",
    "ground_truth": "1",
    "predicted_answer": "1.0",
    "reference": "1708.00160v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5,
      "phrase_overlap": 0.5,
      "is_correct": true
    }
  },
  "1708.00160v2_q1": {
    "paper_id": "1708.00160v2",
    "question_index": 1,
    "question": "What is the support value of the node \"ana=NAM\"?",
    "ground_truth": "2",
    "predicted_answer": "(2,1)",
    "reference": "1708.00160v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3333333333333333,
      "phrase_overlap": 0.3333333333333333,
      "is_correct": true
    }
  },
  "1708.00160v2_q2": {
    "paper_id": "1708.00160v2",
    "question_index": 2,
    "question": "Which coreference model performs best on the CoNLL test set according to the F$_1$ score? Is this performance statistically significant compared to all other models in the table?",
    "ground_truth": "The \"ensemble\" model of e2ef achieves the highest F$_1$ score of 68.83 on the CoNLL test set. Yes, this performance is statistically significant compared to all other models listed in the table, as indicated by the caption and footnote referencing the approximate randomization test.",
    "predicted_answer": "E",
    "reference": "1708.00160v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1708.00160v2_q3": {
    "paper_id": "1708.00160v2",
    "question_index": 3,
    "question": "Which deep-coref model performed best on the WikiCoref dataset, according to the table?",
    "ground_truth": "The CoNLL model performed best on the WikiCoref dataset, with an F1 score of 53.40 when using the +linguistic evaluation metric.",
    "predicted_answer": "E",
    "reference": "1708.00160v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.015503875968992248,
      "phrase_overlap": 0.015503875968992248,
      "is_correct": true
    }
  },
  "1809.00458v1_q0": {
    "paper_id": "1809.00458v1",
    "question_index": 0,
    "question": "Which dataset shows a higher variance in F1 score with increasing buffer size?",
    "ground_truth": "ENRON",
    "predicted_answer": "NETFLIX",
    "reference": "1809.00458v1-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.16666666666666666,
      "phrase_overlap": 0.16666666666666666,
      "is_correct": false
    }
  },
  "1809.00458v1_q1": {
    "paper_id": "1809.00458v1",
    "question_index": 1,
    "question": "Which algorithm performs best on the ENRON dataset?",
    "ground_truth": "GB-KMV",
    "predicted_answer": "GB-KMV performs best on the ENRON dataset.",
    "reference": "1809.00458v1-Figure15-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.25,
      "phrase_overlap": 0.25,
      "is_correct": true
    }
  },
  "1809.00458v1_q2": {
    "paper_id": "1809.00458v1",
    "question_index": 2,
    "question": "Which of the two algorithms (GB-KMV or LSH-E) is the fastest on average across all datasets?",
    "ground_truth": "LSH-E",
    "predicted_answer": "B",
    "reference": "1809.00458v1-Figure14-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1809.00458v1_q3": {
    "paper_id": "1809.00458v1",
    "question_index": 3,
    "question": "Which method performs better, GB-KMV or LSH-E, and how does the performance change with different values of eleFreq and recSize?",
    "ground_truth": "GB-KMV generally performs better than LSH-E, as indicated by the higher F1 scores across the range of eleFreq and recSize values. For both methods, the F1 score tends to decrease as recSize increases, while the impact of eleFreq varies depending on the method.",
    "predicted_answer": "GB-KMV performs better than LSH-E across all values of eleFreq and recSize. GB-KMV's performance improves with increasing eleFreq z-value and decreases with increasing recSize z-value, while LSH-E shows similar trends but consistently lower scores.",
    "reference": "1809.00458v1-Figure16-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.20866141732283464,
      "phrase_overlap": 0.20866141732283464,
      "is_correct": false
    }
  },
  "1809.00458v1_q4": {
    "paper_id": "1809.00458v1",
    "question_index": 4,
    "question": "What is the difference between the Jaccard similarity and the containment similarity?",
    "ground_truth": "The Jaccard similarity measures the overlap between two sets, while the containment similarity measures how much one set is contained within another set.",
    "predicted_answer": "D",
    "reference": "1809.00458v1-TableI-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.012987012987012988,
      "phrase_overlap": 0.012987012987012988,
      "is_correct": true
    }
  },
  "1809.00458v1_q5": {
    "paper_id": "1809.00458v1",
    "question_index": 5,
    "question": "Which dataset requires the most storage space when using the LSH-E method?",
    "ground_truth": "REUTERS",
    "predicted_answer": "REUTERS",
    "reference": "1809.00458v1-TableIII-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1809.00458v1_q6": {
    "paper_id": "1809.00458v1",
    "question_index": 6,
    "question": "Which algorithm performs better in terms of F1 score and precision when the space used is 5%?",
    "ground_truth": "GB-KMV performs better in terms of F1 score and precision when the space used is 5%.",
    "predicted_answer": "GB-KMV performs better in terms of both F1 score and precision when the space used is 5%.",
    "reference": "1809.00458v1-Figure12-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9710982658959537,
      "phrase_overlap": 0.9710982658959537,
      "is_correct": true
    }
  },
  "1809.00458v1_q7": {
    "paper_id": "1809.00458v1",
    "question_index": 7,
    "question": "Which method has the highest F-1 score when space used is 10%?",
    "ground_truth": "GB-KMV",
    "predicted_answer": "GB-KMV",
    "reference": "1809.00458v1-Figure13-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1809.00458v1_q8": {
    "paper_id": "1809.00458v1",
    "question_index": 8,
    "question": "Which dataset has the highest average record length?",
    "ground_truth": "CaOpenData",
    "predicted_answer": "CaOpenData",
    "reference": "1809.00458v1-TableII-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1809.00458v1_q9": {
    "paper_id": "1809.00458v1",
    "question_index": 9,
    "question": "What is the relationship between the element-hash value pairs and the signature size?",
    "ground_truth": "The element-hash value pairs are the elements of the signature, and the signature size is the number of element-hash value pairs in the signature.",
    "predicted_answer": "E",
    "reference": "1809.00458v1-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.013605442176870748,
      "phrase_overlap": 0.013605442176870748,
      "is_correct": true
    }
  },
  "1809.00458v1_q10": {
    "paper_id": "1809.00458v1",
    "question_index": 10,
    "question": "How does the running time of GB-KM vary with the F-1 score?",
    "ground_truth": "The running time of GB-KM increases as the F-1 score increases.",
    "predicted_answer": "The running time of GB-KMV remains almost constant as the F-1 score increases.",
    "reference": "1809.00458v1-Figure19-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.8368794326241135,
      "phrase_overlap": 0.8368794326241135,
      "is_correct": true
    }
  },
  "1809.00458v1_q11": {
    "paper_id": "1809.00458v1",
    "question_index": 11,
    "question": "Which method has a lower running time for all datasets - GB-KMV or LSH-E?",
    "ground_truth": "GB-KMV",
    "predicted_answer": "GB-KMV",
    "reference": "1809.00458v1-Figure18-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1809.00458v1_q12": {
    "paper_id": "1809.00458v1",
    "question_index": 12,
    "question": "What is the containment similarity of Q in X1?",
    "ground_truth": "0.67",
    "predicted_answer": "0.67",
    "reference": "1809.00458v1-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1809.00458v1_q13": {
    "paper_id": "1809.00458v1",
    "question_index": 13,
    "question": "Which method is more efficient at utilizing space while maintaining high accuracy?",
    "ground_truth": "GB-KMV is more efficient at utilizing space while maintaining high accuracy.",
    "predicted_answer": "GB-KMV is more efficient at utilizing space while maintaining high accuracy.",
    "reference": "1809.00458v1-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1809.00458v1_q14": {
    "paper_id": "1809.00458v1",
    "question_index": 14,
    "question": "Which algorithm performs better in terms of F1 Score and Precision on ENRON?",
    "ground_truth": "GB-KMV performs better than LSH-E in terms of F1 Score and Precision.",
    "predicted_answer": "GB-KMV",
    "reference": "1809.00458v1-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.16,
      "phrase_overlap": 0.16,
      "is_correct": true
    }
  },
  "1708.01425v4_q0": {
    "paper_id": "1708.01425v4",
    "question_index": 0,
    "question": "What are the different steps involved in reconstructing implicit warrants for argument reasoning comprehension?",
    "ground_truth": "The different steps involved in reconstructing implicit warrants for argument reasoning comprehension are:\n1. Sampling comments\n2. Stance annotation\n3. Reason span annotations\n4. Reason gist summarization\n5. Reason disambiguation\n6. Alternative warrant\n7. Alternative warrant validation\n8. Warrant for original claim\n9. Warrant validation",
    "predicted_answer": "A",
    "reference": "1708.01425v4-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1708.01425v4_q1": {
    "paper_id": "1708.01425v4",
    "question_index": 1,
    "question": "What is the relationship between the number of workers per \"expert\" and Cohen's kappa agreement for stance annotation?",
    "ground_truth": "The Cohen's kappa agreement for stance annotation increases as the number of workers per \"expert\" increases.",
    "predicted_answer": "The relationship is positive: Cohen's kappa agreement increases as the number of workers per 'expert' increases, with higher MACE thresholds generally yielding higher agreement values.\n\n根据题目的要求，只输出选项字母：\n\nA",
    "reference": "1708.01425v4-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5401929260450161,
      "phrase_overlap": 0.5401929260450161,
      "is_correct": false
    }
  },
  "1708.01425v4_q2": {
    "paper_id": "1708.01425v4",
    "question_index": 2,
    "question": "Which approach performs best on the development set?",
    "ground_truth": "Intra-warrant attention with context.",
    "predicted_answer": "Intra-warrant attention",
    "reference": "1708.01425v4-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7666666666666667,
      "phrase_overlap": 0.7666666666666667,
      "is_correct": true
    }
  },
  "1708.01425v4_q3": {
    "paper_id": "1708.01425v4",
    "question_index": 3,
    "question": "Does formal training in reasoning, logic, or argumentation seem to have a significant effect on argument reasoning comprehension accuracy for people with graduate degrees?",
    "ground_truth": "No, it does not appear to have a significant effect.",
    "predicted_answer": "Yes, formal training in reasoning, logic, or argumentation has a significant effect on argument reasoning comprehension accuracy for people with graduate degrees, as accuracy improves notably with extensive training.",
    "reference": "1708.01425v4-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2462686567164179,
      "phrase_overlap": 0.2462686567164179,
      "is_correct": false
    }
  },
  "1708.01425v4_q4": {
    "paper_id": "1708.01425v4",
    "question_index": 4,
    "question": "Which step in the methodology resulted in the largest decrease in the size of the dataset?",
    "ground_truth": "Step 4, Reason disambiguation.",
    "predicted_answer": "Step 4 (Reason disambiguation) resulted in the largest decrease in the size of the dataset.",
    "reference": "1708.01425v4-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4793388429752066,
      "phrase_overlap": 0.4793388429752066,
      "is_correct": true
    }
  },
  "1708.01425v4_q5": {
    "paper_id": "1708.01425v4",
    "question_index": 5,
    "question": "How does the intra-warrant attention mechanism work?",
    "ground_truth": "The intra-warrant attention mechanism uses a BiLSTM to encode the reason and claim, and then provides this encoded information as an attention vector to LSTM layers for each warrant. The attention vector allows the model to focus on specific parts of the reason and claim that are most relevant to each warrant.",
    "predicted_answer": "The intra-warrant attention mechanism works by using bi-directional LSTMs to process claims and reasons, attention-based LSTMs to process warrants, aggregating outputs via max pooling, and refining them through dense layers.",
    "reference": "1708.01425v4-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.19813084112149532,
      "phrase_overlap": 0.19813084112149532,
      "is_correct": false
    }
  },
  "1708.02153v2_q0": {
    "paper_id": "1708.02153v2",
    "question_index": 0,
    "question": "How does increasing the parameter value (ρ for LIME with Euclidean distance, μ for LIME with cosine similarity, and σ for Parzen) seem to affect the influence vectors?",
    "ground_truth": "As the parameter value increases, the influence vectors generally become smoother and less noisy.",
    "predicted_answer": "Increasing the parameter values results in more distinct and localized influence vectors, with clearer patterns emerging as the parameters increase.",
    "reference": "1708.02153v2-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.49795918367346936,
      "phrase_overlap": 0.49795918367346936,
      "is_correct": false
    }
  },
  "1708.02153v2_q1": {
    "paper_id": "1708.02153v2",
    "question_index": 1,
    "question": "Which explanation method seems to place the most emphasis on specific, localized features rather than smooth, gradual changes in pixel intensity?",
    "ground_truth": "LIME appears to place the most emphasis on specific, localized features.",
    "predicted_answer": "LIME",
    "reference": "1708.02153v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.10526315789473684,
      "phrase_overlap": 0.10526315789473684,
      "is_correct": true
    }
  },
  "1708.02153v2_q2": {
    "paper_id": "1708.02153v2",
    "question_index": 2,
    "question": "Explain why the \"Last contact\" feature has a significant positive influence on the SSL score in both examples, even though it is not directly used by the SSL algorithm.",
    "ground_truth": "The \"Last contact\" feature shows a strong positive influence on the SSL score because it is likely correlated with other features that are used by the algorithm. As the passage mentions, data-driven methods like MIM can assign influence to features that are not directly used by the model if they are correlated with other influential features. In this case, a recent \"Last contact\" date might be correlated with a higher number of recent offenses or other factors that contribute to a higher SSL score.",
    "predicted_answer": "The 'Last contact' feature has a significant positive influence on the SSL score in both examples because it likely serves as a proxy for recency of interaction or activity, which indirectly correlates with risk assessment. This influence is derived from contextual or historical patterns in the data, even though the feature is not directly used by the SSL algorithm.\n\n答案：E",
    "reference": "1708.02153v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1735159817351598,
      "phrase_overlap": 0.1735159817351598,
      "is_correct": false
    }
  },
  "1708.05239v3_q0": {
    "paper_id": "1708.05239v3",
    "question_index": 0,
    "question": "Which MCMC sampler appears to have the most consistent performance across the different variables?",
    "ground_truth": "PE-HMC (N=5)",
    "predicted_answer": "E",
    "reference": "1708.05239v3-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.15384615384615385,
      "phrase_overlap": 0.15384615384615385,
      "is_correct": true
    }
  },
  "1708.05239v3_q1": {
    "paper_id": "1708.05239v3",
    "question_index": 1,
    "question": "How does the performance of the PE-N=5 sampler compare to the HMC sampler?",
    "ground_truth": "The PE-N=5 sampler performs better than the HMC sampler.",
    "predicted_answer": "The PE-N=5 sampler has a lower median and larger variability compared to the HMC sampler.",
    "reference": "1708.05239v3-Figure10-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.593103448275862,
      "phrase_overlap": 0.593103448275862,
      "is_correct": true
    }
  },
  "1809.01246v1_q0": {
    "paper_id": "1809.01246v1",
    "question_index": 0,
    "question": "How does the square hashing process work?",
    "ground_truth": "Square hashing is a process that uses two hash functions to map a source/destination pair to a bucket in a two-dimensional array. The first hash function, h_i(s), maps the source address to a row in the array, and the second hash function, h_i(d), maps the destination address to a column in the array. The intersection of the row and column is the bucket where the fingerprint is stored.",
    "predicted_answer": "The square hashing process works by mapping items to specific buckets using hash functions (h1, h2, h3) and storing fingerprints (<f(s), f(d)>) at the calculated index (<1, 3>).",
    "reference": "1809.01246v1-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.07787610619469026,
      "phrase_overlap": 0.07787610619469026,
      "is_correct": false
    }
  },
  "1809.01246v1_q1": {
    "paper_id": "1809.01246v1",
    "question_index": 1,
    "question": "What is the relationship between the buffer percentage and the width of the room?",
    "ground_truth": "The buffer percentage decreases as the width of the room increases.",
    "predicted_answer": "Buffer percentage decreases as the width of the room increases. \n\n根据提供的信息，最终的答案是：B",
    "reference": "1809.01246v1-Figure13-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.8571428571428571,
      "phrase_overlap": 0.8571428571428571,
      "is_correct": true
    }
  },
  "1809.01246v1_q2": {
    "paper_id": "1809.01246v1",
    "question_index": 2,
    "question": "How are edges aggregated in the graph sketch $G_h$?",
    "ground_truth": "Edges are aggregated by adding their weights together.",
    "predicted_answer": "Edges in the graph sketch $G_h$ are aggregated by summing their weights over time.",
    "reference": "1809.01246v1-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6764705882352942,
      "phrase_overlap": 0.6764705882352942,
      "is_correct": true
    }
  },
  "1809.01246v1_q3": {
    "paper_id": "1809.01246v1",
    "question_index": 3,
    "question": "How does the Average Relative Error (ARE) of node queries change as the width increases for different configurations of GSS and TCM?",
    "ground_truth": "The ARE of node queries generally decreases as the width increases for all configurations of GSS and TCM. However, there are some fluctuations in the ARE for some configurations.",
    "predicted_answer": "A",
    "reference": "1809.01246v1-Figure11-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0111731843575419,
      "phrase_overlap": 0.0111731843575419,
      "is_correct": true
    }
  },
  "1809.01246v1_q4": {
    "paper_id": "1809.01246v1",
    "question_index": 4,
    "question": "What is the relationship between the table and the graph sketch in the figure?",
    "ground_truth": "The table provides the mapping between the nodes in the original graph and their corresponding hash values, which are used to create the graph sketch.",
    "predicted_answer": "E",
    "reference": "1809.01246v1-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.013245033112582781,
      "phrase_overlap": 0.013245033112582781,
      "is_correct": true
    }
  },
  "1809.01246v1_q5": {
    "paper_id": "1809.01246v1",
    "question_index": 5,
    "question": "Which graph shows the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method?",
    "ground_truth": "The graph for the Caida-networkflow dataset shows the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method.",
    "predicted_answer": "Caida-networkflow",
    "reference": "1809.01246v1-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.20118343195266272,
      "phrase_overlap": 0.20118343195266272,
      "is_correct": true
    }
  },
  "1809.01246v1_q6": {
    "paper_id": "1809.01246v1",
    "question_index": 6,
    "question": "Which data structure is the fastest for updating on the email-EuAll dataset?",
    "ground_truth": "GSS (no sampling)",
    "predicted_answer": "Adjacency Lists",
    "reference": "1809.01246v1-TableI-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.1875,
      "phrase_overlap": 0.1875,
      "is_correct": false
    }
  },
  "1809.01246v1_q7": {
    "paper_id": "1809.01246v1",
    "question_index": 7,
    "question": "How does the average precision of TCM(256*memory) compare to the other two algorithms in the email-EuAll dataset?",
    "ground_truth": "The average precision of TCM(256*memory) is lower than the other two algorithms in the email-EuAll dataset.",
    "predicted_answer": "The average precision of TCM(256*memory) is significantly lower compared to GSS(size=12) and GSS(size=16) in the email-EuAll dataset.",
    "reference": "1809.01246v1-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7083333333333334,
      "phrase_overlap": 0.7083333333333334,
      "is_correct": true
    }
  },
  "1809.01246v1_q8": {
    "paper_id": "1809.01246v1",
    "question_index": 8,
    "question": "Which query type has the highest accuracy when M/|V| is small?",
    "ground_truth": "Edge query.",
    "predicted_answer": "A",
    "reference": "1809.01246v1-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1708.03797v1_q0": {
    "paper_id": "1708.03797v1",
    "question_index": 0,
    "question": "What is the role of the code layer in the HDMF architecture?",
    "ground_truth": "The code layer is responsible for generating a compressed representation of the input data. This compressed representation is then used by the decoder to reconstruct the original data.",
    "predicted_answer": "The code layer (Hidden Layer K) in the HDMF architecture is responsible for generating transformed representations X̃ and Ỹ, which are used to compute the relationship R̂.",
    "reference": "1708.03797v1-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5056179775280899,
      "phrase_overlap": 0.5056179775280899,
      "is_correct": false
    }
  },
  "1708.03797v1_q1": {
    "paper_id": "1708.03797v1",
    "question_index": 1,
    "question": "Which model achieved the best overall performance in terms of ranking relevant tags for users?",
    "ground_truth": "HDMF achieved the best overall performance.",
    "predicted_answer": "H",
    "reference": "1708.03797v1-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.045454545454545456,
      "phrase_overlap": 0.045454545454545456,
      "is_correct": true
    }
  },
  "1709.02755v5_q0": {
    "paper_id": "1709.02755v5",
    "question_index": 0,
    "question": "How does the validation accuracy of the SRU model compare to that of the cuDNN LSTM and CNN models?",
    "ground_truth": "The SRU model achieves comparable or slightly higher validation accuracy than the cuDNN LSTM and CNN models on all six benchmarks.",
    "predicted_answer": "A",
    "reference": "1709.02755v5-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.015267175572519083,
      "phrase_overlap": 0.015267175572519083,
      "is_correct": true
    }
  },
  "1709.02755v5_q1": {
    "paper_id": "1709.02755v5",
    "question_index": 1,
    "question": "How does the processing time of SRU compare to that of cuDNN LSTM and word-level convolution with filter widths k=2 and k=3?",
    "ground_truth": "The processing time of SRU is significantly faster than that of cuDNN LSTM and word-level convolution with filter widths k=2 and k=3.",
    "predicted_answer": "E",
    "reference": "1709.02755v5-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.014925373134328358,
      "phrase_overlap": 0.014925373134328358,
      "is_correct": true
    }
  },
  "1709.02755v5_q2": {
    "paper_id": "1709.02755v5",
    "question_index": 2,
    "question": "According to Table 1 and the passage, how does the performance of the SRU model compare to the LSTM model in terms of both accuracy and training speed on the SQuAD dataset?",
    "ground_truth": "The SRU model outperforms the LSTM model in both accuracy and training speed on the SQuAD dataset.",
    "predicted_answer": "The SRU model outperforms the LSTM model in both accuracy and training speed on the SQuAD dataset. For accuracy, SRU achieves higher Dev_EM (71.4 vs. 69.5) and Dev_F1 (80.2 vs. 78.8). For training speed, SRU is significantly faster, with RNN time per epoch being 58s (3 layers) compared to 316s for LSTM, and total time per epoch being 159s (3 layers) compared to 431s for LSTM.",
    "reference": "1709.02755v5-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4117647058823529,
      "phrase_overlap": 0.4117647058823529,
      "is_correct": true
    }
  },
  "1709.02755v5_q3": {
    "paper_id": "1709.02755v5",
    "question_index": 3,
    "question": "Based on the table, how does the training process handle large vocabulary sizes?",
    "ground_truth": "The training process uses several techniques to handle large vocabulary sizes. These include:\n\n1. **Token-based batching:** Instead of grouping sentences of similar lengths together, the training process batches together a fixed number of tokens (5120 tokens per batch). This approach ensures that the model sees a consistent amount of vocabulary regardless of sentence length variation.\n2. **Shared embedding:** This technique maps both source and target words to the same embedding space, effectively reducing the memory footprint needed to store word representations. \n3. **Positional encoding:** This method injects information about the position of words in a sentence into the model, helping it better understand long-range dependencies within the text. ",
    "predicted_answer": "The training process handles large vocabulary sizes by using shared embeddings and position encoding, along with token-based normalization and batch types.",
    "reference": "1709.02755v5-Table6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.09868421052631579,
      "phrase_overlap": 0.09868421052631579,
      "is_correct": false
    }
  },
  "1709.02755v5_q4": {
    "paper_id": "1709.02755v5",
    "question_index": 4,
    "question": "How does the variance of the hidden state $h_t$ compare to the variance of the input $x_t$ in deep layers of the SRU model?",
    "ground_truth": "According to the passage, the variance of the hidden state is approximately equal to the variance of the input in deep layers.",
    "predicted_answer": "In deep layers of the SRU model, the variance of the hidden state $h_t$ approaches and eventually equals the variance of the input $x_t$.",
    "reference": "1709.02755v5-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6844106463878327,
      "phrase_overlap": 0.6844106463878327,
      "is_correct": true
    }
  },
  "1709.02755v5_q5": {
    "paper_id": "1709.02755v5",
    "question_index": 5,
    "question": "How does scaling correction affect the training of SRU models?",
    "ground_truth": "Scaling correction improves the training progress of SRU models, especially for deeper models with many stacked layers.",
    "predicted_answer": "E",
    "reference": "1709.02755v5-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.016666666666666666,
      "phrase_overlap": 0.016666666666666666,
      "is_correct": true
    }
  },
  "1709.02755v5_q6": {
    "paper_id": "1709.02755v5",
    "question_index": 6,
    "question": "How does the performance of SRU with 8 layers compare to the best reported results on the SUBJ dataset, and how does its training time compare to the other models in the \"Our setup\" section?",
    "ground_truth": "While SRU with 8 layers achieves high test accuracy within the \"Our setup\" section on the SUBJ dataset (93.7%), it falls slightly short of the best reported result of 95.5% achieved by Zhao et al. (2015). However, SRU's training time of 879 seconds for 100 epochs on the SST dataset is faster than the LSTM model (2409 seconds) but slower than the CNN model (417 seconds) and the QRNN models (345 and 371 seconds).",
    "predicted_answer": "B/E",
    "reference": "1709.02755v5-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.004796163069544364,
      "phrase_overlap": 0.004796163069544364,
      "is_correct": false
    }
  },
  "1809.02731v3_q0": {
    "paper_id": "1809.02731v3",
    "question_index": 0,
    "question": "Which model performed best on average across all tasks?",
    "ground_truth": "The Linear model performed best on average with a score of 70.0.",
    "predicted_answer": "Linear model performed best on average across all tasks.",
    "reference": "1809.02731v3-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7333333333333333,
      "phrase_overlap": 0.7333333333333333,
      "is_correct": true
    }
  },
  "1809.02731v3_q1": {
    "paper_id": "1809.02731v3",
    "question_index": 1,
    "question": "Which corpus has more sentences, and by how much?",
    "ground_truth": "The UMBC News corpus has more sentences, by approximately 60.5 million.",
    "predicted_answer": "UMBC News has 60.5 million more sentences than BookCorpus.",
    "reference": "1809.02731v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5116279069767442,
      "phrase_overlap": 0.5116279069767442,
      "is_correct": true
    }
  },
  "1809.02731v3_q2": {
    "paper_id": "1809.02731v3",
    "question_index": 2,
    "question": "Which model performs the best on the STS16 task with unsupervised training?",
    "ground_truth": "The Bijective model performs the best on the STS16 task with unsupervised training.",
    "predicted_answer": "Bijective_U",
    "reference": "1809.02731v3-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.2127659574468085,
      "phrase_overlap": 0.2127659574468085,
      "is_correct": false
    }
  },
  "1809.03149v2_q0": {
    "paper_id": "1809.03149v2",
    "question_index": 0,
    "question": "What is the relationship between position and click-through rate (CTR)?",
    "ground_truth": "The relationship between position and CTR is complex and non-linear. In general, CTR decreases as position increases, but there are also local peaks and valleys in the CTR curve. This suggests that there are other factors besides position that affect CTR.",
    "predicted_answer": "B",
    "reference": "1809.03149v2-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0078125,
      "phrase_overlap": 0.0078125,
      "is_correct": true
    }
  },
  "1809.03149v2_q1": {
    "paper_id": "1809.03149v2",
    "question_index": 1,
    "question": "How does the advertising rate for the \"Fix\" curve compare to the \"Oracle\" curve at hour 14?",
    "ground_truth": "The advertising rate for the \"Fix\" curve is lower than the \"Oracle\" curve at hour 14.",
    "predicted_answer": "B",
    "reference": "1809.03149v2-Figure8-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1809.03149v2_q2": {
    "paper_id": "1809.03149v2",
    "question_index": 2,
    "question": "What is the role of the Higher Level Policy in the framework?",
    "ground_truth": "The Higher Level Policy sets constraints for the next sub-trajectory and provides information about the previous stage to the Lower Level Policy.",
    "predicted_answer": "A",
    "reference": "1809.03149v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0136986301369863,
      "phrase_overlap": 0.0136986301369863,
      "is_correct": true
    }
  },
  "1809.03149v2_q3": {
    "paper_id": "1809.03149v2",
    "question_index": 3,
    "question": "How does the advertising system select the best items to show to the user?",
    "ground_truth": "The advertising system selects the best items to show to the user by first generating a candidate set of items from the recommender system. This candidate set is then shuffled and sorted by their score, which is determined by the network. The network takes into account the features of the items and the scoring factors, which are likely based on the user's past behavior and preferences.",
    "predicted_answer": "E",
    "reference": "1809.03149v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1809.03149v2_q4": {
    "paper_id": "1809.03149v2",
    "question_index": 4,
    "question": "What is the effect of using CHER on the percentage of ads displayed for each user?",
    "ground_truth": "The percentage of ads displayed for each user is higher when CHER is used.",
    "predicted_answer": "Using CHER results in the percentage of ads displayed being closer to the target across all policy objectives (0.35, 0.4, 0.45) with reduced variance compared to without CHER.\n\n由于具体要求是选择题的选项字母，而提供的答案是一个结论性语句。按照规则，这里没有对应的单一选项字母（A/B/C/D/E）。但如果需要选择一个最符合描述的选项，可以理解为：\n\n**E**\n\n注：此回答基于给定信息和格式要求进行调整。实际上，这应该被视为对图表内容的总结而非选择题答案。",
    "reference": "1809.03149v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.27249357326478146,
      "phrase_overlap": 0.27249357326478146,
      "is_correct": false
    }
  },
  "1708.06832v3_q0": {
    "paper_id": "1708.06832v3",
    "question_index": 0,
    "question": "Which method achieves the lowest error rate on ILSVRC at 1/4 of the total cost?",
    "ground_truth": "MSDNNet38",
    "predicted_answer": "A",
    "reference": "1708.06832v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1708.06832v3_q1": {
    "paper_id": "1708.06832v3",
    "question_index": 1,
    "question": "What is the effect of increasing the budget in FLOPS on the test Top-1 error rate for the three different training strategies?",
    "ground_truth": "The test Top-1 error rate decreases as the budget in FLOPS increases for all three training strategies.",
    "predicted_answer": "Decreases",
    "reference": "1708.06832v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.16071428571428573,
      "phrase_overlap": 0.16071428571428573,
      "is_correct": true
    }
  },
  "1708.06832v3_q2": {
    "paper_id": "1708.06832v3",
    "question_index": 2,
    "question": "Which model performs the best on CIFAR100 and ILSVRC datasets?",
    "ground_truth": "EANN with AdaLoss performs the best on both CIFAR100 and ILSVRC datasets.",
    "predicted_answer": "E",
    "reference": "1708.06832v3-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.02702702702702703,
      "phrase_overlap": 0.02702702702702703,
      "is_correct": true
    }
  },
  "1906.06589v3_q0": {
    "paper_id": "1906.06589v3",
    "question_index": 0,
    "question": "Which model performed the best on the test data?",
    "ground_truth": "P-FC",
    "predicted_answer": "P",
    "reference": "1906.06589v3-Table10-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.4,
      "phrase_overlap": 0.4,
      "is_correct": true
    }
  },
  "1906.06589v3_q1": {
    "paper_id": "1906.06589v3",
    "question_index": 1,
    "question": "How does the size of the reference set ($X_\\textsf{ref}$) used for DMP training differ between the Purchase/Texas datasets and the CIFAR datasets? Explain the rationale behind this difference.",
    "ground_truth": "For Purchase and Texas datasets, $X_\\textsf{ref}$ is specifically selected and contains 10,000 data points. In contrast, for CIFAR datasets, the entire remaining data (25,000 points) after selecting $D_\\textsf{tr}$ is used as $X_\\textsf{ref}$. This difference is due to the smaller size of the CIFAR datasets. Using the entire remaining data as $X_\\textsf{ref}$ ensures sufficient data for effective DMP training in these cases.",
    "predicted_answer": "The size of the reference set ($X_\textsf{ref}$) used for DMP training is larger for the CIFAR datasets (25000) compared to the Purchase and Texas datasets (10000). This difference is likely due to the higher complexity and larger scale of the CIFAR datasets, which require more samples for effective training.",
    "reference": "1906.06589v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.08412483039348712,
      "phrase_overlap": 0.08412483039348712,
      "is_correct": false
    }
  },
  "1906.06589v3_q2": {
    "paper_id": "1906.06589v3",
    "question_index": 2,
    "question": "What is the relationship between the average X_ref entropy and the generalization gap?",
    "ground_truth": "The generalization gap increases as the average X_ref entropy increases.",
    "predicted_answer": " Increases with Average X_ref entropy",
    "reference": "1906.06589v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6296296296296297,
      "phrase_overlap": 0.6296296296296297,
      "is_correct": true
    }
  },
  "1809.03449v3_q0": {
    "paper_id": "1809.03449v3",
    "question_index": 0,
    "question": "How does the average number of inter-word semantic connections per word change as the value of κ increases?",
    "ground_truth": "The average number of inter-word semantic connections per word increases as the value of κ increases.",
    "predicted_answer": "The average number of inter-word semantic connections per word increases as the value of κ increases.",
    "reference": "1809.03449v3-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1809.03449v3_q1": {
    "paper_id": "1809.03449v3",
    "question_index": 1,
    "question": "Which model performed the best on the AddOneSent dataset?",
    "ground_truth": "KAR",
    "predicted_answer": "KAR (ours)",
    "reference": "1809.03449v3-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.46153846153846156,
      "phrase_overlap": 0.46153846153846156,
      "is_correct": true
    }
  },
  "1809.03449v3_q2": {
    "paper_id": "1809.03449v3",
    "question_index": 2,
    "question": "What is the role of the Knowledge Aided Similarity Matrix in the KAR model?",
    "ground_truth": "The Knowledge Aided Similarity Matrix is used to compute the similarity between the question and passage context embeddings. This similarity score is then used to weight the passage context embeddings, giving more weight to those parts of the passage that are most relevant to the question.",
    "predicted_answer": "C",
    "reference": "1809.03449v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1809.03449v3_q3": {
    "paper_id": "1809.03449v3",
    "question_index": 3,
    "question": "How does the performance of KAR, SAN, and QANet (without data augmentation) change as the proportion of available training examples decreases?",
    "ground_truth": "The performance of all three models decreases as the proportion of available training examples decreases.",
    "predicted_answer": "The performance of KAR, SAN, and QANet (without data augmentation) decreases as the proportion of available training examples decreases. KAR consistently performs better than SAN and QANet, while QANet performs the worst.",
    "reference": "1809.03449v3-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.588957055214724,
      "phrase_overlap": 0.588957055214724,
      "is_correct": false
    }
  },
  "1809.03449v3_q4": {
    "paper_id": "1809.03449v3",
    "question_index": 4,
    "question": "Which model performs the best when trained on 60% of the training data?",
    "ground_truth": "KAR",
    "predicted_answer": "KAR performs the best when trained on 60% of the training data with an F1 score of 82.",
    "reference": "1809.03449v3-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.06741573033707865,
      "phrase_overlap": 0.06741573033707865,
      "is_correct": true
    }
  },
  "1809.03449v3_q5": {
    "paper_id": "1809.03449v3",
    "question_index": 5,
    "question": "Which of the three models, KAR, SAN, or QANet (without data augmentation), performs the best on AddSent when trained on the full training set?",
    "ground_truth": "KAR",
    "predicted_answer": "KAR performs the best on AddSent when trained on the full training set.",
    "reference": "1809.03449v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.08108108108108109,
      "phrase_overlap": 0.08108108108108109,
      "is_correct": true
    }
  },
  "1710.05654v2_q0": {
    "paper_id": "1710.05654v2",
    "question_index": 0,
    "question": "How does the time needed for learning a graph with a subset of allowed edges $\\mathcal{E}^\\text{allowed}$ change as the number of edges per node increases?",
    "ground_truth": "The time needed for learning a graph with a subset of allowed edges $\\mathcal{E}^\\text{allowed}$ increases linearly as the number of edges per node increases.",
    "predicted_answer": "B",
    "reference": "1710.05654v2-Figure15-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.012578616352201259,
      "phrase_overlap": 0.012578616352201259,
      "is_correct": true
    }
  },
  "1710.05654v2_q1": {
    "paper_id": "1710.05654v2",
    "question_index": 1,
    "question": "Which method is the fastest for computing a graph with a small average node degree?",
    "ground_truth": "The proposed method in this paper (k=5) is the fastest for computing a graph with a small average node degree.",
    "predicted_answer": "This paper (k=5) is the fastest method for computing a graph with a small average node degree.",
    "reference": "1710.05654v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.8529411764705882,
      "phrase_overlap": 0.8529411764705882,
      "is_correct": true
    }
  },
  "1710.05654v2_q2": {
    "paper_id": "1710.05654v2",
    "question_index": 2,
    "question": "Which digit has the highest average squared distance to other digits in the MNIST dataset?",
    "ground_truth": "Digit \"1\"",
    "predicted_answer": "E",
    "reference": "1710.05654v2-Figure11-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1710.05654v2_q3": {
    "paper_id": "1710.05654v2",
    "question_index": 3,
    "question": "How well do the approximate bounds of $\\theta$ predict sparsity in the \"spherical\" dataset?",
    "ground_truth": "The approximate bounds of $\\theta$ are very effective at predicting sparsity in the \"spherical\" dataset.",
    "predicted_answer": "The approximate bounds of θ predict sparsity well for the 'Large scale Log' method, as the 'degree obtained' closely matches the 'degree requested'. However, for the A-NN method, the obtained degrees are consistently higher than requested, indicating less sparsity.",
    "reference": "1710.05654v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3089430894308943,
      "phrase_overlap": 0.3089430894308943,
      "is_correct": false
    }
  },
  "1710.05654v2_q4": {
    "paper_id": "1710.05654v2",
    "question_index": 4,
    "question": "Which model is the most effective at connecting digits with larger distances?",
    "ground_truth": "The log model.",
    "predicted_answer": "A",
    "reference": "1710.05654v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": false
    }
  },
  "1710.05654v2_q5": {
    "paper_id": "1710.05654v2",
    "question_index": 5,
    "question": "What is the effect of adding Gaussian noise to the images on the measured sparsity?",
    "ground_truth": "Adding Gaussian noise to the images increases the measured sparsity.",
    "predicted_answer": "Adding Gaussian noise to the images causes a slight deviation between the measured sparsity and the theoretical bounds, particularly at lower threshold values.",
    "reference": "1710.05654v2-Figure12-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5638766519823789,
      "phrase_overlap": 0.5638766519823789,
      "is_correct": false
    }
  },
  "1710.05654v2_q6": {
    "paper_id": "1710.05654v2",
    "question_index": 6,
    "question": "How does the connectivity of the Daitch hard scalable model compare to the Daitch soft scalable model?",
    "ground_truth": "The Daitch hard scalable model has a higher connectivity than the Daitch soft scalable model. This can be seen in the figure, where the bars for the hard model are generally higher than the bars for the soft model.",
    "predicted_answer": "The Daitch hard scalable model has relatively uniform connectivity across labels, while the Daitch soft scalable model has a significant peak at label 0 and much lower connectivity for other labels.",
    "reference": "1710.05654v2-Figure14-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.21359223300970873,
      "phrase_overlap": 0.21359223300970873,
      "is_correct": false
    }
  },
  "1710.05654v2_q7": {
    "paper_id": "1710.05654v2",
    "question_index": 7,
    "question": "How does the relevance of terms assigned by the learned graph compare to the relevance assigned by k-NN and A-NN graphs?",
    "ground_truth": "The learned graph assigns weights that correspond much better to the relevance of the terms compared to k-NN and A-NN graphs.",
    "predicted_answer": "The Learned graph assigns significantly higher relevance scores to certain terms compared to k-NN and A-NN graphs.",
    "reference": "1710.05654v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.7196652719665272,
      "phrase_overlap": 0.7196652719665272,
      "is_correct": true
    }
  },
  "1710.05654v2_q8": {
    "paper_id": "1710.05654v2",
    "question_index": 8,
    "question": "How does the graph diameter change with increasing average degree for different methods and datasets?",
    "ground_truth": "The graph diameter generally decreases with increasing average degree for all methods and datasets. However, the rate of decrease and the final diameter value vary depending on the method and dataset.",
    "predicted_answer": "A",
    "reference": "1710.05654v2-Figure9-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1906.10843v1_q0": {
    "paper_id": "1906.10843v1",
    "question_index": 0,
    "question": "Which estimator performs best in the presence of noisy confounders and how does it compare to the Covariate Control (CC) estimator?",
    "ground_truth": "The Entropy Balancing (EB) estimator performs best across all measures (Bias, MAE, and MSE) when confounders are noisy. While the CC estimator also performs well, it exhibits slightly higher bias and MAE compared to EB.",
    "predicted_answer": "EB",
    "reference": "1906.10843v1-Table5-1.png",
    "question_type": "multiple_choice",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1906.10843v1_q1": {
    "paper_id": "1906.10843v1",
    "question_index": 1,
    "question": "Which ATE estimator is most affected by the presence of noisy confounders?",
    "ground_truth": "Outcome Regression (OR)",
    "predicted_answer": "Outcome Regression (OR) is most affected by the presence of noisy confounders.",
    "reference": "1906.10843v1-Figure6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.45544554455445546,
      "phrase_overlap": 0.45544554455445546,
      "is_correct": true
    }
  },
  "1906.10843v1_q2": {
    "paper_id": "1906.10843v1",
    "question_index": 2,
    "question": "Which estimator has the smallest bias and best MSE performance in the case of fully observed confounders?",
    "ground_truth": "The Entropy Balancing (EB) and Covariate Control (CC) estimators.",
    "predicted_answer": "E",
    "reference": "1906.10843v1-Figure7-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.030303030303030304,
      "phrase_overlap": 0.030303030303030304,
      "is_correct": true
    }
  },
  "1809.03550v3_q0": {
    "paper_id": "1809.03550v3",
    "question_index": 0,
    "question": "Why is the optimal threshold chosen to be at the right margin of the region around the mode of the histogram?",
    "ground_truth": "The region around the mode of the histogram mostly contains noise. Therefore, the optimal threshold is chosen to be at the right margin of this region to avoid including too much noise in the thresholded image.",
    "predicted_answer": "E",
    "reference": "1809.03550v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1809.03550v3_q1": {
    "paper_id": "1809.03550v3",
    "question_index": 1,
    "question": "Which method achieves the best overall F1 score across all categories? Is this method consistently the best across all individual categories?",
    "ground_truth": "According to the table, Algorithm 2 w/ Geman-McLure) achieves the best overall F1 score of 0.56514. However, this method is not consistently the best across all individual categories. For example, OMoGMF has a higher F1 score for the \"badWeather\" category.",
    "predicted_answer": "B",
    "reference": "1809.03550v3-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1809.03550v3_q2": {
    "paper_id": "1809.03550v3",
    "question_index": 2,
    "question": "Which algorithm achieves the fastest processing time per frame and how much faster is it compared to the slowest algorithm listed?",
    "ground_truth": "Algorithm 12(SCDM with Geman-McLure) achieves the fastest processing time per frame at 0.103 seconds. This is approximately 100 times faster than the slowest algorithm, TTD_3WD, which takes 10.343 seconds per frame.",
    "predicted_answer": "Algorithm 2 (w/ Geman-McLure) B",
    "reference": "1809.03550v3-Table6-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.21951219512195122,
      "phrase_overlap": 0.21951219512195122,
      "is_correct": false
    }
  },
  "1809.03550v3_q3": {
    "paper_id": "1809.03550v3",
    "question_index": 3,
    "question": "What is the relationship between the residuals prior to thresholding and the Boolean map?",
    "ground_truth": "The Boolean map is obtained by thresholding the residuals prior to thresholding.",
    "predicted_answer": "The residuals prior to thresholding represent the differences between the original image and the background-subtracted image. Applying a threshold to these residuals generates the Boolean map, which isolates the detected objects (vehicles).",
    "reference": "1809.03550v3-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.225,
      "phrase_overlap": 0.225,
      "is_correct": false
    }
  },
  "1709.08294v3_q0": {
    "paper_id": "1709.08294v3",
    "question_index": 0,
    "question": "How does the performance of the two-way AdaQA model compare to the one-way AdaQA model and other CNN-based baseline models on the WikiQA dataset?",
    "ground_truth": "The two-way AdaQA model significantly outperforms the one-way AdaQA model and all other CNN-based baseline models on the WikiQA dataset. This is evident from the higher MAP and MRR values achieved by the two-way model (0.7107 and 0.7304 respectively) compared to the one-way model (0.7005 and 0.7161) and the baseline models.",
    "predicted_answer": "E",
    "reference": "1709.08294v3-Table3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1709.08294v3_q1": {
    "paper_id": "1709.08294v3",
    "question_index": 1,
    "question": "How does the ACNN framework learn context-sensitive filters?",
    "ground_truth": "The ACNN framework learns context-sensitive filters through two modules: the filter generation module and the adaptive convolution module. The filter generation module produces a set of filters conditioned on the input sentence, while the adaptive convolution module applies the generated filters to an input sentence. The two modules are jointly differentiable, and the overall architecture can be trained in an end-to-end manner.",
    "predicted_answer": "A",
    "reference": "1709.08294v3-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1709.08294v3_q2": {
    "paper_id": "1709.08294v3",
    "question_index": 2,
    "question": "Which model performed the best on the Quora Question Pairs dataset?",
    "ground_truth": "AdaQA (two-way) + att.",
    "predicted_answer": "AdaQA (two-way) + att.",
    "reference": "1709.08294v3-Table5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1709.08294v3_q3": {
    "paper_id": "1709.08294v3",
    "question_index": 3,
    "question": "Based on the figure, which type of question does ACNN perform the best on?",
    "ground_truth": "ACNN performs best on \"Who\" questions.",
    "predicted_answer": "E",
    "reference": "1709.08294v3-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.05128205128205128,
      "phrase_overlap": 0.05128205128205128,
      "is_correct": true
    }
  },
  "1709.08294v3_q4": {
    "paper_id": "1709.08294v3",
    "question_index": 4,
    "question": "How does the Adaptive Question Answering (AdaQA) model generate context-aware filters?",
    "ground_truth": "The AdaQA model generates context-aware filters through the filter generation module. This module takes the question and answer as input and outputs a set of filters that are specific to the question and answer pair.",
    "predicted_answer": "The AdaQA model generates context-aware filters using a filter generation module that processes the question and answer inputs. These filters are then refined through convolution modules before being combined in the matching module.",
    "reference": "1709.08294v3-Figure2-1.png",
    "question_type": "multiple_choice",
    "evaluation": {
      "similarity_score": 0.45089285714285715,
      "phrase_overlap": 0.45089285714285715,
      "is_correct": false
    }
  },
  "1709.08294v3_q5": {
    "paper_id": "1709.08294v3",
    "question_index": 5,
    "question": "Based on Table 1, which dataset has the largest vocabulary size and how does this compare to the average number of words per document in that dataset?",
    "ground_truth": "The Yelp P. dataset has the largest vocabulary size with 25,709 unique words. This is significantly larger than the average number of words per document in the dataset, which is 138.",
    "predicted_answer": "A",
    "reference": "1709.08294v3-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.01092896174863388,
      "phrase_overlap": 0.01092896174863388,
      "is_correct": true
    }
  },
  "1709.08294v3_q6": {
    "paper_id": "1709.08294v3",
    "question_index": 6,
    "question": "Can you explain why the authors claim that their S-ACNN model with a single filter is \"much more expressive\" than the basic S-CNN model, even though it doesn't achieve the best overall performance on either dataset?",
    "ground_truth": "The authors claim that S-ACNN is more expressive than S-CNN because, despite having only one filter, it significantly outperforms S-CNN on both datasets. This suggests that the filter-generation module in ACNN allows for greater flexibility and adaptability, enabling the model to better capture the specific features of each sentence.",
    "predicted_answer": "E",
    "reference": "1709.08294v3-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1709.08294v3_q7": {
    "paper_id": "1709.08294v3",
    "question_index": 7,
    "question": "Which model performs best on the SelQA dataset and how does it compare to the baseline CNN model reported in Jurczyk et al. (2016)?",
    "ground_truth": "The AdaQA (two-way) + att. model achieves the best performance on the SelQA dataset with a MAP score of 0.9021 and an MRR score of 0.9103. Compared to the baseline CNN model from Jurczyk et al. (2016) which has a MAP score of 0.8320 and an MRR score of 0.8420, the AdaQA (two-way) + att. model demonstrates a significant improvement in both metrics.",
    "predicted_answer": "The AdaQA (two-way + att.) model performs best on the SelQA dataset with MAP of 0.9021 and MRR of 0.9103, significantly outperforming the baseline CNN model reported in Jurczyk et al. (2016), which has MAP of 0.8320 and MRR of 0.8420.",
    "reference": "1709.08294v3-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.5248713550600344,
      "phrase_overlap": 0.5248713550600344,
      "is_correct": true
    }
  },
  "1710.06177v2_q0": {
    "paper_id": "1710.06177v2",
    "question_index": 0,
    "question": "How does the performance of VAGER+Voting compare to other VAGER variants in the 1-shot and 20-shot settings?",
    "ground_truth": "VAGER+Voting consistently outperforms all other VAGER variants in both 1-shot and 20-shot settings, achieving the highest AUC and F1 scores.",
    "predicted_answer": "A/B/C/D/E：B\n\n结论：VADERG+Voting在1-shot和20-shot设置中均表现出色，优于其他VADER变体。",
    "reference": "1710.06177v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.27450980392156865,
      "phrase_overlap": 0.27450980392156865,
      "is_correct": true
    }
  },
  "1710.06177v2_q1": {
    "paper_id": "1710.06177v2",
    "question_index": 1,
    "question": "Can you explain why the performance of VAGER is worse than LR for the \"Bubble\" class in the 1-shot binary classification setting, while it performs better for the other nine classes?",
    "ground_truth": "VAGER leverages transfer learning, while LR does not. This means VAGER attempts to apply knowledge from other classes to improve its performance on new classes. For nine out of the ten novel classes, this strategy seems to be successful, as VAGER consistently outperforms LR. However, for the \"Bubble\" class, the transfer learning approach seems to have a negative impact, causing VAGER to perform worse than LR.",
    "predicted_answer": "The performance of VAGER is worse than LR for the 'Bubble' class likely due to the transfer learning approach of VAGER not being well-suited for the specific characteristics of the 'Bubble' class in the 1-shot binary classification setting.",
    "reference": "1710.06177v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.03374233128834356,
      "phrase_overlap": 0.03374233128834356,
      "is_correct": false
    }
  },
  "1710.06177v2_q2": {
    "paper_id": "1710.06177v2",
    "question_index": 2,
    "question": "What is the relationship between the Similarity Ratio and AUC Increasing?",
    "ground_truth": "There is a positive linear relationship between the Similarity Ratio and AUC Increasing.",
    "predicted_answer": "There is a positive linear relationship between Similarity Ratio and AUC Increasing.",
    "reference": "1710.06177v2-Figure3-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9767441860465116,
      "phrase_overlap": 0.9767441860465116,
      "is_correct": true
    }
  },
  "1710.06177v2_q3": {
    "paper_id": "1710.06177v2",
    "question_index": 3,
    "question": "Which method performs best in the 10 classes 1-shot multi-class classification problem?",
    "ground_truth": "VAGER + Voting",
    "predicted_answer": "VAGER+Voting",
    "reference": "1710.06177v2-Figure5-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.9230769230769231,
      "phrase_overlap": 0.9230769230769231,
      "is_correct": true
    }
  },
  "1710.06177v2_q4": {
    "paper_id": "1710.06177v2",
    "question_index": 4,
    "question": "What is the relationship between the novel class and the top-3 most similar base classes?",
    "ground_truth": "The top-3 most similar base classes are the three classes that are most similar to the novel class, based on the embedding layer in a 5-shot setting.",
    "predicted_answer": "The relationship is that each novel class is matched with its top-3 most similar base classes based on visual or conceptual similarity.",
    "reference": "1710.06177v2-Figure4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3873239436619718,
      "phrase_overlap": 0.3873239436619718,
      "is_correct": true
    }
  },
  "1710.06177v2_q5": {
    "paper_id": "1710.06177v2",
    "question_index": 5,
    "question": "Which method performs the best when the number of shots is 50?",
    "ground_truth": "VAGER+Voting",
    "predicted_answer": "VAGER+Voting",
    "reference": "1710.06177v2-Figure2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 1.0,
      "phrase_overlap": 1.0,
      "is_correct": true
    }
  },
  "1809.04276v2_q0": {
    "paper_id": "1809.04276v2",
    "question_index": 0,
    "question": "Why is the model discouraged even though the generated response (RSP) incorporates relevant content from the N-best response candidates (C#1 and C#2)?",
    "ground_truth": "The model is discouraged because it is trained using the Maximum Likelihood Estimation (MLE) objective, which prioritizes generating responses that are identical to the ground-truth (GT) response. Even though the RSP integrates relevant content from the candidates and seems appropriate in the context, it is penalized because it deviates from the exact wording of the GT.",
    "predicted_answer": "The generated response (RSP), while incorporating relevant content from C#1 and C#2, does not align with the tone and focus of the ground truth (GT), which emphasizes curiosity about the learning process and a cheerful tone.因此答案是：B。",
    "reference": "1809.04276v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.12251655629139073,
      "phrase_overlap": 0.12251655629139073,
      "is_correct": false
    }
  },
  "1809.04276v2_q1": {
    "paper_id": "1809.04276v2",
    "question_index": 1,
    "question": "How does the performance of the discriminator in the proposed approach compare to the conventional discriminator in AL? What evidence suggests this difference in performance?",
    "ground_truth": "The discriminator in the author's approach achieves higher accuracy (95.72%) compared to the conventional discriminator in AL (94.01%).",
    "predicted_answer": "The discriminator in the proposed approach ('Ours') outperforms the conventional discriminator in AL, achieving an accuracy of 95.72% compared to 94.01% in AL.",
    "reference": "1809.04276v2-Table4-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.6054421768707483,
      "phrase_overlap": 0.6054421768707483,
      "is_correct": true
    }
  },
  "1809.04276v2_q2": {
    "paper_id": "1809.04276v2",
    "question_index": 2,
    "question": "Can you explain the discrepancy between the number of messages and responses in each dataset?",
    "ground_truth": "Number of responses is significantly higher than number of messages in each dataset (training, validation, and test). This is because each message can have multiple responses associated with it. The passage mentions that users on Sina Weibo can post messages and also comment on other users' messages. These comments are considered as responses in the context of the table. Therefore, one message can have several responses, leading to a higher total number of responses compared to messages.",
    "predicted_answer": "E",
    "reference": "1809.04276v2-Table2-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1809.04276v2_q3": {
    "paper_id": "1809.04276v2",
    "question_index": 3,
    "question": "How does the discriminator in the proposed REAT approach use the N-best response candidates?",
    "ground_truth": "The discriminator takes as input a response and the N-best response candidates, and outputs the probability that the response is human-generated.",
    "predicted_answer": "B",
    "reference": "1809.04276v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0136986301369863,
      "phrase_overlap": 0.0136986301369863,
      "is_correct": true
    }
  },
  "1710.01507v4_q0": {
    "paper_id": "1710.01507v4",
    "question_index": 0,
    "question": "What is the role of the LSTM network in the model architecture?",
    "ground_truth": "The LSTM network is used to process the post text and generate a post text embedding.",
    "predicted_answer": "A",
    "reference": "1710.01507v4-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.023255813953488372,
      "phrase_overlap": 0.023255813953488372,
      "is_correct": true
    }
  },
  "1709.00139v4_q0": {
    "paper_id": "1709.00139v4",
    "question_index": 0,
    "question": "Which method generally achieved a lower objective function value (OFV) for the different datasets, FISVDD or Incremental SVM? Does this imply that one method is definitively better than the other?",
    "ground_truth": "For all datasets presented, Incremental SVM achieved a slightly lower OFV compared to FISVDD. However, this does not necessarily mean that Incremental SVM is definitively better.",
    "predicted_answer": "FISVDD\n\n不，这并不意味着一种方法明显优于另一种方法。",
    "reference": "1709.00139v4-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.06763285024154589,
      "phrase_overlap": 0.06763285024154589,
      "is_correct": false
    }
  },
  "1809.01989v2_q0": {
    "paper_id": "1809.01989v2",
    "question_index": 0,
    "question": "Which method achieved the highest tracking accuracy in terms of minimizing the sum of absolute percentage errors? Does this necessarily mean it had the best overall performance?",
    "ground_truth": "The Ridge method achieved the lowest sum of absolute percentage errors (136.84), indicating the highest tracking accuracy in terms of minimizing absolute deviations from the index. However, this doesn't necessarily translate to the best overall performance.",
    "predicted_answer": "A",
    "reference": "1809.01989v2-Table1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.0,
      "phrase_overlap": 0.0,
      "is_correct": true
    }
  },
  "1709.02418v2_q0": {
    "paper_id": "1709.02418v2",
    "question_index": 0,
    "question": "What is the effect of performing a left-swap on a binary vector y at index j′?",
    "ground_truth": "The left-swap increases the number of misclassified pairs by one.",
    "predicted_answer": "Performing a left-swap on binary vector y at index j′=6 increases the Hamming distance from 4 to 5.",
    "reference": "1709.02418v2-Figure1-1.png",
    "question_type": "open_ended",
    "evaluation": {
      "similarity_score": 0.3902439024390244,
      "phrase_overlap": 0.3902439024390244,
      "is_correct": false
    }
  }
}