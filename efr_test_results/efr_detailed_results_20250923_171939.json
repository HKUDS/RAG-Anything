{
  "query_results": {
    "timestamp": "2025-09-23T17:17:21.144374",
    "queries": [
      {
        "query_id": 1,
        "question": "What is the main contribution of this paper?",
        "standard_rag": {
          "success": true,
          "response_time": 14.760050058364868,
          "response_length": 2454,
          "response": "### Main Contribution\n\nThe primary contribution of the paper is the introduction of ChartCitor, a multi-agent framework designed to enhance the accuracy and reliability of visual chart understanding in question-answering tasks. ChartCitor focuses on providing fine-grained bounding box citations that help link responses to specific elements within chart images. The system orchestrates several specialized agents to tackle challenges inherent in chart-to-text interpretation, such as integrating visual-semantic contexts and improving citation accuracy through structured reasoning and data extraction processes.\n\n### Key Features\n\n- **Multi-Agent Orchestration**: ChartCitor employs various LLM agents for tasks like chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval, and chart mapping, ensuring comprehensive and reliable visual interpretation for QA[^1^][KG ChartCitor - Information Extraction].\n\n- **Improved Performance**: It consistently outperforms existing methods across multiple chart types, highlighted by a top Intersection over Union (IoU) score of 27.4, demonstrating superior performance in visual grounding tasks[^2^][DC 2502.00989v1.pdf].\n\n- **User Trust and Productivity**: Qualitative user studies have shown that ChartCitor enhances user trust in generating AI-assisted answers and improves the productivity of professionals by providing clear and structured citations[^3^][DC 2502.00989v1.pdf].\n\n### Innovative Techniques\n\n- **Table Extraction and Captioning**: Utilizes GPT-4V and GPT-4o to transform chart images into structured formats and generate detailed row, column, and cell captions to clarify data points.\n- **Pre-filtering and Re-ranking**: Uses RankGPT and other LLM-based techniques to refine citation processes, reducing misinformation and enhancing citation precision[^4^][DC 2502.00989v1.pdf].\n- **Cell Localization and Evidence Mapping**: Determines exact locations of data marks within charts to visualize and verify the cited elements accurately[^5^][DC 2502.00989v1.pdf].\n\n### Conclusion\n\nChartCitor sets a new standard in multimodal language model applications, addressing the challenges of visual data interpretation and improving citation accuracy and user satisfaction.\n\n### References\n\n1. [DC] 2502.00989v1.pdf\n2. [KG] ChartCitor - Performance Comparison of Chart Interpretation Methods (table)\n3. [DC] 2502.00989v1.pdf\n4. [DC] 2502.00989v1.pdf\n5. [DC] 2502.00989v1.pdf"
        },
        "enhanced_efr": {
          "success": true,
          "response_time": 11.109987020492554,
          "response_length": 1492,
          "response": "### Main Contribution of the Paper\n\nThe paper presents **ChartCitor**, a multi-agent framework designed to enhance the accuracy of chart question-answering (ChartQA) systems by providing fine-grained bounding box citations. Leveraging large language models (LLMs), ChartCitor addresses the issue of hallucinations, which often results in semantically plausible but factually incorrect information generated by existing LLMs when interpreting charts. This framework aims to improve the integrity of LLM-generated answers by grounding them in visual data from charts and providing visual evidence for these answers.\n\nChartCitor works by orchestrating multiple LLM agents to perform tasks such as chart-to-table extraction, answer reformulation, table augmentation, and evidence retrieval. It outperforms existing methods, achieving higher Intersection over Union (IoU) scores, indicating its robustness in accurately mapping visual elements to source data. This system enhances user trust by facilitating explainable AI processes, which are demonstrated to improve productivity and verification speed in professional settings.\n\nA user study supports its effectiveness, showing that ChartCitor consistently provides more reliable citations than competing models like GPT-4o. The study emphasized its utility in verifying LLM-generated responses through enhanced visual supporting evidence, leading to higher perceived accuracy by users.\n\n### References\n\n- [KG] ChartCitor\n- [DC] 2502.00989v1.pdf"
        },
        "comparison": {
          "time_difference_percent": -24.729340506563783,
          "length_difference_percent": -39.20130399348003
        }
      },
      {
        "query_id": 2,
        "question": "What are the key technologies used in ChartCitor?",
        "standard_rag": {
          "success": true,
          "response_time": 16.412847995758057,
          "response_length": 2642,
          "response": "## Key Technologies Used in ChartCitor\n\nChartCitor is a sophisticated multi-agent framework designed for extracting and verifying information from chart images. It employs several advanced technologies to enhance its operations in understanding and processing chart data.\n\n### DETR\nDETR (Detection Transformer) is utilized in ChartCitor to identify visual data marks such as bars and pie slices within chart images. This image processing algorithm is pivotal in aiding the visualization process, allowing ChartCitor to accurately map visual elements in charts [KG] ChartCitor - DETR.\n\n### Large Language Models (LLMs)\nChartCitor integrates various Large Language Models to execute diverse tasks associated with chart data:\n- **GPT-4V** is used for tasks like chart-to-table extraction and visual fact-checking. Although its performance in visual grounding was less effective compared to other methods, it plays a role in data extraction and answer reformulation [KG] ChartCitor - GPT-4V.\n- **GPT-4o** and **Claude-3.5 Sonnet** are also employed for generating descriptive captions and processing textual data, crucial for various chart analysis tasks [KG] ChartCitor - GPT-4o, [KG] ChartCitor - Claude-3.5 Sonnet.\n- **Llama-3.2** assists in interpreting chart images and improving question-answering tasks [KG] ChartCitor - Llama-3.2.\n\n### Multi-Agent Framework\nChartCitor uses a multi-agent framework composed of various specialized agents, including:\n- **Table Extraction Agent:** Converts input charts into structured formats like HTML tables [KG] Chart-to-Table Extraction Agent - ChartCitor.\n- **Entity Captioning Agent:** Generates contextual descriptions for table data to improve citation accuracy [KG] ChartCitor - Entity Captioning Agent.\n- **LLM Prefiltering and Re-Ranking Agents:** Work together to ensure that only relevant data is utilized for final output, enhancing efficiency and accuracy in citation retrieval [DC] 2502.00989v1.pdf.\n- **Cell Localization Agent:** Maps cited table cells to visual elements in charts, using algorithms to identify marked data points [KG] Cell Localization Agent - ChartCitor System Workflow Diagram (image).\n\nThese technologies collectively empower ChartCitor to achieve superior performance in visual chart understanding, particularly in citation accuracy and robustness in data synthesis. This framework outperforms other methods in various evaluations, making it a leader in multimodal retrieval and information extraction from visual data.\n\n## References\n- [KG] ChartCitor - DETR\n- [KG] ChartCitor - GPT-4V\n- [KG] ChartCitor - GPT-4o\n- [KG] ChartCitor - Llama-3.2\n- [DC] 2502.00989v1.pdf"
        },
        "enhanced_efr": {
          "success": true,
          "response_time": 14.92525601387024,
          "response_length": 2369,
          "response": "### Key Technologies Used in ChartCitor\n\nChartCitor is a sophisticated multi-agent framework designed to enhance the accuracy of visual chart understanding and question answering. Here are the key technologies involved in its operation:\n\n1. **Large Language Models (LLMs)**: ChartCitor employs a range of LLMs, such as GPT-4o and GPT-4V, for tasks like generating descriptive captions, processing textual data, and integrating multimodal retrieval, which combines information from text and visual data to improve accuracy in chart-based queries [KG] ChartCitor - LLM Agents. Additionally, Claude-3.5 Sonnet and Llama-3.2 are used for contextual learning and interpretation [KG] ChartCitor - Claude-3.5 Sonnet, [KG] ChartCitor - Llama-3.2.\n\n2. **Chart-to-Table Extraction Agent**: This component converts visual chart data into structured tables, such as HTML, allowing for systematic data processing and analysis [KG] Chart-to-Table Extraction Agent - ChartCitor.\n\n3. **DETR Algorithm**: ChartCitor uses the DETR algorithm for visual data processing to identify elements like bars and pie slices within chart images, which helps in visual fact-checking and accuracy enhancement [KG] ChartCitor - DETR.\n\n4. **RankGPT and Re-Ranking Methods**: This is used to improve the citation retrieval process by ranking table cells based on their relevance to the query or facts that need attribution, optimizing citation accuracy [KG] ChartCitor - RankGPT.\n\n5. **Cell Localization Agent**: This agent generates bounding box coordinates for visual elements in charts, ensuring that the visual evidence aligns precisely with cited data points, a crucial feature for fine-grained structured chart attribution [KG] Cell Localization Agent - ChartCitor, [DC] 2502.00989v1.pdf.\n\n### Conclusion\n\nThese technologies collectively enable ChartCitor to leverage both textual and visual data efficiently, ensuring accurate, evidenced-based question answering over chart images. The integration of LLMs with other advanced visual processing techniques results in a model that significantly outperforms its contemporaries in tasks requiring visual grounding and information extraction [DC] 2502.00989v1.pdf.\n\n### References\n\n- [KG] ChartCitor - LLM Agents\n- [KG] Chart-to-Table Extraction Agent - ChartCitor\n- [KG] ChartCitor - DETR\n- [KG] ChartCitor - Claude-3.5 Sonnet\n- [DC] 2502.00989v1.pdf"
        },
        "comparison": {
          "time_difference_percent": -9.063582275740867,
          "length_difference_percent": -10.333080999242998
        }
      },
      {
        "query_id": 3,
        "question": "How does the multi-agent framework work?",
        "standard_rag": {
          "success": true,
          "response_time": 15.185051918029785,
          "response_length": 2884,
          "response": "### Multi-Agent Framework of ChartCitor\n\nThe ChartCitor framework is designed to enhance visual chart understanding through a multi-agent system. It orchestrates multiple Large Language Model (LLM) agents to perform complex tasks associated with chart data, including extraction, citation, and visual grounding.\n\n#### Key Components:\n\n1. **Table Extraction Agent**: The process begins with the `Table Extraction Agent`, which transforms input chart images into structured HTML tables using few-shot examples. This step ensures that raw chart data is accessible in a format suitable for further analysis [DC 2502.00989v1.pdf].\n\n2. **Entity Captioning Agent**: Once the charts are converted into tables, the `Entity Captioning Agent` generates contextual descriptions of rows, columns, and individual cells. This agent enriches the tabular data with semantic context necessary for accurate fact extraction and citation validation [DC 2502.00989v1.pdf].\n\n3. **LLM Prefiltering Agent**: This agent hypothesizes and filters out irrelevant table rows and columns by assigning relevance scores. It reduces noise and ensures efficiency by passing only pertinent data to the next phase [KG LLM Pre-filtering Agent].\n\n4. **LLM Re-Ranking Agent**: After pre-filtering, the `LLM Re-Ranking Agent` further refines the selection by re-ranking table cells based on their relevance to answer claims. It employs `RankGPT` to enhance the precision of citation retrieval [KG LLM Prefiltering Agent - LLM Re-ranking Agent].\n\n5. **Answer Reformulation Agent**: This component reformulates answers into structured facts, highlighting missing citations to ensure that responses are both relevant and accurate [KG Answer Reformulation Agent].\n\n6. **Cell Localization Agent**: This agent maps table cells to visual elements within the chart image, using bounding box coordinates for precise localization of data points [KG Cell Localization Agent - ChartCitor]. This step involves identifying visual segments of the chart that correspond to specific data points extracted from the table [DC 2502.00989v1.pdf].\n\n7. **Answer Citations**: The system concludes with embedding citation links within the reformulated answers. This provides users with reliable citations, enhancing clarity and trust [DC 2502.00989v1.pdf].\n\n#### Evaluation and Performance\n\nChartCitor's performance is robust, achieving high accuracy in visual grounding tasks, as reflected by its top IoU score compared to other methods like LISA and Kosmos-2 [DC 2502.00989v1.pdf]. The system is particularly effective in providing precise citations and has been positively received in user studies, significantly outperforming baseline models like GPT-4o in accuracy [DC 2502.00989v1.pdf].\n\n### References\n\n- [KG] LLM Prefiltering Agent\n- [KG] LLM Prefiltering Agent - LLM Re-ranking Agent\n- [KG] Answer Reformulation Agent\n- [DC] 2502.00989v1.pdf"
        },
        "enhanced_efr": {
          "success": true,
          "response_time": 14.088172912597656,
          "response_length": 2661,
          "response": "### Multi-Agent Framework in ChartCitor\n\nThe ChartCitor framework is designed as a multi-agent system specifically for processing and understanding chart data to provide accurate answer citations. This system orchestrates several specialized agents, each handling a distinct aspect of the task.\n\n1. **Chart-to-Table Extraction Agent**: This agent converts chart images, commonly found in PDFs or scanned documents, into structured data formats like HTML tables. It leverages models like GPT-4V to comprehend and translate chart data into an organized table format, using few-shot examples for improved consistency and accuracy in data representation [DC] 2502.00989v1.pdf.\n\n2. **Entity Captioning Agent**: Focused on enhancing the semantic understanding of table data, this agent uses large language models (LLMs) to generate detailed contextual descriptions for each table component—rows, columns, and individual cells. The descriptions provide a richer narrative around numeric values, symbols, and technical terms within the tables, aiding in effective future evidence extraction [KG] ChartCitor - Entity Captioning Agent.\n\n3. **LLM Pre-filtering and Re-ranking Agents**: These agents work in tandem to refine the data before final retrieval. The Pre-filtering Agent applies a score filter to select relevant table elements, while the Re-ranking Agent uses mechanisms like RankGPT to ensure the retrieved data directly supports answer claims, thus improving the accuracy and relevance of the citations [KG] LLM Pre-filtering Agent - LLM Re-ranking Agent.\n\n4. **Cell Localization Agent**: This agent identifies the corresponding visual elements in the chart images linked to the table cells. It employs algorithms to derive bounding box coordinates that map cited table data points to visual representations, ensuring precise information localization within the chart [KG] Cell Localization Agent - ChartCitor.\n\n5. **Answer Reformulation Agent**: This component is crucial for reorganizing the initial answers into more structured facts. It refines the responses, highlighting any missing citations, and prepares them for integration with relevant citations, thereby enhancing clarity and trust [KG] Answer Reformulation Agent - ChartCitor.\n\nOverall, the multi-agent framework in ChartCitor improves the explainability of LLM-generated responses by grounding them in visual data through comprehensive analysis and precise mapping.\n\n### References\n- [KG] ChartCitor - Entity Captioning Agent\n- [KG] LLM Pre-filtering Agent - LLM Re-ranking Agent\n- [KG] Cell Localization Agent - ChartCitor\n- [KG] Answer Reformulation Agent - ChartCitor\n- [DC] 2502.00989v1.pdf"
        },
        "comparison": {
          "time_difference_percent": -7.223412941576861,
          "length_difference_percent": -7.732316227461858
        }
      },
      {
        "query_id": 4,
        "question": "What are the experimental results and performance metrics?",
        "standard_rag": {
          "success": true,
          "response_time": 13.32047414779663,
          "response_length": 1607,
          "response": "## Experimental Results and Performance Metrics\n\nThe experimental results for the ChartCitor system highlight its superiority in handling visual grounding tasks compared to other models. In terms of the Intersection over Union (IoU) scores, which evaluate the accuracy of visual element mapping, ChartCitor outperformed all evaluated methods with an IoU score of 27.4. This indicates its robustness in accurately mapping visual elements within charts. Compared to historical models like Kosmos-2 and LISA, which had considerably lower IoU scores of 3.89 and 4.34 respectively, ChartCitor demonstrates a significant advancement in visual chart understanding and interpretation [DC] 2502.00989v1.pdf.\n\nAdditionally, an image-based analysis of the user evaluation results showed that ChartCitor achieved higher accuracy levels in chart-based question answering compared to GPT-4o. In a user accuracy comparison, ChartCitor surpassed GPT-4o, especially in the 'Completely ACCURATE' category, where it scored above 60% versus GPT-4o's roughly 40% performance. This further substantiates ChartCitor's capability in providing precise citations and improving user trust in generative AI [DC] 2502.00989v1.pdf.\n\nThe system uses a multi-agent framework to process charts, which includes LLM Prefiltering for filtering irrelevant information, and LLM Re-Ranking to select the most relevant table cells for citation, enhancing the overall accuracy of chart interpretation and citation retrieval [KG] ChartCitor - LLM Prefiltering Agent.\n\n## References\n\n- [DC] 2502.00989v1.pdf\n- [KG] ChartCitor - LLM Prefiltering Agent"
        },
        "enhanced_efr": {
          "success": true,
          "response_time": 12.69719123840332,
          "response_length": 1905,
          "response": "## Experimental Results and Performance Metrics\n\n### Performance Metrics\n\nThe evaluation of ChartCitor demonstrated its superior performance across various metrics. Specifically, the performance was assessed using the Intersection over Union (IoU) score, a common metric in visual grounding tasks. The results indicated that ChartCitor achieved an impressive IoU score of 27.4, significantly outperforming other models such as Kosmos-2 and LISA, which had IoU scores of 3.89 and 4.34, respectively [KG] [DC] 2502.00989v1.pdf. This suggests that ChartCitor is highly effective in accurately mapping visual elements in chart images.\n\n### Accuracy Comparison\n\nIn a user evaluation, ChartCitor was found to surpass GPT-4o in accuracy. It excelled particularly in the 'Completely ACCURATE' category, where it scored above 60%, compared to about 40% for GPT-4o. The evaluation also highlighted ChartCitor's lower incidence in the 'Completely INACCURATE' category, with percentage scores significantly better than GPT-4o [DC3].\n\n### User Evaluation Results\n\nThe user evaluations incorporated feedback from five participants who assessed 250 question-answer pairs involving chart images. ChartCitor's performance was favorably received, with participants rating its citations as either 'Completely Accurate' or 'Somewhat Accurate' more frequently than those of GPT-4o. Specifically, ChartCitor garnered a 'Completely Accurate' rating of 41% compared to 28% for GPT-4o. This reflects positively on its effectiveness in chart question-answering tasks [KG] [DC4].\n\nOverall, the experimental results underscore the robustness and reliability of ChartCitor in visual chart understanding and its ability to enhance user trust by improving multimodal content generation through accurate citations.\n\n### References\n- [KG] ChartCitor - Performance Comparison of Chart Interpretation Methods (table)\n- [DC] 2502.00989v1.pdf"
        },
        "comparison": {
          "time_difference_percent": -4.67913455990911,
          "length_difference_percent": 18.54387056627256
        }
      },
      {
        "query_id": 5,
        "question": "How does ChartCitor compare with other baseline methods?",
        "standard_rag": {
          "success": true,
          "response_time": 14.118351697921753,
          "response_length": 2403,
          "response": "### Evaluation of ChartCitor Performance\n\nChartCitor demonstrates superior performance in visual chart understanding tasks compared to several baseline methods. This evaluation is primarily based on Intersection over Union (IoU) scores, a metric used to assess accuracy in visual grounding tasks. ChartCitor achieves an IoU score of 27.4, which is significantly higher than other models like Kosmos-2 and LISA that have IoU scores of 3.89 and 4.34, respectively. This indicates that ChartCitor is more accurate in mapping visual elements to their corresponding data points in chart images [KG] ChartCitor - Performance Comparison of Chart Interpretation Methods (table); [KG] Performance Scores - Performance Comparison of Chart Interpretation Methods (table).\n\n### Comparison with Specific Models\n\nWhen compared to other advanced models such as DETR with Set-of-Marks Prompting and Claude-3.5 Sonnet, ChartCitor still outperforms these with better IoU ratings. DETR [2] + Set-of-Marks Prompting achieves an IoU of 18.6, while Claude-3.5 has a score of 13.8, indicating that while these models show improvements over historical models, ChartCitor's fine-grained bounding box citation capability positions it at the forefront [KG] Advanced Models - Performance Comparison of Chart Interpretation Methods (table); [DC] 2502.00989v1.pdf.\n\n### User Evaluation Findings\n\nThe user evaluation results signify a strong positive reception towards ChartCitor. In a study involving question-answer pairs related to chart images, users found that ChartCitor's attributions were \"Completely Accurate\" in 41% of cases, a higher percentage compared to GPT-4o's 28%. Furthermore, users noted that ChartCitor reduces the time required for verification significantly, highlighting its utility in practical scenarios [DC] 2502.00989v1.pdf.\n\n### Conclusion\n\nOverall, ChartCitor exhibits superior performance in both quantitative metrics and user evaluations compared to baseline models by effectively grounding generated answers in chart data, thereby enhancing user trust and productivity in Generative AI applications.\n\n### References\n\n- [KG] ChartCitor - Performance Comparison of Chart Interpretation Methods (table)\n- [KG] Performance Scores - Performance Comparison of Chart Interpretation Methods (table)\n- [KG] Advanced Models - Performance Comparison of Chart Interpretation Methods (table)\n- [DC] 2502.00989v1.pdf"
        },
        "enhanced_efr": {
          "success": true,
          "response_time": 12.09553599357605,
          "response_length": 1813,
          "response": "### ChartCitor Performance Comparison\n\nChartCitor is a leading model in visual chart interpretation, outshining both historical and some contemporary models across several metrics. It is a multi-agent framework designed for fine-grained citation and information extraction from chart images, aiming to provide accurate answer citations for chart-based question-answering tasks [KG] ChartCitor.\n\n#### Comparative Analysis with Baselines\n\n1. **Intersection over Union (IoU) Scores**: In a performance comparison focused on visual grounding tasks, ChartCitor achieved the highest Intersection over Union (IoU) score of 27.4. This score significantly surpasses that of other models such as Kosmos-2 with an IoU of 3.89 and LISA, which scored 4.34. Even more recent models like GPT-4V and Claude-3.5 managed only 12.5 and 13.8, respectively, indicating ChartCitor’s superior capability in mapping visual elements [DC] 2502.00989v1.pdf.\n\n2. **User Evaluation**: ChartCitor's effectiveness extends beyond just quantitative metrics. User evaluations positioned ChartCitor as more reliable than GPT-4o, particularly in the \"Completely ACCURATE\" category where ChartCitor's performance exceeded that of GPT-4o. The user study indicated a strong positive reception, with ChartCitor achieving higher accuracy in providing citations for chart-based answers [DC] 2502.00989v1.pdf.\n\n3. **Enhanced Citation and Verification**: ChartCitor utilizes several agents to improve citation accuracy. This includes prefiltering and re-ranking of data to ensure that the most relevant information is highlighted, thus minimizing hallucinations and increasing user trust in AI-generated responses [KG] ChartCitor - LLM Agents.\n\n### References\n\n- [KG] ChartCitor\n- [KG] ChartCitor - LLM Agents\n- [DC] 2502.00989v1.pdf\n- [DC] 2502.00989v1.pdf"
        },
        "comparison": {
          "time_difference_percent": -14.327562789383306,
          "length_difference_percent": -24.552642530170623
        }
      }
    ]
  },
  "efr_features": {
    "multilingual_support": true,
    "hybrid_reranking": true,
    "robustness_check": true,
    "source_reliability": true,
    "adaptive_mmr": true,
    "mmr_top_k": 5,
    "modality_weights": {
      "text": 1.0,
      "table": 1.2,
      "equation": 1.1,
      "image": 0.8
    },
    "language_weights": {
      "zh": 1.0,
      "en": 1.0
    },
    "multimodal_query": {
      "success": true,
      "response_length": 2297,
      "response": "### Analysis of Performance Table\n\nThe performance table analyzes different models used for visual grounding tasks, evaluating them based on their Intersection over Union (IoU) scores, a standard metric to assess the accuracy of object detection models by comparing predicted to actual bounding boxes [KG] Intersection over Union (IoU). \n\n### Best Performing Model\n\n**ChartCitor** is clearly identified as the best-performing model, achieving the highest IoU score of 27.4. This performance significantly surpasses other models such as DETR [2]+ Set-of-Marks Prompting [21], which scores 18.6, and historical models like Kosmos-2 and LISA, which score much lower at 3.89 and 4.34, respectively [DC] 2502.00989v1.pdf. The superior IoU score of ChartCitor indicates its robustness in accurately mapping visual elements, confirming its advanced capabilities in visual chart understanding tasks compared to other models [KG] ChartCitor.\n\n### Performance Comparison\n\n- **ChartCitor**: Highest IoU score (27.4), indicating its effectiveness in visual grounding. It's described as employing a multi-agent framework that enhances its ability to generate fine-grained bounding box citations, which likely contributes to its superior performance [KG] ChartCitor.\n  \n- **DETR [2]+ Set-of-Marks Prompting [21]**: Achieves a moderate IoU score (18.6), reflecting improved performance over historical models but still lagging behind ChartCitor [KG] DETR [2]+ Set-of-Marks Prompting [21].\n\n- **Kosmos-2 and LISA**: Exhibit lower performance with IoU scores of 3.89 and 4.34, respectively, highlighting their limitations in handling complex visual and numerical reasoning tasks [KG] Kosmos-2 [KG] LISA.\n\n### Conclusion\n\nThe analysis underscores ChartCitor's substantial advancements in AI-supported visual grounding tasks, as evidenced by its leading IoU score. The distinct performance gap highlights the model's capability to enhance citation accuracy, reduce hallucination, and improve multi-modal language understanding, an achievement that aligns with the broader claims regarding its effectiveness in chart analysis [DC] 2502.00989v1.pdf.\n\n### References\n\n- [KG] Intersection over Union (IoU)\n- [KG] ChartCitor\n- [KG] DETR [2]+ Set-of-Marks Prompting [21]\n- [KG] Kosmos-2\n- [KG] LISA\n- [DC] 2502.00989v1.pdf"
    }
  }
}