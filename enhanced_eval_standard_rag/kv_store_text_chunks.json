{
  "chunk-674ae697ba691bf3c9f7514577fa45ad": {
    "tokens": 1200,
    "content": "ChartCitor: Answer Citations for ChartQA via Multi-Agent LLM Retrieval \n\nKanika Goswami IGDTUW, Delhi India \n\nPuneet Mathur Adobe Research USA \n\nRyan Rossi Adobe Research USA \n\nFranck Dernoncourt Adobe Research USA \n\nAbstract \n\nLarge Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through prefiltering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLMassisted chart QA and enables professionals to be more productive. \n\nCCS Concepts \n\n• Information systems Information extraction. \n\nKeywords \n\nVisual Fact Checking, Information Extraction, Multimodal Retrieval, LLM Agents \n\nACM Reference Format: \n\nKanika Goswami, Puneet Mathur, Ryan Rossi, and Franck Dernoncourt. 2025. ChartCitor: Answer Citations for ChartQA via Multi-Agent LLM Retrieval. In . ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn \n\n1 Introduction \n\nChart data finds extensive use across diverse domains such as healthcare, finance, and education. Recently, LLMs such as Llama-3.2 [16], Claude-3.5 Sonnet, and GPT-4V [1] have proven effective in utilizing in-context learning and visual prompting to interpret and reason over chart images. However, these models tend to hallucinate — generate answers with semantically plausible but factually incorrect information — which undermines their reliability and erodes user trust [14, 19]. While existing approaches attempt to address hallucination by grounding LLM-generated responses in source documents through citation mechanisms [7], charts present unique challenges: (i) complex mapping between visual elements and underlying data, (ii) limited contextual information due to compressed visual data representation, (iii) difficulty in localizing chart elements across diverse visualization types and layouts, and (iv) ambiguity in alignment between text descriptions and visual elements. Prior research has explored various approaches to address this challenge, including instruction tuning [8], in-context learning [5], and natural-language inference (NLI)-based post-hoc attribution methods [4]. However, these approaches have primarily focused on attributing entire charts rather than specific structural elements [6], limiting their practical utility. To address these limitations, we propose ChartCitor, a system that provides visual evidence for generated answers by identifying and highlighting relevant chart elements through bounding box annotations. ChartCitor works by orchestrating multiple specialized LLM agents to: (1) extract structured data table from charts, (2) break down answers into logical steps, (3) generate contextual descriptions for rows/columns, (4) identify supporting evidence through pre-filtering and re-ranking to connect specific table cells to claims, and (5) localize the selected cells in the chart image. ChartCitor helps professionals save time on fact-checking LLM-generated answers and enhances user trust by providing reliable and logically-explained citations sourced from charts. \n\n2 ChartCitor \n\nWe aim to solve the Fine-grained Structured Chart Attribution task which involves identifying graph elements (e.g bars, lines, pies in chart images) that support factual claims in a generated text response to a user’s question. We propose ChartCitor (Fig. 1) – a multiagent framework that provides fine-grained citations for generated answers grounded in chart image by orchestrating multiple LLM agents, which is explained as follows: \n\n(1) Chart2Table Extraction Agent: Charts are predominantly present in PDFs or scanned documents that need to be converted into structured table formats (e.g., CSV, or HTML). We utilize GPT-4V to comprehend PDF images and output corresponding HTML without the need for external OCR using few shot prompting to identify cell data across each row/column. We use visual self-reflection [13] to provide the GPT-4V with its own rendered HTML and data table output to check for consistency between the re-plotted LLM output and the original chart. In case of inconsistencies in the data extraction, the LLM refines its output until the extracted table data is error-free. (2) Answer Reformulation Agent: The answer to be attributed, which can be AI-generated or otherwise, may be composed of multiple facts, numerical formulations and multi-hop logic. Each fact may be sourced from a different row/column in the chart table. To facilitate precise citations, re-framing the answer statement into a chain of reasoning steps helps to better retrieve the correct citations from the table. We convert the answer statement into a hierarchy of reasoning thoughts/arguments via few-shot in-context prompting, ensuring the resultant answer arguments are independent sentences without any deviation in their collective meaning from the original answer statement. \n\n(3) Entity Captioning Agent: Understanding tabular data extends beyond simple cell interpretation, requiring comprehension of how cell information relate to both the table’s structure and its broader context. Tables often present analytical challenges through ambiguous content, including technical terminology, contextless numeric values, domain-specific symbols, and hierarchical row/column headers. These ambiguities impede reliable evidence extraction and citation validation through semantic matching. Our solution leverages LLMs in an unsupervised manner to generate rich, multi-layered contextual descriptions: (i) Row Captioning: Our",
    "chunk_order_index": 0,
    "full_doc_id": "doc-b720ee4f8234da6eab49046df331006e",
    "file_path": "2502.00989v1.pdf",
    "llm_cache_list": [
      "default:extract:fb606e891fba6d421fb220d90fabbbc1",
      "default:extract:103b6fa92ee8a03dedf206ee2dde5fc6"
    ],
    "create_time": 1758613406,
    "update_time": 1758613406,
    "_id": "chunk-674ae697ba691bf3c9f7514577fa45ad"
  },
  "chunk-b839fa86d69937abbbfac9da3a686d58": {
    "tokens": 1200,
    "content": "ular data extends beyond simple cell interpretation, requiring comprehension of how cell information relate to both the table’s structure and its broader context. Tables often present analytical challenges through ambiguous content, including technical terminology, contextless numeric values, domain-specific symbols, and hierarchical row/column headers. These ambiguities impede reliable evidence extraction and citation validation through semantic matching. Our solution leverages LLMs in an unsupervised manner to generate rich, multi-layered contextual descriptions: (i) Row Captioning: Our system employs GPT-4o to generate comprehensive row-level descriptions that capture complex patterns across features, summarize temporal trends, highlight significant dates and provide comparative analysis with respect to outliers within each row. (ii) Column Captioning: We generate detailed captions for each column using GPT-4o to explain ambiguous measurement units, symbols, empty cell spaces, and technical relationship of it’s contents with corresponding row headers. (iii) Cell Captioning: Row and column-level captions may highlight broader trends but the fine-grained cell level information needs to be contextualized in terms of its associated row and column headers. Captioning agent uses GPT-4o to describe the importance of each cell in the context of its associated row and columns. \n\nTable Cell Retrieval: We use retrieve-then-rank approach to identify the most relevant table cells. Our two-step approach begins with LLM-based pre-filtering to reject irrelevant rows and columns. We then employ LLM re-ranking to retrieve the most precise cell-level matches, ensuring both comprehensive coverage and accuracy in the final selection. \n\n(4) LLM Pre-filtering Agent: We hypothesize that some of the table rows/ columns are likely to be unrelated to the answer facts. Passing irrelevant and distracting table entities to the LLM-based re-ranker can mislead it, negatively impacting the ranking process. Inspired by [11], the LLM-based pre-filtering step uses chain-of-thought [18] followed by Plan and Solve [17] prompting techniques to generate a relevance score for each row/column based on the significance of its descriptive caption to the given answer statement (between 0 to 1). Additionally, we prompt the LLM to explain its rationale behind the score generation to enhance explainability and avoid hallucinations. We establish a specific threshold (usually $0 . 3 - 0 . 5 )$ ) for row/column filtering to retain potential citations that are sent to the re-ranker, discarding those falling below the threshold. This implementation significantly reduces the number of noisy of rows and columns that can misguide the re-ranker, leading to an improved citation retrieval performance. \n\n(5) LLM Re-ranking Agent: LLMs providing citations that don’t directly support their claims can be interpreted as a form of hallucination which may diminish user confidence. To solve this, we retrieve the set of table cells that are collectively both sufficient and directly relevant to the answer claims. We use RankGPT [15], a listwise LLM re-ranker to re-rank the table cells extracted from the intersection of rows and columns selected in the pre-filtering stage. Additionally, we prompt GPT-4o to provide a layer-of-thought [3] explanation of its rationale in ranking the cell items, enhancing the transparency in the citation mechanism by enforcing a logical coherence in the evidence chain. \n\n(6) Cell Localization Agent: The final step maps cited table cells to their corresponding visual elements in the chart image. This agent leverages DETR trained [2] on ChartQA data to identify all possible data marks (bars, line segments, pie slices) using image processing algorithms. GPT-4V with few-shot set-of-marks prompting [20] then identifies elements corresponding to the cited cells. The agent generates bounding box coordinates for the relevant visual elements, employing visual self-reflection to verify precise correspondence between highlighted regions and cited data points. \n\n3 Implementation Details \n\nAll constituent agents utilize textual LLM APIs such as those provided by OpenAI (GPT-4o, GPT-4V) or Claude Sonnet-3.5. We use ChatGPT $- 4 0$ as the base multimodal language model (MLLM) for ChartCitor. We convert data tables from TabCite benchmark [10] into bar/pie/line charts along with paired QA. \n\nEvaluation: We adopt visual Intersection over Union (IoU) as principal metric for chart attribution tasks. Detected regions in the chart image are matched to ground truth regions (e.g., bars in barplot or pies in piechart) based on a threshold value of $\\mathrm { I o U } \\ge 0 . 9 .$ Unlike bar charts and pie charts, where detected regions can be matched to discrete ground truth regions, line charts involve discrete points. Since grounding models generate bounding boxes or regions, we compute the proportion of ground truth points covered within the detected region(s) over total points detected. \n\nBaselines: (1) Zero-shot LLM Bounding Box Prompting – We prompt GPT-4o and Claude 3.5 Sonnet to predict normalized bounding box coordinates for chart components based on input text and the visual chart. (2) Kosmos-2 [12] is a multimodal LLM with textto-visual grounding capabilities. It represents object locations as Markdown links for generating bounding boxes for visual grounding tasks. (3) LISA (Large Language Instructed Segmentation Assistant) [9] is a reasoning-based zero-shot segmentation model that generates masks from implicit and complex textual queries. \n\n4 Results and Discussion \n\nTable 2(a) shows quantitative results that demonstrate that ChartCitor consistently outperforms the baselines across all chart types, highlighting its robustness and effectiveness in visual chart understanding. ChartCitor achieves better performance compared to directly prompting LLMs to predict bounding boxes. Further, even \n\nusing GPT-4V with set of marks prompting",
    "chunk_order_index": 1,
    "full_doc_id": "doc-b720ee4f8234da6eab49046df331006e",
    "file_path": "2502.00989v1.pdf",
    "llm_cache_list": [
      "default:extract:16a34d078334d123c496ff38975b5418",
      "default:extract:8a31cbb2bac47d27430a15320dac9ad0"
    ],
    "create_time": 1758613406,
    "update_time": 1758613406,
    "_id": "chunk-b839fa86d69937abbbfac9da3a686d58"
  },
  "chunk-9af5c81195bb45458709701521978b72": {
    "tokens": 1200,
    "content": "ed Segmentation Assistant) [9] is a reasoning-based zero-shot segmentation model that generates masks from implicit and complex textual queries. \n\n4 Results and Discussion \n\nTable 2(a) shows quantitative results that demonstrate that ChartCitor consistently outperforms the baselines across all chart types, highlighting its robustness and effectiveness in visual chart understanding. ChartCitor achieves better performance compared to directly prompting LLMs to predict bounding boxes. Further, even \n\nusing GPT-4V with set of marks prompting over detected chart elements show weak performance. Kosmos-2 and LISA perform poorly, with very low IoU scores, highlighting their inability to handle factual grounding in charts due to insufficient visual and numerical reasoning. Interestingly, all tested method including our proposed ChartCitor, zero-shot LMM prompting, LISA and KOSMOS2 struggle with interpreting complex geometrical proportions in pie charts due to their difficulty in handling non-rectangular bounding box segmentation task. Further, we conducted a user study (Fig. 2(b) to evaluate the citation accuracy and perceived utility of fine-grained chart attribution provided by ChartCitor. Five participants evaluated 250 randomly sampled question-answer pairs with associated chart images to study the usefulness and accuracy of the citations provided by ChartCitor compared to direct GPT-4o prompting. The evaluation results demonstrated strong positive reception, with participants rating the attributions as Completely Accurate $4 1 \\%$ vs $2 8 \\%$ ) or Somewhat Accurate ( $1 7 \\%$ vs $1 5 \\%$ ) for verifying chart-based question answering accuracy in ChartCitor and GPT-4o, respectively. Attributions were found to be more \"Completely Inaccurate\" ChartCitor than GPT-4o ( $1 7 \\%$ vs $3 1 \\%$ ). Participants described the citations as a handy tool in making verification of LLM-generated answers easier (\"...can help me to quickly verify trends in charts, cutting down the time I spent on 10 documents from 5 hrs to 20 mins.\"). \n\n5 Conclusion \n\nWe introduced ChartCitor, which grounds LLM-generated QA responses to chart elements using agentic orchestration and set-ofmarks prompting. The system outperforms baselines by $9 . 1 5 \\%$ and shows promise for rich document QA over PDF collections. While currently effective for single-chart citations, future work can address multi-chart interactions, hallucination mitigation, and explicit citation-text mapping to enhance trustworthy multimodal content generation. \n\nReferences \n\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). \n\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with transformers. In European conference on computer vision. Springer, 213–229.   \n[3] Wachara Fungwacharakorn, Nguyen Ha Thanh, May Myo Zin, and Ken Satoh. 2024. Layer-of-Thoughts Prompting (LoT): Leveraging LLM-Based Retrieval with Constraint Hierarchies. arXiv preprint arXiv:2410.12153 (2024).   \n[4] Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023. RARR: Researching and Revising What Language Models Say, Using Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 16477–16508. https://doi.org/10.18653/v1/2023.acllong.910   \n[5] Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling Large Language Models to Generate Text with Citations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 6465–6488. https://doi.org/10.18653/v1/2023.emnlp-main.398   \n[6] Siqing Huo, Negar Arabzadeh, and Charles Clarke. 2023. Retrieving supporting evidence for generative question answering. In Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region. 11–20.   \n[7] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. Comput. Surveys 55, 12 (2023), 1–38.   \n[8] Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur, and Jimmy Lin. 2023. HAGRID: A Human-LLM Collaborative Dataset for Generative InformationSeeking with Attribution. arXiv:2307.16883 (2023).",
    "chunk_order_index": 2,
    "full_doc_id": "doc-b720ee4f8234da6eab49046df331006e",
    "file_path": "2502.00989v1.pdf",
    "llm_cache_list": [
      "default:extract:e7cdcf63301d81ea34478564ec052894",
      "default:extract:7b2ab7d5d11986023cb121d9287d36a6"
    ],
    "create_time": 1758613406,
    "update_time": 1758613406,
    "_id": "chunk-9af5c81195bb45458709701521978b72"
  },
  "chunk-30a776bce427dd986de9cf5abd6e011f": {
    "tokens": 1131,
    "content": "ale Fung. 2023. Survey of hallucination in natural language generation. Comput. Surveys 55, 12 (2023), 1–38.   \n[8] Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur, and Jimmy Lin. 2023. HAGRID: A Human-LLM Collaborative Dataset for Generative InformationSeeking with Attribution. arXiv:2307.16883 (2023).   \n[9] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. 2024. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9579– 9589.   \n[10] Puneet Mathur, Alexa Siu, Nedim Lipka, and Tong Sun. 2024. MATSA: MultiAgent Table Structure Attribution. In Conference on Empirical Methods in Natural Language Processing. https://aclanthology.org/2024.emnlp-demo.26/   \n[11] Baharan Nouriinanloo and Maxime Lamothe. 2024. Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models. arXiv preprint arXiv:2406.18740 (2024).   \n[12] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023).   \n[13] Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection. ArXiv abs/2303.11366 (2023). https://api.semanticscholar.org/CorpusID:257636839   \n[14] Ben Snyder, Marius Moisescu, and Muhammad Bilal Zafar. 2023. On Early Detection of Hallucinations in Factual Question Answering. ArXiv abs/2312.14183 (2023). https://api.semanticscholar.org/CorpusID:266521062   \n[15] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 14918–14937. https://doi.org/10.18653/v1/2023.emnlpmain.923   \n[16] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).   \n[17] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-Solve Prompting: Improving Zero-Shot Chainof-Thought Reasoning by Large Language Models. In Annual Meeting of the Association for Computational Linguistics. https://api.semanticscholar.org/CorpusID: 258558102   \n[18] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. ArXiv abs/2201.11903 (2022). https://api.semanticscholar.org/CorpusID:246411621   \n[19] Ziwei Xu, Sanjay Jain, and Mohan S. Kankanhalli. 2024. Hallucination is Inevitable: An Innate Limitation of Large Language Models. ArXiv abs/2401.11817 (2024). https://api.semanticscholar.org/CorpusID:267069207   \n[20] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. 2023. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441 (2023).   \n[21] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chun yue Li, and Jianfeng Gao. 2023. Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V. ArXiv abs/2310.11441 (2023). https://api.semanticscholar.org/CorpusID: 266149987",
    "chunk_order_index": 3,
    "full_doc_id": "doc-b720ee4f8234da6eab49046df331006e",
    "file_path": "2502.00989v1.pdf",
    "llm_cache_list": [
      "default:extract:d40ba024b7df6d6811ebb29887189efa",
      "default:extract:99cf49eb9347037f5ca10b8e6bfe5b7b"
    ],
    "create_time": 1758613406,
    "update_time": 1758613406,
    "_id": "chunk-30a776bce427dd986de9cf5abd6e011f"
  },
  "chunk-b7255d18a8df30a4afaa5abde11b9392": {
    "tokens": 31,
    "content": "ArXiv abs/2310.11441 (2023). https://api.semanticscholar.org/CorpusID: 266149987",
    "chunk_order_index": 4,
    "full_doc_id": "doc-b720ee4f8234da6eab49046df331006e",
    "file_path": "2502.00989v1.pdf",
    "llm_cache_list": [
      "default:extract:3a4313a2df9c59576affe7fe7f6fda9d",
      "default:extract:9f5d3b5c5e5bf266012f7b36dcbadf36"
    ],
    "create_time": 1758613406,
    "update_time": 1758613406,
    "_id": "chunk-b7255d18a8df30a4afaa5abde11b9392"
  },
  "chunk-59d9dff684ce807b28de26d05d7110e4": {
    "content": "\nImage Content Analysis:\nImage Path: /Users/gozachary/Downloads/Data-2/RAG-Anything/output/2502.00989v1/auto/images/dc80eb40fe8039ab9fcd844aa4438af28967c504f10187490fad92565b27f5ce.jpg\nCaptions: None\nFootnotes: None\n\nVisual Analysis: Error: Unable to process image",
    "tokens": 89,
    "full_doc_id": "doc-b720ee4f8234da6eab49046df331006e",
    "chunk_order_index": 5,
    "file_path": "2502.00989v1.pdf",
    "llm_cache_list": [
      "default:extract:183c68e121665c7de1cb657c19907bd7",
      "default:extract:359989c0a440f0b44ad3e6f6b470b80d"
    ],
    "is_multimodal": true,
    "modal_entity_name": "image_b6df1c945927ec7cf0f1433b75757883",
    "original_type": "image",
    "page_idx": 1,
    "create_time": 1758613406,
    "update_time": 1758613406,
    "_id": "chunk-59d9dff684ce807b28de26d05d7110e4"
  },
  "chunk-51d0df2e95c066061264879f74149837": {
    "content": "Table Analysis:\nImage Path: /Users/gozachary/Downloads/Data-2/RAG-Anything/output/2502.00989v1/auto/images/6aad72893fcc4ac2ad7d8bd0522ca5f9d7aafe941ccd14891339efc94e0fb015.jpg\nCaption: None\nStructure: <table><tr><td>Method</td><td>IoU</td></tr><tr><td>Kosmos-2</td><td>3.89</td></tr><tr><td>LISA</td><td>4.34</td></tr><tr><td>GPT-4V (Direct Bbox Decoding)</td><td>12.5</td></tr><tr><td>Claude-3.5 (Sonnet Direct Bbox Decoding)</td><td>13.8</td></tr><tr><td>DETR [2]+ Set-of-Marks Prompting [21]</td><td>18.6</td></tr><tr><td>ChartCitor</td><td>27.4</td></tr></table>\nFootnotes: None\n\nAnalysis: Error: Unable to generate response",
    "tokens": 257,
    "full_doc_id": "doc-b720ee4f8234da6eab49046df331006e",
    "chunk_order_index": 6,
    "file_path": "2502.00989v1.pdf",
    "llm_cache_list": [
      "default:extract:f3015a066580670731e802ff2719a340",
      "default:extract:8af5b7ef0834f412216b0fb6fc845f61"
    ],
    "is_multimodal": true,
    "modal_entity_name": "table_2ed41aac883ab4c8d601342f8cf41860",
    "original_type": "table",
    "page_idx": 2,
    "create_time": 1758613406,
    "update_time": 1758613406,
    "_id": "chunk-51d0df2e95c066061264879f74149837"
  },
  "chunk-7434f0605f7d63432a2806c12f18d8e5": {
    "content": "\nImage Content Analysis:\nImage Path: /Users/gozachary/Downloads/Data-2/RAG-Anything/output/2502.00989v1/auto/images/721f52a4142cec5ef6887349a53981cff863972945eec15ef60eaf78038ad5b7.jpg\nCaptions: None\nFootnotes: None\n\nVisual Analysis: Error: Unable to process image",
    "tokens": 89,
    "full_doc_id": "doc-b720ee4f8234da6eab49046df331006e",
    "chunk_order_index": 7,
    "file_path": "2502.00989v1.pdf",
    "llm_cache_list": [
      "default:extract:bac9ad093de78f160a100f0de5d090d2",
      "default:extract:bd45697830a328309c531563348e8f2d"
    ],
    "is_multimodal": true,
    "modal_entity_name": "image_b6df1c945927ec7cf0f1433b75757883",
    "original_type": "image",
    "page_idx": 2,
    "create_time": 1758613406,
    "update_time": 1758613406,
    "_id": "chunk-7434f0605f7d63432a2806c12f18d8e5"
  }
}