#!/usr/bin/env python3
"""
SPIQA Test-B ç®€åŒ–ç”Ÿæˆå¼ä»»åŠ¡è¯„ä¼°
ä½¿ç”¨é€‚åˆç”Ÿæˆå¼ä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡
"""

import json
from typing import Dict, List, Any
import re
from collections import defaultdict


def load_testb_dataset():
    """åŠ è½½ Test-B æ•°æ®é›†"""
    with open("test-B/SPIQA_testB.json", "r", encoding="utf-8") as f:
        return json.load(f)


def calculate_text_quality_metrics(text: str) -> Dict[str, float]:
    """è®¡ç®—æ–‡æœ¬è´¨é‡æŒ‡æ ‡"""
    if not text:
        return {
            "length_score": 0.0,
            "sentence_count": 0,
            "avg_sentence_length": 0.0,
            "word_diversity": 0.0,
            "readability_score": 0.0,
        }

    # åŸºæœ¬ç»Ÿè®¡
    words = text.split()
    sentences = re.split(r"[.!?]+", text)
    sentences = [s.strip() for s in sentences if s.strip()]

    # é•¿åº¦åˆ†æ•°ï¼ˆåŸºäºç†æƒ³é•¿åº¦èŒƒå›´ï¼‰
    text_length = len(text)
    if text_length < 50:
        length_score = text_length / 50
    elif text_length > 500:
        length_score = 500 / text_length
    else:
        length_score = 1.0

    # å¥å­æ•°é‡
    sentence_count = len(sentences)

    # å¹³å‡å¥å­é•¿åº¦
    avg_sentence_length = sum(len(s.split()) for s in sentences) / max(
        sentence_count, 1
    )

    # è¯æ±‡å¤šæ ·æ€§ï¼ˆå”¯ä¸€è¯/æ€»è¯æ•°ï¼‰
    unique_words = len(set(word.lower() for word in words))
    total_words = len(words)
    word_diversity = unique_words / max(total_words, 1)

    # å¯è¯»æ€§åˆ†æ•°ï¼ˆåŸºäºå¥å­é•¿åº¦å’Œè¯æ±‡å¤æ‚åº¦ï¼‰
    ideal_sentence_length = 15  # ç†æƒ³å¥å­é•¿åº¦
    sentence_length_score = (
        1.0 - abs(avg_sentence_length - ideal_sentence_length) / ideal_sentence_length
    )
    readability_score = (sentence_length_score + word_diversity) / 2

    return {
        "length_score": length_score,
        "sentence_count": sentence_count,
        "avg_sentence_length": avg_sentence_length,
        "word_diversity": word_diversity,
        "readability_score": readability_score,
    }


def evaluate_question_answer_pair(
    question: str, question_type: str, evidence_info: List[Dict]
) -> Dict[str, Any]:
    """è¯„ä¼°å•ä¸ªé—®é¢˜-ç­”æ¡ˆå¯¹"""

    # æ¨¡æ‹Ÿç”Ÿæˆç­”æ¡ˆï¼ˆå®é™…åº”ç”¨ä¸­è¿™é‡Œä¼šè°ƒç”¨ RAG ç³»ç»Ÿï¼‰
    # è¿™é‡Œæˆ‘ä»¬åŸºäºé—®é¢˜ç±»å‹å’Œè¯æ®ä¿¡æ¯ç”Ÿæˆæ¨¡æ‹Ÿç­”æ¡ˆ

    if "Shallow" in question_type:
        # æµ…å±‚é—®é¢˜ï¼šç”Ÿæˆç®€çŸ­ã€ç›´æ¥çš„ç­”æ¡ˆ
        simulated_answer = f"Based on the evidence, {question.lower().replace('?', '')} can be answered by examining the provided context and rationale."
    elif "Deep" in question_type or "Complex" in question_type:
        # æ·±å±‚é—®é¢˜ï¼šç”Ÿæˆè¯¦ç»†ã€åˆ†ææ€§çš„ç­”æ¡ˆ
        simulated_answer = f"This is a complex question that requires detailed analysis. {question} involves multiple aspects that need to be considered: {', '.join([evid.get('rationale', '')[:50] for evid in evidence_info[:2]])}..."
    elif "Testing" in question_type:
        # æµ‹è¯•é—®é¢˜ï¼šç”Ÿæˆå…·ä½“ã€å¯éªŒè¯çš„ç­”æ¡ˆ
        simulated_answer = f"The answer to {question.lower().replace('?', '')} can be determined from the experimental results and data presented in the evidence."
    else:
        simulated_answer = f"To answer {question.lower().replace('?', '')}, we need to analyze the provided evidence and context."

    # æ¨¡æ‹Ÿè¯æ®æå–
    simulated_evidence = []
    for evid in evidence_info:
        context = evid.get("context", "")
        rationale = evid.get("rationale", "")
        if context:
            simulated_evidence.append(f"Context: {context[:100]}...")
        if rationale:
            simulated_evidence.append(f"Rationale: {rationale[:100]}...")

    # è¯„ä¼°ç­”æ¡ˆè´¨é‡
    answer_metrics = calculate_text_quality_metrics(simulated_answer)

    # è¯„ä¼°è¯æ®è´¨é‡
    evidence_metrics = {
        "evidence_count": len(simulated_evidence),
        "evidence_coverage": min(
            1.0, len(simulated_evidence) / max(len(evidence_info), 1)
        ),
        "evidence_relevance": 0.8,  # æ¨¡æ‹Ÿç›¸å…³æ€§åˆ†æ•°
    }

    # è®¡ç®—ç»¼åˆåˆ†æ•°
    overall_score = (
        answer_metrics["readability_score"] * 0.3
        + answer_metrics["length_score"] * 0.2
        + evidence_metrics["evidence_coverage"] * 0.3
        + evidence_metrics["evidence_relevance"] * 0.2
    )

    return {
        "question": question,
        "question_type": question_type,
        "simulated_answer": simulated_answer,
        "simulated_evidence": simulated_evidence,
        "answer_metrics": answer_metrics,
        "evidence_metrics": evidence_metrics,
        "overall_score": overall_score,
    }


def run_testb_simple_evaluation():
    """è¿è¡Œ Test-B ç®€åŒ–è¯„ä¼°"""
    print("ğŸš€ å¼€å§‹ SPIQA Test-B ç”Ÿæˆå¼ä»»åŠ¡ç®€åŒ–è¯„ä¼°...")

    # åŠ è½½æ•°æ®é›†
    print("ğŸ“¥ åŠ è½½ Test-B æ•°æ®é›†...")
    dataset = load_testb_dataset()
    print(f"âœ… åŠ è½½å®Œæˆï¼š{len(dataset)} ç¯‡è®ºæ–‡")

    results = []
    total_questions = 0

    # ç»Ÿè®¡ä¿¡æ¯
    stats = defaultdict(int)
    type_performance = defaultdict(list)

    print("ğŸ” å¼€å§‹å¤„ç†é—®é¢˜...")

    for paper_id, paper_data in dataset.items():
        questions = paper_data.get("question", [])
        question_types = paper_data.get("question_type", [])
        evidential_info = paper_data.get("evidential_info", [])

        print(f"ğŸ“„ å¤„ç†è®ºæ–‡ {paper_id}ï¼Œå…± {len(questions)} ä¸ªé—®é¢˜")

        for i, (question, qtype, evidence) in enumerate(
            zip(questions, question_types, evidential_info)
        ):
            total_questions += 1
            stats[qtype] += 1

            print(f"  â“ é—®é¢˜ {i+1}: {question[:50]}...")

            try:
                # è¯„ä¼°é—®é¢˜-ç­”æ¡ˆå¯¹
                result = evaluate_question_answer_pair(question, qtype, evidence)
                result["paper_id"] = paper_id
                result["question_index"] = i

                results.append(result)
                type_performance[qtype].append(result["overall_score"])

                print(f"  âœ… å®Œæˆï¼Œåˆ†æ•°: {result['overall_score']:.3f}")

            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
                results.append(
                    {
                        "paper_id": paper_id,
                        "question_index": i,
                        "question": question,
                        "question_type": qtype,
                        "error": str(e),
                        "overall_score": 0.0,
                    }
                )

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")

    # æŒ‰é—®é¢˜ç±»å‹è®¡ç®—æ€§èƒ½
    type_stats = {}
    for qtype, scores in type_performance.items():
        if scores:
            type_stats[qtype] = {
                "count": len(scores),
                "average_score": sum(scores) / len(scores),
                "median_score": sorted(scores)[len(scores) // 2],
                "min_score": min(scores),
                "max_score": max(scores),
            }

    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    all_scores = [r["overall_score"] for r in results if "overall_score" in r]
    overall_stats = {
        "total_questions": total_questions,
        "successful_evaluations": len(all_scores),
        "overall_average_score": sum(all_scores) / len(all_scores)
        if all_scores
        else 0.0,
        "question_type_distribution": dict(stats),
        "type_performance": type_stats,
    }

    # ä¿å­˜ç»“æœ
    output_file = "spiqa_testb_simple_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(
            {"results": results, "statistics": overall_stats},
            f,
            indent=2,
            ensure_ascii=False,
        )

    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # æ‰“å°æ€»ç»“
    print("\nğŸ“ˆ è¯„ä¼°æ€»ç»“:")
    print(f"æ€»é—®é¢˜æ•°: {total_questions}")
    print(f"æˆåŠŸè¯„ä¼°: {len(all_scores)}")
    print(f"æ•´ä½“å¹³å‡åˆ†æ•°: {overall_stats['overall_average_score']:.3f}")

    print("\nğŸ“Š æŒ‰é—®é¢˜ç±»å‹ç»Ÿè®¡:")
    for qtype, stats in type_stats.items():
        print(f"  {qtype}:")
        print(f"    æ•°é‡: {stats['count']}")
        print(f"    å¹³å‡åˆ†æ•°: {stats['average_score']:.3f}")
        print(f"    ä¸­ä½æ•°: {stats['median_score']:.3f}")
        print(f"    èŒƒå›´: {stats['min_score']:.3f} - {stats['max_score']:.3f}")

    return results, overall_stats


if __name__ == "__main__":
    results, stats = run_testb_simple_evaluation()
