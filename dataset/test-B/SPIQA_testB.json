{"1506.06579": {"paper_id": "paper_10", "title": "Understanding Neural Networks Through Deep Visualization", "arxiv_url": "https://arxiv.org/abs/1506.06579", "s2orc_url": "https://www.semanticscholar.org/paper/1b5a24639fa80056d1a17b15f6997d10e76cc731", "all_figures_tables": {"1b5a24639fa80056d1a17b15f6997d10e76cc731/3-Figure1-1.png": "Figure 1. The bottom shows a screenshot from the interactive visualization software. The webcam input is shown, along with the whole layer of conv5 activations. The selected channel pane shows an enlarged version of the 13x13 conv5151 channel activations. Below it, the deconv starting at the selected channel is shown. On the right, three selections of nine images are shown: synthetic images produced using the regularized gradient ascent methods described in Section 3, the top 9 image patches from the training set (the images from the training set that caused the highest activations for the selected channel), and the deconv of the those top 9 images. All areas highlighted with a green star relate to the particular selected channel, here conv5151; when the selection changes, these panels update. The top depicts enlarged numerical optimization results for this and other channels. conv52 is a channel that responds most strongly to dog faces (as evidenced by the top nine images, which are not shown due to space constraints), but it also responds to flowers on the blanket on the bottom and half way up the right side of the image (as seen in the inset red highlight). This response to flowers can be partially seen in the optimized images but would be missed in an analysis focusing only on the top nine images and their deconv versions, which contain no flowers. conv5151 detects different types of faces. The top nine images are all of human faces, but here we see it responds also to the cat\u2019s face (and in Figure 2 a lion\u2019s face). Finally, conv5111 activates strongly for the cat\u2019s face, the optimized images show catlike fur and ears, and the top nine images (not shown here) are also all of cats. For this image, the softmax output layer top two predictions are \u201cEgyptian Cat\u201d and \u201cComputer Keyboard.\u201d All figures in this paper are best viewed digitally, in color, significantly zoomed in.", "1b5a24639fa80056d1a17b15f6997d10e76cc731/5-Figure2-1.png": "Figure 2. A view of the 13\u00d713 activations of the 151st channel on the conv5 layer of a deep neural network trained on ImageNet, a dataset that does not contain a face class, but does contain many images with faces. The channel responds to human and animal faces and is robust to changes in scale, pose, lighting, and context, which can be discerned by a user by actively changing the scene in front of a webcam or by loading static images (e.g. of the lions) and seeing the corresponding response of the unit. Photo of lions via Flickr user arnolouise, licensed under CC BY-NC-SA 2.0.", "1b5a24639fa80056d1a17b15f6997d10e76cc731/7-Figure4-1.png": "Figure 4. Visualizations of the preferred inputs for different class units on layer fc8, the 1000-dimensional output of the network just before the final softmax. In the lower left are 9 visualizations each (in 3\u00d73 grids) for four different sets of regularization hyperparameters for the Gorilla class (Table 1). For all other classes, we have selected four interpretable visualizations produced by our regularized optimization method. We chose the four combinations of regularization hyperparameters by performing a random hyperparameter search and selecting combinations that complement each other. For example, the lower left quadrant tends to show lower frequency patterns, the upper right shows high frequency patterns, and the upper left shows a sparse set of important regions. Often greater intuition can be gleaned by considering all four at once. In nearly every case, we have found that one can guess what class a neuron represents by viewing sets of these optimized, preferred images. Best viewed electronically, zoomed in.7", "1b5a24639fa80056d1a17b15f6997d10e76cc731/8-Figure5-1.png": "Figure 5. Visualization of example features of eight layers of a deep, convolutional neural network. The images reflect the true sizes of the features at different layers. In each layer, we show visualizations from 4 random gradient descent runs for each channel. While these images are hand picked to showcase the diversity and interpretability of the visualizations, one image for each filter of all five convolutional layers is shown in Figure S1 in supplementary information. One can recognize important features of objects at different scales, such as edges, corners, wheels, eyes, shoulders, faces, handles, bottles, etc. The visualizations show the increase in complexity and variation on higher layers, comprised of simpler components from lower layers. The variation of patterns increases with increasing layer number, indicating that increasingly invariant representations are learned. In particular, the jump from Layer 5 (the last convolution layer) to Layer 6 (the first fully-connected layer) brings about a large increase in variation. Best viewed electronically, zoomed in.", "1b5a24639fa80056d1a17b15f6997d10e76cc731/9-Figure3-1.png": "Figure 3. The effects of each regularization method from Section 3 when used individually. Each of the four rows shows a linear sweep in hyperparameter space from no regularization (left) to strong regularization (right). When applied too strongly, some regularizations cause the optimization to fail (e.g. L2 decay, top row) or the images to be less interpretable (small norm and small contribution clipping, bottom two rows). For this reason, a random hyperparameter search was useful for finding joint hyperparameter settings that worked well together (see Figure 4). Best viewed electronically, zoomed in.", "1b5a24639fa80056d1a17b15f6997d10e76cc731/9-Table1-1.png": "Table 1. Four hyperparameter combinations that produce different styles of recognizable images. We identified these four after reviewing images produced by 300 randomly selected hyperparameter combinations. From top to bottom, they are the hyperparameter combinations that produced the top-left, top-right, bottomleft, and bottom-right Gorilla class visualizations, respectively, in Figure 4. The third row hyperparameters produced most of the visualizations for the other classes in Figure 4, and all of those in Figure 5."}, "referred_figures_tables": [["1b5a24639fa80056d1a17b15f6997d10e76cc731/9-Figure3-1.png", "1b5a24639fa80056d1a17b15f6997d10e76cc731/7-Figure4-1.png", "1b5a24639fa80056d1a17b15f6997d10e76cc731/8-Figure5-1.png", "1b5a24639fa80056d1a17b15f6997d10e76cc731/9-Table1-1.png"], ["1b5a24639fa80056d1a17b15f6997d10e76cc731/9-Figure3-1.png", "1b5a24639fa80056d1a17b15f6997d10e76cc731/7-Figure4-1.png"], ["1b5a24639fa80056d1a17b15f6997d10e76cc731/9-Figure3-1.png", "1b5a24639fa80056d1a17b15f6997d10e76cc731/7-Figure4-1.png", "1b5a24639fa80056d1a17b15f6997d10e76cc731/8-Figure5-1.png", "1b5a24639fa80056d1a17b15f6997d10e76cc731/9-Table1-1.png"]], "question_id": [15, 17, 16], "question": ["For the images used for visualization in the paper, were they selected randomly or picked by the authors?", "What is meant by \"linear sweep\" in hyperparameter space?", "How many hyperparameter combinations were used for the random hyperparameter search?"], "question_section": ["Visualizing via Regularized Optimization", "Visualizing via Regularized Optimization", "Visualizing via Regularized Optimization"], "question_trigger_sentence": ["Figure 5. Visualization of example features of eight layers of a deep, convolutional neural network. The images reflect the true sizes of the features at different layers. In each layer, we show visualizations from 4 random gradient descent runs for each channel.", "Figure 3. The effects of each regularization method from Section 3 when used individually. Each of the four rows shows a linear sweep in hyperparameter space from no regularization (left) to strong regularization (right).", "While each of these regularization methods helps on its own, in combination they are even more effective. We found useful combinations via a random hyperparameter search"], "question_type": ["Shallow question", "Shallow question", "Testing question"], "evidential_info": [[{"context": "If the above regularization methods are applied individually, they aresomewhat effective at producing more interpretable images; Figure\u00a03 shows the effects of each individual hyperparameter.However, preliminary experiments uncovered that their combinedeffect produces better visualizations. To pick a reasonable set ofhyperparameters for all methods at once, we ran a randomhyperparameter search of 300 possible combinations and settled on fourthat complement each other well. The four selected combinations arelisted in Table\u00a01 and optimized images using each are shown for the \u201cGorilla\u201d class output unit in Figure\u00a04. Of the four, some show highfrequency information, others low frequency; some contain densepixel data, and others contain only sparse outlines of importantregions.We found the version in the lower-left quadrant to be the best single set of hyperparameters, but often greater intuition canbe gleaned by considering all four at once.Figure\u00a05 shows the optimization results computed for a selection of units on all layers. A single image for every filter of all five convolutional layers is shown in Supplementary Figure\u00a0S1. Nine images for each filter of all layers, including each of the 1000 ImageNet output classes, can be viewed at http://yosinski.com/deepvis.", "rationale": "If the above regularization methods are applied individually, they are somewhat effective at producing more interpretable images; Figure 3 shows the effects of each individual hyperparameter. However, preliminary experiments uncovered that their combined effect produces better visualizations.To pick a reasonable set of hyperparameters for all methods at once, we ran a random hyperparameter search of 300 possible combinations and settled on four that complement each other well. The four selected combinations are listed in Table 1 and optimized images using each are shown for the \u201cGorilla\u201d class output unit in Figure 4. Of the four, some show high frequency information, others low frequency; some contain dense pixel data, and others contain only sparse outlines of important regions."}], [{"context": "Figure 3. The effects of each regularization method from Section 3 when used individually. Each of the four rows shows a linear sweep in hyperparameter space from no regularization (left) to strong regularization (right). When applied too strongly, some regularizations cause the optimization to fail (e.g. L2 decay, top row) or the images to be less interpretable (small norm and small contribution clipping, bottom two rows). For this reason, a random hyperparameter search was useful for finding joint hyperparameter settings that worked well together (see Figure 4). Best viewed electronically, zoomed in.", "rationale": "Each of the four rows shows a linear sweep in hyperparameter space from no regularization (left) to strong regularization (right)."}], [{"context": "If the above regularization methods are applied individually, they aresomewhat effective at producing more interpretable images; Figure\u00a03 shows the effects of each individual hyperparameter.However, preliminary experiments uncovered that their combinedeffect produces better visualizations. To pick a reasonable set ofhyperparameters for all methods at once, we ran a randomhyperparameter search of 300 possible combinations and settled on fourthat complement each other well. The four selected combinations arelisted in Table\u00a01 and optimized images using each are shown for the \u201cGorilla\u201d class output unit in Figure\u00a04. Of the four, some show highfrequency information, others low frequency; some contain densepixel data, and others contain only sparse outlines of importantregions.We found the version in the lower-left quadrant to be the best single set of hyperparameters, but often greater intuition canbe gleaned by considering all four at once.Figure\u00a05 shows the optimization results computed for a selection of units on all layers. A single image for every filter of all five convolutional layers is shown in Supplementary Figure\u00a0S1. Nine images for each filter of all layers, including each of the 1000 ImageNet output classes, can be viewed at http://yosinski.com/deepvis.", "rationale": "To pick a reasonable set of hyperparameters for all methods at once, we ran a random hyperparameter search of 300 possible combinations and settled on four that complement each other well."}]], "composition": ["Authors best practices were to combine effects of different ways of regularization to produce interpretable images. They first search randomly through 300 different combinations of  hyperparameters, then they pick the best four sets of hyperparameters that are compliments to each other and then these sets would be used to visualize preferred images for different classes.", "linear sweep can be seen as a regular increment in the values of some regularization hyperparameter (from leftmost where there is no regularization to rightmost where strong regularization occur ) to see the variation of their effects on the corresponding activations.", "300 sets of possible hyperparameter combinations then choose four of them that complement each other well."], "Is_figure_in_evidence": [false, true, false], "Is_table_in_evidence": [true, false, true], "question_key": ["9", "12", "24"], "passages": ["The last several years have produced tremendous progress in training powerful, deep neural network models that are approaching and even surpassing human abilities on a variety of challenging machine learning tasks (Taigman et\u00a0al., 2014; Schroff et\u00a0al., 2015; Hannun et\u00a0al., 2014). A flagship example is training deep, convolutional neural networks (CNNs) with supervised learning to classify natural images (Krizhevsky et\u00a0al., 2012). That area has benefitted from the combined effects of faster computing (e.g. GPUs), better training techniques (e.g. dropout (Hinton et\u00a0al., 2012)), better activation units (e.g. rectified linear units (Glorot et\u00a0al., 2011)), and larger labeled datasets (Deng et\u00a0al., 2009; Lin et\u00a0al., 2014).", "While there has thus been considerable improvements in our knowledge of how to create high-performing architectures and learning algorithms, our understanding of how these large neural models operate has lagged behind. Neural networks have long been known as \u201cblack boxes\u201d because it is difficult to understand exactly how any particular, trained neural network functions due to the large number of interacting, non-linear parts. Large modern neural networks are even harder to study because of their size; for example, understanding the widely-used AlexNet DNN involves making sense of the values taken by the 60 million trained network parameters. Understanding what is learned is interesting in its own right, but it is also one key way of further improving models: the intuitions provided by understanding the current generation of models should suggest ways to make them better. For example, the deconvolutional technique for visualizing the features learned by the hidden units of DNNs suggested an architectural change of smaller convolutional filters that led to state of the art performance on the ImageNet benchmark in 2013\u00a0(Zeiler & Fergus, 2013).", "We also note that tools that enable understanding will especially benefit the vast numbers of newcomers to deep learning, who would like to take advantage of off-the-shelf software packages \u2014 like Theano (Bergstra et\u00a0al., 2010), Pylearn2 (Goodfellow et\u00a0al., 2013), Caffe (Jia et\u00a0al., 2014), and Torch (Collobert et\u00a0al., 2011) \u2014 in new domains, but who may not have any intuition for why their models work (or do not). Experts can also benefit as they iterate ideas for new models or when they are searching for good hyperparameters. We thus believe that both experts and newcomers will benefit from tools that provide intuitions about the inner workings of DNNs. This paper provides two such tools, both of which are open source so that scientists and practitioners can integrate them with their own DNNs to better understand them.", "The first tool is software that interactively plots the activations produced on each layer of a trained DNN for user-provided images or video. Static images afford a slow, detailed investigation of a particular input, whereas video input highlights the DNNs changing responses to dynamic input. At present, the videos are processed live from a user\u2019s computer camera, which is especially helpful because users can move different items around the field of view, occlude and combine them, and perform other manipulations to actively learn how different features in the network respond.", "The second tool we introduce enables better visualization of the learned features computed by individual neurons at every layer of a DNN. Seeing what features have been learned is important both to understand how current DNNs work and to fuel intuitions for how to improve them.", "Attempting to understand what computations are performed at each layer in DNNs is an increasingly popular direction of research. One approach is to study each layer as a group and investigate the type of computation performed by the set of neurons on a layer as a whole\u00a0(Yosinski et\u00a0al., 2014; Mahendran & Vedaldi, 2014). This approach is informative because the neurons in a layer interact with each other to pass information to higher layers, and thus each neuron\u2019s contribution to the entire function performed by the DNN depends on that neuron\u2019s context in the layer.", "Another approach is to try to interpret the function computed by each individual neuron. Past studies in this vein roughly divide into two different camps: dataset-centric and network-centric. The former requires both a trained DNN and running data through that network; the latter requires only the trained network itself. One dataset-centric approach is to display images from the training or test set that cause high or low activations for individual units. Another is the deconvolution method of Zeiler & Fergus\u00a0(2013), which highlights the portions of a particular image that are responsible for the firing of each neural unit.", "Network-centric approaches investigate a network directly without any data from a dataset. For example, Erhan et al.\u00a0(2009) synthesized images that cause high activations for particular units. Starting with some initial input \ud835\udc31=\ud835\udc31\ud835\udfce\ud835\udc31subscript\ud835\udc310\\mathbf{x}=\\mathbf{x_{0}}bold_x = bold_x start_POSTSUBSCRIPT bold_0 end_POSTSUBSCRIPT, the activation ai(\ud835\udc31)subscript\ud835\udc4e\ud835\udc56\ud835\udc31a_{i}(\\mathbf{x})italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) caused at some unit i\ud835\udc56iitalic_i by this input is computed, and then steps are taken in input spacealong the gradient \u2202ai(\ud835\udc31)/\u2202\ud835\udc31subscript\ud835\udc4e\ud835\udc56\ud835\udc31\ud835\udc31\\partial a_{i}(\\mathbf{x})/\\partial\\mathbf{x}\u2202 italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) / \u2202 bold_x to synthesize inputs that cause higher and higher activations of unit i\ud835\udc56iitalic_i, eventually terminating at some \ud835\udc31*superscript\ud835\udc31\\mathbf{x^{*}}bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT which is deemed to be a preferred input stimulus for the unit in question. In the case where the input space is an image, \ud835\udc31*superscript\ud835\udc31\\mathbf{x^{*}}bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT can be displayed directly for interpretation. Others have followed suit, using the gradient to find images that cause higher activations (Simonyan et\u00a0al., 2013; Nguyen et\u00a0al., 2014) or lower activations (Szegedy et\u00a0al., 2013) for output units.", "These gradient-based approaches are attractive in their simplicity, but the optimization process tends to produce images that do not greatly resemble natural images. Instead, they are composed of a collection of \u201chacks\u201d that happen to cause high (or low) activations: extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure\u00a0(Simonyan et\u00a0al., 2013; Nguyen et\u00a0al., 2014; Szegedy et\u00a0al., 2013; Goodfellow et\u00a0al., 2014). The fact that activations may be effected by such hacks is better understood thanks to several recent studies. Specifically, it has been shown that such hacks may be applied to correctly classified images to cause them to be misclassified even via imperceptibly small changes (Szegedy et\u00a0al., 2013), that such hacks can be found even without the gradient information to produce unrecognizable \u201cfooling examples\u201d (Nguyen et\u00a0al., 2014), and that the abundance of non-natural looking images that cause extreme activations can be explained by the locally linear behavior of neural nets(Goodfellow et\u00a0al., 2014).", "With such strong evidence that optimizing images to cause high activations produces unrecognizable images, is there any hope of using such methods to obtain useful visualizations? It turns out there is, if one is able to appropriately regularize the optimization. Simonyan et al.\u00a0(2013) showed that slightly discernible images for the final layers of a convnet could be produced with L2subscript\ud835\udc3f2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-regularization. Mahendran and Vedaldi (2014) also showed the importance of incorporating natural-image priors in the optimization process when producing images that mimic an entire-layer\u2019s firing pattern produced by a specific input image. We build on these works and contribute three additional forms of regularization that, when combined, produce more recognizable, optimization-based samples than previous methods. Because the optimization is stochastic, by starting at different random initial images, we can produce a set of optimized images whose variance provides information about the invariances learned by the unit.", "To summarize, this paper makes the following two contributions:", "1.We describe and release a software tool that provides a live, interactive visualization of every neuron in a trained convnet as it responds to a user-provided image or video. The tool displays forward activation values, preferred stimuli via gradient ascent, top images for each unit from the training set, deconv highlighting (Zeiler & Fergus, 2013) of top images, and backward diffs computed via backprop or deconv starting from arbitrary units. The combined effect of these complementary visualizations promotes a greater understanding of what a neuron computes than any single method on its own. We also describe a few insights we have gained from using this tool.(Section\u00a02).2.We extend past efforts to visualize preferred activation patterns in input space by adding several new types of regularization, which produce what we believe are the most interpretable images for large convnets so far (Section\u00a03).", "Both of our tools are released as open source and are available athttp://yosinski.com/deepvis. While the tools could be adapted to integrate with any DNN software framework, they work out of the box withthe popular Caffe DNN software package (Jia et\u00a0al., 2014).Users may run visualizations with their own Caffe DNN or our pre-trained DNN, which comes with pre-computed images optimized to activate each neuron in this trained network. Our pre-trained network is nearly identical to the \u201cAlexNet\u201d architecture (Krizhevsky et\u00a0al., 2012), but with local reponse normalization layers after pooling layers following (Jia et\u00a0al., 2014). It was trained with the Caffe framework on the ImageNet 2012 dataset (Deng et\u00a0al., 2009).", "Our first visualization method is straightforward: plotting the activation values for the neurons in each layer of a convnet in response to an image or video. In fully connected neural networks, the order of the units is irrelevant, so plots of these vectors are not spatially informative. However, in convolutional networks, filters are applied in a way that respects the underlying geometry of the input; in the case of 2D images, filters are applied in a 2D convolution over the two spatial dimensions of the image. This convolution produces activations on subsequent layers that are, for each channel, also arranged spatially.", "Figure\u00a01 shows examples of this type of plot for the \ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\\mathsf{conv5}sansserif_conv5 layer.The \ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\\mathsf{conv5}sansserif_conv5 layer has size 256\u00d7\\times\u00d713\u00d7\\times\u00d713, which we depict as 256 separate 13\u00d7\\times\u00d713 grayscale images. Each of the 256 small images contains activations in the same spatial x\ud835\udc65xitalic_x-y\ud835\udc66yitalic_y spatial layout as the input data, and the 256 images are simply and arbitrarily tiled into a 16\u00d7\\times\u00d716 grid in row-major order.Figure\u00a02 shows a zoomed in view of one particular channel, \ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\ud835\udfe3\ud835\udfe7\ud835\udfe3subscript\ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7151\\mathsf{conv5_{151}}sansserif_conv5 start_POSTSUBSCRIPT sansserif_151 end_POSTSUBSCRIPT, that responds to human and animal faces. All layers can be viewed in the software tool, including pooling and normalization layers. Visualizing these layers provides intuitions about their effects and functions.", "Although this visualization is simple to implement, we find it informative becauseall data flowing through the network can be visualized. There is nothing mysterious happening behind the scenes. Because this convnet contains only a single path from input to output, every layer is a bottleneck through which all information must pass en-route to a classification decision. The layer sizes are all small enough that any one layer can easily fit on a computer screen.111The layer with the most activations is \ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe3\ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe3\\mathsf{conv1}sansserif_conv1 which, when tiled, is only 550x550 before adding padding. So far, we have gleaned several surprising intuitions from using the tool:", "\u2022One of the most interesting conclusions so far has been that representations on some layers seem to be surprisingly local. Instead of finding distributed representations on all layers, we see, for example, detectors for text, flowers, fruit, and faces on \ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe6\ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe6\\mathsf{conv4}sansserif_conv4 and \ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\\mathsf{conv5}sansserif_conv5. These conclusions can be drawn either from the live visualization or the optimized images (or, best, by using both in concert) and suggest several directions for future research (discussed in Section\u00a04).\u2022When using direct file input to classify photos from Flickr or Google Images, classifications are often correct and highly confident (softmax probability for correct class near 1). On the other hand, when using input from a webcam, predictions often cannot be correct because no items from the training set are shown in the image. The training set\u2019s 1000 classes, though numerous, do not cover most common household objects. Thus, when shown a typical webcam view of a person with no ImageNet classes present, the output has no single high probability, as is expected. Surprisingly, however, this probability vector is noisy and varies significantly in response to tiny changes in the input, often changing merely in response to the noise from the webcam. We might have instead expected unchanging and low confidence predictions for a given scene when no object the network has been trained to classify is present. Plotting the fully connected layers (\ud835\uddbf\ud835\uddbc\ud835\udfe8\ud835\uddbf\ud835\uddbc\ud835\udfe8\\mathsf{fc6}sansserif_fc6 and \ud835\uddbf\ud835\uddbc\ud835\udfe9\ud835\uddbf\ud835\uddbc\ud835\udfe9\\mathsf{fc7}sansserif_fc7) also reveals a similar sensitivity to small input changes.\u2022Although the last three layers are sensitive to small input changes, much of the lower layer computation is more robust. For example, when visualizing the \ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\\mathsf{conv5}sansserif_conv5 layer, one can find many invariant detectors for faces, shoulders, text, etc. by moving oneself or objects in front of the camera. Even though the 1000 classes contain no explicitly labeled faces or text, the network learns to identify these concepts simply because they represent useful partial information for making a later classification decision. One face detector, denoted \ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\ud835\udfe3\ud835\udfe7\ud835\udfe3subscript\ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7151\\mathsf{conv5_{151}}sansserif_conv5 start_POSTSUBSCRIPT sansserif_151 end_POSTSUBSCRIPT (channel number 151 on \ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\\mathsf{conv5}sansserif_conv5), is shown in Figure\u00a02 activating for human and lion faces and in Figure\u00a01 activating for a cat face. Zhou et al.\u00a0(2014) recently observed a similar effect where convnets trained only to recognize different scene types \u2014 playgrounds, restaurant patios, living rooms, etc. \u2014 learn object detectors (e.g. for chairs, books, and sofas) on intermediate layers.", "The reader is encouraged to try this visualization tool out for him or herself. The code, together with pre-trained models and images synthesized by gradient ascent, can be downloaded athttp://yosinski.com/deepvis.", "The second contribution of this work is introducing several regularization methods to bias images found via optimization toward more visually interpretable examples. While each of these regularization methods helps on its own, in combination they are even more effective. We found useful combinations via a random hyperparameter search, as discussed below.", "Formally, consider an image \ud835\udc31\u2208\u211dC\u00d7H\u00d7W\ud835\udc31superscript\u211d\ud835\udc36\ud835\udc3b\ud835\udc4a\\mathbf{x}\\in\\mathbb{R}^{C\\times H\\times W}bold_x \u2208 blackboard_R start_POSTSUPERSCRIPT italic_C \u00d7 italic_H \u00d7 italic_W end_POSTSUPERSCRIPT, where\u00a0C=3\ud835\udc363C=3italic_C = 3 color channels and the height (H\ud835\udc3bHitalic_H) and width (W\ud835\udc4aWitalic_W) are both 227 pixels. When this image is presented to a neural network, it causes an activation ai(\ud835\udc31)subscript\ud835\udc4e\ud835\udc56\ud835\udc31a_{i}(\\mathbf{x})italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) for some unit i\ud835\udc56iitalic_i, where for simplicity i\ud835\udc56iitalic_i is an index that runs over all units on all layers. We also define a parameterized regularization function R\u03b8(\ud835\udc31)subscript\ud835\udc45\ud835\udf03\ud835\udc31R_{\\theta}(\\mathbf{x})italic_R start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x ) that penalizes images in various ways.", "Our network was trained on ImageNet by first subtracting the per-pixel mean of examples in ImageNet before inputting training examples to the network. Thus, the direct input to the network, \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, can be thought of as a zero-centered input. We may pose the optimization problem as finding an image \ud835\udc31*superscript\ud835\udc31\\mathbf{x^{*}}bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT where", "\ud835\udc31*=argmax\ud835\udc31\u2061(ai(\ud835\udc31)\u2212R\u03b8(\ud835\udc31))superscript\ud835\udc31subscriptargmax\ud835\udc31subscript\ud835\udc4e\ud835\udc56\ud835\udc31subscript\ud835\udc45\ud835\udf03\ud835\udc31\\mathbf{x^{*}}=\\operatorname*{arg\\,max}_{\\mathbf{x}}(a_{i}(\\mathbf{x})-R_{\\theta}(\\mathbf{x}))bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) - italic_R start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x ) )(1)", "In practice, we use a slightly different formulation. Because we search for \ud835\udc31*superscript\ud835\udc31\\mathbf{x^{*}}bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT by starting at some \ud835\udc31\ud835\udfcesubscript\ud835\udc310\\mathbf{x_{0}}bold_x start_POSTSUBSCRIPT bold_0 end_POSTSUBSCRIPT and taking gradient steps, we instead define the regularization via an operator r\u03b8(\u22c5)subscript\ud835\udc5f\ud835\udf03\u22c5r_{\\theta}(\\cdot)italic_r start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( \u22c5 ) that maps \ud835\udc31\ud835\udc31\\mathbf{x}bold_x to a slightly more regularized version of itself. This latter definition is strictly more expressive, allowing regularization operators r\u03b8subscript\ud835\udc5f\ud835\udf03r_{\\theta}italic_r start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT that are not the gradient of any R\u03b8subscript\ud835\udc45\ud835\udf03R_{\\theta}italic_R start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT.This method is easy to implement within a gradient descent framework by simply alternating between taking a step toward the gradient of ai(\ud835\udc31)subscript\ud835\udc4e\ud835\udc56\ud835\udc31a_{i}(\\mathbf{x})italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) and taking a step in the direction given by r\u03b8subscript\ud835\udc5f\ud835\udf03r_{\\theta}italic_r start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT. With a gradient descent step size of \u03b7\ud835\udf02\\etaitalic_\u03b7, a single step in this process applies the update:", "\ud835\udc31\u2190r\u03b8(\ud835\udc31+\u03b7\u2202ai\u2202\ud835\udc31)\u2190\ud835\udc31subscript\ud835\udc5f\ud835\udf03\ud835\udc31\ud835\udf02subscript\ud835\udc4e\ud835\udc56\ud835\udc31\\mathbf{x}\\leftarrow r_{\\theta}\\left(\\mathbf{x}+\\eta\\frac{\\partial a_{i}}{\\partial\\mathbf{x}}\\right)\\\\bold_x \u2190 italic_r start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x + italic_\u03b7 divide start_ARG \u2202 italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG \u2202 bold_x end_ARG )(2)", "We investigated the following four regularizations. All are designed to overcome different pathologies commonly encountered by gradient descent without regularization.", "L2subscript\ud835\udc3f2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT decay: A common regularization, L2subscript\ud835\udc3f2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT decay penalizes large values and is implemented as r\u03b8(\ud835\udc31)=(1\u2212\u03b8decay)\u22c5\ud835\udc31subscript\ud835\udc5f\ud835\udf03\ud835\udc31\u22c51subscript\ud835\udf03decay\ud835\udc31r_{\\theta}(\\mathbf{x})=(1-\\theta_{\\mathrm{decay}})\\cdot\\mathbf{x}italic_r start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x ) = ( 1 - italic_\u03b8 start_POSTSUBSCRIPT roman_decay end_POSTSUBSCRIPT ) \u22c5 bold_x. L2subscript\ud835\udc3f2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT decay tends to prevent a small number of extreme pixel values from dominating the example image. Such extreme single-pixel values neither occur naturally with great frequency nor are useful for visualization. L2subscript\ud835\udc3f2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT decay was also used by Simonyan et al.\u00a0(2013).", "Gaussian blur: Producing images via gradient ascent tends to produce examples with high frequency information (see Supplementary Section\u00a0S1 for a possible reason). While these images cause high activations, they are neither realistic nor interpretable (Nguyen et\u00a0al., 2014). A useful regularization is thus to penalize high frequency information. We implement this as a Gaussian blur step r\u03b8(\ud835\udc31)=GaussianBlur(\ud835\udc31,\u03b8b_width)subscript\ud835\udc5f\ud835\udf03\ud835\udc31GaussianBlur\ud835\udc31subscript\ud835\udf03b_widthr_{\\theta}(\\mathbf{x})=\\mathrm{GaussianBlur}(\\mathbf{x},\\theta_{\\mathrm{b\\_width}})italic_r start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x ) = roman_GaussianBlur ( bold_x , italic_\u03b8 start_POSTSUBSCRIPT roman_b _ roman_width end_POSTSUBSCRIPT ). Convolving with a blur kernel is more computationally expensive than the other regularization methods, so we added another hyperparameter \u03b8b_everysubscript\ud835\udf03b_every\\theta_{\\mathrm{b\\_every}}italic_\u03b8 start_POSTSUBSCRIPT roman_b _ roman_every end_POSTSUBSCRIPT to allow, for example, blurring every several optimization steps instead of every step. Blurring an image multiple times with a small width Gaussian kernel is equivalent to blurring once with a larger width kernel, and the effect will be similar even if the image changes slightly during the optimization process. This technique thus lowers computational costs without limiting the expressiveness of the regularization. Mahendran & Vedaldi\u00a0(2014) used a penalty with a similar effect to blurring, called total variation, in their work reconstructing images from layer codes.", "Clipping pixels with small norm: The first two regularizations suppress high amplitude and high frequency information, so after applying both, we are left with an \ud835\udc31*superscript\ud835\udc31\\mathbf{x^{*}}bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT that contains somewhat small, somewhat smooth values. However, \ud835\udc31*superscript\ud835\udc31\\mathbf{x^{*}}bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT will still tend to contain non-zero pixel values everywhere. Even if some pixels in \ud835\udc31*superscript\ud835\udc31\\mathbf{x^{*}}bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT show the primary object or type of input causing the unit under consideration to activate, the gradient with respect to all other pixels in \ud835\udc31*superscript\ud835\udc31\\mathbf{x^{*}}bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT will still generally be non-zero, so these pixels will also shift to show some pattern as well, contributing in whatever small way they can to ultimately raise the chosen unit\u2019s activation. We wish to bias the search away from such behavior and instead show only the main object, letting other regions be exactly zero if they are not needed. We implement this bias using an r\u03b8(\ud835\udc31)subscript\ud835\udc5f\ud835\udf03\ud835\udc31r_{\\theta}(\\mathbf{x})italic_r start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x ) that computes the norm of each pixel (over red, green, and blue channels) and then sets any pixels with small norm to zero. The threshold for the norm, \u03b8n_pctsubscript\ud835\udf03n_pct\\theta_{\\mathrm{n\\_pct}}italic_\u03b8 start_POSTSUBSCRIPT roman_n _ roman_pct end_POSTSUBSCRIPT, is specified as a percentile of all pixel norms in \ud835\udc31\ud835\udc31\\mathbf{x}bold_x.", "Clipping pixels with small contribution: Instead of clipping pixels with small norms, we can try something slightly smarter and clip pixels with small contributions to the activation. One way of computing a pixel\u2019s contribution to an activation is to measure how much the activation increases or decreases when the pixel is set to zero; that is, to compute the contribution as |ai(\ud835\udc31)\u2212ai(\ud835\udc31\u2212j)|subscript\ud835\udc4e\ud835\udc56\ud835\udc31subscript\ud835\udc4e\ud835\udc56subscript\ud835\udc31\ud835\udc57|a_{i}(\\mathbf{x})-a_{i}(\\mathbf{x}_{-j})|| italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) - italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT - italic_j end_POSTSUBSCRIPT ) |, where \ud835\udc31\u2212jsubscript\ud835\udc31\ud835\udc57\\mathbf{x}_{-j}bold_x start_POSTSUBSCRIPT - italic_j end_POSTSUBSCRIPT is \ud835\udc31\ud835\udc31\\mathbf{x}bold_x but with the jthsuperscript\ud835\udc57\ud835\udc61\u210ej^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT pixel set to zero. This approach is straightforward but prohibitively slow, requiring a forward pass for every pixel. Instead, we approximate this process by linearizing ai(\ud835\udc31)subscript\ud835\udc4e\ud835\udc56\ud835\udc31a_{i}(\\mathbf{x})italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) around \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, in which case the contribution of each dimension of \ud835\udc31\ud835\udc31\\mathbf{x}bold_x can be estimated as the elementwise product of \ud835\udc31\ud835\udc31\\mathbf{x}bold_x and the gradient. We then sum over all three channels and take the absolute value, computing |\u2211c\ud835\udc31\u2218\u2207\ud835\udc31ai(\ud835\udc31)|subscript\ud835\udc50\ud835\udc31subscript\u2207\ud835\udc31subscript\ud835\udc4e\ud835\udc56\ud835\udc31\\left|\\sum_{c}\\mathbf{x}\\circ\\nabla_{\\mathbf{x}}a_{i}(\\mathbf{x})\\right|| \u2211 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT bold_x \u2218 \u2207 start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) |. We use the absolute value to find pixels with small contribution in either direction, positive or negative. While we could choose to keep the pixel transitions where setting the pixel to zero would result in a large activation increase, these shifts are already handled by gradient ascent, and here we prefer to clip only the pixels that are deemed not to matter, not to take large gradient steps outside the region where the linear approximation is most valid. We define this r\u03b8(\ud835\udc31)subscript\ud835\udc5f\ud835\udf03\ud835\udc31r_{\\theta}(\\mathbf{x})italic_r start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x ) as the operation that sets pixels with contribution under the \u03b8c_pctsubscript\ud835\udf03c_pct\\theta_{\\mathrm{c\\_pct}}italic_\u03b8 start_POSTSUBSCRIPT roman_c _ roman_pct end_POSTSUBSCRIPT percentile to zero.", "If the above regularization methods are applied individually, they aresomewhat effective at producing more interpretable images; Figure\u00a03 shows the effects of each individual hyperparameter.However, preliminary experiments uncovered that their combinedeffect produces better visualizations. To pick a reasonable set ofhyperparameters for all methods at once, we ran a randomhyperparameter search of 300 possible combinations and settled on fourthat complement each other well. The four selected combinations arelisted in Table\u00a01 and optimized images using each are shown for the \u201cGorilla\u201d class output unit in Figure\u00a04. Of the four, some show highfrequency information, others low frequency; some contain densepixel data, and others contain only sparse outlines of importantregions.We found the version in the lower-left quadrant to be the best single set of hyperparameters, but often greater intuition canbe gleaned by considering all four at once.Figure\u00a05 shows the optimization results computed for a selection of units on all layers. A single image for every filter of all five convolutional layers is shown in Supplementary Figure\u00a0S1. Nine images for each filter of all layers, including each of the 1000 ImageNet output classes, can be viewed at http://yosinski.com/deepvis.", "We have introduced two visual tools for aiding in the interpretationof trained neural nets.Intuition gained from these tools may prompt ideas for improved methods and future research. Here we discuss several such ideas.", "The interactive tool reveals that representations on later convolutional layers tend to be somewhat local, where channels correspond to specific, natural parts (e.g. wheels, faces) instead of being dimensions in a completely distributed code. That said, not all features correspond to natural parts, raising the possibility of a different decomposition of the world than humans might expect. These visualizations suggest that further study into the exact nature of learned representations \u2014 whether they are local to a single channel or distributed across several \u2014 is likely to be interesting (see Zhou et al.\u00a0(2014) for work in this direction). The locality of the representation also suggests that during transfer learning, when new models are trained atop the \ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe6\ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe6\\mathsf{conv4}sansserif_conv4 or \ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\ud835\uddbc\ud835\uddc8\ud835\uddc7\ud835\uddcf\ud835\udfe7\\mathsf{conv5}sansserif_conv5 representations, a bias toward sparse connectivity could be helpful because it may be necessary to combine only a few features from these layers to create important features at higher layers.", "The second tool \u2014 new regularizations that enable improved, interpretable, optimized visualizations of learned features \u2014 will help researchers and practitioners understand, debug, and improve their models. The visualizations also reveal a new twist in an ongoing story. Previous studieshave shown that discriminative networks can easily be fooled or hacked by the addition of certain structurednoise in image space (Szegedy et\u00a0al., 2013; Nguyen et\u00a0al., 2014).An oft-cited reason for this property is that discriminative training leads networksto ignore non-discriminative information in their input, e.g. learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs. For this reason it has been seen as a hopeless endeavor to create a generative model in which one randomly samples an x\ud835\udc65xitalic_x from a broad distribution on the space of all possible imagesand then iteratively transforms x\ud835\udc65xitalic_x into a recognizable image by moving it to a region that satisfies both a prior p(x)\ud835\udc5d\ud835\udc65p(x)italic_p ( italic_x ) and posterior p(y|x)\ud835\udc5dconditional\ud835\udc66\ud835\udc65p(y|x)italic_p ( italic_y | italic_x ) for some class label y\ud835\udc66yitalic_y.Past attempts have largely supported this view by producing unrealistic images using this method (Nguyen et\u00a0al., 2014; Simonyan et\u00a0al., 2013).", "However, the results presented here suggest an alternate possibility: the previously used priors may simply have been too weak (see Section\u00a0S1 for one hypothesis of why a strong p(x)\ud835\udc5d\ud835\udc65p(x)italic_p ( italic_x ) model is needed). With the careful design or learning of a p(x)\ud835\udc5d\ud835\udc65p(x)italic_p ( italic_x ) model that biases toward realism,one may be able to harnessthe large number of parameters present in a discriminately learned p(y|x)\ud835\udc5dconditional\ud835\udc66\ud835\udc65p(y|x)italic_p ( italic_y | italic_x ) modelto generate realistic images by enforcing probability under both models simultaneously.Even with the simple, hand-coded p(x)\ud835\udc5d\ud835\udc65p(x)italic_p ( italic_x ) models we use in this paper as regularizers, complex dependencies between distant pixels already arise (cf. the beetles with structure spanning over 100 pixels in Figure\u00a04). This implies that the discriminative parameters also contain significant \u201cgenerative\u201d structure from thetraining dataset; that is, the parameters encodenot only the jaguar\u2019s spots, but to some extent also its four legs.With better, learned probabilistic models over the input and activations of higher layers, much more structure may be apparent. Work by Dai et al.\u00a0(2015) shows some interesting results in this direction.While the images generated in this paper are far from being photo-realistic, they do suggest thattransferring discriminatively trained parameters to generative models \u2014 opposite the direction of the usual unsupervised pretraining approach \u2014 may be a fruitful area for further investigation."], "figure_types": {"1b5a24639fa80056d1a17b15f6997d10e76cc731/3-Figure1-1.png": "schematic", "1b5a24639fa80056d1a17b15f6997d10e76cc731/5-Figure2-1.png": "photograph(s)", "1b5a24639fa80056d1a17b15f6997d10e76cc731/7-Figure4-1.png": "photograph(s)", "1b5a24639fa80056d1a17b15f6997d10e76cc731/9-Figure3-1.png": "plot", "1b5a24639fa80056d1a17b15f6997d10e76cc731/9-Table1-1.png": "table", "1b5a24639fa80056d1a17b15f6997d10e76cc731/8-Figure5-1.png": "photograph(s)"}}, "2204.11673": {"paper_id": "paper_106", "title": "Incorporating Explicit Knowledge in Pre-trained Language Models for Passage Re-ranking", "arxiv_url": "https://arxiv.org/abs/2204.11673", "s2orc_url": "https://www.semanticscholar.org/paper/ca8f25b6c49c7f34380a7c6623e74f63f3a48771", "all_figures_tables": {"ca8f25b6c49c7f34380a7c6623e74f63f3a48771/10-Table5-1.png": "Table 5: Comparisons on different settings of knowledge graph distillation on TREC 2019 DL.", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/10-Table6-1.png": "Table 6: Ranking performance comparison on different domains.", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/10-Table7-1.png": "Table 7: Quantitative analysis of knowledge in meta-graph on different domains.", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/2-Figure1-1.png": "Figure 1: The workflow of KERM.", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/4-Figure2-1.png": "Figure 2: The illustration of global graph pruning.", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/5-Figure3-1.png": "Figure 3: The illustration of a meta-graph constructed between the target entities of a query-passage pair. Note that some entities do not appear in the meta-graph, as they might not have any path that reaches the entities on the other side.", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/6-Figure4-1.png": "Figure 4: The architecture of KERM.", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/8-Table1-1.png": "Table 1: Statistics of datasets.", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/9-Table2-1.png": "Table 2: Performance comparison on MARCO-DEV and TREC 2019 DL.", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/9-Table3-1.png": "Table 3: Performance Comparisons between different settings of knowledge injector on MSMARCO DEV.", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/9-Table4-1.png": "Table 4: MRR@10 results with different number of layers in knowledge injector on MSMARCO."}, "referred_figures_tables": [["ca8f25b6c49c7f34380a7c6623e74f63f3a48771/9-Table3-1.png", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/10-Table5-1.png"], ["ca8f25b6c49c7f34380a7c6623e74f63f3a48771/10-Table5-1.png"], ["ca8f25b6c49c7f34380a7c6623e74f63f3a48771/8-Table1-1.png"], ["ca8f25b6c49c7f34380a7c6623e74f63f3a48771/4-Figure2-1.png", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/4-Figure2-1.png"]], "question_id": [4, 0, 1, 6], "question": ["Does the paper show how each component of KERM can contribute to passage re-ranking performance quantitatively and qualitatively?", "Does the author showed that the distillation on the knowledge graph can be useful for re-ranking task?", "Who collected the queries from MSMARCO-Passage dataset to make MSMARCO-TRAIN query set?", "What is the example of unreliable relations in knowledge graph for passage re-ranking scenario?"], "question_section": ["7.Conclusion", "4.KERM", "5.1.Datasets", "1.Introduction"], "question_trigger_sentence": ["The role of each module in KERM is also comprehensively analyzed.", "Notably, the main challenges of incorporating explicit knowledge are to 1) distill a knowledge graph that is useful for re-ranking task, and 2) aggregate the explicit knowledge with the current implicit knowledge in an appropriate manner that can improve the overall performance. Hence our proposed approach is mainly composed of two parts, i.e., knowledge graph distillation and knowledge aggregation, to tackle two challenges respectively.", "We use a large-scale public available corpus, i.e., MSMARCO-Passage collection (Nguyen et al., 2016), as our passage collection. This collection contains approximately 8.8 million passages extracted from 3.2 million web documents covering multiple fields. We train our model on the MSMARCO-TRAIN query set of 502,939 queries and evaluate KERM on three query sets.", "Specifically, we first distill a graph globally for passage re-ranking scenario from an existing knowledge graph by pruning some unreliable or noisy relations based on TransE embedding."], "question_type": ["Shallow question", "Shallow question", "Shallow question", "Testing question"], "evidential_info": [[{"context": "Here we compare ranking performances of KERM and other PLMs based re-rankers on the first two widely used query sets. Moreover, ablation studies for each component of KERM are also explored. All experimental results were reported under the same BM25 setting.", "rationale": "This work conducted ablation studies for each component of KERM."}, {"context": "Table\u00a03 shows the performance comparisons between different settings of knowledge injector, which is statistically significant. From this table, we can observe the following phenomena. (1) MRR@10 of KERM without interaction and propagation process decreases at least 1\\% respectively. This indicates both knowledge interaction and propagation processes play an indispensable role in ranking performance. (2) The performance of KERM without propagation is comparable to vanilla ERNIE. Not only query and passage entities, but also their multi-hop neighbors are essential for the ranking performance. (3) MRR@10 of KERM without knowledge interaction drops the most. It suggests the simple and straightforward way to aggregate knowledge graph with text does not work in the passage re-ranking scenario. The text and knowledge graph need to be refined with each other mutually in the interaction, which will be further analyzed in detail as follows.", "rationale": "This work compared the performance of KERM with different settings for the knowledge injector, and showed that performance decreases without knowledge interaction and also decreases without knowledge propagation. This shows the significance of the knowledge injector component."}, {"context": "Knowledge graph distillation is performed in both global and local perspectives. To explore their roles in the ranking performance, we remove the graph pruning globally and sentence selection locally respectively, keep other settings unchanged, and derive KERM without graph pruning and sentence selection respectively. From results on TREC 2019 DL in Table 5, observations are listed as below. (1) Without global graph pruning, MRR@10 and the average edge score, calculated through Eq.(3), decrease the most, and the time efficiency drops slightly. This indicates the original knowledge graph exists noise data that affect performance. (2) Without sentence selection, the time efficiency drops the most and the average edge score decreases slightly, which proves that not every sentence in a passage has a positive effect on semantic matching. Overall, knowledge graph distillation is significant to KERM.", "rationale": "This work investigated the roles of knowledge graph distillation both globally and locally in the performance of KERM, and demonstrated that performance decreased without global distillation and efficiency decreases without either global or local distillation. This shows the significance of the distillation component."}], [{"context": "Existing knowledge graphs are usually incomplete and noisy. It is unsuitable for direct introduction of them to the current model. Specially, there is no knowledge base particularly for passage re-ranking task. For example, ConceptNet\u00a0(Speeret\u00a0al., 2017) is a general knowledge graph that contains common sense knowledge, where the information might not be useful for our passage re-ranking task. Therefore, it is critical for us to propose a knowledge graph distillation process from both global and local perspectives.", "rationale": "This work proposes using knowledge graph distillation as it can help retain only informative knowledge needed for passage re-ranking."}, {"context": "Knowledge graph distillation is performed in both global and local perspectives. To explore their roles in the ranking performance, we remove the graph pruning globally and sentence selection locally respectively, keep other settings unchanged, and derive KERM without graph pruning and sentence selection respectively. From results on TREC 2019 DL in Table\u00a05, observations are listed as below. (1) Without global graph pruning, MRR@10 and the average edge score, calculated through Eq.(3), decrease the most, and the time efficiency drops slightly. This indicates the original knowledge graph exists noise data that affect performance. (2) Without sentence selection, the time efficiency drops the most and the average edge score decreases slightly, which proves that not every sentence in a passage has a positive effect on semantic matching. Overall, knowledge graph distillation is significant to KERM.", "rationale": "The work demonstrated that knowledge graph distillation was empirically effective for final performance."}, {"context": "The main goal of this paper is to reasonably introduce external knowledge graph to PLMs for passage re-ranking. We first design a novel knowledge meta graph construction method to distill reliable and query related knowledge from a general and noisy knowledge graph. The knowledge meta graph bridges the semantic gap between each query and passage. Then we propose a knowledge injector layer for mutually updating text and knowledge representations, which transformers word to entity representations for graph meta network, vice versa. Knowledge Enhanced Ranking Model is pretrained with Masked Language Model (MLM) Sentence Relation Prediction (SRP) [38] tasks, and fine-tuned with cross entropy loss function for passage re-ranking task. Experimental results on public benchmark datasets show the effectiveness of the proposed method compared with state-of-the-art baselines without external knowledge due to its first attempt. The role of each module in KERM is also comprehensively analyzed. Since this work was limited to the one-to-one meta-graph of a query-passage pair built online, continued efforts are needed to make knowledge enhancement more efficient for both retrieval and re-ranking stage.", "rationale": "As existing knowledge graphs can be noisy and incomplete, this work proposes distilling only the information useful for passage re-ranking from knowledge graphs."}, {"context": "Despite that the knowledge graph distillation in our method is empirically shown to be effective for the final performance, the implementation of graph pruning and meta-graph construction is still based on simple heuristics. A more promising way of formulating a useful meta-graph is to jointly learn a graph generator with the reranker in an end-to-end fashion, which enables more flexibility.Besides, it is currently infeasible to exploit the external knowledge in the retrieval stage, which needs to exhaustively build massive meta-graphs for a large scale of candidates. A further study could focus on how to use external knowledge in PLM based retriever.", "rationale": "By investigating the effect of global and local distillation separately, this work found that the MRR@10 score and efficiency decreased slightly without global distillation, and that time efficiency decreased the most without local distillation."}, {"context": "For knowledge graph distillation, we propose a novel pipeline to establish knowledge meta graphs, which only retain informative knowledge for passage re-ranking. Specifically, we first distill a graph globally for passage re-ranking scenario from an existing knowledge graph by pruning some unreliable or noisy relations based on TransE embedding. Then for a specific query-passage pair, we extract entities from both the query and passage, and construct a query-document bipartite entity graph based on query and passage entities and their k-hop neighbors, namely knowledge meta graph. Challenge 1. could be addressed in this distillation process.", "rationale": "The authors propose a method of knowledge distillation and knowledge injection to effectively introduce external knowledge graphs to PLMs for passage re-ranking. In their experimental results, they showed the effectiveness of their method compared to state-of-the-art baselines."}], [{"context": "We use a large-scale public available corpus, i.e., MSMARCO-Passage collection\u00a0(Nguyen et\u00a0al., 2016), as our passage collection. This collection contains approximately 8.8 million passages extracted from 3.2 million web documents covering multiple fields. We train our model on the MSMARCO-TRAIN query set of 502,939 queries and evaluate KERM on three query sets. Table 1 provides the detailed information of these query sets.The first test set is MSMARCO-DEV, which includes 6,980 sparsely-judged queries mixed with multiple domains. Each query has an average of 1.1 relevant passages with binary relevance label.The second test set is TREC 2019 DL\u00a0(Craswell et\u00a0al., 2020), which contains 43 densely-judged queries with fine-grained relevance labels, i.e., irrelevant, relevant, highly relevant and perfectly relevant. On average, a query has 95.4 relevant passages, and most queries have more than 10 relevant passages. With fine-grained labels and multiple relevant passages per query, TREC 2019 DL can be used to reflect the fine-grained ranking performance between relevant passages.To evaluate KERM on specific domains, we further introduce Ohsumed\u00a0111http://disi.unitn.it/moschitti/corpora.htm query set, which contains 63 queries on bio-medical domain.The collection of Ohsumed is constructed from the first 20,000 passages in Mesh categories of the year 1991.Following the previous work\u00a0(Joachims, 1998), the test collection including 10,000 passages are utilized for performance comparison on Ohsumed query set.Each query has an average of 50.9 relevant passages with three graded relevance labels. In section\u00a06.4, we demonstrate that the quality of external knowledge constructed by KERM in such domain could be more useful.", "rationale": "MARCO-Passage collection is a large-scale publicly available corpus and two query sets derived from this corpus are used in the paper: MSMARCO-TRAIN for training the model and MSMARCO-DEV for testing."}], [{"context": "\u2022Challenge 1. Existing knowledge graph are not constructed for re-ranking task. They usually contain trivial factual triples, which can hardly bring information gain. The inappropriate selection of external knowledge could even jeopardize the re-ranker performance. How to utilize existing knowledge graph to re-ranking task is remain a challenge.\u2022Challenge 2.The explicit knowledge and implicit knowledge are highly heterogeneous due to the different sources, which makes the aggregation of the two difficult.How to mutually refine each other and effectively aggregate explicit knowledge into implicit knowledge to alleviate the semantic gap between query and passage is still a challenge.", "rationale": "Existing knowledge graphs can contain trivial factual triplets, which do not bring substantial information gain."}, {"context": "Fig. 2 shows a real case of our global graph pruning method on ConceptNet, i.e., a general knowledge graph. In this case, the entity hepatitis has various relations to disease, infectious disease, adult, etc. From the distance of nodes in Fig. 2, we can clearly observe that the knowledge hepatitis is an infectious disease is more reliable and informative than hepatitis is located at adult. To hepatitis, the concept adult is more general than infectious disease. This indicates that our pruning method can effectively eliminate less informative knowledge.", "rationale": "In ConceptNet, the entity \u201chepatitis\u201d has relations with both \u201cinfectious disease\u201d and \u201cadult\u201d. In this case, to the concept \u201chepatitis\u201d,  the concept \u201cadults\u201d is more general than \u201cinfectious disease\u201d and thus the relationship between \u201chepatitis\u201d and \u201cinfectious disease\u201d is more reliable and informative."}]], "composition": ["This work conducted ablation studies to investigate the contribution of each component in the performance of KERM. By testing different settings for the knowledge injector, this work found that performance decreases without knowledge interaction and also without knowledge propagation. By testing the model without global or local distillation, they also demonstrated that performance decreases without global distillation and efficiency decreases without either global or local distillation. These experiments demonstrate that each component of KERM contributes to passage re-ranking performance quantitatively.", "This work proposes using knowledge graph distillation as it can help retain only informative knowledge needed for passage re-ranking. By investigating the effect of global and local distillation separately, this work found that the MRR@10 score and efficiency decreased slightly without global distillation, and that time efficiency decreased the most without local distillation. Therefore, this work demonstrates that both global and local distillation of knowledge graphs is useful for re-ranking tasks in terms of performance and efficiency.", "MARCO-Passage collection is a large-scale publicly available corpus and two query sets derived from this corpus are used in the paper: MSMARCO-TRAIN and MSMARCO-DEV. How and who collected the queries from MARCO-Passage to construct MSMARCO-TRAIN cannot be answered from this paper.", "Unreliable relations in a knowledge graph involve trivial factual triplets that do not bring substantial information gain. For example, in ConceptNet, the entity \u201chepatitis\u201d has relations with both \u201cinfectious disease\u201d and \u201cadult\u201d. To the concept \u201chepatitis\u201d,  the concept \u201cadults\u201d is more general than \u201cinfectious disease\u201d and thus the relationship between \u201chepatitis\u201d and \u201cinfectious disease\u201d is more reliable and informative."], "Is_figure_in_evidence": [false, false, false, true], "Is_table_in_evidence": [true, true, true, false], "question_key": ["69", "70", "71", "73"], "passages": ["Passage Re-ranking is a crucial stage in modern information retrieval systems, which aims to reorder a small set of candidate passages to be presented to users. To put the most relevant passages on top of a ranking list, a re-ranker is usually designed with powerful capacity in modeling semantic relevance, which attracted a wealth of research studies in the past decade\u00a0(Guo et\u00a0al., 2020). Recently,large-scale pre-trained language models (PLMs), e.g. BERT\u00a0(Devlinet\u00a0al., 2018), ERNIE\u00a0(Sun et\u00a0al., 2019) and RoBERTa\u00a0(Liu et\u00a0al., 2019), have dominated many natural language processing tasks, and have also achieved remarkable success on passage re-ranking.For example, PLM based re-rankers\u00a0(MacAvaney et\u00a0al., 2019; Liet\u00a0al., 2020; Dong and Niu, 2021; Donget\u00a0al., 2022) have achieved state-of-the-art performance, which takes the concatenation of query-passage pair as input, and applies multi-layer full-attention to model their semantic relevance. Their superiority can be attributed to the expressive transformer structure and the pretrain-then-finetune paradigm, which allow the model to learn useful implicit knowledge (i.e., semantic relevance in the latent space) from massive textual corpus\u00a0(Fan et\u00a0al., 2021).", "However, implicit knowledge still has some inherent weaknesses, which limits the applicability of PLMs based re-rankers. First,queries and passages are usually created by different persons and have different expression ways\u00a0(Nogueiraet\u00a0al., 2019b), such as word usage and language style.Worse still, the data distributions of search queries and web contents are highly heterogeneous\u00a0(Liuet\u00a0al., 2021), where various specialized domains (e.g., bio-medical) may only have few training examples in a general corpus. Domain-specific knowledge can hardly be revealed and captured by the model, and thus the processing of domain-specific queries is often inaccurate.", "To overcome the limitations, it is essential to incorporate the knowledge graph as explicit knowledge to PLM based re-rankers. Thus we propose Knowledge Enhanced Re-ranking Model (KERM), which utilizes external knowledge to explicitly enhance the semantic matching process in PLM based re-rankers.Intuitively, the difference in expression ways can be mitigated by the triplet with \"synonymy\" as relation in knowledge graph, and all the triplets can enrich the domain knowledge.The overall workflow of KERM is depicted in Fig.\u00a01.To the the best of our knowledge, this is the first attempt for knowledge enhanced PLMs for passage re-ranking.", "Despite the knowledge graph is a desirable source of explicit knowledge, it is non-trivial to take advantage of explicit knowledge directly for passage re-ranking due to the following two challenges:", "\u2022Challenge 1. Existing knowledge graph are not constructed for re-ranking task. They usually contain trivial factual triples, which can hardly bring information gain. The inappropriate selection of external knowledge could even jeopardize the re-ranker performance. How to utilize existing knowledge graph to re-ranking task is remain a challenge.\u2022Challenge 2.The explicit knowledge and implicit knowledge are highly heterogeneous due to the different sources, which makes the aggregation of the two difficult.How to mutually refine each other and effectively aggregate explicit knowledge into implicit knowledge to alleviate the semantic gap between query and passage is still a challenge.", "In general, the workflow of KERM can be divided into knowledge graph distillation and knowledge aggregation to tackle the above challenges.", "For knowledge graph distillation, we propose a novel pipeline to establish knowledge meta graphs, which only retain informative knowledge for passage re-ranking. Specifically, we first distill a graph globally for passage re-ranking scenario from an existing knowledge graph by pruning some unreliable or noisy relations based on TransE embedding. Then for a specific query-passage pair, we extract entities from both the query and passage, and construct a query-document bipartite entity graph based on query and passage entities and their k-hop neighbors, namely knowledge meta graph. Challenge 1. could be addressed in this distillation process.", "For knowledge aggregation, we design a novel interaction module between text and knowledge graph to combine the implicit and explicit knowledge. To derive implicit knowledge from text, we employ PLM as text encoder. To be aligned with implicit knowledge, knowledge meta graph is encoded with a multi-layer graph neural network (i.e. k-hop), namely Graph Meta Network (GMN). Each transformer layer outputs word representations. Each graph meta network layer outputs entity representations. Both word and entity representations are aggregated as the input of the following transformer and GMN layer, respectively in a novelly designed module, namely knowledge injector. Therefore through knowledge aggregation, implicit knowledge from text corpus and explicit knowledge from existing knowledge graph can mutually boost each other to achieve a better re-ranking performance, in which the issues in Challenge 2. could be mitigated.", "Overall, our contributions can be summarized as follows:\u2022It is the first attempt to solve the knowledge enhanced PLMs problem for passage re-ranking. The key motivation lies in that bridging the semantic gap between the query and passage with the help of both kinds of knowledge.\u2022We design a novel knowledge graph distillation method. It refines a reliable knowledge graph from the existing one globally and constructs a knowledge meta graph based on the refined graph locally.\u2022We propose a novel aggregation of PLM and graph neural network framework to model the interaction between explicit knowledge and implicit knowledge.\u2022Experimental results show the effectiveness of KERM on both general and domain specific data, achieving state-of-the-art performance for passage re-ranking. We also conduct a comprehensive study for the effects of each module in our method. The code is available at https://github.com/DQ0408 /KERM.", "In this section, we introduce several recently-proposed PLMs based re-rankers and retrievers. Moreover, we also present the general background of the related techniques involved in this paper, i.e. Knowledge Enhanced Pre-trained Language Models (KE-PLMs) and Graph Neural Network.", "Existing PLMs based re-rankers typically improve ranking performance from two aspects: (1) By optimizing the ranking procedure: monoBERT\u00a0(Nogueira and Cho, 2019) is the first work that re-purposed BERT as a passage re-ranker and achieves state-of-the-art results. duoBERT\u00a0(Nogueiraet\u00a0al., 2019a) integrates monoBERT in a multistage ranking architecture and adopts a pairwise classification approach to passage relevance computation. UED\u00a0(Yanet\u00a0al., 2021) proposes a cascade pre-training manner that can jointly enhance the retrieval stage through passage expansion with a pre-trained query generator and thus elevate the re-ranking stage with a pre-trained transformer encoder. The two stages can facilitate each other in a unified pre-training framework. H-ERNIE\u00a0(Chuet\u00a0al., 2022) proposes a multi-granularity PLM for web search.(2) By designing rational distillation procedure: LM Distill + Fine-Tuning\u00a0(Gaoet\u00a0al., 2020) explores a variety of distillation methods to equip a smaller re-ranker with both general-purpose language modeling knowledge learned in pre-training and search- specific\ud835\udc60\ud835\udc5d\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc53\ud835\udc56\ud835\udc50specificitalic_s italic_p italic_e italic_c italic_i italic_f italic_i italic_c relevance modeling knowledge learned in fine-tuning, and produces a faster re-ranker with better ranking performance. CAKD\u00a0(Hofst\u00e4tter et\u00a0al., 2020) proposes a cross-architecture knowledge distillation procedure with a Margin-MSE loss, which can distill knowledge from multiple teachers at the same time. RocketQAv1\u00a0(Qu et\u00a0al., 2021) trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. RocketQAv2\u00a0(Ren et\u00a0al., 2021) proposes a novel approach that jointly trains the dense passage retriever and passage re-ranker. The parameters of RocketQAv2 are inherited from RocketQAv1. Besides, RocketQAv2 utilizes a large PLM for data augmentation and denoising, which can also be regarded as a distillation procedure. Notably, these two types of studies anticipate more insightful information to be captured by the advanced ranking and training procedures, while neglecting the limitations of implicit knowledge extracted from noisy and heterogeneous data. Therefore, in this paper, we proposed the first knowledge-enhanced PLM based re-ranker, which thoughtfully leverages explicit external knowledge that improve the effectiveness of the model.", "The low-dimensional dense representations for query and passage are computed by PLMs based retrievers from the dual-encoder architecture. Afterward, the candidate passage set could be retrieved efficiently via approximate nearest neighbor algorithms.Existing studies could be categorized into two parts:(1) By optimizing the matching stage: DPR\u00a0(Karpukhin et\u00a0al., 2020) is the first study to leverage PLM to empower the retriever by a single vector. Other researches, such asRepBERT\u00a0(Zhanet\u00a0al., 2020), ColBERT\u00a0(Khattab andZaharia, 2020), COIL\u00a0(Gaoet\u00a0al., 2021) and Interactor\u00a0(Yeet\u00a0al., 2022), obtain multiple vectors for query and passage for matching.(2) By optimizing the representation learning module: RocketQAv1\u00a0(Qu et\u00a0al., 2021) and RocketQAv2\u00a0(Ren et\u00a0al., 2021) boost the representation learning of retriever by leveraging the power of cross-encoder in a cascade or joint manner. Other studies boost the representation learning by designed IR-oriented pre-training tasks.ICT\u00a0(Leeet\u00a0al., 2019) treats sentences as pseudo-queries and matched them to the passage they originate from. Condenser\u00a0(Gao and Callan, 2021) utilizes a novel pre-training task, which can produces an information-rich representation to condense an input sequence.", "Existing KE-PLMs can be categorized by the granularity of knowledge they incorporate from knowledge graph (KG), as text-based knowledge, entity knowledge and KG meta-graphs.To integrate text-based knowledge, RAG\u00a0(Lewiset\u00a0al., 2020) and KIF\u00a0(Fanet\u00a0al., 2020) first retrieve top-k documents from Wikipedia using KNN-based retrieval, and the PLM model is employed to generate the output conditioned on these retrieved documents. Entity-level information can be highly useful for a variety of natural language understanding tasks. Hence, many existing KE-PLMs target this type of simple yet powerful knowledge. ERNIE(BAIDU)\u00a0(Sun et\u00a0al., 2019) introduces a new pre-training strategy of language model which masking phrases or entities in order to implicitly learn both synaptic and semantic knowledge from these units. ERNIE(THU)\u00a0(Zhanget\u00a0al., 2019) integrates informative entity representations in the knowledge module into the underlying layers of the semantic module based on the alignments between text and entity to equip the model with the ability of knowledge awareness. As knowledge graphs provide richer information than simply entity, more and more researchers start to explore integration of more sophisticated knowledge, such as meta-graphs in KG. CokeBERT\u00a0(Su et\u00a0al., 2021) proposes a novel semantic-driven Graph Neural Network (GNN) to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text.CoLake\u00a0(Sunet\u00a0al., 2020a) also uses GNN to aggregate information from the constructed meta-graph in both pre-training and inference. CoLake converts the meta-graph into token sequence and appends it to input sequence for PLMs, which is distinctive to CokeBERT. Although extensive research has been proposed up to now to address the knowledge-aware problem, none exists which constrained on how to use knowledge to empower PLMs particularly for re-ranking tasks.", "Existing Graph Neural Networks (GNNs) mainly fall into two categories: graph-based and path-based. Graph-based GNNs learn the structured information by directly passing nodes massage on the graph structure.GCNs\u00a0(Kipf and Welling, 2016) introduce a novel approach on graph-structured data by aggregating messages from its direct neighbors to learn the graph-structured feature efficiently and effectively. R-GCNs\u00a0(Schlichtkrull et\u00a0al., 2018) are developed specifically to encode the highly multi-relational graphs by defining relation-specific weight matrix for each edge type.In contrast, path-based GNNs first decompose the graph into paths and then pass nodes massage on the path level, which can naturally utilize the relationship between neighbors to transmit messages.RNs\u00a0(Santoro et\u00a0al., 2017) use MLPs to encode all paths in a graph and then pool the representation of paths to generate a global representation for the graph. KagNet\u00a0(Linet\u00a0al., 2019) is a combination of GCNs, LSTMs and a hierarchical path-based attention mechanism, which forms an architecture for modeling nondegenerate paths in a graph. In this work, we use path-based GNNs to formulate our GMN module for its good scalability on modeling relationship information in heterogeneous graphs.", "Given a query q, passage re-ranking aims at ordering a set of \u03f0italic-\u03f0\\varkappaitalic_\u03f0 passages, i.e.,\ud835\udcab={\ud835\udc29\u03ba}\u03ba=1\u03f0\ud835\udcabsuperscriptsubscriptsubscript\ud835\udc29\ud835\udf05\ud835\udf051italic-\u03f0\\mathcal{P}=\\left\\{\\textbf{p}_{\\kappa}\\right\\}_{\\kappa=1}^{\\varkappa}caligraphic_P = { p start_POSTSUBSCRIPT italic_\u03ba end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_\u03ba = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03f0 end_POSTSUPERSCRIPT, which is usually retrieved from a large-scale passage collection by a retriever, e.g. BM25\u00a0(Yanget\u00a0al., 2017), DPR\u00a0(Karpukhin et\u00a0al., 2020) etc. In particular, a passage is a sequence of words \ud835\udc29={wp}p=1|\ud835\udc29|\ud835\udc29superscriptsubscriptsubscript\ud835\udc64\ud835\udc5d\ud835\udc5d1\ud835\udc29\\textbf{p}=\\{w_{p}\\}_{p=1}^{|\\textbf{p}|}p = { italic_w start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_p = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | p | end_POSTSUPERSCRIPT, where |\ud835\udc29|\ud835\udc29|\\textbf{p}|| p | is the length of passage p. Similarly, a query is a sequence of words \ud835\udc2a={wq}q=1|\ud835\udc2a|\ud835\udc2asuperscriptsubscriptsubscript\ud835\udc64\ud835\udc5e\ud835\udc5e1\ud835\udc2a\\textbf{q}=\\{w_{q}\\}_{q=1}^{|\\textbf{q}|}q = { italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_q = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | q | end_POSTSUPERSCRIPT. Note that a passage p consists of T\ud835\udc47Titalic_T sentences \ud835\udc29={\ud835\udc2c\u03c4}\u03c4=1T\ud835\udc29superscriptsubscriptsubscript\ud835\udc2c\ud835\udf0f\ud835\udf0f1\ud835\udc47\\textbf{p}=\\{\\textbf{s}_{\\tau}\\}_{\\tau=1}^{T}p = { s start_POSTSUBSCRIPT italic_\u03c4 end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_\u03c4 = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT.", "Following a previous study\u00a0(Zou et\u00a0al., 2021), a desirable re-ranker is a scoring function f*(\u22c5,\u22c5)superscript\ud835\udc53\u22c5\u22c5f^{*}(\\cdot,\\cdot)italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( \u22c5 , \u22c5 ) that maximizes the consistency between its predictions (denoted as Y^\ud835\udc2a,\ud835\udcab={f(\ud835\udc2a,\ud835\udc29\u03ba)|\ud835\udc29\u03ba\u2208\ud835\udcab}subscript^\ud835\udc4c\ud835\udc2a\ud835\udcabconditional-set\ud835\udc53\ud835\udc2asubscript\ud835\udc29\ud835\udf05subscript\ud835\udc29\ud835\udf05\ud835\udcab\\hat{Y}_{\\textbf{q},\\mathcal{P}}=\\{f(\\mathbf{q},\\mathbf{p}_{\\kappa})\\leavevmode\\nobreak\\ |\\leavevmode\\nobreak\\ \\mathbf{p}_{\\kappa}\\in\\mathcal{P}\\}over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT q , caligraphic_P end_POSTSUBSCRIPT = { italic_f ( bold_q , bold_p start_POSTSUBSCRIPT italic_\u03ba end_POSTSUBSCRIPT ) | bold_p start_POSTSUBSCRIPT italic_\u03ba end_POSTSUBSCRIPT \u2208 caligraphic_P }) and the ground truth labels (denoted as Y={y\u03ba}\u03ba=1\u03f0\ud835\udc4csuperscriptsubscriptsubscript\ud835\udc66\ud835\udf05\ud835\udf051italic-\u03f0Y=\\{y_{\\kappa}\\}_{\\kappa=1}^{\\varkappa}italic_Y = { italic_y start_POSTSUBSCRIPT italic_\u03ba end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_\u03ba = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03f0 end_POSTSUPERSCRIPT), i.e.,", "(1)f*=maxf\u2061\ud835\udd3c{\ud835\udc2a,\ud835\udcab,Y}\u03d1(Y,Y^\ud835\udc2a,\ud835\udcab),superscript\ud835\udc53subscript\ud835\udc53subscript\ud835\udd3c\ud835\udc2a\ud835\udcab\ud835\udc4citalic-\u03d1\ud835\udc4csubscript^\ud835\udc4c\ud835\udc2a\ud835\udcabf^{*}=\\max_{f}\\mathbb{E}_{\\{\\textbf{q},\\mathcal{P},Y\\}}{\\vartheta(Y,\\hat{Y}_{\\textbf{q},\\mathcal{P}})},italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT = roman_max start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT { q , caligraphic_P , italic_Y } end_POSTSUBSCRIPT italic_\u03d1 ( italic_Y , over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT q , caligraphic_P end_POSTSUBSCRIPT ) ,where \u03d1italic-\u03d1\\varthetaitalic_\u03d1 is a ranking metric (e.g., MRR@10) that measures the consistency between the predictions and the labels.", "A knowledge base is usually represented as a directed graph \ud835\udca2={\u2130,\u211b}\ud835\udca2\u2130\u211b\\mathcal{G}=\\{\\mathcal{E},\\mathcal{R}\\}caligraphic_G = { caligraphic_E , caligraphic_R }, where the node set \u2130\u2130\\mathcal{E}caligraphic_E represents entities, and the edge set \u211b\u211b\\mathcal{R}caligraphic_R is composed of relations between entities.A triplet (eh,r,et)subscript\ud835\udc52\u210e\ud835\udc5fsubscript\ud835\udc52\ud835\udc61(e_{h},r,e_{t})( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_r , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is the basic unit in the knowledge graph, where eh,et\u2208\u2130subscript\ud835\udc52\u210esubscript\ud835\udc52\ud835\udc61\u2130e_{h},e_{t}\\in\\mathcal{E}italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2208 caligraphic_E are head and tail entity respectively, and r\u2208\u211b\ud835\udc5f\u211br\\in\\mathcal{R}italic_r \u2208 caligraphic_R refers to their relations.For example, (apple,used_for,eating)\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc59\ud835\udc52\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51_\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc54(apple,used\\_{for},eating)( italic_a italic_p italic_p italic_l italic_e , italic_u italic_s italic_e italic_d _ italic_f italic_o italic_r , italic_e italic_a italic_t italic_i italic_n italic_g ) means that \"apple is used for eating\".", "To leverage explicit knowledge in \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G for passage re-ranking, we anticipate building a novel knowledge-enhanced passage re-ranker, whose objective can be defined as(2)f*=maxf\u2061\ud835\udd3c{\ud835\udc2a,\ud835\udcab,Y}\u03d1(Y,Y^\ud835\udc2a,\ud835\udcab,\ud835\udca2),superscript\ud835\udc53subscript\ud835\udc53subscript\ud835\udd3c\ud835\udc2a\ud835\udcab\ud835\udc4citalic-\u03d1\ud835\udc4csubscript^\ud835\udc4c\ud835\udc2a\ud835\udcab\ud835\udca2f^{*}=\\max_{f}\\mathbb{E}_{\\{\\textbf{q},\\mathcal{P},Y\\}}{\\vartheta(Y,\\hat{Y}_{\\textbf{q},\\mathcal{P},\\mathcal{G}})},italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT = roman_max start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT { q , caligraphic_P , italic_Y } end_POSTSUBSCRIPT italic_\u03d1 ( italic_Y , over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT q , caligraphic_P , caligraphic_G end_POSTSUBSCRIPT ) ,where Y^\ud835\udc2a,\ud835\udcab,\ud835\udca2={f(\ud835\udc2a,\ud835\udc29\u03ba|\ud835\udca2)|\ud835\udc29\u03ba\u2208\ud835\udcab}subscript^\ud835\udc4c\ud835\udc2a\ud835\udcab\ud835\udca2conditional\ud835\udc53\ud835\udc2aconditionalsubscript\ud835\udc29\ud835\udf05\ud835\udca2subscript\ud835\udc29\ud835\udf05\ud835\udcab\\hat{Y}_{\\textbf{q},\\mathcal{P},\\mathcal{G}}=\\{f(\\mathbf{q},\\mathbf{p}_{\\kappa}\\leavevmode\\nobreak\\ |\\leavevmode\\nobreak\\ \\mathcal{G})\\leavevmode\\nobreak\\ |\\leavevmode\\nobreak\\ \\mathbf{p}_{\\kappa}\\in\\mathcal{P}\\}over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT q , caligraphic_P , caligraphic_G end_POSTSUBSCRIPT = { italic_f ( bold_q , bold_p start_POSTSUBSCRIPT italic_\u03ba end_POSTSUBSCRIPT | caligraphic_G ) | bold_p start_POSTSUBSCRIPT italic_\u03ba end_POSTSUBSCRIPT \u2208 caligraphic_P }, and f(\ud835\udc2a,\ud835\udc29\u03ba|\ud835\udca2)\ud835\udc53\ud835\udc2aconditionalsubscript\ud835\udc29\ud835\udf05\ud835\udca2f(\\mathbf{q},\\mathbf{p}_{\\kappa}\\leavevmode\\nobreak\\ |\\leavevmode\\nobreak\\ \\mathcal{G})italic_f ( bold_q , bold_p start_POSTSUBSCRIPT italic_\u03ba end_POSTSUBSCRIPT | caligraphic_G ) represents the ranking score that is aware of the explicit knowledge extracted from \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G.", "In this section, we introduce Knowledge Enhanced Re-ranking Model (KERM), which leverages explicit knowledge that improves conventional cross-encoder for passage re-ranking.Notably, the main challenges of incorporating explicit knowledge are to 1) distill a knowledge graph that is useful for re-ranking task, and 2) aggregate the explicit knowledge with the current implicit knowledge in an appropriate manner that can improve the overall performance. Hence our proposed approach is mainly composed of two parts, i.e., knowledge graph distillation and knowledge aggregation, to tackle two challenges respectively. In the rest of this section, we first describe how to distill a reliable knowledge graph globally and build a knowledge meta graph locally from it for a specific query-passage pair. Then, we present how to combine the distilled knowledge graph and existing text corpus to derive a knowledge enhanced passage re-ranker.", "Existing knowledge graphs are usually incomplete and noisy. It is unsuitable for direct introduction of them to the current model. Specially, there is no knowledge base particularly for passage re-ranking task. For example, ConceptNet\u00a0(Speeret\u00a0al., 2017) is a general knowledge graph that contains common sense knowledge, where the information might not be useful for our passage re-ranking task. Therefore, it is critical for us to propose a knowledge graph distillation process from both global and local perspectives.", "Given a global knowledge graph \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G, the first step is to eliminate those knowledge that might be noisy to be applied. To achieve this, we use TransE\u00a0(Bordes et\u00a0al., 2013) to measure the reliability of a given knowledge triplet. In particular, TransE is an unsupervised learning method that learns latent representations for a knowledge triplet (eh,r,et)subscript\ud835\udc52\u210e\ud835\udc5fsubscript\ud835\udc52\ud835\udc61(e_{h},r,e_{t})( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_r , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ). Intuitively, it models the latent distribution of knowledge in a given knowledge graph, and those who are out of this distribution can be viewed as less informative knowledge, which should not be used. Based on this,we use the entity embeddings pre-trained by TransE to calculate a distance metric between two linked entities as(3)Rele(eh,r,et)=\ud835\udc04(eh)\u22c5\ud835\udc04(r)+\ud835\udc04(eh)\u22c5\ud835\udc04(et)+\ud835\udc04(r)\u22c5\ud835\udc04(et),\ud835\udc45\ud835\udc52subscript\ud835\udc59\ud835\udc52subscript\ud835\udc52\u210e\ud835\udc5fsubscript\ud835\udc52\ud835\udc61\u22c5\ud835\udc04subscript\ud835\udc52\u210e\ud835\udc04\ud835\udc5f\u22c5\ud835\udc04subscript\ud835\udc52\u210e\ud835\udc04subscript\ud835\udc52\ud835\udc61\u22c5\ud835\udc04\ud835\udc5f\ud835\udc04subscript\ud835\udc52\ud835\udc61Rel_{e}(e_{h},r,e_{t})=\\mathbf{E}({e_{h}})\\cdot\\mathbf{E}(r)+\\mathbf{E}({e_{h}})\\cdot\\mathbf{E}({e_{t}})+\\mathbf{E}({r})\\cdot\\mathbf{E}({e_{t}}),italic_R italic_e italic_l start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_r , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = bold_E ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) \u22c5 bold_E ( italic_r ) + bold_E ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) \u22c5 bold_E ( italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + bold_E ( italic_r ) \u22c5 bold_E ( italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,(4)Dist(eh,et)=1Rele(eh,r,et),\ud835\udc37\ud835\udc56\ud835\udc60\ud835\udc61subscript\ud835\udc52\u210esubscript\ud835\udc52\ud835\udc611\ud835\udc45\ud835\udc52subscript\ud835\udc59\ud835\udc52subscript\ud835\udc52\u210e\ud835\udc5fsubscript\ud835\udc52\ud835\udc61Dist(e_{h},e_{t})=\\frac{1}{Rel_{e}(e_{h},r,e_{t})},italic_D italic_i italic_s italic_t ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG italic_R italic_e italic_l start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_r , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG ,where \ud835\udc04(e)\ud835\udc04\ud835\udc52\\mathbf{E}({e})bold_E ( italic_e ) and \ud835\udc04(r)\ud835\udc04\ud835\udc5f\\mathbf{E}({r})bold_E ( italic_r ) are the TransE embeddings of entity and relation, respectively, and the inner product measures the relevance between two vectors. As the objective of TranE is aligned with minimizing the distance shown in Eq.(4), we can consider those knowledge triplets with small distance values as informative knowledge.", "After measuring the reliability of knowledge, we prune \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G by only keep the top-\u03a0\u03a0\\Piroman_\u03a0 neighboring entities \ud835\udca9(eh)\ud835\udca9subscript\ud835\udc52\u210e\\mathcal{N}(e_{h})caligraphic_N ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) of a given entity ehsubscript\ud835\udc52\u210ee_{h}italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT, which can formally be defined as(5)\ud835\udca9(eh)=\u222a\u03c0=1\u03a0{et\u03c0},whereDist(eh,et\u03c0)\u2264Dist(eh,et\u03c0+1).formulae-sequence\ud835\udca9subscript\ud835\udc52\u210esuperscriptsubscript\ud835\udf0b1\u03a0superscriptsubscript\ud835\udc52\ud835\udc61\ud835\udf0b\ud835\udc64\u210e\ud835\udc52\ud835\udc5f\ud835\udc52\ud835\udc37\ud835\udc56\ud835\udc60\ud835\udc61subscript\ud835\udc52\u210esuperscriptsubscript\ud835\udc52\ud835\udc61\ud835\udf0b\ud835\udc37\ud835\udc56\ud835\udc60\ud835\udc61subscript\ud835\udc52\u210esuperscriptsubscript\ud835\udc52\ud835\udc61\ud835\udf0b1\\mathcal{N}(e_{h})=\\cup_{\\pi=1}^{\\Pi}\\{e_{t}^{\\pi}\\},where\\,Dist(e_{h},e_{t}^{\\pi})\\leq Dist(e_{h},e_{t}^{\\pi+1}).caligraphic_N ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) = \u222a start_POSTSUBSCRIPT italic_\u03c0 = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_\u03a0 end_POSTSUPERSCRIPT { italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT } , italic_w italic_h italic_e italic_r italic_e italic_D italic_i italic_s italic_t ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT ) \u2264 italic_D italic_i italic_s italic_t ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03c0 + 1 end_POSTSUPERSCRIPT ) .Thus, the pruned global graph \ud835\udca2\u00af\u00af\ud835\udca2\\overline{\\mathcal{G}}over\u00af start_ARG caligraphic_G end_ARG can be denoted as(6)\ud835\udca2\u00af={(eh,r,et)|eh,et\u2208\u2130\u2227r\u2208\u211b\u2227et\u2208\ud835\udca9(eh)}.\u00af\ud835\udca2conditional-setsubscript\ud835\udc52\u210e\ud835\udc5fsubscript\ud835\udc52\ud835\udc61subscript\ud835\udc52\u210esubscript\ud835\udc52\ud835\udc61\u2130\ud835\udc5f\u211bsubscript\ud835\udc52\ud835\udc61\ud835\udca9subscript\ud835\udc52\u210e\\overline{\\mathcal{G}}=\\{(e_{h},r,e_{t})|e_{h},e_{t}\\in\\mathcal{E}\\wedge r\\in\\mathcal{R}\\wedge e_{t}\\in\\mathcal{N}(e_{h})\\}.over\u00af start_ARG caligraphic_G end_ARG = { ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_r , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) | italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2208 caligraphic_E \u2227 italic_r \u2208 caligraphic_R \u2227 italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2208 caligraphic_N ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) } .", "Fig.\u00a02 shows a real case of our global graph pruning method on ConceptNet, i.e., a general knowledge graph. In this case, the entity hepatitis has various relations to disease, infectious disease, adult, etc. From the distance of nodes in Fig.\u00a02, we can clearly observe that the knowledge hepatitis is an infectious disease is more reliable and informative than hepatitis is located at adult. To hepatitis, the concept adult is more general than infectious disease. This indicates that our pruning method can effectively eliminate less informative knowledge.", "Different from existing knowledge-enhanced PLMs for other NLP tasks, our aim for the re-ranking task is particularly on the relevance modeling between query and passage.Thus, we further leverage the knowledge in the global graph \ud835\udca2\u00af\u00af\ud835\udca2\\overline{\\mathcal{G}}over\u00af start_ARG caligraphic_G end_ARG to construct \u201cbridges\u201d between query and passage, which alleviates the semantic gap and improves semantic modeling.More specifically, for a given query-passage pair (i.e., (\ud835\udc2a,\ud835\udc29)\ud835\udc2a\ud835\udc29(\\mathbf{q},\\mathbf{p})( bold_q , bold_p )), we propose to construct a bipartite meta-graph that connects those entities in the \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q and those in \ud835\udc29\ud835\udc29\\mathbf{p}bold_p.", "The construction process is shown in Alg.\u00a01, which contains three sub-steps: key sentence selection, target entity recognition and path discovery.", "(1)Key sentence selection. The actual information need of a user usually concentrates on a small part of a relevant passage\u00a0(Guo et\u00a0al., 2020). To this end, we mimic human judgment and only focus on the sentence of each passage that is the most related to a query\u00a0(Zou et\u00a0al., 2021).In particular, we define the relevance score between a query q and a sentence \ud835\udc2cisubscript\ud835\udc2c\ud835\udc56\\textbf{s}_{i}s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as(7)Relqs(\ud835\udc2a,\ud835\udc2ci)=\u2211q=1|\ud835\udc2a|\ud835\udc04(wq)|\ud835\udc2a|\u22c5\u2211s=1|\ud835\udc2ci|\ud835\udc04(ws)|\ud835\udc2ci|.\ud835\udc45\ud835\udc52subscript\ud835\udc59\ud835\udc5e\ud835\udc60\ud835\udc2asubscript\ud835\udc2c\ud835\udc56\u22c5superscriptsubscript\ud835\udc5e1\ud835\udc2a\ud835\udc04subscript\ud835\udc64\ud835\udc5e\ud835\udc2asuperscriptsubscript\ud835\udc601subscript\ud835\udc2c\ud835\udc56\ud835\udc04subscript\ud835\udc64\ud835\udc60subscript\ud835\udc2c\ud835\udc56Rel_{qs}(\\textbf{q},\\textbf{s}_{i})=\\frac{\\sum_{q=1}^{|\\textbf{q}|}\\textbf{E}(w_{q})}{|\\textbf{q}|}\\cdot\\frac{\\sum_{s=1}^{|\\textbf{s}_{i}|}\\textbf{E}(w_{s})}{|\\textbf{s}_{i}|}.italic_R italic_e italic_l start_POSTSUBSCRIPT italic_q italic_s end_POSTSUBSCRIPT ( q , s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = divide start_ARG \u2211 start_POSTSUBSCRIPT italic_q = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | q | end_POSTSUPERSCRIPT E ( italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) end_ARG start_ARG | q | end_ARG \u22c5 divide start_ARG \u2211 start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | end_POSTSUPERSCRIPT E ( italic_w start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) end_ARG start_ARG | s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | end_ARG .For the sake of efficiency, we initialize \ud835\udc04(w)\ud835\udc04\ud835\udc64\\textbf{E}(w)E ( italic_w ) from Word2Vec\u00a0(Mikolovet\u00a0al., 2013) embedding.Based on Eq.(7), we select the most relevant sentence \ud835\udc2c*superscript\ud835\udc2c\\textbf{s}^{*}s start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT in p to build the meta-graph for \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q and \ud835\udc29\ud835\udc29\\mathbf{p}bold_p.(2)Target entity recognition.Next, we select the entities in q and \ud835\udc2c*superscript\ud835\udc2c\\textbf{s}^{*}s start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT to construct the meta-graph. Specifically, we only consider the entities that exactly match in \u2130\u2130\\mathcal{E}caligraphic_E. Meanwhile, we omit those entity phrases that are sub-sequences of other recognized entities.For example, in the query \"what causes low liver enzymes\", both \"liver\" and \"liver enzyme\" are entities, but the entity \"liver enzyme\" is more informative to be recognized as the target entity, and \"liver\" should be omitted.(3)Path discovery. Finally, given the target entities of q and \ud835\udc2c*superscript\ud835\udc2c\\textbf{s}^{*}s start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT (denoted as \u03d5\ud835\udc2asubscriptitalic-\u03d5\ud835\udc2a\\phi_{\\mathbf{q}}italic_\u03d5 start_POSTSUBSCRIPT bold_q end_POSTSUBSCRIPT and \u03d5\ud835\udc2c*subscriptitalic-\u03d5superscript\ud835\udc2c\\phi_{\\mathbf{s}^{*}}italic_\u03d5 start_POSTSUBSCRIPT bold_s start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, respectively), we perform Breadth First Search (BFS) on \ud835\udca2\u00af\u00af\ud835\udca2\\overline{\\mathcal{G}}over\u00af start_ARG caligraphic_G end_ARG to discover the paths within K\ud835\udc3eKitalic_K-hop between \u03d5\ud835\udc2asubscriptitalic-\u03d5\ud835\udc2a\\phi_{\\mathbf{q}}italic_\u03d5 start_POSTSUBSCRIPT bold_q end_POSTSUBSCRIPT and \u03d5\ud835\udc2c*subscriptitalic-\u03d5superscript\ud835\udc2c\\phi_{\\mathbf{s}^{*}}italic_\u03d5 start_POSTSUBSCRIPT bold_s start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. Note that we only keep the within-K\ud835\udc3eKitalic_K-hop paths that might be the most useful for the downstream re-ranking task. Meanwhile, the knowledge could be complemented from the K\ud835\udc3eKitalic_K-hop paths.", "After taking the series of processes, the meta-graph \ud835\udc06\ud835\udc2a,\ud835\udc29={\u2130\ud835\udc2a,\ud835\udc29,\u211b\ud835\udc2a,\ud835\udc29}subscript\ud835\udc06\ud835\udc2a\ud835\udc29subscript\u2130\ud835\udc2a\ud835\udc29subscript\u211b\ud835\udc2a\ud835\udc29\\mathbf{G}_{\\mathbf{q},\\mathbf{p}}=\\{\\mathcal{E}_{\\mathbf{q},\\mathbf{p}},\\mathcal{R}_{\\mathbf{q},\\mathbf{p}}\\}bold_G start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT = { caligraphic_E start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT , caligraphic_R start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT } is constructed with the multi-hop paths discovered between \u03d5\ud835\udc2asubscriptitalic-\u03d5\ud835\udc2a\\phi_{\\mathbf{q}}italic_\u03d5 start_POSTSUBSCRIPT bold_q end_POSTSUBSCRIPT and \u03d5\ud835\udc2c*subscriptitalic-\u03d5superscript\ud835\udc2c\\phi_{\\mathbf{s}^{*}}italic_\u03d5 start_POSTSUBSCRIPT bold_s start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. Fig. 3 shows an example of the meta-graph, which contains rich knowledge about the semantic relevance between the query and passage. Notably, a better key sentence selector or entity linker such as Sentence-BERT\u00a0(Reimers andGurevych, 2019) and DER\u00a0(Wu et\u00a0al., 2019) may benefit the ranking performance, but can burden the entire model inference time, which is infeasible to a qualified re-ranker.", "Given a meta-graph \ud835\udc06\ud835\udc2a,\ud835\udc29subscript\ud835\udc06\ud835\udc2a\ud835\udc29\\mathbf{G}_{\\mathbf{q},\\mathbf{p}}bold_G start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT, we propose a PLM based re-ranker that performs knowledge-enhanced relevance computation, i.e., f(\ud835\udc2a,\ud835\udc29|\ud835\udca2)\ud835\udc53\ud835\udc2aconditional\ud835\udc29\ud835\udca2f(\\mathbf{q},\\mathbf{p}\\leavevmode\\nobreak\\ |\\leavevmode\\nobreak\\ \\mathcal{G})italic_f ( bold_q , bold_p | caligraphic_G ). In the following, we first introduce the text encoder, and then present how we inject explicit knowledge from \ud835\udc06\ud835\udc2a,\ud835\udc29subscript\ud835\udc06\ud835\udc2a\ud835\udc29\\mathbf{G}_{\\mathbf{q},\\mathbf{p}}bold_G start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT into the encoder.", "We adopt the commonly-used cross-encoder as the text encoder. The input is formulated asthe concatenation of a query-passage pair and the input layer converts the token indexes to a set of token embeddings\u00a0(Vaswani et\u00a0al., 2017) (i.e., \ud835\udc0e0subscript\ud835\udc0e0\\mathbf{O}_{0}bold_O start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT) as(8)\ud835\udc0e0=InputLayer([[CLS],{wq}q=1|\ud835\udc2a|,[SEP],{wp}p=1|\ud835\udc29|,[SEP]]).subscript\ud835\udc0e0InputLayerdelimited-[]\ud835\udc36\ud835\udc3f\ud835\udc46superscriptsubscriptsubscript\ud835\udc64\ud835\udc5e\ud835\udc5e1\ud835\udc2adelimited-[]\ud835\udc46\ud835\udc38\ud835\udc43superscriptsubscriptsubscript\ud835\udc64\ud835\udc5d\ud835\udc5d1\ud835\udc29delimited-[]\ud835\udc46\ud835\udc38\ud835\udc43\\mathbf{O}_{0}=\\text{InputLayer}([[CLS],\\{w_{q}\\}_{q=1}^{|\\mathbf{q}|},[SEP],\\{w_{p}\\}_{p=1}^{|\\mathbf{p}|},[SEP]]).bold_O start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = InputLayer ( [ [ italic_C italic_L italic_S ] , { italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_q = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | bold_q | end_POSTSUPERSCRIPT , [ italic_S italic_E italic_P ] , { italic_w start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_p = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | bold_p | end_POSTSUPERSCRIPT , [ italic_S italic_E italic_P ] ] ) .In the l\ud835\udc59litalic_l-th transformer layer, text context features are extracted via multi-head self-attention and Feed Forward Network (FFN) as(9)\ud835\udc07^l=MultiHeadAttention\u2061(\ud835\udc0el\u22121),subscript^\ud835\udc07\ud835\udc59MultiHeadAttentionsubscript\ud835\udc0e\ud835\udc591\\hat{\\mathbf{H}}_{l}=\\operatorname{MultiHeadAttention}(\\mathbf{O}_{l-1}),over^ start_ARG bold_H end_ARG start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = roman_MultiHeadAttention ( bold_O start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT ) ,(10)\ud835\udc0el=\u03c3(\ud835\udc07l^\ud835\udc16l1+bl1)\ud835\udc16l2+bl2,subscript\ud835\udc0e\ud835\udc59\ud835\udf0e^subscript\ud835\udc07\ud835\udc59superscriptsubscript\ud835\udc16\ud835\udc591superscriptsubscript\ud835\udc4f\ud835\udc591superscriptsubscript\ud835\udc16\ud835\udc592superscriptsubscript\ud835\udc4f\ud835\udc592\\mathbf{O}_{l}=\\sigma\\left(\\hat{\\mathbf{H}_{l}}\\mathbf{W}_{l}^{1}+b_{l}^{1}\\right)\\mathbf{W}_{l}^{2}+b_{l}^{2},bold_O start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = italic_\u03c3 ( over^ start_ARG bold_H start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG bold_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT + italic_b start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) bold_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_b start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,where \ud835\udc16lsubscript\ud835\udc16\ud835\udc59\\mathbf{W}_{l}bold_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and blsubscript\ud835\udc4f\ud835\udc59b_{l}italic_b start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT are the parameters of FFN and \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 is an activation function, and \ud835\udc0elsubscript\ud835\udc0e\ud835\udc59\\mathbf{O}_{l}bold_O start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT is the output of layer l\ud835\udc59litalic_l.", "Based on the text encoder, we develop a knowledge injector that can seamlessly integrate explicit knowledge. Moreover, inspired by CokeBERT\u00a0(Su et\u00a0al., 2021), our knowledge injector is equipped with a GMN module to dynamically refine the knowledge context on the basis of text context features learned by text encoder, which further improves the flexibility and usability of the knowledge enhancement. Besides, our method allows the text context and knowledge context to interact and mutually boost each other.", "Knowledge injection.As shown in Fig.\u00a04, the knowledge injector consists of multiple transformer layers, which is the same as the text encoder. Given a query-passage pair (\ud835\udc2a,\ud835\udc29)\ud835\udc2a\ud835\udc29(\\mathbf{q},\\mathbf{p})( bold_q , bold_p ), we first find the entities in \ud835\udc06\ud835\udc2a,\ud835\udc29subscript\ud835\udc06\ud835\udc2a\ud835\udc29\\mathbf{G}_{\\mathbf{q},\\mathbf{p}}bold_G start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT that can be enhanced by external knowledge. For these entities, we define \ud835\udc04\ud835\udc04\\mathbf{E}bold_E as the knowledge embeddings to be applied in the knowledge injection layers, where \ud835\udc04\ud835\udc04\\mathbf{E}bold_E is initialized by TransE embeddings extracted from the pruned global graph \ud835\udca2\u00af\u00af\ud835\udca2\\overline{\\mathcal{G}}over\u00af start_ARG caligraphic_G end_ARG.", "Next, we align each entity with the first token of the corresponding phrase in the selected key sentence\u00a0(Zhanget\u00a0al., 2019), and define the knowledge injection process as(11)\ud835\udc07^l=MultiHeadAttention\u2061(\ud835\udc0el\u22121),subscript^\ud835\udc07\ud835\udc59MultiHeadAttentionsubscript\ud835\udc0e\ud835\udc591\\hat{\\mathbf{H}}_{l}=\\operatorname{MultiHeadAttention}(\\mathbf{O}_{l-1}),over^ start_ARG bold_H end_ARG start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = roman_MultiHeadAttention ( bold_O start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT ) ,(12)\ud835\udc05l=\u03c3((\ud835\udc07l^\ud835\udc16l1+bl1)\u2295\u039b(\ud835\udc04\ud835\udc16l3+bl3)),subscript\ud835\udc05\ud835\udc59\ud835\udf0edirect-sum^subscript\ud835\udc07\ud835\udc59superscriptsubscript\ud835\udc16\ud835\udc591superscriptsubscript\ud835\udc4f\ud835\udc591\u039bsuperscriptsubscript\ud835\udc04\ud835\udc16\ud835\udc593superscriptsubscript\ud835\udc4f\ud835\udc593\\mathbf{F}_{l}=\\sigma\\left((\\hat{\\mathbf{H}_{l}}\\mathbf{W}_{l}^{1}+b_{l}^{1})\\oplus\\Lambda(\\mathbf{E}\\mathbf{W}_{l}^{3}+b_{l}^{3})\\right),bold_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = italic_\u03c3 ( ( over^ start_ARG bold_H start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG bold_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT + italic_b start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) \u2295 roman_\u039b ( bold_EW start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT + italic_b start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) ) ,(13)\ud835\udc0el=\ud835\udc05l\ud835\udc16l2+bl2.subscript\ud835\udc0e\ud835\udc59subscript\ud835\udc05\ud835\udc59subscriptsuperscript\ud835\udc162\ud835\udc59subscriptsuperscript\ud835\udc4f2\ud835\udc59\\mathbf{O}_{l}=\\mathbf{F}_{l}\\mathbf{W}^{2}_{l}+b^{2}_{l}.bold_O start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = bold_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT bold_W start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT + italic_b start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT .In Eq. (12), \u2295direct-sum\\oplus\u2295 means element-wise addition and \u039b(\u22c5)\u039b\u22c5\\Lambda(\\cdot)roman_\u039b ( \u22c5 ) represents the alignment function maps the entities to the corresponding positions of the tokens. By doing this, the external knowledge \ud835\udc04\ud835\udc04\\mathbf{E}bold_E is integrated in the output \ud835\udc0elsubscript\ud835\udc0e\ud835\udc59\\mathbf{O}_{l}bold_O start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT of the knowledge injection layer. The final relevance score of this query-passage pair is defined as(14)f(\ud835\udc2a,\ud835\udc29|\ud835\udca2)=\u03c3(\ud835\udc0eM[CLS]\ud835\udc164+b4).\ud835\udc53\ud835\udc2aconditional\ud835\udc29\ud835\udca2\ud835\udf0esuperscriptsubscript\ud835\udc0eM[CLS]superscript\ud835\udc164superscript\ud835\udc4f4f(\\mathbf{q},\\mathbf{p}\\leavevmode\\nobreak\\ |\\leavevmode\\nobreak\\ \\mathcal{G})=\\sigma\\left(\\mathbf{O}_{\\textrm{M}}^{\\textrm{[CLS]}}\\mathbf{W}^{4}+b^{4}\\right).italic_f ( bold_q , bold_p | caligraphic_G ) = italic_\u03c3 ( bold_O start_POSTSUBSCRIPT M end_POSTSUBSCRIPT start_POSTSUPERSCRIPT [CLS] end_POSTSUPERSCRIPT bold_W start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT + italic_b start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) .", "Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \ud835\udca2\u00af\u00af\ud835\udca2\\overline{\\mathcal{G}}over\u00af start_ARG caligraphic_G end_ARG. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \ud835\udc06\ud835\udc2a,\ud835\udc29subscript\ud835\udc06\ud835\udc2a\ud835\udc29\\mathbf{G}_{\\mathbf{q},\\mathbf{p}}bold_G start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT, The multi-hop paths of \ud835\udc06\ud835\udc2a,\ud835\udc29subscript\ud835\udc06\ud835\udc2a\ud835\udc29\\mathbf{G}_{\\mathbf{q},\\mathbf{p}}bold_G start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap.", "More specifically, each knowledge injection layer has a multi-layer GMN (as shown in Fig. 4) to propagate knowledge on \ud835\udc06\ud835\udc2a,\ud835\udc29subscript\ud835\udc06\ud835\udc2a\ud835\udc29\\mathbf{G}_{\\mathbf{q},\\mathbf{p}}bold_G start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT.First, the input of GMN is formulated with the fused feature \ud835\udc05lsubscript\ud835\udc05\ud835\udc59\\mathbf{F}_{l}bold_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT as(15)\ud835\udc04^l(0)=\u0393(\ud835\udc05l\ud835\udc16l5+bl5),superscriptsubscript^\ud835\udc04\ud835\udc590\u0393subscript\ud835\udc05\ud835\udc59subscriptsuperscript\ud835\udc165\ud835\udc59subscriptsuperscript\ud835\udc4f5\ud835\udc59\\hat{\\mathbf{E}}_{l}^{(0)}=\\Gamma(\\mathbf{F}_{l}\\mathbf{W}^{5}_{l}+b^{5}_{l}),over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT = roman_\u0393 ( bold_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT bold_W start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT + italic_b start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) ,where \u0393\u0393\\Gammaroman_\u0393 represents the slice operation that extracts the fused information of the target entities in \ud835\udc06\ud835\udc2a,\ud835\udc29={\u2130\ud835\udc2a,\ud835\udc29,\u211b\ud835\udc2a,\ud835\udc29}subscript\ud835\udc06\ud835\udc2a\ud835\udc29subscript\u2130\ud835\udc2a\ud835\udc29subscript\u211b\ud835\udc2a\ud835\udc29\\mathbf{G}_{\\mathbf{q},\\mathbf{p}}=\\{\\mathcal{E}_{\\mathbf{q},\\mathbf{p}},\\mathcal{R}_{\\mathbf{q},\\mathbf{p}}\\}bold_G start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT = { caligraphic_E start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT , caligraphic_R start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT }, and thus \ud835\udc04^l(0)superscriptsubscript^\ud835\udc04\ud835\udc590\\hat{\\mathbf{E}}_{l}^{(0)}over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT consists of fused entities representation \ud835\udc04^e1(0),\ud835\udc04^e2(0),\u2026,\ud835\udc04^e\u03a8(0)subscriptsuperscript^\ud835\udc040subscript\ud835\udc521subscriptsuperscript^\ud835\udc040subscript\ud835\udc522\u2026subscriptsuperscript^\ud835\udc040subscript\ud835\udc52\u03a8\\hat{\\mathbf{E}}^{(0)}_{e_{1}},\\hat{\\mathbf{E}}^{(0)}_{e_{2}},...,\\hat{\\mathbf{E}}^{(0)}_{e_{\\Psi}}over^ start_ARG bold_E end_ARG start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , over^ start_ARG bold_E end_ARG start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , \u2026 , over^ start_ARG bold_E end_ARG start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT roman_\u03a8 end_POSTSUBSCRIPT end_POSTSUBSCRIPT, i.e., \u03a8=|\u2130\ud835\udc2a,\ud835\udc29|\u03a8subscript\u2130\ud835\udc2a\ud835\udc29\\Psi=|\\mathcal{E}_{\\mathbf{q},\\mathbf{p}}|roman_\u03a8 = | caligraphic_E start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT |.", "Next, in the k\ud835\udc58kitalic_k-th layer of GMN, an entity embedding ehsubscript\ud835\udc52\u210ee_{h}italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT is updated via an attentive aggregation from its neighbors \ud835\udca9(eh)\ud835\udca9subscript\ud835\udc52\u210e\\mathcal{N}(e_{h})caligraphic_N ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) as(16)\ud835\udc04^eh(k)=\ud835\udc04^eh(k\u22121)+\u2211et\u2208\ud835\udca9(eh)\ud835\udc1aht(k)\ud835\udc04^et(k\u22121).superscriptsubscript^\ud835\udc04subscript\ud835\udc52\u210e\ud835\udc58superscriptsubscript^\ud835\udc04subscript\ud835\udc52\u210e\ud835\udc581subscriptsubscript\ud835\udc52\ud835\udc61\ud835\udca9subscript\ud835\udc52\u210esuperscriptsubscript\ud835\udc1a\u210e\ud835\udc61\ud835\udc58superscriptsubscript^\ud835\udc04subscript\ud835\udc52\ud835\udc61\ud835\udc581\\hat{\\mathbf{E}}_{e_{h}}^{(k)}=\\hat{\\mathbf{E}}_{e_{h}}^{(k-1)}+\\sum_{e_{t}\\in\\mathcal{N}(e_{h})}\\mathbf{a}_{ht}^{(k)}\\hat{\\mathbf{E}}_{e_{t}}^{(k-1)}.over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT = over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT + \u2211 start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2208 caligraphic_N ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT bold_a start_POSTSUBSCRIPT italic_h italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT .Here, \ud835\udc1aht(k)superscriptsubscript\ud835\udc1a\u210e\ud835\udc61\ud835\udc58\\mathbf{a}_{ht}^{(k)}bold_a start_POSTSUBSCRIPT italic_h italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT is the attention value, which can be defined as(17)\ud835\udc1aht(k)=exp(\ud835\udc26ht(k))\u2211en\u2208\ud835\udca9(eh)exp(\ud835\udc26hn(k)),superscriptsubscript\ud835\udc1a\u210e\ud835\udc61\ud835\udc58\ud835\udc52\ud835\udc65\ud835\udc5dsuperscriptsubscript\ud835\udc26\u210e\ud835\udc61\ud835\udc58subscriptsubscript\ud835\udc52\ud835\udc5b\ud835\udca9subscript\ud835\udc52\u210e\ud835\udc52\ud835\udc65\ud835\udc5dsuperscriptsubscript\ud835\udc26\u210e\ud835\udc5b\ud835\udc58\\mathbf{a}_{ht}^{(k)}=\\frac{exp(\\mathbf{m}_{ht}^{(k)})}{\\sum_{e_{n}\\in\\mathcal{N}(e_{h})}exp(\\mathbf{m}_{hn}^{(k)})},bold_a start_POSTSUBSCRIPT italic_h italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT = divide start_ARG italic_e italic_x italic_p ( bold_m start_POSTSUBSCRIPT italic_h italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT \u2208 caligraphic_N ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_e italic_x italic_p ( bold_m start_POSTSUBSCRIPT italic_h italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) end_ARG ,and the logits \ud835\udc26ht(k)superscriptsubscript\ud835\udc26\u210e\ud835\udc61\ud835\udc58\\mathbf{m}_{ht}^{(k)}bold_m start_POSTSUBSCRIPT italic_h italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT is computed as(18)\ud835\udc26ht(k)=\u03c3superscriptsubscript\ud835\udc26\u210e\ud835\udc61\ud835\udc58\ud835\udf0e\\displaystyle\\mathbf{m}_{ht}^{(k)}=\\sigmabold_m start_POSTSUBSCRIPT italic_h italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT = italic_\u03c3(\u03b1(\ud835\udc04^eh(k\u22121)\u2225\ud835\udc04^et(k\u22121))+\u03b2(\ud835\udc04^eh(k\u22121)\u2225\ud835\udc04^rht(k\u22121))\\displaystyle\\left(\\alpha\\left(\\hat{\\mathbf{E}}_{e_{h}}^{(k-1)}\\|\\hat{\\mathbf{E}}_{e_{t}}^{(k-1)}\\right)+\\beta\\left(\\hat{\\mathbf{E}}_{e_{h}}^{(k-1)}\\|\\hat{\\mathbf{E}}_{r_{ht}}^{(k-1)}\\right)\\right.( italic_\u03b1 ( over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT \u2225 over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT ) + italic_\u03b2 ( over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT \u2225 over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_h italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT )+\u03b3(\ud835\udc04^rht(k\u22121)\u2225\ud835\udc04^et(k\u22121))).\\displaystyle\\left.+\\gamma\\left(\\hat{\\mathbf{E}}_{r_{ht}}^{(k-1)}\\|\\hat{\\mathbf{E}}_{e_{t}}^{(k-1)}\\right)\\right).+ italic_\u03b3 ( over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_h italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT \u2225 over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k - 1 ) end_POSTSUPERSCRIPT ) ) .In Eq. (18), the functions \u03b1(\u22c5)\ud835\udefc\u22c5\\alpha(\\cdot)italic_\u03b1 ( \u22c5 ), \u03b2(\u22c5)\ud835\udefd\u22c5\\beta(\\cdot)italic_\u03b2 ( \u22c5 ) and \u03b3(\u22c5)\ud835\udefe\u22c5\\gamma(\\cdot)italic_\u03b3 ( \u22c5 )are full-connected layers, and \u22c5\u2225\u22c5\\cdot\\|\\cdot\u22c5 \u2225 \u22c5 represents concatenation operation.", "By applying a K\ud835\udc3eKitalic_K-layer GMN in each layer of the knowledge injector, the output entity representation \ud835\udc04^eh(K)superscriptsubscript^\ud835\udc04subscript\ud835\udc52\u210e\ud835\udc3e\\hat{\\mathbf{E}}_{e_{h}}^{(K)}over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_K ) end_POSTSUPERSCRIPT can ensemble knowledge from all the K\ud835\udc3eKitalic_K-hop neighbors. As described in Section 4.1.2 that all the paths of \ud835\udc06\ud835\udc2a,\ud835\udc29subscript\ud835\udc06\ud835\udc2a\ud835\udc29\\mathbf{G}_{\\mathbf{q},\\mathbf{p}}bold_G start_POSTSUBSCRIPT bold_q , bold_p end_POSTSUBSCRIPT between \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q and \ud835\udc29\ud835\udc29\\mathbf{p}bold_p is within K\ud835\udc3eKitalic_K hops, the GMN module can attentively propagate knowledge along the paths from entities in \ud835\udc29\ud835\udc29\\mathbf{p}bold_p to those in \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q, and vice versa, which can enrich the semantics of the entities that benefit the relevance modeling.", "Subsequently, the updated entity embeddings could be used as the knowledge to be injected in the next layer, i.e., \ud835\udc04:=\ud835\udc04^(K)assign\ud835\udc04superscript^\ud835\udc04\ud835\udc3e\\mathbf{E}:=\\hat{\\mathbf{E}}^{(K)}bold_E := over^ start_ARG bold_E end_ARG start_POSTSUPERSCRIPT ( italic_K ) end_POSTSUPERSCRIPT. In other words, we can re-define Eq. (12) as(19)\ud835\udc05l=\u03c3((\ud835\udc07l^\ud835\udc16l1+bl1)\u2295\u039b(\ud835\udc04l\ud835\udc16l3+bl3)),subscript\ud835\udc05\ud835\udc59\ud835\udf0edirect-sum^subscript\ud835\udc07\ud835\udc59superscriptsubscript\ud835\udc16\ud835\udc591superscriptsubscript\ud835\udc4f\ud835\udc591\u039bsubscript\ud835\udc04\ud835\udc59superscriptsubscript\ud835\udc16\ud835\udc593superscriptsubscript\ud835\udc4f\ud835\udc593\\mathbf{F}_{l}=\\sigma\\left((\\hat{\\mathbf{H}_{l}}\\mathbf{W}_{l}^{1}+b_{l}^{1})\\oplus\\Lambda(\\mathbf{E}_{l}\\mathbf{W}_{l}^{3}+b_{l}^{3})\\right),bold_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = italic_\u03c3 ( ( over^ start_ARG bold_H start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG bold_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT + italic_b start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) \u2295 roman_\u039b ( bold_E start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT bold_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT + italic_b start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) ) ,where \ud835\udc04lsubscript\ud835\udc04\ud835\udc59\\mathbf{E}_{l}bold_E start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT is defined as(20)\ud835\udc04l={\ud835\udc04^l\u22121(K),l\u2208[2,M]TransE embeddings.l=1subscript\ud835\udc04\ud835\udc59casessuperscriptsubscript^\ud835\udc04\ud835\udc591\ud835\udc3e\ud835\udc592\ud835\udc40TransE embeddings\ud835\udc591\\mathbf{E}_{l}=\\begin{cases}\\hat{\\mathbf{E}}_{l-1}^{(K)},&l\\in[2,M]\\\\\\text{TransE embeddings}.&l=1\\end{cases}bold_E start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = { start_ROW start_CELL over^ start_ARG bold_E end_ARG start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_K ) end_POSTSUPERSCRIPT , end_CELL start_CELL italic_l \u2208 [ 2 , italic_M ] end_CELL end_ROW start_ROW start_CELL TransE embeddings . end_CELL start_CELL italic_l = 1 end_CELL end_ROW", "Knowledge-enhanced pre-training.Following previous studies\u00a0(Nogueiraet\u00a0al., 2019a; Yanet\u00a0al., 2021; Kim and Ko, 2021), we conduct continual pre-training on MSMARCO corpus to warm up the parameters of GMN module.We apply Masked Language Model (MLM)\u00a0(Devlinet\u00a0al., 2018) and Sentence Relation Prediction (SRP)\u00a0(Wang et\u00a0al., 2019) as the pre-training tasks in KERM.Compared to conventional Next Sentence Prediction (NSP)\u00a0(Devlinet\u00a0al., 2018), the task of SRP is to predict whether a given sentence is the next sentence, previous sentence relation or no relation with another sentence. To incorporate knowledge during the pre-training stage, we construct a meta-graph for each sentence pair, and apply the knowledge aggregation process as introduced above.The pre-training loss is defined as\u2112p=\u2112MLM+\u2112SRPsubscript\u2112\ud835\udc5dsubscript\u2112\ud835\udc40\ud835\udc3f\ud835\udc40subscript\u2112\ud835\udc46\ud835\udc45\ud835\udc43\\mathcal{L}_{p}=\\mathcal{L}_{MLM}+\\mathcal{L}_{SRP}caligraphic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT italic_M italic_L italic_M end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_S italic_R italic_P end_POSTSUBSCRIPT.", "Knowledge-enhanced fine-tuning.We adopt a cross-entropy loss to fine-tune KERM:(21)\u2112f=\u22121|\ud835\udcac|\u2211q\u2208\ud835\udcaclog\u2061exp(f(\ud835\udc2a,\ud835\udc29+|\ud835\udca2))exp(f(\ud835\udc2a,\ud835\udc29+|\ud835\udca2))+\u2211p\u2212exp(f(\ud835\udc2a,\ud835\udc29\u2212|\ud835\udca2))subscript\u2112\ud835\udc531\ud835\udcacsubscript\ud835\udc5e\ud835\udcacexp\ud835\udc53\ud835\udc2aconditionalsuperscript\ud835\udc29\ud835\udca2exp\ud835\udc53\ud835\udc2aconditionalsuperscript\ud835\udc29\ud835\udca2subscriptsuperscript\ud835\udc5dexp\ud835\udc53\ud835\udc2aconditionalsuperscript\ud835\udc29\ud835\udca2\\mathcal{L}_{f}=-\\frac{1}{|\\mathcal{Q}|}\\sum_{q\\in\\mathcal{Q}}\\log\\frac{\\mathrm{exp}({f(\\mathbf{q},\\mathbf{p}^{+}\\leavevmode\\nobreak\\ |\\leavevmode\\nobreak\\ \\mathcal{G})})}{\\mathrm{exp}({f(\\mathbf{q},\\mathbf{p}^{+}\\leavevmode\\nobreak\\ |\\leavevmode\\nobreak\\ \\mathcal{G})})+\\sum_{p^{-}}\\mathrm{exp}({f(\\mathbf{q},\\mathbf{p}^{-}\\leavevmode\\nobreak\\ |\\leavevmode\\nobreak\\ \\mathcal{G})})}caligraphic_L start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = - divide start_ARG 1 end_ARG start_ARG | caligraphic_Q | end_ARG \u2211 start_POSTSUBSCRIPT italic_q \u2208 caligraphic_Q end_POSTSUBSCRIPT roman_log divide start_ARG roman_exp ( italic_f ( bold_q , bold_p start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT | caligraphic_G ) ) end_ARG start_ARG roman_exp ( italic_f ( bold_q , bold_p start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT | caligraphic_G ) ) + \u2211 start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_exp ( italic_f ( bold_q , bold_p start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT | caligraphic_G ) ) end_ARGwhere |\ud835\udcac|\ud835\udcac|\\mathcal{Q}|| caligraphic_Q | is the number of queries in training set, and p+superscript\ud835\udc5dp^{+}italic_p start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT and p\u2212superscript\ud835\udc5dp^{-}italic_p start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT denote the positive passage and negative passage in \u2119\u2119\\mathbb{P}blackboard_P for current query \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q, respectively.", "We use a large-scale public available corpus, i.e., MSMARCO-Passage collection\u00a0(Nguyen et\u00a0al., 2016), as our passage collection. This collection contains approximately 8.8 million passages extracted from 3.2 million web documents covering multiple fields. We train our model on the MSMARCO-TRAIN query set of 502,939 queries and evaluate KERM on three query sets. Table 1 provides the detailed information of these query sets.The first test set is MSMARCO-DEV, which includes 6,980 sparsely-judged queries mixed with multiple domains. Each query has an average of 1.1 relevant passages with binary relevance label.The second test set is TREC 2019 DL\u00a0(Craswell et\u00a0al., 2020), which contains 43 densely-judged queries with fine-grained relevance labels, i.e., irrelevant, relevant, highly relevant and perfectly relevant. On average, a query has 95.4 relevant passages, and most queries have more than 10 relevant passages. With fine-grained labels and multiple relevant passages per query, TREC 2019 DL can be used to reflect the fine-grained ranking performance between relevant passages.To evaluate KERM on specific domains, we further introduce Ohsumed\u00a0111http://disi.unitn.it/moschitti/corpora.htm query set, which contains 63 queries on bio-medical domain.The collection of Ohsumed is constructed from the first 20,000 passages in Mesh categories of the year 1991.Following the previous work\u00a0(Joachims, 1998), the test collection including 10,000 passages are utilized for performance comparison on Ohsumed query set.Each query has an average of 50.9 relevant passages with three graded relevance labels. In section\u00a06.4, we demonstrate that the quality of external knowledge constructed by KERM in such domain could be more useful.", "We use ConceptNet\u00a0(Speeret\u00a0al., 2017), a general knowledge graph as our external knowledge base \ud835\udca2\ud835\udca2\\mathcal{G}caligraphic_G. Following KagNet\u00a0(Linet\u00a0al., 2019), we merge relation types to increase graph density and construct a multi-relational graph with 17 relation types, including atlocation\ud835\udc4e\ud835\udc61\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5batlocationitalic_a italic_t italic_l italic_o italic_c italic_a italic_t italic_i italic_o italic_n, causes\ud835\udc50\ud835\udc4e\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc60causesitalic_c italic_a italic_u italic_s italic_e italic_s, createdby\ud835\udc50\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc51\ud835\udc4f\ud835\udc66createdbyitalic_c italic_r italic_e italic_a italic_t italic_e italic_d italic_b italic_y, etc.", "We include several PLMs based re-rankers in our evaluation, including the state-of-the-art:\u2022monoBERT\u00a0(Nogueira and Cho, 2019): The first study that re-purposes BERT as a re-ranker and achieves state-of-the-art results.\u2022duoBERT\u00a0(Nogueiraet\u00a0al., 2019a):This work proposes a pairwise classification approach using BERT, which obtains the ability to be more sensitive to semantics through greater computation.\u2022UED\u00a0(Yanet\u00a0al., 2021): A unified pre-training framework that jointly refines re-ranker and query generator. For a fair comparison, we only use the re-ranker in UED without passage expansion.\u2022LM Distill+Fine-Tuning (LDFT)\u00a0(Gaoet\u00a0al., 2020):A variety of distillation methods are compared in this paper. The experimental results indicate that a proper distillation procedure (i.e. first distill the language model, and then fine-tune on the ranking task) could produce a faster re-ranker with better ranking performance.\u2022CAKD\u00a0(Hofst\u00e4tter et\u00a0al., 2020): This work proposes a cross-architecture knowledge distillation procedure with Margin-MSE loss, which can distill knowledge from multiple teachers.\u2022RocketQAv1\u00a0(Qu et\u00a0al., 2021): This work mainly focuses on the training of PLM based retriever, where the re-ranker is an intermediate product of its training process.\u2022RocketQAv2\u00a0(Ren et\u00a0al., 2021): Based on RocketQAv1, this work proposes a novel approach that jointly trains the PLM based retriever and re-ranker.To compare the performance of different methods, we resort to two ranking metrics.For MSMARCO-DEV, We adopt Mean Reciprocal Rank (i.e., MRR@10).For TREC 2019 DL, we use Mean Average Precision, i.e., MAP@10 and MAP@30.For Ohsumed, both Mean Reciprocal Rank and Mean Average Precision (i.e., MRR@10 and MAP@10) are employed for comprehensive performance analysis in queries requiring in-depth domain knowledge.", "We use the traditional sparse retriever BM25\u00a0(Yanget\u00a0al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle\u00a0(Maet\u00a0al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL)\u00a0222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer\u00a0(Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM\u2019s backbone are initialized from ERNIE-2.0 base\u00a0(Sunet\u00a0al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on ERNIEbasesubscriptERNIEbase\\textrm{ERNIE}_{\\textrm{base}}ERNIE start_POSTSUBSCRIPT base end_POSTSUBSCRIPT re-ranker and all models in ablation studies.", "Here we compare ranking performances of KERM and other PLMs based re-rankers on the first two widely used query sets. Moreover, ablation studies for each component of KERM are also explored. All experimental results were reported under the same BM25 setting.", "Table\u00a02 shows the ranking performance of KERM and baselines on MSMARCO-DEV and TREC 2019 DL. In the second column, model settings are displayed, including the PLMs used in models, whether distillation is enabled and computing resources required for model training. From Table\u00a02, we observe the following phenomena.", "(1) Compared with the best SOTA methods on the sparsely-judged MARCO-DEV query set, KERM outperforms all other baseline models except RocketQAv2.It utilizes a well-trained cross-encoder ERNIElargesubscriptERNIElarge\\textrm{ERNIE}_{\\textrm{large}}ERNIE start_POSTSUBSCRIPT large end_POSTSUBSCRIPT in RocketQAv1 to remove the predicted negatives with low confidence scores and include the predicted positives with high confidence scores. This can be regarded as a distillation. Meanwhile, RocketQAv2 achieves promising performance through a very large batch size on enormous computational resources, which is hardly comparable to our technique that only requires up to 4 GPUs. In addition to RocketQAv2, both KERMbasesubscriptKERMbase\\textrm{KERM}_{\\textrm{base}}KERM start_POSTSUBSCRIPT base end_POSTSUBSCRIPT and KERMlargesubscriptKERMlarge\\textrm{KERM}_{\\textrm{large}}KERM start_POSTSUBSCRIPT large end_POSTSUBSCRIPT exceed strong baseline models, including duoBERT with sophisticated multiple re-ranking stages and CAKD distilled from multiple large models. It demonstrates the effectiveness of external knowledge injection.", "(2) Among both kinds of baselines, KERMlargesubscriptKERMlarge\\textrm{KERM}_{\\textrm{large}}KERM start_POSTSUBSCRIPT large end_POSTSUBSCRIPT achieves the best performance on the densely-judged TREC 2019 DL query set. MAP @10 and MAP@30 measure the quality of the ranking result over related passages. Baseline models with larger networks usually perform better in MAP, which indicates that complex structure helps model capture the fine-grained differences between related passages. With the well-designed GMN module and introduced reliable external knowledge, KERMbasesubscriptKERMbase\\textrm{KERM}_{\\textrm{base}}KERM start_POSTSUBSCRIPT base end_POSTSUBSCRIPT achieves the best performance on MAP@10 even compared to various large baseline models.", "(3) Distilled models typically perform better at putting the relevant passage at top positions, but the subtle differences between relevant passages cannot be captured effectively through relatively small distilled models. On the MARCO-DEV query set, LDFT\u00a0(Gaoet\u00a0al., 2020) performs better than duoBERT on MRR@10 and the former model size is much smaller than the latter. It shows that distillation plays a great role in performance improvement. Because LDFT\u00a0(Gaoet\u00a0al., 2020) neither release the code nor report MAP in the original paper, we omit its result on TREC 2019 DL query set. Additionally, models that perform well on MAP do not lead in MRR and vice versa, demonstrating that two metrics are to measure different aspects of the ranking quality. KERM shows the most stable performance on both metrics among all baseline models.", "(4) Compared with ERNIEbasesubscriptERNIEbase\\textrm{ERNIE}_{\\textrm{base}}ERNIE start_POSTSUBSCRIPT base end_POSTSUBSCRIPT we trained, KERMbasesubscriptKERMbase\\textrm{KERM}_{\\textrm{base}}KERM start_POSTSUBSCRIPT base end_POSTSUBSCRIPT shows a significant improvement on both two query sets. This indicates the explicit introduction of external knowledge can alleviate the semantic gap and heterogeneity between query and passage, and improve the semantic matching performance.", "Knowledge injector module including knowledge injection and propagation process realized as Graph Meta Network (GMN), is mainly responsible for the interaction between text and knowledge graph. To explore their roles in the ranking performance, we remove the knowledge injection, aggregation process and the whole module separately and keep other units unchanged in KERM. Experimental results of three base models are shown in Table\u00a03. KERM without knowledge injector module is degraded to vanilla ERNIE. KERM without knowledge propagation process is formally equivalent to ERNIE(THU)\u00a0(Zhanget\u00a0al., 2019). KERM without knowledge injection process takes the text of query-passage pair and meta graph as separate inputs, and then concatenate two parts of outputs to feed into a linear layer by redefining Eq.(19) and Eq.(14) respectively as(22)\ud835\udc05l={\u03c3(\ud835\udc07l^\ud835\udc16l1+bl1),forEq.(13)\u03c3(\ud835\udc04l\ud835\udc16l3+bl3),forEq.(15)subscript\ud835\udc05\ud835\udc59cases\ud835\udf0e^subscript\ud835\udc07\ud835\udc59superscriptsubscript\ud835\udc16\ud835\udc591superscriptsubscript\ud835\udc4f\ud835\udc591forEq.13\ud835\udf0esubscript\ud835\udc04\ud835\udc59superscriptsubscript\ud835\udc16\ud835\udc593superscriptsubscript\ud835\udc4f\ud835\udc593forEq.15\\mathbf{F}_{l}=\\begin{cases}\\sigma\\left(\\hat{\\mathbf{H}_{l}}\\mathbf{W}_{l}^{1}+b_{l}^{1}\\right),&\\textrm{for}\\;\\textrm{Eq.}(\\ref{eq:textoutput})\\\\\\sigma\\left(\\mathbf{E}_{l}\\mathbf{W}_{l}^{3}+b_{l}^{3}\\right),&\\textrm{for}\\;\\textrm{Eq.}(\\ref{eq:gmninput})\\end{cases}bold_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = { start_ROW start_CELL italic_\u03c3 ( over^ start_ARG bold_H start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG bold_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT + italic_b start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) , end_CELL start_CELL for Eq. ( ) end_CELL end_ROW start_ROW start_CELL italic_\u03c3 ( bold_E start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT bold_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT + italic_b start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) , end_CELL start_CELL for Eq. ( ) end_CELL end_ROW(23)f(\ud835\udc2a,\ud835\udc29|\ud835\udca2)=\u03c3((\ud835\udc0eM[CLS]\u2225\ud835\udc04M(K))\ud835\udc166+b6).\ud835\udc53\ud835\udc2aconditional\ud835\udc29\ud835\udca2\ud835\udf0econditionalsuperscriptsubscript\ud835\udc0eM[CLS]superscriptsubscript\ud835\udc04M\ud835\udc3esuperscript\ud835\udc166superscript\ud835\udc4f6f(\\mathbf{q},\\mathbf{p}\\leavevmode\\nobreak\\ |\\leavevmode\\nobreak\\ \\mathcal{G})=\\sigma\\left(\\left(\\mathbf{O}_{\\textrm{M}}^{\\textrm{[CLS]}}\\|\\mathbf{E}_{\\textrm{M}}^{(K)}\\right)\\mathbf{W}^{6}+b^{6}\\right).italic_f ( bold_q , bold_p | caligraphic_G ) = italic_\u03c3 ( ( bold_O start_POSTSUBSCRIPT M end_POSTSUBSCRIPT start_POSTSUPERSCRIPT [CLS] end_POSTSUPERSCRIPT \u2225 bold_E start_POSTSUBSCRIPT M end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_K ) end_POSTSUPERSCRIPT ) bold_W start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT + italic_b start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) .", "Table\u00a03 shows the performance comparisons between different settings of knowledge injector, which is statistically significant. From this table, we can observe the following phenomena. (1) MRR@10 of KERM without interaction and propagation process decreases at least 1%percent11\\%1 % respectively. This indicates both knowledge interaction and propagation processes play an indispensable role in ranking performance. (2) The performance of KERM without propagation is comparable to vanilla ERNIE. Not only query and passage entities, but also their multi-hop neighbors are essential for the ranking performance. (3) MRR@10 of KERM without knowledge interaction drops the most. It suggests the simple and straightforward way to aggregate knowledge graph with text does not work in the passage re-ranking scenario. The text and knowledge graph need to be refined with each other mutually in the interaction, which will be further analyzed in detail as follows.", "To further explore the text-knowledge interaction influence on the ranking performance, we compare ranking performances from KERM with different numbers of knowledge injector layers. All experiments in Table\u00a04 are conducted with the same experimental settings except the number of knowledge injector layers (denoted as M\ud835\udc40Mitalic_M). Note that in our setting, the number of text encoder layers N\ud835\udc41Nitalic_N plus M\ud835\udc40Mitalic_M is always 12121212, i.e. the number of layers in ERNIEbasesubscriptERNIEbase\\textrm{ERNIE}_{\\textrm{base}}ERNIE start_POSTSUBSCRIPT base end_POSTSUBSCRIPT. No knowledge injector layer (M=0\ud835\udc400M=0italic_M = 0) represents the vanilla ERNIEbasesubscriptERNIEbase\\textrm{ERNIE}_{\\textrm{base}}ERNIE start_POSTSUBSCRIPT base end_POSTSUBSCRIPT re-ranker without explicit knowledge enhancement.", "With the increase of M\ud835\udc40Mitalic_M in Table\u00a04, the ranking performance is not improved linearly. Instead, the performance achieves the best at M=3\ud835\udc403M=3italic_M = 3 and then falls down (statistically significant). This performance variation trend is contradictory to our intuition that the more injector layers, the deeper interaction between text and knowledge, and the more performance improvement is expected. The possible reason lies in that the knowledge injector layer makes pretrained parameters from ERNIEbasesubscriptERNIEbase\\textrm{ERNIE}_{\\textrm{base}}ERNIE start_POSTSUBSCRIPT base end_POSTSUBSCRIPT not reusable, which means the implicit knowledge learned from large-scale is not applicable to these layers. Hence the number choice of the knowledge injector layer is somehow determined by the trade-off between implicit and explicit knowledge.", "Knowledge graph distillation is performed in both global and local perspectives. To explore their roles in the ranking performance, we remove the graph pruning globally and sentence selection locally respectively, keep other settings unchanged, and derive KERM without graph pruning and sentence selection respectively. From results on TREC 2019 DL in Table\u00a05, observations are listed as below. (1) Without global graph pruning, MRR@10 and the average edge score, calculated through Eq.(3), decrease the most, and the time efficiency drops slightly. This indicates the original knowledge graph exists noise data that affect performance. (2) Without sentence selection, the time efficiency drops the most and the average edge score decreases slightly, which proves that not every sentence in a passage has a positive effect on semantic matching. Overall, knowledge graph distillation is significant to KERM.", "We further investigate the ranking effect of KERM on a specific domain. Specifically, we conduct experiments on OHSUMED from bio-medical field, and a bio-medical query subset of MSMARCO-DEV including 1,11011101,1101 , 110 queries. This query subset is derived from the mixed domain query set of MSMARCO-DEV by k-means clustering method\u00a0(Hartigan and Wong, 1979), while the remaining subset with 5,87058705,8705 , 870 queires is denoted as the general domain subset. Performance comparisons between KERM and BM25, ERNIE are shown in Table\u00a06.", "Results are obtained from Table\u00a06. (1) Poor ranking performances of all models on bio-medical domain indicates that it is more challenging in the data scarcity scenario, where textual data is not covered widely in the PLMs\u2019 pretraining datasets. (2) Compared with ERNIE, KERM has a higher relative improvement in bio-medical domain than general domain. This demonstrates that the incorporation of knowledge graph is more useful for a data scarcity domain. To verify this idea, we compare the size of knowledge meta graph used for different domains as follows.", "We quantify the knowledge desirability as the size of average knowledge meta graph used in one domain. Specifically, we use the average number of edges as the size and average edge score calculated through Eq.(3) as the reliability of the knowledge meta graph. From Table 7, we can see that the meta-graph constructed on Bio-Medical domains is better in terms of quantity and quality. It indicates that the external knowledge found on professional domains contains more information.", "The main goal of this paper is to reasonably introduce external knowledge graph to PLMs for passage re-ranking. We first design a novel knowledge meta graph construction method to distill reliable and query related knowledge from a general and noisy knowledge graph. The knowledge meta graph bridges the semantic gap between each query and passage. Then we propose a knowledge injector layer for mutually updating text and knowledge representations, which transformers word to entity representations for graph meta network, vice versa. Knowledge Enhanced Ranking Model is pretrained with Masked Language Model (MLM) Sentence Relation Prediction (SRP) [38] tasks, and fine-tuned with cross entropy loss function for passage re-ranking task. Experimental results on public benchmark datasets show the effectiveness of the proposed method compared with state-of-the-art baselines without external knowledge due to its first attempt. The role of each module in KERM is also comprehensively analyzed. Since this work was limited to the one-to-one meta-graph of a query-passage pair built online, continued efforts are needed to make knowledge enhancement more efficient for both retrieval and re-ranking stage.", "Despite that the knowledge graph distillation in our method is empirically shown to be effective for the final performance, the implementation of graph pruning and meta-graph construction is still based on simple heuristics. A more promising way of formulating a useful meta-graph is to jointly learn a graph generator with the reranker in an end-to-end fashion, which enables more flexibility.Besides, it is currently infeasible to exploit the external knowledge in the retrieval stage, which needs to exhaustively build massive meta-graphs for a large scale of candidates. A further study could focus on how to use external knowledge in PLM based retriever."], "figure_types": {"ca8f25b6c49c7f34380a7c6623e74f63f3a48771/10-Table5-1.png": "table", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/10-Table6-1.png": "table", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/10-Table7-1.png": "table", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/2-Figure1-1.png": "schematic", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/4-Figure2-1.png": "schematic", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/5-Figure3-1.png": "schematic", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/6-Figure4-1.png": "schematic", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/8-Table1-1.png": "table", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/9-Table2-1.png": "table", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/9-Table3-1.png": "table", "ca8f25b6c49c7f34380a7c6623e74f63f3a48771/9-Table4-1.png": "table"}}, "1804.02767": {"paper_id": "paper_113", "title": "YOLOv3: An Incremental Improvement", "arxiv_url": "https://arxiv.org/abs/1804.02767", "s2orc_url": "https://www.semanticscholar.org/paper/ebc96892b9bcbf007be9a1d7844e4b09fde9d961", "all_figures_tables": {"ebc96892b9bcbf007be9a1d7844e4b09fde9d961/1-Figure1-1.png": "Figure 1. We adapt this figure from the Focal Loss paper [9]. YOLOv3 runs significantly faster than other detection methods with comparable performance. Times from either an M40 or Titan X, they are basically the same GPU.", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/2-Figure2-1.png": "Figure 2. Bounding boxes with dimension priors and location prediction. We predict the width and height of the box as offsets from cluster centroids. We predict the center coordinates of the box relative to the location of filter application using a sigmoid function. This figure blatantly self-plagiarized from [15].", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/2-Table1-1.png": "Table 1. Darknet-53.", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/3-Table2-1.png": "Table 2. Comparison of backbones. Accuracy, billions of operations, billion floating point operations per second, and FPS for various networks.", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/3-Table3-1.png": "Table 3. I\u2019m seriously just stealing all these tables from [9] they take soooo long to make from scratch. Ok, YOLOv3 is doing alright. Keep in mind that RetinaNet has like 3.8\u00d7 longer to process an image. YOLOv3 is much better than SSD variants and comparable to state-of-the-art models on the AP50 metric.", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/4-Figure3-1.png": "Figure 3. Again adapted from the [9], this time displaying speed/accuracy tradeoff on the mAP at .5 IOU metric. You can tell YOLOv3 is good because it\u2019s very high and far to the left. Can you cite your own paper? Guess who\u2019s going to try, this guy\u2192 [16]. Oh, I forgot, we also fix a data loading bug in YOLOv2, that helped by like 2 mAP. Just sneaking this in here to not throw off layout.", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/6-Figure4-1.png": "Figure 4. Zero-axis charts are probably more intellectually honest... and we can still screw with the variables to make ourselves look good!", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/6-Figure5-1.png": "Figure 5. These two hypothetical detectors are perfect according to mAP over these two images. They are both perfect. Totally equal."}, "referred_figures_tables": [["ebc96892b9bcbf007be9a1d7844e4b09fde9d961/3-Table3-1.png"], ["ebc96892b9bcbf007be9a1d7844e4b09fde9d961/6-Figure5-1.png"]], "question_id": [18, 0], "question": ["What are some of the limitations of the YOLOv3 object detection model?", "How does YOLOv3 improve upon previous versions of the YOLO object detection algorithm?"], "question_section": ["Feature Extractor", "Training"], "question_trigger_sentence": ["Each network is trained with identical settings and tested at , single crop accuracy. Run times are measured on a Titan X at . Thus Darknet-53 performs on par with state-of-the-art classifiers but with fewer floating point operations and more speed.", "We use a new network for performing feature extraction. Our new network is a hybrid approach between the network used in YOLOv2, Darknet-19, and that newfangled residual network stuff."], "question_type": ["Shallow question", "Testing question"], "evidential_info": [[{"context": "YOLOv3 is pretty good! See table 3. In terms of COCOs weird average mean AP metric it is on par with the SSD variants but is 3\u00d73\\times3 \u00d7 faster. It is still quite a bit behind other models like RetinaNet in this metric though.", "rationale": "However, it has comparatively worse performance on medium and larger size objects."}, {"context": "However, when we look at the \u201cold\u201d detection metric of mAP at IOU=.5 (or AP{}_{50} in the chart) YOLOv3 is very strong. It is almost on par with RetinaNet and far above the SSD variants. This indicates that YOLOv3 is a very strong detector that excels at producing decent boxes for objects. However, performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object.", "rationale": "However, performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object."}, {"context": "In the past YOLO struggled with small objects. However, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high AP{}_{S} performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.", "rationale": "It is still quite a bit behind other models like RetinaNet in this metric though."}, {"context": "YOLOv3 is a good detector. It\u2019s fast, it\u2019s accurate. It\u2019s not as great on the COCO average AP between .5 and .95 IOU metric. But it\u2019s very good on the old detection metric of .5 IOU.", "rationale": "It\u2019s not as great on the COCO average AP between 95 IOU metric."}], [{"context": "We use a new network for performing feature extraction. Our new network is a hybrid approach between the network used in YOLOv2, Darknet-19, and that newfangled residual network stuff. Our network uses successive 3\\times 3 and 1\\times 1 convolutional layers but now has some shortcut connections as well and is significantly larger. It has 53 convolutional layers so we call it\u2026. wait for it\u2026.. Darknet-53!", "rationale": "Namely, it\u2019s faster and better."}, {"context": "When we plot accuracy vs speed on the AP{}_{50} metric (see figure 5) we see YOLOv3 has significant benefits over other detection systems. Namely, it\u2019s faster and better.", "rationale": "So here\u2019s the deal with YOLOv3: We mostly took good ideas from other people. We also trained a new classifier network that\u2019s better than the other ones."}, {"context": "So here\u2019s the deal with YOLOv3: We mostly took good ideas from other people. We also trained a new classifier network that\u2019s better than the other ones. We\u2019ll just take you through the whole system from scratch so you can understand it all.", "rationale": "Our network uses successive 3\\times 3 and 1\\times 1 convolutional layers but now has some shortcut connections as well and is significantly larger. It has 53 convolutional layers so we call it\u2026. wait for it\u2026.. Darknet-53!"}]], "composition": ["Some of the limitations of YOLOv3, based on the information given in the paper are: it is still quite a bit behind other models like RetinaNet in the \"COCO's weired average mAP\" metric (COCO average AP between 95 IOU metric), performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object, it has comparatively worse performance on medium and larger size objects.", "YOLOv3 is faster and better than YOLO. It has more layers. The authors also tried some small tricks and experiments which further improved the overall performance."], "Is_figure_in_evidence": [false, true], "Is_table_in_evidence": [true, false], "question_key": ["115", "119"], "passages": ["Sometimes you just kinda phone it in for a year, you know? I didn\u2019t do a whole lot of research this year. Spent a lot of time on Twitter. Played around with GANs a little. I had a little momentum left over from last year [12] [1]; I managed to make some improvements to YOLO. But, honestly, nothing like super interesting, just a bunch of small changes that make it better. I also helped out with other people\u2019s research a little.", "Actually, that\u2019s what brings us here today. We have a camera-ready deadline [4] and we need to cite some of the random updates I made to YOLO but we don\u2019t have a source. So get ready for a TECH REPORT!", "The great thing about tech reports is that they don\u2019t need intros, y\u2019all know why we\u2019re here. So the end of this introduction will signpost for the rest of the paper. First we\u2019ll tell you what the deal is with YOLOv3. Then we\u2019ll tell you how we do. We\u2019ll also tell you about some things we tried that didn\u2019t work. Finally we\u2019ll contemplate what this all means.", "So here\u2019s the deal with YOLOv3: We mostly took good ideas from other people. We also trained a new classifier network that\u2019s better than the other ones. We\u2019ll just take you through the whole system from scratch so you can understand it all.", "Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes [15]. The network predicts 4 coordinates for each bounding box, txsubscript\ud835\udc61\ud835\udc65t_{x}italic_t start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT, tysubscript\ud835\udc61\ud835\udc66t_{y}italic_t start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT, twsubscript\ud835\udc61\ud835\udc64t_{w}italic_t start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT, thsubscript\ud835\udc61\u210et_{h}italic_t start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT. If the cell is offset from the top left corner of the image by (cx,cy)subscript\ud835\udc50\ud835\udc65subscript\ud835\udc50\ud835\udc66(c_{x},c_{y})( italic_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) and the bounding box prior has width and height pwsubscript\ud835\udc5d\ud835\udc64p_{w}italic_p start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT, phsubscript\ud835\udc5d\u210ep_{h}italic_p start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT, then the predictions correspond to:", "bxsubscript\ud835\udc4f\ud835\udc65\\displaystyle b_{x}italic_b start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT=\u03c3(tx)+cxabsent\ud835\udf0esubscript\ud835\udc61\ud835\udc65subscript\ud835\udc50\ud835\udc65\\displaystyle=\\sigma(t_{x})+c_{x}= italic_\u03c3 ( italic_t start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) + italic_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPTbysubscript\ud835\udc4f\ud835\udc66\\displaystyle b_{y}italic_b start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT=\u03c3(ty)+cyabsent\ud835\udf0esubscript\ud835\udc61\ud835\udc66subscript\ud835\udc50\ud835\udc66\\displaystyle=\\sigma(t_{y})+c_{y}= italic_\u03c3 ( italic_t start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) + italic_c start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPTbwsubscript\ud835\udc4f\ud835\udc64\\displaystyle b_{w}italic_b start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT=pwetwabsentsubscript\ud835\udc5d\ud835\udc64superscript\ud835\udc52subscript\ud835\udc61\ud835\udc64\\displaystyle=p_{w}e^{t_{w}}= italic_p start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT end_POSTSUPERSCRIPTbhsubscript\ud835\udc4f\u210e\\displaystyle b_{h}italic_b start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT=phethabsentsubscript\ud835\udc5d\u210esuperscript\ud835\udc52subscript\ud835\udc61\u210e\\displaystyle=p_{h}e^{t_{h}}= italic_p start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUPERSCRIPT", "During training we use sum of squared error loss. If the ground truth for some coordinate prediction is t^*subscript^\ud835\udc61*\\hat{t}_{\\mbox{*}}over^ start_ARG italic_t end_ARG start_POSTSUBSCRIPT * end_POSTSUBSCRIPT our gradient is the ground truth value (computed from the ground truth box) minus our prediction: t^*\u2212t*subscript^\ud835\udc61*subscript\ud835\udc61*\\hat{t}_{\\mbox{*}}-t_{\\mbox{*}}over^ start_ARG italic_t end_ARG start_POSTSUBSCRIPT * end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT * end_POSTSUBSCRIPT. This ground truth value can be easily computed by inverting the equations above.", "YOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following [17]. We use the threshold of .5.5.5.5. Unlike [17] our system only assigns one bounding box prior for each ground truth object. If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.", "Each box predicts the classes the bounding box may contain using multilabel classification. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classifiers. During training we use binary cross-entropy loss for the class predictions.", "This formulation helps when we move to more complex domains like the Open Images Dataset [7]. In this dataset there are many overlapping labels (i.e. Woman and Person). Using a softmax imposes the assumption that each box has exactly one class which is often not the case. A multilabel approach better models the data.", "YOLOv3 predicts boxes at 3 different scales. Our system extracts features from those scales using a similar concept to feature pyramid networks [8]. From our base feature extractor we add several convolutional layers. The last of these predicts a 3-d tensor encoding bounding box, objectness, and class predictions. In our experiments with COCO [10] we predict 3 boxes at each scale so the tensor is N\u00d7N\u00d7[3*(4+1+80)]\ud835\udc41\ud835\udc41delimited-[]34180N\\times N\\times[3*(4+1+80)]italic_N \u00d7 italic_N \u00d7 [ 3 * ( 4 + 1 + 80 ) ] for the 4 bounding box offsets, 1 objectness prediction, and 80 class predictions.", "Next we take the feature map from 2 layers previous and upsample it by 2\u00d72\\times2 \u00d7. We also take a feature map from earlier in the network and merge it with our upsampled features using concatenation. This method allows us to get more meaningful semantic information from the upsampled features and finer-grained information from the earlier feature map. We then add a few more convolutional layers to process this combined feature map, and eventually predict a similar tensor, although now twice the size.", "We perform the same design one more time to predict boxes for the final scale. Thus our predictions for the 3rd scale benefit from all the prior computation as well as fine-grained features from early on in the network.", "We still use k-means clustering to determine our bounding box priors. We just sort of chose 9 clusters and 3 scales arbitrarily and then divide up the clusters evenly across scales. On the COCO dataset the 9 clusters were: (10\u00d713),(16\u00d730),(33\u00d723),(30\u00d761),(62\u00d745),(59\u00d7119),(116\u00d790),(156\u00d7198),(373\u00d7326)101316303323306162455911911690156198373326(10\\times 13),(16\\times 30),(33\\times 23),(30\\times 61),(62\\times 45),(59\\times 119),(116\\times 90),(156\\times 198),(373\\times 326)( 10 \u00d7 13 ) , ( 16 \u00d7 30 ) , ( 33 \u00d7 23 ) , ( 30 \u00d7 61 ) , ( 62 \u00d7 45 ) , ( 59 \u00d7 119 ) , ( 116 \u00d7 90 ) , ( 156 \u00d7 198 ) , ( 373 \u00d7 326 ).", "We use a new network for performing feature extraction. Our new network is a hybrid approach between the network used in YOLOv2, Darknet-19, and that newfangled residual network stuff. Our network uses successive 3\u00d73333\\times 33 \u00d7 3 and 1\u00d71111\\times 11 \u00d7 1 convolutional layers but now has some shortcut connections as well and is significantly larger. It has 53 convolutional layers so we call it\u2026. wait for it\u2026.. Darknet-53!", "This new network is much more powerful than Darknet-19 but still more efficient than ResNet-101 or ResNet-152. Here are some ImageNet results:", "Each network is trained with identical settings and tested at 256\u00d7256256256256\\times 256256 \u00d7 256, single crop accuracy. Run times are measured on a Titan X at 256\u00d7256256256256\\times 256256 \u00d7 256. Thus Darknet-53 performs on par with state-of-the-art classifiers but with fewer floating point operations and more speed. Darknet-53 is better than ResNet-101 and 1.5\u00d71.5\\times1.5 \u00d7 faster. Darknet-53 has similar performance to ResNet-152 and is 2\u00d72\\times2 \u00d7 faster.", "Darknet-53 also achieves the highest measured floating point operations per second. This means the network structure better utilizes the GPU, making it more efficient to evaluate and thus faster. That\u2019s mostly because ResNets have just way too many layers and aren\u2019t very efficient.", "We still train on full images with no hard negative mining or any of that stuff. We use multi-scale training, lots of data augmentation, batch normalization, all the standard stuff. We use the Darknet neural network framework for training and testing [14].", "YOLOv3 is pretty good! See table 3. In terms of COCOs weird average mean AP metric it is on par with the SSD variants but is 3\u00d73\\times3 \u00d7 faster. It is still quite a bit behind other models like RetinaNet in this metric though.", "However, when we look at the \u201cold\u201d detection metric of mAP at IOU=.5absent.5=.5= .5 (or AP5050{}_{50}start_FLOATSUBSCRIPT 50 end_FLOATSUBSCRIPT in the chart) YOLOv3 is very strong. It is almost on par with RetinaNet and far above the SSD variants. This indicates that YOLOv3 is a very strong detector that excels at producing decent boxes for objects. However, performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object.", "In the past YOLO struggled with small objects. However, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high APS\ud835\udc46{}_{S}start_FLOATSUBSCRIPT italic_S end_FLOATSUBSCRIPT performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.", "When we plot accuracy vs speed on the AP5050{}_{50}start_FLOATSUBSCRIPT 50 end_FLOATSUBSCRIPT metric (see figure 5) we see YOLOv3 has significant benefits over other detection systems. Namely, it\u2019s faster and better.", "We tried lots of stuff while we were working on YOLOv3. A lot of it didn\u2019t work. Here\u2019s the stuff we can remember.", "Anchor box x,y\ud835\udc65\ud835\udc66x,yitalic_x , italic_y offset predictions. We tried using the normal anchor box prediction mechanism where you predict the x,y\ud835\udc65\ud835\udc66x,yitalic_x , italic_y offset as a multiple of the box width or height using a linear activation. We found this formulation decreased model stability and didn\u2019t work very well.", "Linear x,y\ud835\udc65\ud835\udc66x,yitalic_x , italic_y predictions instead of logistic. We tried using a linear activation to directly predict the x,y\ud835\udc65\ud835\udc66x,yitalic_x , italic_y offset instead of the logistic activation. This led to a couple point drop in mAP.", "Focal loss. We tried using focal loss. It dropped our mAP about 2 points. YOLOv3 may already be robust to the problem focal loss is trying to solve because it has separate objectness predictions and conditional class predictions. Thus for most examples there is no loss from the class predictions? Or something? We aren\u2019t totally sure.", "Dual IOU thresholds and truth assignment. Faster R-CNN uses two IOU thresholds during training. If a prediction overlaps the ground truth by .7 it is as a positive example, by [.3\u2212.7]delimited-[].3.7[.3-.7][ .3 - .7 ] it is ignored, less than .3 for all ground truth objects it is a negative example. We tried a similar strategy but couldn\u2019t get good results.", "We quite like our current formulation, it seems to be at a local optima at least. It is possible that some of these techniques could eventually produce good results, perhaps they just need some tuning to stabilize the training.", "YOLOv3 is a good detector. It\u2019s fast, it\u2019s accurate. It\u2019s not as great on the COCO average AP between .5 and .95 IOU metric. But it\u2019s very good on the old detection metric of .5 IOU.", "Why did we switch metrics anyway? The original COCO paper just has this cryptic sentence: \u201cA full discussion of evaluation metrics will be added once the evaluation server is complete\u201d. Russakovsky et al report that that humans have a hard time distinguishing an IOU of .3 from .5! \u201cTraining humans to visually inspect a bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is surprisingly difficult.\u201d [18] If humans have a hard time telling the difference, how much does it matter?", "But maybe a better question is: \u201cWhat are we going to do with these detectors now that we have them?\u201d A lot of the people doing this research are at Google and Facebook. I guess at least we know the technology is in good hands and definitely won\u2019t be used to harvest your personal information and sell it to\u2026. wait, you\u2019re saying that\u2019s exactly what it will be used for?? Oh.", "Well the other people heavily funding vision research are the military and they\u2019ve never done anything horrible like killing lots of people with new technology oh wait\u2026..111The author is funded by the Office of Naval Research and Google.", "I have a lot of hope that most of the people using computer vision are just doing happy, good stuff with it, like counting the number of zebras in a national park [13], or tracking their cat as it wanders around their house [19]. But computer vision is already being put to questionable use and as researchers we have a responsibility to at least consider the harm our work might be doing and think of ways to mitigate it. We owe the world that much.", "In closing, do not @ me. (Because I finally quit Twitter)."], "figure_types": {"ebc96892b9bcbf007be9a1d7844e4b09fde9d961/1-Figure1-1.png": "plot", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/2-Figure2-1.png": "schematic", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/2-Table1-1.png": "table", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/3-Table2-1.png": "table", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/3-Table3-1.png": "table", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/4-Figure3-1.png": "plot", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/6-Figure4-1.png": "plot", "ebc96892b9bcbf007be9a1d7844e4b09fde9d961/6-Figure5-1.png": "photograph(s)"}}, "1502.04681": {"paper_id": "paper_114", "title": "Unsupervised Learning of Video Representations using LSTMs", "arxiv_url": "https://arxiv.org/abs/1502.04681", "s2orc_url": "https://www.semanticscholar.org/paper/829510ad6f975c939d589eeb01a3cf6fc6c8ce4d", "all_figures_tables": {"829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/10-Figure11-1.png": "Figure 11. LSTM Classifier.", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/10-Table2-1.png": "Table 2. Future prediction results on MNIST and image patches. All models use 2 layers of LSTMs.", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/11-Figure12-1.png": "Figure 12. Effect of pretraining on action recognition with change in the size of the labelled training set. The error bars are over 10 different samples of training sets.", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/11-Table3-1.png": "Table 3. Comparison of different unsupervised pretraining methods. UCF-101 small is a subset containing 10 videos per class. HMDB51 small contains 4 videos per class.", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/11-Table4-1.png": "Table 4. Comparison with state-of-the-art action recognition models.", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/3-Figure1-1.png": "Figure 1. LSTM unit", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/3-Figure2-1.png": "Figure 2. LSTM Autoencoder Model", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/4-Figure3-1.png": "Figure 3. LSTM Future Predictor Model", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/4-Figure4-1.png": "Figure 4. The Composite Model: The LSTM predicts the future as well as the input sequence.", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/6-Figure5-1.png": "Figure 5. Reconstruction and future prediction obtained from the Composite Model on a dataset of moving MNIST digits.", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/7-Figure6-1.png": "Figure 6. Reconstruction and future prediction obtained from the Composite Model on a dataset of natural image patches. The first two rows show ground truth sequences. The model takes 16 frames as inputs. Only the last 10 frames of the input sequence are shown here. The next 13 frames are the ground truth future. In the rows that follow, we show the reconstructed and predicted frames for two instances of the model.", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/8-Figure7-1.png": "Figure 7. Pattern of activity in 200 randomly chosen LSTM units in the Future Predictor of a 1 layer (unconditioned) Composite Model trained on moving MNIST digits. The vertical axis corresponds to different LSTM units. The horizontal axis is time. The model was only trained to predict the next 10 frames, but here we let it run to predict the next 100 frames. Top: The dynamics has a periodic quality which does not die out. Bottom : The pattern of activity, if the trained weights in the future predictor are replaced by random weights. The dynamics quickly dies out.", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/9-Figure10-1.png": "Figure 10. Output features from the two decoder LSTMs of a Composite Model trained on moving MNIST digits. These figures show the top-200 features ordered by L2 norm.", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/9-Figure8-1.png": "Figure 8. Out-of-domain runs. Reconstruction and Future prediction for test sequences of one and three moving digits. The model was trained on sequences of two moving digits.", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/9-Figure9-1.png": "Figure 9. Input features from a Composite Model trained on moving MNIST digits. In an LSTM, each input frame is connected to four sets of units - the input, the input gate, forget gate and output gate. These figures show the top-200 features ordered by L2 norm of the input features. The features in corresponding locations belong to the same LSTM unit."}, "referred_figures_tables": [["829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/8-Figure7-1.png", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/8-Figure7-1.png", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/9-Figure9-1.png"], ["829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/4-Figure3-1.png", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/6-Figure5-1.png", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/10-Table2-1.png"], ["829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/11-Table4-1.png"]], "question_id": [22, 9, 25], "question": ["How many different types of experiments are performed to test the proposed models?", "Which variants of LSTM encoder-decoder models are used in this study?", "Which evaluation criteria was used to compare the performance of action recognition models?"], "question_section": ["Action Recognition on UCF-101/HMDB-51", "LSTM Autoencoder Model", "Conclusions"], "question_trigger_sentence": ["The aim of this set of experiments is to see if the features learned by unsupervised learning can help improve performance on supervised tasks.", "The key advantage of using an LSTM unit over a traditional neuron in an RNN is that the cell state in an LSTM unit sums activities over time. Since derivatives distribute over sums, the error derivatives don\u2019t vanish quickly as they get sent back into time. This makes it easy to do credit assignment over long sequences and discover long-range features.", "Future prediction results are summarized in Table 2. For MNIST we compute the cross entropy of the predictions with respect to the ground truth, both of which are 64 64 patches. For natural image patches, we compute the squared loss."], "question_type": ["Testing question ", "Shallow question ", "Shallow question "], "evidential_info": [[{"context": "Generalization over time scalesIn the next experiment, we test if the model can work at time scales that aredifferent than what it was trained on. We take a one hidden layer unconditionedComposite Model trained on moving MNIST digits. The model has 2048 LSTM unitsand looks at a 64 \\times 64 input. It was trained on input sequences of 10frames to reconstruct those 10 frames as well as predict 10 frames into thefuture. In order to test if the future predictor is able to generalize beyond 10frames, we let the model run for 100 steps into the future.Fig.\u00a07(a) shows the pattern of activity in the LSTM units of thefuture predictorpathway for a randomly chosen test input. It shows the activity at each of thethree sigmoidal gates (input, forget, output), the input (after the tanhnon-linearity, before being multiplied by the input gate), the cell state andthe final output (after being multiplied by the output gate). Even though theunits are ordered randomly along the vertical axis, we can see that the dynamicshas a periodic quality to it. The model is able to generate persistent motionfor long periods of time. In terms of reconstruction, the model only outputsblobs after the first 15 frames, but the motion is relatively well preserved.More results, including long range future predictions over hundreds of time steps can see been athttp://www.cs.toronto.edu/~nitish/unsupervised_video.To show that setting up a periodic behaviour is not trivial,Fig.\u00a07(b) shows the activity from a randomly initialized futurepredictor. Here, the LSTM state quickly converges and the outputs blur completely.", "rationale": "Generalization over time scales. In the next experiment, we test if the model can work at time scales that are different than what it was trained on."}, {"context": "Experiments on MNIST; We first trained our models on a dataset of moving MNIST digits. In this dataset, each video was 20 frames long and consisted of two digits moving inside a 64 \u00d7 64 patch. The digits were chosen randomly from the training set and placed initially at random locations inside the patch.", "rationale": "Experiments on MNIST"}, {"context": "Experiments on Natural Image Patches; Next, we tried to see if our models can also work with natural image patches. For this, we trained the models on sequences of 32 \u00d7 32 natural image patches extracted from the UCF-101 dataset. In this case, we used linear output units and the squared error loss function.", "rationale": "Experiments on Natural Image Patches"}, {"context": "Out-of-domain Inputs; Next, we test this model\u2019s ability to deal with out-of domain inputs. For this, we test the model on sequences of one and three moving digits. The model was trained on sequences of two moving digits, so it has never seen inputs with just one digit or three digits.", "rationale": "Out-of-domain Inputs"}, {"context": "Visualizing Features; Next, we visualize the features learned by this model. Fig. 9 shows the weights that connect each input frame to the encoder LSTM. There are four sets of weights. One set of weights connects the frame to the input units. There are three other sets, one corresponding to each of the three gates (input, forget and output). Each weight has a size of 64 \u00d7 64.", "rationale": "Visualizing Features"}], [{"context": "Another natural unsupervised learning task for sequences is predicting thefuture. This is the approach used in language models for modeling sequences ofwords. The design of the Future Predictor Model is same as that of theAutoencoder Model, except that the decoder LSTM in this case predicts frames ofthe video that come after the input sequence (Fig.\u00a03).Ranzato et\u00a0al. (2014) use a similar model but predict only the next frame at eachtime step. This model, on the other hand, predicts a long sequence into thefuture. Here again we can consider two variants of the decoder \u2013 conditionaland unconditioned.", "rationale": "Two variants, in first one the decoder LSTM is conditioned on the last generated frame and the other in which it is not."}, {"context": "For each of these two models, we can consider two possibilities - one in whichthe decoder LSTM is conditioned on the last generated frame and the other inwhich it is not. In the experimental section, we explore these choicesquantitatively. Here we briefly discuss arguments for and against a conditionaldecoder. A strong argument in favour of using a conditional decoder is that itallows the decoder to model multiple modes in the target sequence distribution.Without that, we would end up averaging the multiple modes in the low-levelinput space. However, this is an issue only if we expect multiple modes in thetarget sequence distribution. For the LSTM Autoencoder, there is only onecorrect target and hence a unimodal target distribution. But for the LSTM FuturePredictor there is a possibility of multiple targets given an input because evenif we assume a deterministic universe, everything needed to predict the futurewill not necessarily be observed in the input.", "rationale": "The design of the Future Predictor Model is same as that of the Auto encoder Model, except that the decoder LSTM in this case predicts frames of the video that come after the input sequence. Here again we can consider two variants of the decoder, conditional and unconditioned."}, {"context": "Future prediction results are summarized in Table 2. For MNIST we compute the cross entropy of the predictions with respect to the ground truth, both of which are 64 \u00d7 64 patches. For natural image patches, we compute the squared loss. We see that the Composite Model always does a better job of predicting the future compared to the Future Predictor. This indicates that having the autoencoder along with the future predictor to force the model to remember more about the inputs actually helps predict the future better. Next, we can compare each model with its conditional variant. Here, we find that the conditional models perform better, as was also noted in Fig. 5.", "rationale": "Composite Model always does a better job of predicting the future compared to the Future Predictor."}], [{"context": "Finally, we compare our models to the state-of-the-art action recognitionresults. The performance is summarized in Table\u00a04. The table isdivided into three sets. The first set compares models that use only RGB data(single or multiple frames). The second set compares models that use explicitlycomputed flow features only. Models in the third set use both.", "rationale": "The first set compares models that use only RGB data(single or multiple frames). The second set compares models that use explicitly computed flow features only. Models in the third set use both."}]], "composition": ["5 different types of experiments are performed to test the proposed models. They are Generalization over time scales, Experiments on MNIST, Experiments on Natural Image Patches, Out-of-domain Inputs, and Visualizing Features.", "Future Predictor, Composite Model, Conditional Future Predictor, Composite Model with Conditional Future Predictor are the variants of LSTM encoder-decoder models are used in this study.", "Evaluation criteria are measure on RGB data(single or multiple frames) and flow features."], "Is_figure_in_evidence": [true, true, false], "Is_table_in_evidence": [false, false, true], "question_key": ["122", "123", "143"], "passages": ["Understanding temporal sequences is important for solving many problems in theAI-set. Recently, recurrent neural networks using the Long Short Term Memory(LSTM) architecture (Hochreiter & Schmidhuber, 1997) have been used successfully to perform various supervisedsequence learning tasks, such as speech recognition (Graves & Jaitly, 2014), machinetranslation (Sutskever et\u00a0al., 2014; Cho et\u00a0al., 2014), and caption generation for images(Vinyals et\u00a0al., 2014). They have also been applied on videos for recognizingactions and generating natural language descriptions (Donahue et\u00a0al., 2014). Ageneral sequence to sequence learning framework was described by Sutskever et\u00a0al. (2014)in which a recurrent network is used to encode a sequence into a fixed lengthrepresentation, and then another recurrent network is used to decode a sequenceout of that representation. In this work, we apply and extend this framework tolearn representations of sequences of images. We choose to work in theunsupervised setting where we only have access to a dataset of unlabelledvideos.", "Videos are an abundant and rich source of visual information and can be seen asa window into the physics of the world we live in, showing us examples of whatconstitutes objects, how objects move against backgrounds, what happens whencameras move and how things get occluded. Being able to learn a representationthat disentangles these factors would help in making intelligent machines thatcan understand and act in their environment. Additionally, learning good videorepresentations is essential for a number of useful tasks, such as recognizingactions and gestures.", "Supervised learning has been extremely successful in learning good visualrepresentations that not only produce good results at the task they are trainedfor, but also transfer well to other tasks and datasets. Therefore, it isnatural to extend the same approach to learning video representations. This hasled to research in 3D convolutional nets (Ji et\u00a0al., 2013; Tran et\u00a0al., 2014), different temporalfusion strategies (Karpathy et\u00a0al., 2014) and exploring different ways ofpresenting visual information to convolutional nets (Simonyan & Zisserman, 2014a).However, videos are much higher dimensional entities compared to single images.Therefore, it becomes increasingly difficult to do credit assignment and learn longrange structure, unless we collect much more labelled data or do a lot offeature engineering (for example computing the right kinds of flow features) tokeep the dimensionality low. The costly work of collecting more labelled dataand the tedious work of doing more clever engineering can go a long way insolving particular problems, but this is ultimately unsatisfying as a machinelearning solution. This highlights the need for using unsupervised learning tofind and represent structure in videos. Moreover, videos have a lot ofstructure in them (spatial and temporal regularities) which makes themparticularly well suited as a domain for building unsupervised learning models.", "When designing any unsupervised learning model, it is crucial to have the rightinductive biases and choose the right objective function so that the learningsignal points the model towards learning useful features. Inthis paper, we use the LSTM Encoder-Decoder framework to learn videorepresentations. The key inductive bias here is that the same operation must beapplied at each time step to propagate information to the next step. Thisenforces the fact that the physics of the world remains the same, irrespective ofinput. The same physics acting on any state, at any time, must produce the nextstate. Our model works as follows.The Encoder LSTM runs through a sequence of frames to come upwith a representation. This representation is then decoded through another LSTMto produce a target sequence. We consider different choices of the targetsequence. One choice is to predict the same sequence as the input. Themotivation is similar to that of autoencoders \u2013 we wish to capture all that isneeded to reproduce the input but at the same time go through the inductivebiases imposed by the model. Another option is to predict the future frames.Here the motivation is to learn a representation that extracts all that isneeded to extrapolate the motion and appearance beyond what has been observed. These twonatural choices can also be combined. In this case, there are two decoder LSTMs\u2013 one that decodes the representation into the input sequence and another thatdecodes the same representation to predict the future.", "The inputs to the model can, in principle, be any representation of individualvideo frames. However, for the purposes of this work, we limit our attention totwo kinds of inputs. The first is image patches. For this we use natural imagepatches as well as a dataset of moving MNIST digits. The second ishigh-level \u201cpercepts\u201d extracted by applying a convolutional net trained onImageNet. These percepts are the states of last (and/or second-to-last) layers ofrectified linear hidden states from a convolutional neural net model.", "In order to evaluate the learned representations we qualitatively analyze thereconstructions and predictions made by the model. For a more quantitativeevaluation, we use these LSTMs as initializations for the supervised task ofaction recognition. If the unsupervised learning model comes up with usefulrepresentations then the classifier should be able to perform better, especiallywhen there are only a few labelled examples. We find that this is indeed thecase.", "The first approaches to learning representations of videos in an unsupervisedway were based on ICA (van Hateren & Ruderman, 1998; Hurri & Hyv\u00e4rinen, 2003). Le et\u00a0al. (2011) approached thisproblem using multiple layers of Independent Subspace Analysis modules. Generativemodels for understanding transformations between pairs of consecutive images arealso well studied (Memisevic, 2013; Memisevic & Hinton, 2010; Susskind et\u00a0al., 2011).This work was extended recently by Michalski et\u00a0al. (2014) to model longersequences.", "Recently, Ranzato et\u00a0al. (2014) proposed a generative model for videos. The modeluses a recurrent neural network to predict the next frame or interpolate betweenframes. In this work, the authors highlight the importance of choosing the rightloss function. It is argued that squared loss in input space is not the rightobjective because it does not respond well to small distortions in input space.The proposed solution is to quantize image patches into a large dictionary andtrain the model to predict the identity of the target patch. This does solvesome of the problems of squared loss but it introduces an arbitrary dictionarysize into the picture and altogether removes the idea of patches being similaror dissimilar to one other.Designing an appropriate lossfunction that respects our notion of visual similarity is a very hard problem(in a sense, almost as hard as the modeling problem we want to solve in thefirst place). Therefore, in this paper, we use the simple squared lossobjective function as a starting point and focus on designing an encoder-decoderRNN architecture that can be used with any loss function.", "In this section, we describe several variants of our LSTM Encoder-Decoder model.The basic unit of our network is the LSTM cell block.Our implementation of LSTMs follows closely the one discussed by Graves (2013).", "In this section we briefly describe the LSTM unit which is the basic building block ofour model. The unit is shown in Fig.\u00a01 (reproduced from Graves (2013)).", "Each LSTM unit has a cell which has a state ctsubscript\ud835\udc50\ud835\udc61c_{t}italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at time t\ud835\udc61titalic_t. This cell can bethought of as a memory unit. Access to this memory unit for reading or modifyingit is controlled through sigmoidal gates \u2013 input gate itsubscript\ud835\udc56\ud835\udc61i_{t}italic_i start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, forget gate ftsubscript\ud835\udc53\ud835\udc61f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTand output gate otsubscript\ud835\udc5c\ud835\udc61o_{t}italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The LSTM unit operates as follows. At each time step itreceives inputs from two external sources at each of the four terminals (thethree gates and the input). The first source is the current frame \ud835\udc31tsubscript\ud835\udc31\ud835\udc61{{\\bf x}_{t}}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.The second source is the previous hidden states of all LSTM units in the samelayer \ud835\udc21t\u22121subscript\ud835\udc21\ud835\udc611{\\bf h}_{t-1}bold_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT. Additionally, each gate has an internal source, the cellstate ct\u22121subscript\ud835\udc50\ud835\udc611c_{t-1}italic_c start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT of its cell block. The links between a cell and its own gatesare called peephole connections. The inputs coming from different sourcesget added up, along with a bias. The gates are activated by passing their totalinput through the logistic function. The total input at the input terminal ispassed through the tanh non-linearity. The resulting activation is multiplied bythe activation of the input gate. This is then added to the cell state aftermultiplying the cell state by the forget gate\u2019s activation ftsubscript\ud835\udc53\ud835\udc61f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The final outputfrom the LSTM unit htsubscript\u210e\ud835\udc61h_{t}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is computed by multiplying the output gate\u2019s activationotsubscript\ud835\udc5c\ud835\udc61o_{t}italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with the updated cell state passed through a tanh non-linearity. Theseupdates are summarized for a layer of LSTM units as follows\ud835\udc22tsubscript\ud835\udc22\ud835\udc61\\displaystyle{\\bf i}_{t}bold_i start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT=\\displaystyle==\u03c3(Wxi\ud835\udc31t+Whi\ud835\udc21t\u22121+Wci\ud835\udc1ct\u22121+\ud835\udc1bi),\ud835\udf0esubscript\ud835\udc4a\ud835\udc65\ud835\udc56subscript\ud835\udc31\ud835\udc61subscript\ud835\udc4a\u210e\ud835\udc56subscript\ud835\udc21\ud835\udc611subscript\ud835\udc4a\ud835\udc50\ud835\udc56subscript\ud835\udc1c\ud835\udc611subscript\ud835\udc1b\ud835\udc56\\displaystyle\\sigma\\left(W_{xi}{\\bf x}_{t}+W_{hi}{\\bf h}_{t-1}+W_{ci}{\\bf c}_{t-1}+{\\bf b}_{i}\\right),italic_\u03c3 ( italic_W start_POSTSUBSCRIPT italic_x italic_i end_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_W start_POSTSUBSCRIPT italic_h italic_i end_POSTSUBSCRIPT bold_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + italic_W start_POSTSUBSCRIPT italic_c italic_i end_POSTSUBSCRIPT bold_c start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + bold_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,\ud835\udc1ftsubscript\ud835\udc1f\ud835\udc61\\displaystyle{\\bf f}_{t}bold_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT=\\displaystyle==\u03c3(Wxf\ud835\udc31t+Whf\ud835\udc21t\u22121+Wcf\ud835\udc1ct\u22121+\ud835\udc1bf),\ud835\udf0esubscript\ud835\udc4a\ud835\udc65\ud835\udc53subscript\ud835\udc31\ud835\udc61subscript\ud835\udc4a\u210e\ud835\udc53subscript\ud835\udc21\ud835\udc611subscript\ud835\udc4a\ud835\udc50\ud835\udc53subscript\ud835\udc1c\ud835\udc611subscript\ud835\udc1b\ud835\udc53\\displaystyle\\sigma\\left(W_{xf}{\\bf x}_{t}+W_{hf}{\\bf h}_{t-1}+W_{cf}{\\bf c}_{t-1}+{\\bf b}_{f}\\right),italic_\u03c3 ( italic_W start_POSTSUBSCRIPT italic_x italic_f end_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_W start_POSTSUBSCRIPT italic_h italic_f end_POSTSUBSCRIPT bold_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + italic_W start_POSTSUBSCRIPT italic_c italic_f end_POSTSUBSCRIPT bold_c start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + bold_b start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) ,\ud835\udc1ctsubscript\ud835\udc1c\ud835\udc61\\displaystyle{\\bf c}_{t}bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT=\\displaystyle==\ud835\udc1ft\ud835\udc1ct\u22121+\ud835\udc22ttanh\u2061(Wxc\ud835\udc31t+Whc\ud835\udc21t\u22121+\ud835\udc1bc),subscript\ud835\udc1f\ud835\udc61subscript\ud835\udc1c\ud835\udc611subscript\ud835\udc22\ud835\udc61subscript\ud835\udc4a\ud835\udc65\ud835\udc50subscript\ud835\udc31\ud835\udc61subscript\ud835\udc4a\u210e\ud835\udc50subscript\ud835\udc21\ud835\udc611subscript\ud835\udc1b\ud835\udc50\\displaystyle{\\bf f}_{t}{\\bf c}_{t-1}+{\\bf i}_{t}\\tanh\\left(W_{xc}{\\bf x}_{t}+W_{hc}{\\bf h}_{t-1}+{\\bf b}_{c}\\right),bold_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_c start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + bold_i start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_tanh ( italic_W start_POSTSUBSCRIPT italic_x italic_c end_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_W start_POSTSUBSCRIPT italic_h italic_c end_POSTSUBSCRIPT bold_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + bold_b start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) ,\ud835\udc28tsubscript\ud835\udc28\ud835\udc61\\displaystyle{\\bf o}_{t}bold_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT=\\displaystyle==\u03c3(Wxo\ud835\udc31t+Who\ud835\udc21t\u22121+Wco\ud835\udc1ct+\ud835\udc1bo),\ud835\udf0esubscript\ud835\udc4a\ud835\udc65\ud835\udc5csubscript\ud835\udc31\ud835\udc61subscript\ud835\udc4a\u210e\ud835\udc5csubscript\ud835\udc21\ud835\udc611subscript\ud835\udc4a\ud835\udc50\ud835\udc5csubscript\ud835\udc1c\ud835\udc61subscript\ud835\udc1b\ud835\udc5c\\displaystyle\\sigma\\left(W_{xo}{\\bf x}_{t}+W_{ho}{\\bf h}_{t-1}+W_{co}{\\bf c}_{t}+{\\bf b}_{o}\\right),italic_\u03c3 ( italic_W start_POSTSUBSCRIPT italic_x italic_o end_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_W start_POSTSUBSCRIPT italic_h italic_o end_POSTSUBSCRIPT bold_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + italic_W start_POSTSUBSCRIPT italic_c italic_o end_POSTSUBSCRIPT bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + bold_b start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) ,\ud835\udc21tsubscript\ud835\udc21\ud835\udc61\\displaystyle{\\bf h}_{t}bold_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT=\\displaystyle==\ud835\udc28ttanh\u2061(\ud835\udc1ct).subscript\ud835\udc28\ud835\udc61subscript\ud835\udc1c\ud835\udc61\\displaystyle{\\bf o}_{t}\\tanh({\\bf c}_{t}).bold_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_tanh ( bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .", "Note that all Wc\u2063\u2219subscript\ud835\udc4a\ud835\udc50\u2219W_{c\\bullet}italic_W start_POSTSUBSCRIPT italic_c \u2219 end_POSTSUBSCRIPT matrices are diagonal, whereas the rest are dense.The key advantage of using an LSTM unit over a traditional neuron in an RNN isthat the cell state in an LSTM unit sums activities over time. Since derivativesdistribute over sums, the error derivatives don\u2019t vanish quickly as they get sent backinto time. This makes it easy to do credit assignment over long sequences anddiscover long-range features.", "In this section, we describe a model that uses Recurrent Neural Nets (RNNs) madeof LSTM units to do unsupervised learning. The model consists of two RNNs \u2013the encoder LSTM and the decoder LSTM as shown in Fig.\u00a02. Theinput to the model is a sequence of vectors (image patches or features). Theencoder LSTM reads in this sequence. After the last input has been read, thedecoder LSTM takes over and outputs a prediction for the target sequence. Thetarget sequence is same as the input sequence, but in reverse order. Reversingthe target sequence makes the optimization easier because the model can get offthe ground by looking at low range correlations. This is also inspired by howlists are represented in LISP. The encoder can be seen as creating a list byapplying the cons function on the previously constructed list and the newinput. The decoder essentially unrolls this list, with the hidden to outputweights extracting the element at the top of the list (car function) andthe hidden to hidden weights extracting the rest of the list (cdrfunction). Therefore, the first element out is the last element in.", "The decoder can be of two kinds \u2013 conditional or unconditioned.A conditional decoder receives the last generated output frame asinput, i.e., the dotted input in Fig.\u00a02 is present.An unconditioned decoder does not receive that input. This is discussed in moredetail in Sec.\u00a02.4. Fig.\u00a02 shows a singlelayer LSTM Autoencoder. The architecture can be extend to multiple layers bystacking LSTMs on top of each other.", "Why should this learn good features?The state of the encoder LSTM after the last input has been read is therepresentation of the input video. The decoder LSTM is being asked toreconstruct back the input sequence from this representation. In order to do so,the representation must retain information about the appearance of the objectsand the background as well as the motion contained in the video.However, an important question for any autoencoder-style model is what preventsit from learning an identity mapping and effectively copying the input to theoutput. In that case all the information about the input would still be presentbut the representation will be no better than the input. There are two factorsthat control this behaviour. First, the fact that there are only a fixed numberof hidden units makes it unlikely that the model can learn trivial mappings forarbitrary length input sequences. Second, the same LSTM operation is used todecode the representation recursively. This means that the same dynamics must beapplied on the representation at any stage of decoding. This further preventsthe model from learning an identity mapping.", "Another natural unsupervised learning task for sequences is predicting thefuture. This is the approach used in language models for modeling sequences ofwords. The design of the Future Predictor Model is same as that of theAutoencoder Model, except that the decoder LSTM in this case predicts frames ofthe video that come after the input sequence (Fig.\u00a03).Ranzato et\u00a0al. (2014) use a similar model but predict only the next frame at eachtime step. This model, on the other hand, predicts a long sequence into thefuture. Here again we can consider two variants of the decoder \u2013 conditionaland unconditioned.", "Why should this learn good features?In order to predict the next few frames correctly, the model needs informationabout which objects and background are present and how they are moving so thatthe motion can be extrapolated. The hidden state coming out from theencoder will try to capture this information. Therefore, this state can be seen asa representation of the input sequence.", "For each of these two models, we can consider two possibilities - one in whichthe decoder LSTM is conditioned on the last generated frame and the other inwhich it is not. In the experimental section, we explore these choicesquantitatively. Here we briefly discuss arguments for and against a conditionaldecoder. A strong argument in favour of using a conditional decoder is that itallows the decoder to model multiple modes in the target sequence distribution.Without that, we would end up averaging the multiple modes in the low-levelinput space. However, this is an issue only if we expect multiple modes in thetarget sequence distribution. For the LSTM Autoencoder, there is only onecorrect target and hence a unimodal target distribution. But for the LSTM FuturePredictor there is a possibility of multiple targets given an input because evenif we assume a deterministic universe, everything needed to predict the futurewill not necessarily be observed in the input.", "There is also an argument against using a conditional decoder from theoptimization point-of-view. There are strong short-range correlations invideo data, for example, most of the content of a frame is same as the previousone. If the decoder was given access to the last few frames while generating aparticular frame at training time, it would find it easy to pick up on thesecorrelations. There would only be a very small gradient that tries to fix up theextremely subtle errors that require long term knowledge about the inputsequence. In an unconditioned decoder, this input is removed and the model isforced to look for information deep inside the encoder.", "The two tasks \u2013 reconstructing the input and predicting the future can becombined to create a composite model as shown in Fig.\u00a04. Herethe encoder LSTM is asked to come up with a state from which we can both predictthe next few frames as well as reconstruct the input.", "This composite model tries to overcome the shortcomings that each model sufferson its own. A high-capacity autoencoder would suffer from the tendency to learntrivial representations that just memorize the inputs. However, thismemorization is not useful at all for predicting the future. Therefore, thecomposite model cannot just memorize information. On the other hand, the futurepredictor suffers form the tendency to store information only about the last fewframes since those are most important for predicting the future, i.e., in orderto predict vtsubscript\ud835\udc63\ud835\udc61v_{t}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, the frames {vt\u22121,\u2026,vt\u2212k}subscript\ud835\udc63\ud835\udc611\u2026subscript\ud835\udc63\ud835\udc61\ud835\udc58\\{v_{t-1},\\ldots,v_{t-k}\\}{ italic_v start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , \u2026 , italic_v start_POSTSUBSCRIPT italic_t - italic_k end_POSTSUBSCRIPT } are much moreimportant than v0subscript\ud835\udc630v_{0}italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, for some small value of k\ud835\udc58kitalic_k. Therefore the representationat the end of the encoder will have forgotten about a large part of the input.But if we ask the model to also predict all of the input sequence, thenit cannot just pay attention to the last few frames.", "We design experiments to accomplish the following objectives:\u2022Get a qualitative understanding of what the LSTM learns to do.\u2022Measure the benefit of initializing networks for supervised learning taskswith the weights found by unsupervised learning, especially with very few training examples.\u2022Compare the different proposed models - Autoencoder, Future Predictor andComposite models and their conditional variants.\u2022Compare with state-of-the-art action recognition benchmarks.", "We use the UCF-101 and HMDB-51 datasets for supervised tasks.The UCF-101 dataset (Soomro et\u00a0al., 2012) contains 13,320 videos with an average length of6.2 seconds belonging to 101 different action categories. The dataset has 3standard train/test splits with the training set containing around 9,500 videosin each split (the rest are test).The HMDB-51 dataset (Kuehne et\u00a0al., 2011) contains 5100 videos belonging to 51 differentaction categories. Mean length of the videos is 3.2 seconds. This also has 3train/test splits with 3570 videos in the training set and rest in test.", "To train the unsupervised models, we used a subset of the Sports-1M dataset(Karpathy et\u00a0al., 2014), that contains 1\u00a0million YouTube clips.Even though this dataset is labelled for actions, we didnot do any supervised experiments on it because of logistical constraints withworking with such a huge dataset. We instead collected 300 hours of video byrandomly sampling 10 second clips from the dataset. It is possible to collectbetter samples if instead of choosing randomly, we extracted videos where a lot ofmotion is happening and where there are no shot boundaries. However, we did notdo so in the spirit of unsupervised learning, and because we did not want tointroduce any unnatural bias in the samples. We also used the superviseddatasets (UCF-101 and HMDB-51) for unsupervised training. However, we found thatusing them did not give any significant advantage over just using the YouTubevideos.", "We extracted percepts using the convolutional neural net model ofSimonyan & Zisserman (2014b). The videos have a resolution of 240 \u00d7\\times\u00d7 320 and weresampled at almost 30 frames per second. We took the central 224 \u00d7\\times\u00d7 224patch from each frame and ran it through the convnet. This gave us the RGBpercepts. Additionally, for UCF-101, we computed flow percepts by extracting flows using the Broxmethod and training the temporal stream convolutional network as described bySimonyan & Zisserman (2014a). We found that the fc6 features worked better than fc7 forsingle frame classification using both RGB and flow percepts. Therefore, weused the 4096-dimensional fc6 layer as the input representation of our data.Besides these percepts, we also trained the proposed models on 32 \u00d7\\times\u00d7 32patches of pixels.", "All models were trained using backprop on a single NVIDIA Titan GPU. A two layer2048 unit Composite model that predicts 13 frames and reconstructs 16 framestook 18-20 hours to converge on 300 hours of percepts. We initialized weightsby sampling from a uniform distribution whose scale was set to 1/sqrt(fan-in).Biases at all the gates were initialized to zero. Peep-hole connections wereinitialized to zero. The supervised classifiers trained on 16 frames took 5-15minutes to converge. The code can be found at https://github.com/emansim/unsupervised-videos.", "The aim of this set of experiments to visualize the properties of the proposedmodels.", "Experiments on MNISTWe first trained our models on a dataset of moving MNIST digits. In thisdataset, eachvideo was 20 frames long and consisted of two digits moving inside a 64 \u00d7\\times\u00d7 64patch. The digits were chosen randomly from the training set and placedinitially at random locations inside the patch. Each digit was assigned avelocity whose direction was chosen uniformly randomly on a unit circle andwhose magnitude was also chosen uniformly at random over a fixed range. Thedigits bounced-off the edges of the 64 \u00d7\\times\u00d7 64 frame and overlapped if theywere at the same location. The reason for working with this dataset is that it isinfinite in size and can be generated quickly on the fly. This makes it possibleto explore the model without expensive disk accesses or overfitting issues. Italso has interesting behaviours due to occlusions and the dynamics of bouncingoff the walls.", "We first trained a single layer Composite Model. Each LSTM had 2048 units. Theencoder took 10 frames as input. The decoder tried to reconstruct these 10frames and the future predictor attempted to predict the next 10 frames. Weused logistic output units with a cross entropy loss function.Fig.\u00a05 shows two examples of running this model. The truesequences are shown in the first two rows. The next two rows show thereconstruction and future prediction from the one layer Composite Model. It isinteresting to note that the model figures out how to separate superimposeddigits and can model them even as they pass through each other. This shows someevidence of disentangling the two independent factors of variation inthis sequence. The model can also correctly predict the motion after bouncingoff the walls. In order to see if adding depth helps, we trained a two layerComposite Model, with each layer having 2048 units. We can see that addingdepth helps the model make better predictions. Next, we changed the futurepredictor by making it conditional. We can see that this model makes sharperpredictions.", "Experiments on Natural Image PatchesNext, we tried to see if our models can also work with natural image patches.For this, we trained the models on sequences of 32 \u00d7\\times\u00d7 32 natural imagepatches extracted from the UCF-101 dataset. In this case, we used linear outputunits and the squared error loss function. The input was 16 frames and the modelwas asked to reconstruct the 16 frames and predict the future 13 frames.Fig.\u00a06 shows the results obtained from a two layer Compositemodel with 2048 units. We found that the reconstructions and the predictions areboth very blurry. We then trained a bigger model with 4096 units. The outputsfrom this model are also shown in Fig.\u00a06. We can see thatthe reconstructions get much sharper.", "Generalization over time scalesIn the next experiment, we test if the model can work at time scales that aredifferent than what it was trained on. We take a one hidden layer unconditionedComposite Model trained on moving MNIST digits. The model has 2048 LSTM unitsand looks at a 64 \u00d7\\times\u00d7 64 input. It was trained on input sequences of 10frames to reconstruct those 10 frames as well as predict 10 frames into thefuture. In order to test if the future predictor is able to generalize beyond 10frames, we let the model run for 100 steps into the future.Fig.\u00a07(a) shows the pattern of activity in the LSTM units of thefuture predictorpathway for a randomly chosen test input. It shows the activity at each of thethree sigmoidal gates (input, forget, output), the input (after the tanhnon-linearity, before being multiplied by the input gate), the cell state andthe final output (after being multiplied by the output gate). Even though theunits are ordered randomly along the vertical axis, we can see that the dynamicshas a periodic quality to it. The model is able to generate persistent motionfor long periods of time. In terms of reconstruction, the model only outputsblobs after the first 15 frames, but the motion is relatively well preserved.More results, including long range future predictions over hundreds of time steps can see been athttp://www.cs.toronto.edu/~nitish/unsupervised_video.To show that setting up a periodic behaviour is not trivial,Fig.\u00a07(b) shows the activity from a randomly initialized futurepredictor. Here, the LSTM state quickly converges and the outputs blur completely.", "Out-of-domain InputsNext, we test this model\u2019s ability to deal with out-of-domain inputs. For this,we test the model on sequences of one and three moving digits. The model wastrained on sequences of two moving digits, so it has never seen inputs with justone digit or three digits. Fig.\u00a08 shows the reconstructionand future prediction results. For one moving digit, we can see that the modelcan do a good job but it really tries to hallucinate a second digit overlappingwith the first one. The second digit shows up towards the end of the futurereconstruction. For three digits, the model merges digits into blobs. However,it does well at getting the overall motion right. This highlights a key drawbackof modeling entire frames of input in a single pass. In order to model videoswith variable number of objects, we perhaps need models that not only have an attentionmechanism in place, but can also learn to execute themselves a variable numberof times and do variable amounts of computation.", "Visualizing FeaturesNext, we visualize the features learned by this model.Fig.\u00a09 shows the weights that connect each input frame tothe encoder LSTM. There are four sets of weights. One set of weights connectsthe frame to the input units. There are three other sets, one corresponding toeach of the three gates (input, forget and output). Each weight has a size of 64\u00d7\\times\u00d7 64. A lot of features look like thin strips. Others look like higherfrequency strips. It is conceivable that the high frequency features help inencoding the direction and velocity of motion.", "Fig.\u00a010 shows the output features from the two LSTMdecoders of a Composite Model. These correspond to the weights connecting theLSTM output units to the output layer. They appear to be somewhat qualitativelydifferent from the input features shown in Fig.\u00a09. Thereare many more output features that are local blobs, whereas those are rare inthe input features. In the output features, the ones that do look like stripsare much shorter than those in the input features. One way to interpret this isthe following. The model needs to know about motion (which direction and howfast things are moving) from the input. This requires precise informationabout location (thin strips) and velocity (high frequency strips). But when itis generating the output, the model wants to hedge its bets so that itdoes notsuffer a huge loss for predicting things sharply at the wrong place. This couldexplain why the output features have somewhat bigger blobs. The relativeshortness of the strips in the output features can be explained by the fact thatin the inputs, it does not hurt to have a longer feature than what is needed todetect a location because information is coarse-coded through multiple features.But in the output, the model may not want to put down a feature that is biggerthan any digit because other units will have to conspire to correct for it.", "The aim of this set of experiments is to see if the features learned byunsupervised learning can help improve performance on supervised tasks.", "We trained a two layer Composite Model with 2048 hidden units with no conditioning oneither decoders. The model was trained on percepts extracted from 300 hours ofYouTube data. The model was trained to autoencode 16 frames and predict thenext 13 frames. We initialize an LSTM classifier with theweights learned by the encoder LSTM from this model. The classifier is shown inFig.\u00a011. The output from each LSTM in the second layer goes into a softmaxclassifier that makes a prediction about the action being performed at each timestep. Since only one action is being performed in each video in the datasets weconsider, the target is the same at each time step. At test time, thepredictions made at each time step are averaged. To get a prediction for theentire video, we average the predictions from all 16 frame blocks in the videowith a stride of 8 frames. Using a smaller stride did not improve results.", "The baseline for comparing these models is an identical LSTM classifier but withrandomly initialized weights. All classifiers used dropout regularization,where we dropped activations as they were communicated across layers but notthrough time within the same LSTM as proposed in Zaremba et\u00a0al. (2014). Weemphasize that this is a very strong baseline and does significantly better thanjust using single frames. Using dropout was crucial in order to train goodbaseline models especially with very few training examples.", "Fig.\u00a012 compares three models - single frame classifier(logistic regression), baseline LSTM classifier and the LSTM classifierinitialized with weights from the Composite Model as the number of labelledvideos per class is varied. Note that having one labelled video means havingmany labelled 16 frame blocks. We can see that for the case of very fewtraining examples, unsupervised learning gives a substantial improvement. Forexample, for UCF-101, the performance improves from 29.6% to 34.3% whentraining on only one labelled video. As the size of the labelled dataset grows,the improvement becomes smaller. Even for the full UCF-101 dataset we still get aconsiderable improvement from 74.5% to 75.8%. On HMDB-51, the improvement isfrom 42.8% to 44.0% for the full dataset (70 videos per class) and 14.4% to19.1% for one video per class. Although, the improvement in classification byusing unsupervised learning was not as big as we expected, we still managed toyield an additional improvement over a strong baseline. We discuss some avenuesfor improvements later.", "We further ran similar experiments on the optical flow percepts extracted fromthe UCF-101 dataset. A temporal stream convolutional net, similar to the oneproposed by Simonyan & Zisserman (2014b), was trained on single frame optical flows aswell as on stacks of 10 optical flows. This gave an accuracy of 72.2% and77.5% respectively. Here again, our models took 16 frames as input,reconstructed them and predicted 13 frames into the future. LSTMs with 128hidden units improved the accuracy by 2.1% to 74.3% for the single framecase. Bigger LSTMs did not improve results. By pretraining the LSTM, we wereable to further improve the classification to 74.9% (\u00b10.1plus-or-minus0.1\\pm 0.1\u00b1 0.1). For stacks of10 frames we improved very slightly to 77.7%. These results are summarized inTable\u00a01.", "The aim of this set of experiments is to compare the different variants of themodel proposed in this paper. Since it is always possible to get lowerreconstruction error by copying the inputs, we cannot use input reconstructionerror as a measure of how good a model is doing. However, we can use the errorin predicting the future as a reasonable measure of how good the model isdoing. Besides, we can use the performance on supervised tasks as a proxy forhow good the unsupervised model is doing. In this section, we present results fromthese two analyses.", "Future prediction results are summarized in Table\u00a02. For MNISTwe compute the cross entropy of the predictions with respect to the groundtruth, both of which are 64 \u00d7\\times\u00d7 64 patches. For natural image patches, wecompute the squared loss. We see that the Composite Model always does a betterjob of predicting the future compared to the Future Predictor. This indicatesthat having the autoencoder along with the future predictor to force the modelto remember more about the inputs actually helps predict the future better.Next, we can compare each model with its conditional variant. Here, we find thatthe conditional models perform better, as was also noted in Fig.\u00a05.", "Next, we compare the models using performance on a supervised task.Table\u00a03 shows the performance on actionrecognition achieved by finetuning different unsupervised learning models.Besides running the experiments on the full UCF-101 and HMDB-51 datasets, we also ran theexperiments on small subsets of these to better highlight the case where we havevery few training examples. We find that all unsupervised models improve over thebaseline LSTM which is itself well-regularized by using dropout. The Autoencodermodel seems to perform consistently better than the Future Predictor. TheComposite model which combines the two does better than either one alone.Conditioning on the generated inputs does not seem to give a clearadvantage over not doing so. The Composite Model with a conditional futurepredictor works the best, although its performance is almost same as that of theComposite Model.", "Finally, we compare our models to the state-of-the-art action recognitionresults. The performance is summarized in Table\u00a04. The table isdivided into three sets. The first set compares models that use only RGB data(single or multiple frames). The second set compares models that use explicitlycomputed flow features only. Models in the third set use both.", "On RGB data, our model performs at par with the best deep models. It performs3% better than the LRCN model that also used LSTMs on top of convnet features111However,the improvement is only partially from unsupervised learning, since weused a better convnet model.. Our model performs better than C3D features thatuse a 3D convolutional net. However, when the C3D features are concatenatedwith fc6 percepts, they do slightly better than our model.", "The improvement for flow features over using a randomly initialized LSTM networkis quite small. We believe this is atleast partly due to the fact that the flow perceptsalready capture a lot of the motion information that the LSTM would otherwisediscover.", "When we combine predictions from the RGB and flow models, we obtain 84.3accuracy on UCF-101. We believe further improvements can be made by running themodel over different patch locations and mirroring the patches. Also, our modelcan be applied deeper inside the convnet instead of just at the top-level. Thatcan potentially lead to further improvements.In this paper, we focus on showing that unsupervised training helpsconsistently across both datasets and across different sized training sets.", "We proposed models based on LSTMs that can learn good video representations. Wecompared them and analyzed their properties through visualizations. Moreover, wemanaged to get an improvement on supervised tasks. The best performing model wasthe Composite Model that combined an autoencoder and a future predictor.Conditioning on generated outputs did not have a significant impact on theperformance for supervised tasks, however it made the future predictions lookslightly better. The model was able to persistently generate motion well beyondthe time scales it was trained for. However, it lost the precise object featuresrapidly after the training time scale. The features at the input and outputlayers were found to have some interesting properties.", "To further get improvements for supervised tasks, we believe that the model canbe extended by applying it convolutionally across patches of the video andstacking multiple layers of such models. Applying this model in the lower layersof a convolutional net could help extract motion information that wouldotherwise be lost across max-pooling layers. In our future work, we plan tobuild models based on these autoencoders from the bottom up instead of applyingthem only to percepts."], "figure_types": {"829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/10-Figure11-1.png": "schematic", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/10-Table2-1.png": "table", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/11-Figure12-1.png": "plot", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/11-Table3-1.png": "table", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/11-Table4-1.png": "table", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/3-Figure1-1.png": "schematic", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/3-Figure2-1.png": "schematic", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/4-Figure3-1.png": "schematic", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/4-Figure4-1.png": "schematic", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/6-Figure5-1.png": "schematic", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/7-Figure6-1.png": "photograph(s)", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/8-Figure7-1.png": "plot", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/9-Figure10-1.png": "plot", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/9-Figure8-1.png": "schematic", "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d/9-Figure9-1.png": "other"}}, "1505.07293": {"paper_id": "paper_115", "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation", "arxiv_url": "https://arxiv.org/abs/1505.07293", "s2orc_url": "https://www.semanticscholar.org/paper/6f9f143ec602aac743e07d092165b708fa8f1473", "all_figures_tables": {"6f9f143ec602aac743e07d092165b708fa8f1473/2-Figure1-1.png": "Figure 1. A 4 layer SegNet which takes in an RGB input image and performs feed-forward computation to obtain pixel-wise labelling. A stack of feature encoders is followed by a corresponding decoders. The soft-max layer classifies each pixel independently using the features input by the last decoder. An encoder uses the convolution-ReLU-max pooling-subsampling pipeline. A decoder upsamples its input using the transferred pool indices from its encoder. It then performs convolution with a trainable filter bank.", "6f9f143ec602aac743e07d092165b708fa8f1473/4-Figure2-1.png": "Figure 2. (a) The modular training starts by optimizing the first encoder and decoder weights. The soft-max can be pre-trained or randomly initialized. (b) Once the first pair is trained, we insert an inner deeper encoder-decoder pair and optimize these weights while holding the outer encoder-decoder and soft-max weights fixed. More deeper pairs are then subsequently trained. Note encoder and decoder weights are untied.", "6f9f143ec602aac743e07d092165b708fa8f1473/5-Figure3-1.png": "Figure 3. SegNet feature ablation study. All layers use 64 features in the SegNet. The four columns from right to left show the predictions obtained at various depths when only a fraction of the feature activations are used and the remaining set to zero. Note the quality of the labelling improves strikingly at depth 4 even when only the top-1 feature activations are considered. Interestingly, these activations seem to be tuned largely for the static scene classes and the other classes such as cars are labelled only when more features are activated. When fewer features are activated, missing categories (cars) are filled with sidewalk which is reasonable. Also shown are the percentage of activated features as part of the top \u2019N\u2019 activations; deeper layers have fewer but more finely tuned activations.", "6f9f143ec602aac743e07d092165b708fa8f1473/6-Table1-1.png": "Table 1. Quantitative results on CamVid [1]. We consider SegNet - 4 layer for comparisons with other methods. SegNet performs best on several challenging classes (cars, pedestrians, poles) while maintaining competitive accuracy on remaining classes. The class average and global average is the highest even when compared to methods using structure from motion [2], CRF [36, 20], dense depth maps [43], temporal cues [39].", "6f9f143ec602aac743e07d092165b708fa8f1473/6-Table2-1.png": "Table 2. Quantitative results on the NYU v2 [33]. The SegNet performs better than the multi-scale convnet which uses the same inputs (and post-processing) on 9 out of 13 classes. The method in [13] uses additional cues such as ground plane detect and column-wise depth normalization to achieve better accuracy. .", "6f9f143ec602aac743e07d092165b708fa8f1473/6-Table3-1.png": "Table 3. Quantitative results on the KITTI dataset [9, 29]. The SegNet performance is better globally and comparable among classes. The fence class resembles buildings and needs other cues such as temporal information used in [29] for better accuracy.", "6f9f143ec602aac743e07d092165b708fa8f1473/7-Figure4-1.png": "Figure 4. Result samples on CamVid day and dusk test sequences. The evolution of various unary predictions and unaries combined with externally trained detectors [20] and CRF models [36]. SegNet predictions retain small categories such as poles (column 2,4), bicyclist (column 3), far side sidewalk (column 2) better than other methods while producing overall smooth predictions. CRF results, although smooth, miss several important categories even when SfM based cues are used. In the dusk scenario, SfM cues are particularly valuable (row 3). Here the SegNet fails to label the car (column 4) however, it fills this part with very reasonable predictions.", "6f9f143ec602aac743e07d092165b708fa8f1473/8-Figure5-1.png": "Figure 5. Row 1,2 show KITTI test samples. Notice the illumination differences to CamVid samples in Fig. 4. Row 3: Predictions when all layers are trained afresh from a random initialization using the KITTI training set. Row 4: SegNet is pre-trained with the CamVid dataset and only layer 4 is trained for two epochs on the KITTI training set. Supervised pre-training can produce good results with a small extra computational effort. Row 5: poor results obtained when starting from pre-trained weights and training only a soft-max classifier with a hidden layer. Unknown class is blackened.", "6f9f143ec602aac743e07d092165b708fa8f1473/8-Figure6-1.png": "Figure 6. Row 1,2: Indoor scenes from NYU dataset version 2 and their ground truth. Row 3: SegNet predictions with RGBD input. No extra information such as ground plane fitting, column wise pixel depth normalization [13] or multi-scale inputs [6] was used. Although the predictions are largely correct, the edges between categories are not sharp mainly because of low input resolution, interpolated depth values close to class edges."}, "referred_figures_tables": [["6f9f143ec602aac743e07d092165b708fa8f1473/2-Figure1-1.png"], ["6f9f143ec602aac743e07d092165b708fa8f1473/4-Figure2-1.png"], ["6f9f143ec602aac743e07d092165b708fa8f1473/2-Figure1-1.png"], ["6f9f143ec602aac743e07d092165b708fa8f1473/2-Figure1-1.png"]], "question_id": [5, 6, 7, 8], "question": ["What is kernel size used in each layer of SegNet?", "What are the total number of encoders and decoders used in SegNet?", "How many features are used in each layer of SegNet?", "What are the advantages of using a flat architecture in SegNet?"], "question_section": ["Introduction", "SegNet Architecture and Learning Scheme", "SegNet Architecture and Learning Scheme", "SegNet Architecture and Learning Scheme"], "question_trigger_sentence": ["The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a layer SegNet with kernels and non-overlapping max pooling in each layer has a spatial context of pixels when a feature-map is backtracked to the input image.", "SegNet maintains a constant number of features per layer which is typically set to", "SegNet uses a \u201cflat\u201d architecture, i.e, the number of features in each layer remains the same ( in our case) but with full connectivity.", "We perform local contrast normalization (LCN) as a pre-processing step to the input"], "question_type": ["Testing question ", "Testing question ", "Testing question ", "Deep/complex question "], "evidential_info": [[{"context": "We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\\times 7 kernels and 2\\times 2 non-overlapping max pooling in each layer has a spatial context of 106\\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.", "rationale": "A constant kernel size of 7*7 over all the layers was chosen to provide a wide context for smooth labelling."}, {"context": "SegNet uses a \u201cflat\u201d architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\\times 2 with a stride of non-overlapping 2 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\\times 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\\times 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.", "rationale": "A 4 layer SegNet with 7*7 kernels and 2*2 non-overlapping max pooling in each layer has a spatial context of 106*106 pixels when a feature-map is backtracked to the input image."}], [{"context": "We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\\times 7 kernels and 2\\times 2 non-overlapping max pooling in each layer has a spatial context of 106\\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.", "rationale": "The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\\times 7 kernels and 2\\times 2 non-overlapping max pooling in each layer has a spatial context of 106\\times 106 pixels when a feature-map is backtracked to the input image."}, {"context": "We use mini-batches that maximize GPU usage and avoid GPU-CPU memory transfers. Typically, 25-50 randomly chosen images (with replacement) per mini-batch. The optimizer is run for 20 iterations per mini-batch and 10 epochs for each layer. We empirically observe that the objective plateaus after 5-6 epochs and so we run another 4 epochs as a margin. Note that, after 10 epochs, each input sample approximately \u201cinfluences\u201d the optimizer200 times. We train the encoder-decoder pair weights closest to the input layer. The soft-max layer can be trained first or randomly initialised. It then remains fixed throughout the experiment. Next, we introduce a deeper layer of encoder-decoder (see Fig. 2) and train their weights while holding the shallower layer encoder-decoder weights fixed. Note that the objective remains the same, i.e., to minimize label cross-entropy loss over the mini-batch. This is unlike unsupervised feature learning approaches which reconstruct the input of the layer in question [27, 16], thus varying the objective with each layer. The deconvolution network [42] on the other hand optimizes the same reconstruction objective with each deeper layer. The difference to our approach is (i) the objective is unsupervised, (ii) there is no encoder to learn a feed-forward representation thus requiring an optimisation step during test time to produce features for recognition. We successively add deeper encoder-decoder pairs and train them while holding the preceeding pair\u2019s weights fixed. In total, we use 4 layer networks, i.e., 4 encoders and 4 decoders in our experiments. Once the encoder-decoder stack is trained, we find that there is no advantage to training the soft-max layer as it only relies on a linear discriminant function.We wrote our own Matlab GPU compatible implementation of SegNet that uses the minFunc optimization library [31]. Our code has been tested on NVIDIA Tesla K40, GTX GeForce 880M and GTXGeForce780 GPUs. We will make our light-weight Matlab code available publicly soon. With the current state of code optimisation, training a 4 layer deep SegNet on the CamVid dataset (367 training images of 360\\times 480) takes about a week. The unoptimized test time is in the order of 2secs/frame: bulk of the computation time is spent performing tensor convolutions in the feedforward path and FFT based convolutions during backpropagation 333more speedup can be gained https://developer.nvidia.com/cuDNN.", "rationale": "In total, we use 4 layer networks, i.e., 4 encoders and 4 decoders in our experiments."}], [{"context": "We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\\times 7 kernels and 2\\times 2 non-overlapping max pooling in each layer has a spatial context of 106\\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.", "rationale": "SegNet uses a \u201cflat\u201d architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity."}, {"context": "SegNet uses a \u201cflat\u201d architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\\times 2 with a stride of non-overlapping 2 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\\times 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\\times 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.", "rationale": "SegNet maintains a constant number of features per layer which is typically set to 64."}], [{"context": "SegNet uses a \u201cflat\u201d architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\\times 2 with a stride of non-overlapping 2 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\\times 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\\times 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.", "rationale": "SegNet uses a \u201cflat\u201d architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster."}]], "composition": ["The kernel size used in each layer of SegNet is 7*7.", "4 encoders and 4 decoders are used in SegNet.", "64 features are used in each layer of SegNet.", "The flat architecture avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder) and the training time remains almost same for each additional/deeper encoder-decoder pair."], "Is_figure_in_evidence": [true, true, true, true], "Is_table_in_evidence": [false, false, false, false], "question_key": ["146", "152", "153", "154"], "passages": ["Semantic segmentation is an important step towards understanding and inferring different objects and their arrangements observed in a scene. This has wide array of applications ranging from estimating scene geometry, inferring support-relationships among objects to autonomous vehicle driving. Early methods that relied on low-level vision cues have fast been superseded by popular machine learning algorithms. In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images [37, 34] also seen growing interest in semantic pixel-wise labelling problems [7, 14, 35]. However, these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling. The results, although very encouraging, have not been quite satisfactory. Primarily, the deepest layer representations/feature maps are of a small resolution as compared to input image dimensions due to several pooling layers e.g. if 2\u00d72222\\times 22 \u00d7 2 non-overlapping max-pooling-subsampling layers are used three times, the resulting feature map is 1/8th1superscript8\ud835\udc61\u210e1/8^{th}1 / 8 start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT of the input dimension. Therefore, an ad hoc technique is used to upsample the deepest layer feature map to match the input image dimensions by replicating features within a block i.e. all pixels within a block (8\u00d78888\\times 88 \u00d7 8 in our example) have the same features. This often results in predictions that appear blocky222see http://david.grangier.info/scene_parsing/. This is exactly what we improve using our proposed SegNet architecture, wherein the decoders learn to map the deepest layer features to full image dimensions. Learning to decode has two other advantages.First, deeper layers each with pooling-subsampling can be introduced which increases the spatial context for pixel labelling. This results in smooth predictions unlike patch based classifiers [36, 2]. Second, ablation studies to understand the effects of features such as in [41] can be performed using the decoder stack.", "We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4444 layer SegNet with 7\u00d77777\\times 77 \u00d7 7 kernels and 2\u00d72222\\times 22 \u00d7 2 non-overlapping max pooling in each layer has a spatial context of 106\u00d7106106106106\\times 106106 \u00d7 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64646464. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.", "In Sec. 2 we review related recent literature. We describe in detail the SegNet architecture in Sec. 3 along with its qualitative analysis. Our quantitative experiments with SegNet on several well known benchmark datasets are described in Sec. 4. We also discuss the advantages and drawbacks of our approach including computational times. We conclude with pointers to future work in Sec. 5. For most of our experiments, we use outdoor RGB road scene analysis [1, 9] and indoor RGBD scene analysis [33] datasets to measure the quantitative performance.", "Semantic pixel-wise segmentation is an ongoing topic of research, fuelled by challenging datasets [1, 33, 9]. Current best performing methods all mostly rely on hand engineered features generally used for per-pixel independent classification. Typically, a patch is fed into a classifier e.g. Random Forest [32, 2] or Boosting [36, 20] to predict the class probabilities of the center pixel. Features based on appearance [32], SfM and appearance [2, 36, 20] have been explored for the CamVid test. These per-pixel noisy predictions (often called unary terms) from the classifiers are then smoothed by using a pair-wise or higher order CRF [36, 20] to improve the accuracy. More recent approaches have aimed to produce high quality unaries by trying to predict the labels for all the pixels in a patch as opposed to only the center pixel. This improves the results of Random Forest based unaries [18] but thin structured classes are classfied poorly. Dense depth maps computed from the CamVid video have also been used as input for classification using Random Forests [43]. Another approach argues for the use of a combination of popular hand designed features and spatio temporal super-pixelization to obtain higher accuracy [39]. Recent top performing technique on the CamVid test [20] addresses the imbalance among label frequencies by using additional training data from the PASCAL VOC dataset to learn object detectors. The result of all these techniques indicates the need for improved classification as increases in accuracy have mostly come from adding new features or modalities to the classifier. Post-processing using CRF models of various orders [36] has mainly resulted in improving the accuracy of dominant classes such as sky, road, buildings with little effect on the accuracy of thin structured but equally important classes such as signs, poles, pedestrians. This highlights the need for better pixel-wise classification when imbalanced label frequencies exist.Meanwhile, indoor RGBD pixel-wise semantic segmentation has also gained popularity since the release of the NYU dataset [33] which showed the usefulness of the depth channel to improve segmentation. Their approach used features such as RGB-SIFT, depth-SIFT, location as input to a neural network classifier to predict pixel unaries. The noisy unaries are then smoothed using a CRF. Improvements were made using a richer feature set including LBP and region segmentation to obtain higher accuracy [28] followed by a CRF. In more recent work [33], both class segmentation and support relationships are inferred together using a combination of RGB and depth based cues. Another approach focusses on real-time joint reconstruction and semantic segmentation, where Random Forests are used as the classifier [13]. Gupta et al. [12] use boundary detection and hierarchical grouping before performing category segmentation. The common attribute along all these approaches is the use of hand engineered features for pixel-wise classifiction of either RGB or RGBD images. The application of deep learning for scene segmentation has only just begun. There have also been a few attempts to apply networks designed for categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [7, 6, 11, 8]. However, the resulting classification is blocky [11]. Another approach using recurrent neural networks [26] merges several low resolution predictions to create input image resolution predictions. On the whole, although some of these techniques already present improvements over hand engineered features [7].", "Our work is inspired by the unsupervised feature learning architecture proposed by Ranzato et. al [27]. The key learning module is an encoder-decoder network where the encoder consists of a filter bank convolution, tanh squashing function, max pooling followed by sub-sampling to obtain the feature maps. For each sample, the indices of the max locations computed during pooling are stored and passed to the decoder. The decoder upsamples the feature maps by using the already stored pooled indices, also called switches, and learns a decoder filter bank to reconstruct the input image. This architecture was used for unsupervised pre-training of feature hierarchies. A similar decoding technique is used for visualizing trained convolutional networks[42] for object classification; the transposed encoder kernels are set as the decoder kernels which are followed by a non-linearity and the pooling indices are used for upsampling. The architecture of Ranzato mainly concentrated on layer wise feature learning using small input patches although during test time a full sized image was the input. This discrepancy was corrected for by Kavukcuoglu et. al. [16] by using test size images/feature maps to learn hierarchical encoders. Both these approaches however did not attempt to use deep encoder-decoder networks for unsupervised feature training as they discarded the decoders after each encoder training. Here, the SegNet architecture differs from these approaches as the objective used for training all the encoder-decoder pairs is the same, i.e., to minimise the cross-entropy label loss.", "Other applications where pixel wise predictions are made using deep networks are image super-resolution [4] and depth map prediction from a single image [5]. The authors in [5] discuss the need for learning to upsample from low resolution feature maps which is the central topic of this paper.", "A four layer SegNet architecture used in our experiments is illustrated in Fig. 1. Each encoder performs dense convolutions, ReLU non-linearity, a non-overlapping max pooling with a 2\u00d72222\\times 22 \u00d7 2 window and finally down-sampling. Each decoder upsamples its input using the memorized pooled indices and convolves it with a trainable filter bank. No ReLU non-linearity is used in the decoder unlike the deconvolution network [41, 42]. This makes it easier to optimize the filters in each pair. The encoder and decoder filters are also untied to provide additional degrees of freedom to minimize the objective. The final layer is a soft-max classifier (with no bias term) which classifies each pixel independently. The output of the soft-max is a K channel image where K is the number of classes.", "SegNet uses a \u201cflat\u201d architecture, i.e, the number of features in each layer remains the same (64646464 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\u00d72222\\times 22 \u00d7 2 with a stride of non-overlapping 2222 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\u00d77777\\times 77 \u00d7 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\u00d7106106106106\\times 106106 \u00d7 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.", "The input to the SegNet can be any arbitrary multi-channel image or feature map(s), e.g., RGB, RGBD, map of normals, depth etc. We perform local contrast normalization (LCN) as a pre-processing step to the input [23, 15]. The advantage of this step are many, (i) to correct for non-uniform scene illumination thus reducing the dynamic range (increases contrast in shadowed parts). (ii) highlighting edges which leads the network to learn category shape, (iii) improves convergence as it decorrelates the input dimensions [23]. LCN is performed independently for each modality, i.e., RGB is contrast normalized as a three channel input and depth as a single channel for RGBD inputs. This avoids highlighting pseudo depth edges due to RGB edges and vice-versa.", "Most deep learning methods use stochastic gradient descent (SGD) for training [22]. SGD needs sufficient expertise to initialize weights with appropriate magnitudes, adapting appropriately learning rates and momentum parameters which both control the step sizes. Therefore, we adopt L-BFGS [25] based on the comparative study by Ngiam et. al [21] who advocate the use of L-BFGS particularly for auto-encoders. L-BFGS has faster and more stable convergence than SGD. It also works well in large batches which is useful to maximize the throughput of powerful GPUs. We initialize the weights in all the layers and the soft-max weights from a zero mean unit variance Gaussian \ud835\udca9(0,1)\ud835\udca901\\mathcal{N}(0,1)caligraphic_N ( 0 , 1 ) and normalized the kernels to unit L2 norm. We obtained good predictive performance from the network without the need for special layer-wise weight initialization or any learning rate tuning. We also use inverse frequency weighting for the classes to correct for any label imbalances in the training set [32].", "We use mini-batches that maximize GPU usage and avoid GPU-CPU memory transfers. Typically, 25\u221250255025-5025 - 50 randomly chosen images (with replacement) per mini-batch. The optimizer is run for 20202020 iterations per mini-batch and 10101010 epochs for each layer. We empirically observe that the objective plateaus after 5\u22126565-65 - 6 epochs and so we run another 4444 epochs as a margin. Note that, after 10101010 epochs, each input sample approximately \u201cinfluences\u201d the optimizer200200200200 times. We train the encoder-decoder pair weights closest to the input layer. The soft-max layer can be trained first or randomly initialised. It then remains fixed throughout the experiment. Next, we introduce a deeper layer of encoder-decoder (see Fig. 2) and train their weights while holding the shallower layer encoder-decoder weights fixed. Note that the objective remains the same, i.e., to minimize label cross-entropy loss over the mini-batch. This is unlike unsupervised feature learning approaches which reconstruct the input of the layer in question [27, 16], thus varying the objective with each layer. The deconvolution network [42] on the other hand optimizes the same reconstruction objective with each deeper layer. The difference to our approach is (i) the objective is unsupervised, (ii) there is no encoder to learn a feed-forward representation thus requiring an optimisation step during test time to produce features for recognition. We successively add deeper encoder-decoder pairs and train them while holding the preceeding pair\u2019s weights fixed. In total, we use 4 layer networks, i.e., 4 encoders and 4 decoders in our experiments. Once the encoder-decoder stack is trained, we find that there is no advantage to training the soft-max layer as it only relies on a linear discriminant function.We wrote our own Matlab GPU compatible implementation of SegNet that uses the minFunc optimization library [31]. Our code has been tested on NVIDIA Tesla K40, GTX GeForce 880M and GTXGeForce780 GPUs. We will make our light-weight Matlab code available publicly soon. With the current state of code optimisation, training a 4 layer deep SegNet on the CamVid dataset (367 training images of 360\u00d7480360480360\\times 480360 \u00d7 480) takes about a week. The unoptimized test time is in the order of 2222secs/frame: bulk of the computation time is spent performing tensor convolutions in the feedforward path and FFT based convolutions during backpropagation 333more speedup can be gained https://developer.nvidia.com/cuDNN.", "We perform an ablation study to gain some insight into about the SegNet features. The work of Zeiler et al. [41] study the effects of feature activations in each layer of a trained network [19]. The feature activations are mapped back to image pixel space using a deconvolutional network. The SegNet architecture by construction is trained to decode the encoder activations and we use this to visualize the effect of feature activations (which layer) in the pixel label space. A recent study [38] has shown that in each layer of a deep network it is the \u201cdirection\u201d or \u201cspace\u201d (ensemble of feature activations) which encodes useful class information rather than individual units (feature activations). We therefore focus our study on the predictive effect of a subset of feature activations at each layer. For a given layer, we compute the feature activations/maps for each sample in the training set. We then compute the root mean square value of each map i.e. \u2200j\u2208{1..64}for-all\ud835\udc571..64\\forall j\\in\\{1..64\\}\u2200 italic_j \u2208 { 1..64 } 1N\u2211i\u2208\u2110(fji)21\ud835\udc41subscript\ud835\udc56\u2110superscriptsuperscriptsubscript\ud835\udc53\ud835\udc57\ud835\udc562\\sqrt{\\frac{1}{N}\\sum_{i\\in\\mathcal{I}}(f_{j}^{i})^{2}}square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_N end_ARG \u2211 start_POSTSUBSCRIPT italic_i \u2208 caligraphic_I end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG where fjisuperscriptsubscript\ud835\udc53\ud835\udc57\ud835\udc56f_{j}^{i}italic_f start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT is jthsuperscript\ud835\udc57\ud835\udc61\u210ej^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT feature map value at pixel i\ud835\udc56iitalic_i at a given layer. This assigns each map a single value e.g., the CamVid training set would have a 64646464 dimensional vector for each training sample for layer 4 of the SegNet. We now compute a histogram of the top \u2018N\u2019 elements of each such vector over all the samples. This histogram shows the most activated features in that layer over the training set. For any \u2018N\u2019, we set the remainder of feature maps to zero (ablation) and decode the pixel-wise labelling for a given input sample. Note that since our training is modular, this can be done after each deeper layer has been added. Some results of the top \u2019N\u2019 feature activations based labelling across all the layers are shown in Fig. 3. We observe firstly that the predictions get smoother as depth is increased which is a consequence of larger spatial context in the input space. More interestingly, the top-1 4th layer features predict almost entirely the static scene classes and \u201cfill in\u201d the missing cars e.g. with sidewalk. Given the feature(s) which get activated for cars are zeroed out, this prediction is reasonable and indicates the network is able to learn spatial context/class location information. Similarly, trees are filled in with buildings and bollards are extended to poles. In contrast, this effect is less clear and gets worse for shallower layers. This suggests subsets of features in the deeper layers are more \u201ctuned\u201d to certain scene categories in agreement with earlier work [41].We would like to add here that our efforts to perform an ablation study by choosing each feature map in turn and setting the remaining to zero produced results which were not clearly interpretable. It is also interesting to note that for shallower layers to produce qualitatively better predictions \u2019N\u2019 has to be set to about 5 or 10. The corresponding histogram has atleast 50%percent5050\\%50 % of the features activated as opposed to about 15%percent1515\\%15 % for the top-1 in layer 4, indicating deeper features are tuned to groups of related categories.", "A number of outdoor scene datasets are available for semantic parsing [10, 30, 1, 9]. Out of these, we chose the CamVid [1] and KITTI [9] datasets which contains 11 semantic classes such as road, building, cars, pedestrians etc.. There is a large imbalance in their frequencies [1]. Road, Sky, Building pixels are approximately 40\u221250405040-5040 - 50 times more than pedestrian, poles, sign-symbols, cars, bicyclists in the dataset making it very challenging to label smaller categories. This dataset contains video sequences, thus we are able to benchmark our approach with those which use motion and structure [20, 36, 2] and video segments [39]. Other datasets have more balanced label frequencies and are still image datasets. Another reason for choosing CamVid as compared to SIFT-flow, LabelMe is that the size of the training set is small (367367367367) making it feasible to train the SegNet given a standard GPU in reasonable time. The CamVid dataset also contains train and test images (233233233233) in day and dusk (poor lighting) conditions. The qualitative comparisons of SegNet predictions with several well known algorithms (unaries, unaries+CRF) are shown in Fig. 4. The qualitative results show the ability of the SegNet to segment small (cars, pedestrians, bicyclist) classes while producing a smooth segmentation of the overall scene. The other methods shown in Fig. 4 use structure from motion based cues. Lacking this cue, the SegNet misses some labels (cars) but fills it in with other reasonable context related classes. The CRF based results are smooth but do not retain small classes. More dense models [17] can be better but with additional cost of inference. Table 1 compares the algorithms numerically and demonstrates its superiority over recent competing methods.The KITTI dataset is the largest publicly available road scene dataset. Recently, some images from this dataset have been hand-labelled (8888 classes) for inferring dense 3D semantic maps [29]. Note that the image sizes are approximately, 376\u00d712413761241376\\times 1241376 \u00d7 1241, and so we cropped the centre 360\u00d7480360480360\\times 480360 \u00d7 480 to make it compatible with the CamVid dataset. We use this dataset to analyse the effect of supervised pre-training using the CamVid data on the KITTI test set. First, we add here that testing on the KITTI samples with only the pre-trained SegNet (using CamVid data) resulted in poor performance. This is because of illumination related differences between the datasets. Therefore, we experimented with three other training variants for the KITTI dataset; (i) training all the layers of the SegNet from a random initialization, denoted SegNet(R), (ii) initializing the parameters with CamVid trained values and training only a soft-max classifier with a hidden layer, denoted SegNet(SM), and (iii) initializing the parameters with CamVid trained values and training only the 4th layer of the SegNet for just 2222 epochs, denoted SegNet(L4). High quality predictions are obtained in scenario SegNet(R) as expected (Fig. 5). The good performance with CamVid pre-training and layer 4 training shows that, (i) useful semantic cues can be transferred across datasets using the shallower layers, and (ii) it is beneficial to train the deepest layer of the SegNet first given a small computational budget. Table 3 shows the SegNet(R) is competitive even when temporal cues [29] are not used.For indoor RGBD scenes, the NYU dataset (version 2)[33] is the largest benchmark dataset containing 795795795795 training and 654654654654 testing images with 14141414 class (objects, furniture, wall, ceiling etc.) labelling comparison. The NYU dataset has been used to benchmark Farabet et. al\u2019s [7] multi-scale deep learning approach to scene parsing. This benchmark is therefore useful to compare their method, which uses ad hoc feature upsampling, with our learning to upsample based approach. We also note that they learn approximately 1.2M1.2\ud835\udc401.2M1.2 italic_M parameters as compared to SegNet\u2019s 1.4M1.4\ud835\udc401.4M1.4 italic_M parameters. Other methods either use the smaller NYU dataset [28], different performance measures [12] or test on a small set of classes citeraey. The quantitative analysis shown in Table 2 show that the SegNet predictions are better the multi-scale convnet (2 pooling layers only) in 9 out of 13 classes. This suggests the SegNet can deal with scale changes by increasing context using deeper layers. The overall results are still far from satisfactory and the lack of cues such as height from ground, depth normalization (used in [13]) are needed to achieve better performance. The qualitative results in Fig. 6 show that the predictions are largely correct but lack sharp edges. This is due to low input resolution of 320\u00d7240320240320\\times 240320 \u00d7 240, lack of ground truth around class edges,and errors in depth interpolation. Another reason is that over the different datasets we tested on, the parameters of the SegNet remained the same. We plan to study the NYU dataset in more detail in the future. Additional results can be viewed in the supplementary material.", "We presented SegNet, a fully trainable deep architecture for joint feature learning and mapping an input image in a feed-forward manner to its pixel-wise semantic labels. A highlight of the proposed architecture is its ability to produce smooth segment labels when compared with local patch based classifiers. This is due to deep layers of feature encoding that employ a large spatial context for pixel-wise labelling. To the best of our knowledge this is the first deep learning method to learn to map low resolution encoder feature maps to semantic labels. Both qualitative and numerical accuracy of the SegNet for outdoor and indoor scenes is very competitive, even without use of any CRF post-processing. We have also demonstrated the use of pre-trained SegNet for obtaining good performance on other datasets with a small extra computational effort. The encoder-decoder architecture of the SegNet can also be trained unsupervised and to handle missing data in the input during test time."], "figure_types": {"6f9f143ec602aac743e07d092165b708fa8f1473/2-Figure1-1.png": "schematic", "6f9f143ec602aac743e07d092165b708fa8f1473/4-Figure2-1.png": "schematic", "6f9f143ec602aac743e07d092165b708fa8f1473/5-Figure3-1.png": "photograph(s)", "6f9f143ec602aac743e07d092165b708fa8f1473/6-Table1-1.png": "table", "6f9f143ec602aac743e07d092165b708fa8f1473/6-Table2-1.png": "table", "6f9f143ec602aac743e07d092165b708fa8f1473/6-Table3-1.png": "table", "6f9f143ec602aac743e07d092165b708fa8f1473/7-Figure4-1.png": "photograph(s)", "6f9f143ec602aac743e07d092165b708fa8f1473/8-Figure5-1.png": "photograph(s)", "6f9f143ec602aac743e07d092165b708fa8f1473/8-Figure6-1.png": "photograph(s)"}}, "1506.02640": {"paper_id": "paper_119", "title": "You Only Look Once: Unified, Real-Time Object Detection", "arxiv_url": "https://arxiv.org/abs/1506.02640", "s2orc_url": "https://www.semanticscholar.org/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd", "all_figures_tables": {"f8e79ac0ea341056ef20f2616628b3e964764cfd/1-Figure1-1.png": "Figure 1: The YOLO Detection System. Processing images with YOLO is simple and straightforward. Our system (1) resizes the input image to 448\u00d7 448, (2) runs a single convolutional network on the image, and (3) thresholds the resulting detections by the model\u2019s confidence.", "f8e79ac0ea341056ef20f2616628b3e964764cfd/2-Figure2-1.png": "Figure 2: The Model. Our system models detection as a regression problem. It divides the image into an S\u00d7S grid and for each grid cell predicts B bounding boxes, confidence for those boxes, and C class probabilities. These predictions are encoded as an S \u00d7 S \u00d7 (B \u2217 5 + C) tensor.", "f8e79ac0ea341056ef20f2616628b3e964764cfd/3-Figure3-1.png": "Figure 3: The Architecture. Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating 1\u00d7 1 convolutional layers reduce the features space from preceding layers. We pretrain the convolutional layers on the ImageNet classification task at half the resolution (224\u00d7 224 input image) and then double the resolution for detection.", "f8e79ac0ea341056ef20f2616628b3e964764cfd/6-Figure4-1.png": "Figure 4: Error Analysis: Fast R-CNN vs. YOLO These charts show the percentage of localization and background errors in the top N detections for various categories (N = # objects in that category).", "f8e79ac0ea341056ef20f2616628b3e964764cfd/6-Table1-1.png": "Table 1: Real-Time Systems on PASCAL VOC 2007. Comparing the performance and speed of fast detectors. Fast YOLO is the fastest detector on record for PASCAL VOC detection and is still twice as accurate as any other real-time detector. YOLO is 10 mAP more accurate than the fast version while still well above real-time in speed.", "f8e79ac0ea341056ef20f2616628b3e964764cfd/6-Table2-1.png": "Table 2: Model combination experiments on VOC 2007. We examine the effect of combining various models with the best version of Fast R-CNN. Other versions of Fast R-CNN provide only a small benefit while YOLO provides a significant performance boost.", "f8e79ac0ea341056ef20f2616628b3e964764cfd/7-Table3-1.png": "Table 3: PASCAL VOC 2012 Leaderboard. YOLO compared with the full comp4 (outside data allowed) public leaderboard as of November 6th, 2015. Mean average precision and per-class average precision are shown for a variety of detection methods. YOLO is the only real-time detector. Fast R-CNN + YOLO is the forth highest scoring method, with a 2.3% boost over Fast R-CNN.", "f8e79ac0ea341056ef20f2616628b3e964764cfd/8-Figure5-1.png": "Figure 5: Generalization results on Picasso and People-Art datasets.", "f8e79ac0ea341056ef20f2616628b3e964764cfd/8-Figure6-1.png": "Figure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it does think one person is an airplane."}, "referred_figures_tables": [["f8e79ac0ea341056ef20f2616628b3e964764cfd/7-Table3-1.png"], ["f8e79ac0ea341056ef20f2616628b3e964764cfd/7-Table3-1.png"], ["f8e79ac0ea341056ef20f2616628b3e964764cfd/1-Figure1-1.png"], ["f8e79ac0ea341056ef20f2616628b3e964764cfd/1-Figure1-1.png"], ["f8e79ac0ea341056ef20f2616628b3e964764cfd/6-Table1-1.png"], ["f8e79ac0ea341056ef20f2616628b3e964764cfd/7-Table3-1.png"], ["f8e79ac0ea341056ef20f2616628b3e964764cfd/3-Figure3-1.png", "f8e79ac0ea341056ef20f2616628b3e964764cfd/3-Figure3-1.png"], ["f8e79ac0ea341056ef20f2616628b3e964764cfd/7-Table3-1.png", "f8e79ac0ea341056ef20f2616628b3e964764cfd/7-Table3-1.png"]], "question_id": [0, 1, 2, 4, 14, 16, 13, 19], "question": ["Fast YOLO processes double the mAP of other real-time detectors, what is the actual value of the mAP ?", "What are the metrics used to compare the performance between YOLO & DPM/RCNN?", "How did the authors verify that YOLO learns very general representation of objects ?", "What does the authors means by reframing object detection as a \"single regression problem\" ?", "According to the authors, the VGG-16 version of Faster R-CNN is 6 time slower than YOLO, what is the actual speed of the model ?", "Why does YOLO struggle in localizing objects correctly ?", "Why did the authors chose to train YOLO using VGG-16 and not other neural network architecture ?", "Why does Yolo outperform R-CNN in other categories such as cat and train ?"], "question_section": ["Abstract", "Introduction", "Introduction", "Introduction", "Experiments", "Experiments", "Experiments", "Experiments"], "question_trigger_sentence": ["Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN,", "Finally, YOLO learns very general representations of objects.", "Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems.", "YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance.", "Each prediction is either correct or it is classified based on the type of error:\n Correct: correct class and IOU > :5\n Localization: correct class, :1 < IOU < :5 [...]", "Fast R-CNN is almost 3x more likely to predict background detections than YOLO.", "The VGG-16 version of Faster R-CNN is 10 mAP higher but is also 6 times slower than YOLO.", "We compare YOLO to other detection systems on the Picasso Dataset [12] and the People-Art Dataset [3], two datasets for testing person detection on artwork."], "question_type": ["Testing question", "Shallow question", "Deep/complex question", "Testing question", "Testing question", "Deep/complex question", "Deep/complex question", "Deep/complex question"], "evidential_info": [[{"context": "First, YOLO is extremely fast. Since we frame detection as a regression problem we don\u2019t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems. For a demo of our system running in real-time on a webcam please see our project webpage: http://pjreddie.com/yolo/.", "rationale": "Fast YOLO has 52.7% mAP on the Pascal VOC dataset, more than twice compared to other real-time detectors. YOLO pushes that value up to 63.4% mAP while still processing images in real time."}, {"context": "Fast YOLO is the fastest object detection method on Pascal; as far as we know, it is the fastest extant object detector. With 52.7\\% mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to 63.4\\% while still maintaining real-time performance.", "rationale": "In YOLO, the object detection problem is framed as a regression problem and the entire network can be trained end-to-end. While Fast YOLO can process images at 150 fps, the baseline YOLO runs at 45 fps."}, {"context": "On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.", "rationale": "Still YOLO shows less accuracy (57.9% mAP) compared to the state-of-the-art on the VOC2012 dataset. It struggles with small objects more relative to other architectures."}, {"context": "Fast YOLO is the fastest general-purpose object detector in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection.", "rationale": "YOLO was observed to generalize well to other domains, making it perfect for fast, robust object detection tasks."}], [{"context": "First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.", "rationale": "The different errors are explored on VOC 2007 made by YOLO and R-CNN respectively. And mAP scores are compared on VOC 2012 dataset. Furthermore, YOLO is shown to boost the performance of Fast R-CNN when combined together."}, {"context": "Many research efforts in object detection focus on making standard detection pipelines fast. [5] [38] [31] [14] [17] [28] However, only Sadeghi et al. actually produce a detection system that runs in real-time (30 frames per second or better) [31]. We compare YOLO to their GPU implementation of DPM which runs either at 30Hz or 100Hz. While the other efforts don\u2019t reach the real-time milestone we also compare their relative mAP and speed to examine the accuracy-performance tradeoffs available in object detection systems.", "rationale": "The mAP and speed are compared with real-time detectors and YOLO. Also, other models that do not reach the real-time milestone are included too."}, {"context": "On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.", "rationale": "The application of YOLO and other object detector models trained on natural images to artwork also shows that YOLO's accuracy degrades much less compared to others. This means YOLO can generalize well to different domains."}, {"context": "YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.", "rationale": "More detailed accuracy (mAP) on each class of objects in Pascal VOC 2012 is also performed to see the kinds of objects each object detection predicts well."}], [{"context": "YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection.", "rationale": "YOLO shows less degradation of accuracy compared to other object detectors when trained on natural images and applied to artwork, which means it will less likely to fail when applied to new domains."}, {"context": "Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can\u2019t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.", "rationale": "YOLO can learn the shapes and sizes of objects and their common appearances, thus when applied to artwork, which is different pixel-wise, it still shows reasonable results."}, {"context": "Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.", "rationale": "YOLO can be trained on full images and it is trained end-to-end."}, {"context": "YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.", "rationale": "Since YOLO sees the full image, it can implicitly encode contextual information about the class and its appearance. It makes less than half the errors with the background objects compared to Fast R-CNN."}, {"context": "We introduce YOLO, a unified model for object detection. Our model is simple to construct and can be trained directly on full images. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.", "rationale": "YOLO can be trained on full images and it is trained end-to-end."}], [{"context": "Current detection systems repurpose classifiers to perform detection. To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image [10].", "rationale": "YOLO predicts bounding boxes and class probabilities straight from the image pixels."}, {"context": "We unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means our network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real-time speeds while maintaining high average precision.", "rationale": "Most detection pipelines perform several steps such as extracting features, and classifying or localizing them. They are run on some portions of the image or in a sliding window manner."}, {"context": "We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are.", "rationale": "Unified YOLO model trains on full images and optimizes detection performance."}, {"context": "YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection.", "rationale": "YOLO's loss function is directly related to detection performance and it is trained end-to-end compared to other classifier-based detectors."}, {"context": "Object detection is a core problem in computer vision. Detection pipelines generally start by extracting a set of robust features from input images (Haar [25], SIFT [23], HOG [4], convolutional features [6]). Then, classifiers [36, 21, 13, 10] or localizers [1, 32] are used to identify objects in the feature space. These classifiers or localizers are run either in sliding window fashion over the whole image or on some subset of regions in the image [35, 15, 39]. We compare the YOLO detection system to several top detection frameworks, highlighting key similarities and differences.", "rationale": "YOLO is trained end-to-end and predicts bounding boxes using the entire image. It also computes class probabilities for each bounding box."}, {"context": "We introduce YOLO, a unified model for object detection. Our model is simple to construct and can be trained directly on full images. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.", "rationale": "Existing methods do object detection by running classifiers at various locations of the image."}], [{"context": "The recent Faster R-CNN replaces selective search with a neural network to propose bounding boxes, similar to Szegedy et al. [8] In our tests, their most accurate model achieves 7 fps while a smaller, less accurate one runs at 18 fps. The VGG-16 version of Faster R-CNN is 10 mAP higher but is also 6 times slower than YOLO. The Zeiler-Fergus Faster R-CNN is only 2.5 times slower than YOLO but is also less accurate.", "rationale": "The Faster R-CNN similar to Szegedy et al. achieved 7fps (18fps on a less accurate model). Also, Faster R-CNN with VGG-16 shows 10 mAP higher accuracy but is 6 times slower than YOLO."}, {"context": "Table 1: Real-Time Systems on PASCAL VOC 2007. Compar-", "rationale": "From table 1, we can see that Faster R-CNN with VGG-16 achieves 73.2% mAP at 7 fps, while YOLO has 63.4% mAP at 45 fps on Pascal VOC 2007 dataset."}], [{"context": "YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict. Our model struggles with small objects that appear in groups, such as flocks of birds.", "rationale": "While YOLO is really fast, it fails to localize objects well, especially small ones."}, {"context": "YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. However, our system puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object. Our system also proposes far fewer bounding boxes, only 98 per image compared to about 2000 from Selective Search. Finally, our system combines these individual components into a single, jointly optimized model.", "rationale": "More than half of the YOLO's total errors come from localization."}, {"context": "YOLO struggles to localize objects correctly. Localization errors account for more of YOLO\u2019s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it\u2019s top detections are false positives that don\u2019t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.", "rationale": "YOLO limits the number of bounding boxes per grid cell, which means small objects in a group, or objects that are close to each other will be hard to detect and localize."}, {"context": "On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.", "rationale": "Evaluation with VOC 2012 shows that YOLO struggles much more with detecting small objects compared to other state-of-the-art models. YOLO performs 8-10% less accurately than R-CNN in categories like bottle, sheep, and tv/monitor, but does well for cats and trains."}, {"context": "YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.", "rationale": "With the spatial constraints of the YOLO model, it predicts much fewer bounding boxes per image compared to R-CNN (98 vs 2000)."}, {"context": "Since our model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the input image.", "rationale": "YOLO has problems with objects with unseen or new aspect ratios or configurations. On top of that, the model uses coarse features for predicting bounding boxes as the network downsamples the input image several times."}, {"context": "Finally, while we train on a loss function that approximates detection performance, our loss function treats errors the same in small bounding boxes versus large bounding boxes. A small error in a large box is generally benign but a small error in a small box has a much greater effect on IOU. Our main source of error is incorrect localizations.", "rationale": "In terms of the loss function, the error for large bounding boxes is penalized the same as for small bounding boxes. But obviously, the same amount of error will have a more drastic effect on a small object compared to a large object. Thus, localization errors are high for YOLO."}], [{"context": "We also train YOLO using VGG-16. This model is more accurate but also significantly slower than YOLO. It is useful for comparison to other detection systems that rely on VGG-16 but since it is slower than real-time the rest of the paper focuses on our faster models.", "rationale": "The authors train YOLO with VGG-16, where it showed better accuracy, but it was significantly slower. Thus, they focused on faster variants of the model."}, {"context": "Our network architecture is inspired by the GoogLeNet model for image classification [34]. Our network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use 1 \u00d7 1 reduction layers followed by 3 \u00d7 3 convolutional layers, similar to Lin et al [22]. The full network is shown in Figure 3.", "rationale": "The YOLO's architecture is similar to the GoogLeNet model for image classification."}, {"context": "We pretrain our convolutional layers on the ImageNet 1000-class competition dataset [30]. For pretraining we use the first 20 convolutional layers from Figure 3 followed by a average-pooling layer and a fully connected layer. We train this network for approximately a week and achieve a single crop top-5 accuracy of 88% on the ImageNet 2012 validation set, comparable to the GoogLeNet models in Caffe\u2019s Model Zoo [24]. We use the Darknet framework for all training and inference [26].", "rationale": "The YOLO's convolutional layers have been pretrained on the ImageNet 1000-class competition dataset."}, {"context": "We train the network for about 135 epochs on the training and validation data sets from PASCAL VOC 2007 and 2012. When testing on 2012 we also include the VOC 2007 test data for training. Throughout training we use a batch size of 64, a momentum of 0.9 and a decay of 0.0005.", "rationale": "The entire network of YOLO is trained on PASCAL VOC 2007 training and validation datasets."}], [{"context": "Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can\u2019t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.", "rationale": "Since YOLO uses the entire image as an input it can learn the contextual information around object categories and their appearances."}, {"context": "On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.", "rationale": "YOLO learns generalizable representations of objects as it shows much better results compared to others when it is trained on natural images and tested on artwork."}, {"context": "Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.", "rationale": "The evaluation of the VOC 2012 dataset shows that the YOLO model is much worse than others when it comes to localizing small objects. It also has 8-10% less accuracy for the bottle, sheep, and tv/monitor categories. But it showed reasonable results for cat and train classes."}, {"context": "Table 3: PASCAL VOC 2012 Leaderboard. YOLO compared with the full comp4 (outside data allowed) public leaderboard as of", "rationale": "As it can be seen in Table 3, YOLO can be comparatively bad with certain categories, but at the same time good with others."}]], "composition": ["The baseline YOLO model shows 63.4% mAP at 45fps on the Pascal VOC dataset, while Fast YOLO is on 52.7 mAP at 150fps. Still, they are more than twice more accurate compared to other real-time detectors. However, the YOLO network was observed to struggle with small objects but is generalizable well to other domains.", "Different approaches to evaluating object detection models are presented in the paper where they mostly use mean average precision (mAP) and frames per second (fps) for accuracy and speed respectively. Qualitatively, the YOLO's errors are compared to R-CNN, and mAP on different classes of objects is shown. Moreover, YOLO was shown to boost the performance of R-CNN, and better generalize for new domains.", "Since YOLO is trained on full images and end-to-end it can encode contextual information about each class and its appearance. Moreover, it can learn shapes, sizes, and the relationship between objects. Thus it was shown to be generalizable to artwork, although pixel-wise they are different from natural images, and it makes twice as less mistakes with background objects compared to R-CNN.", "Reframing object detection as a simple regression problem means predicting bounding boxes and class probabilities directly from image pixels avoiding complex pipelines and steps which most of the existing (classifier-based) methods do. YOLO can be trained end-to-end and can predict bounding boxes and respective class probabilities directly from an entire image. Also, its loss function directly corresponds to detection performance, which makes optimizing it more intuitive and easier.", "Table 1 reveals that the actual speed of Faster R-CNN with VGG-16 is 7fps with 73.2% mAP. At the same time, YOLO has more than 6 times the higher speed of 45 fps with 63.4% mAP on Pascal VOC 2007.", "Although YOLO is a really fast model, it usually struggles with localizing small objects in a group or objects near each other. In fact, localization errors take up more than half of all YOLO's errors. It happens because YOLO has only a limited number of bounding boxes per grid cell and the loss function penalizes the errors in the large and small bounding boxes the same. On top of that, the model uses coarse features to predict bounding boxes, and it may have problems with unusual aspect ratios and configurations of objects.", "In fact, the base YOLO model and Fast YOLO have used GoogLeNet-inspired architecture to VGG-16. The authors claim that they have trained it with VGG-16 and it had better accuracy, however, it was too slow to be real-time. The YOLO model is first pretrained on the ImageNet 1000-class competition dataset and later trained on training and validation data of the Pascal VOC 2007 dataset.", "The paper does not specifically discuss why YOLO is better for cat and train categories in VOC 2012 dataset and worse for the bottle, sheep, and tv/monitor. Thus, it is difficult to answer this question with only the contents of the paper."], "Is_figure_in_evidence": [false, false, true, true, false, false, true, false], "Is_table_in_evidence": [true, true, false, false, true, true, false, true], "question_key": ["166", "167", "168", "170", "175", "179", "180", "181"], "passages": ["Humans glance at an image and instantly know what objects are in the image, where they are, and how they interact. The human visual system is fast and accurate, allowing us to perform complex tasks like driving with little conscious thought. Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems.", "Current detection systems repurpose classifiers to perform detection. To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image [10].", "More recent approaches like R-CNN use region proposal methods to first generate potential bounding boxes in an image and then run a classifier on these proposed boxes. After classification, post-processing is used to refine the bounding boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene [13]. These complex pipelines are slow and hard to optimize because each individual component must be trained separately.", "We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are.", "YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection.", "First, YOLO is extremely fast. Since we frame detection as a regression problem we don\u2019t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems. For a demo of our system running in real-time on a webcam please see our project webpage: http://pjreddie.com/yolo/.", "Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can\u2019t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.", "Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.", "YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.", "All of our training and testing code is open source.A variety of pretrained models are also available to download.", "We unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means our network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real-time speeds while maintaining high average precision.", "Our system divides the input image into an S\u00d7S\ud835\udc46\ud835\udc46S\\times Sitalic_S \u00d7 italic_S grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.", "Each grid cell predicts B\ud835\udc35Bitalic_B bounding boxes and confidence scores for those boxes. These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts. Formally we define confidence as Pr\u2061(Object)*IOUpredtruthPrObjectsuperscriptsubscriptIOUpredtruth\\Pr(\\textrm{Object})*\\textrm{IOU}_{\\textrm{pred}}^{\\textrm{truth}}roman_Pr ( Object ) * IOU start_POSTSUBSCRIPT pred end_POSTSUBSCRIPT start_POSTSUPERSCRIPT truth end_POSTSUPERSCRIPT. If no object exists in that cell, the confidence scores should be zero. Otherwise we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth.", "Each bounding box consists of 5 predictions: x\ud835\udc65xitalic_x, y\ud835\udc66yitalic_y, w\ud835\udc64witalic_w, h\u210ehitalic_h, and confidence. The (x,y)\ud835\udc65\ud835\udc66(x,y)( italic_x , italic_y ) coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the whole image. Finally the confidence prediction represents the IOU between the predicted box and any ground truth box.", "Each grid cell also predicts C\ud835\udc36Citalic_C conditional class probabilities, Pr\u2061(Classi|Object)PrconditionalsubscriptClass\ud835\udc56Object\\Pr(\\textrm{Class}_{i}|\\textrm{Object})roman_Pr ( Class start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | Object ). These probabilities are conditioned on the grid cell containing an object. We only predict one set of class probabilities per grid cell, regardless of the number of boxes B\ud835\udc35Bitalic_B.", "At test time we multiply the conditional class probabilities and the individual box confidence predictions,Pr\u2061(Classi|Object)*Pr\u2061(Object)*IOUpredtruth=Pr\u2061(Classi)*IOUpredtruthPrconditionalsubscriptClass\ud835\udc56ObjectPrObjectsuperscriptsubscriptIOUpredtruthPrsubscriptClass\ud835\udc56superscriptsubscriptIOUpredtruth\\scriptsize\\Pr(\\textrm{Class}_{i}|\\textrm{Object})*\\Pr(\\textrm{Object})*\\textrm{IOU}_{\\textrm{pred}}^{\\textrm{truth}}=\\Pr(\\textrm{Class}_{i})*\\textrm{IOU}_{\\textrm{pred}}^{\\textrm{truth}}roman_Pr ( Class start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | Object ) * roman_Pr ( Object ) * IOU start_POSTSUBSCRIPT pred end_POSTSUBSCRIPT start_POSTSUPERSCRIPT truth end_POSTSUPERSCRIPT = roman_Pr ( Class start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) * IOU start_POSTSUBSCRIPT pred end_POSTSUBSCRIPT start_POSTSUPERSCRIPT truth end_POSTSUPERSCRIPT(1)which gives us class-specific confidence scores for each box. These scores encode both the probability of that class appearing in the box and how well the predicted box fits the object.", "For evaluating YOLO on Pascal VOC, we use S=7\ud835\udc467S=7italic_S = 7, B=2\ud835\udc352B=2italic_B = 2. Pascal VOC has 20 labelled classes so C=20\ud835\udc3620C=20italic_C = 20. Our final prediction is a 7\u00d77\u00d73077307\\times 7\\times 307 \u00d7 7 \u00d7 30 tensor.", "We implement this model as a convolutional neural network and evaluate it on the Pascal VOC detection dataset [9]. The initial convolutional layers of the network extract features from the image while the fully connected layers predict the output probabilities and coordinates.", "Our network architecture is inspired by the GoogLeNet model for image classification [34]. Our network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use 1\u00d71111\\times 11 \u00d7 1 reduction layers followed by 3\u00d73333\\times 33 \u00d7 3 convolutional layers, similar to Lin et al [22]. The full network is shown in Figure 3.", "We also train a fast version of YOLO designed to push the boundaries of fast object detection. Fast YOLO uses a neural network with fewer convolutional layers (9 instead of 24) and fewer filters in those layers. Other than the size of the network, all training and testing parameters are the same between YOLO and Fast YOLO.", "The final output of our network is the 7\u00d77\u00d73077307\\times 7\\times 307 \u00d7 7 \u00d7 30 tensor of predictions.", "We pretrain our convolutional layers on the ImageNet 1000-class competition dataset [30]. For pretraining we use the first 20 convolutional layers from Figure 3 followed by a average-pooling layer and a fully connected layer. We train this network for approximately a week and achieve a single crop top-5 accuracy of 88% on the ImageNet 2012 validation set, comparable to the GoogLeNet models in Caffe\u2019s Model Zoo [24]. We use the Darknet framework for all training and inference [26].", "We then convert the model to perform detection. Ren et al. show that adding both convolutional and connected layers to pretrained networks can improve performance [29]. Following their example, we add four convolutional layers and two fully connected layers with randomly initialized weights. Detection often requires fine-grained visual information so we increase the input resolution of the network from 224\u00d7224224224224\\times 224224 \u00d7 224 to 448\u00d7448448448448\\times 448448 \u00d7 448.", "Our final layer predicts both class probabilities and bounding box coordinates. We normalize the bounding box width and height by the image width and height so that they fall between 0 and 1. We parametrize the bounding box x\ud835\udc65xitalic_x and y\ud835\udc66yitalic_y coordinates to be offsets of a particular grid cell location so they are also bounded between 0 and 1.", "We use a linear activation function for the final layer and all other layers use the following leaky rectified linear activation:", "\u03d5(x)={x,if\u00a0x>00.1x,otherwiseitalic-\u03d5\ud835\udc65cases\ud835\udc65if\u00a0\ud835\udc6500.1\ud835\udc65otherwise\\phi(x)=\\begin{cases}x,&\\text{if }x>0\\\\0.1x,&\\text{otherwise}\\end{cases}italic_\u03d5 ( italic_x ) = { start_ROW start_CELL italic_x , end_CELL start_CELL if italic_x > 0 end_CELL end_ROW start_ROW start_CELL 0.1 italic_x , end_CELL start_CELL otherwise end_CELL end_ROW(2)", "We optimize for sum-squared error in the output of our model. We use sum-squared error because it is easy to optimize, however it does not perfectly align with our goal of maximizing average precision. It weights localization error equally with classification error which may not be ideal. Also, in every image many grid cells do not contain any object. This pushes the \u201cconfidence\u201d scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, causing training to diverge early on.", "To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don\u2019t contain objects. We use two parameters, \u03bbcoordsubscript\ud835\udf06coord\\lambda_{\\textrm{coord}}italic_\u03bb start_POSTSUBSCRIPT coord end_POSTSUBSCRIPT and \u03bbnoobjsubscript\ud835\udf06noobj\\lambda_{\\textrm{noobj}}italic_\u03bb start_POSTSUBSCRIPT noobj end_POSTSUBSCRIPT to accomplish this. We set \u03bbcoord=5subscript\ud835\udf06coord5\\lambda_{\\textrm{coord}}=5italic_\u03bb start_POSTSUBSCRIPT coord end_POSTSUBSCRIPT = 5 and \u03bbnoobj=.5subscript\ud835\udf06noobj.5\\lambda_{\\textrm{noobj}}=.5italic_\u03bb start_POSTSUBSCRIPT noobj end_POSTSUBSCRIPT = .5.", "Sum-squared error also equally weights errors in large boxes and small boxes. Our error metric should reflect that small deviations in large boxes matter less than in small boxes. To partially address this we predict the square root of the bounding box width and height instead of the width and height directly.", "YOLO predicts multiple bounding boxes per grid cell. At training time we only want one bounding box predictor to be responsible for each object. We assign one predictor to be \u201cresponsible\u201d for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors. Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall.", "During training we optimize the following, multi-part loss function:\u03bb\ud835\udc1c\ud835\udc28\ud835\udc28\ud835\udc2b\ud835\udc1d\u2211i=0S2\u2211j=0B\ud835\udfd9ijobj[(xi\u2212x^i)2+(yi\u2212y^i)2]+\u03bb\ud835\udc1c\ud835\udc28\ud835\udc28\ud835\udc2b\ud835\udc1d\u2211i=0S2\u2211j=0B\ud835\udfd9ijobj[(wi\u2212w^i)2+(hi\u2212h^i)2]+\u2211i=0S2\u2211j=0B\ud835\udfd9ijobj(Ci\u2212C^i)2+\u03bbnoobj\u2211i=0S2\u2211j=0B\ud835\udfd9ijnoobj(Ci\u2212C^i)2+\u2211i=0S2\ud835\udfd9iobj\u2211c\u2208classes(pi(c)\u2212p^i(c))2subscript\ud835\udf06\ud835\udc1c\ud835\udc28\ud835\udc28\ud835\udc2b\ud835\udc1dsuperscriptsubscript\ud835\udc560superscript\ud835\udc462superscriptsubscript\ud835\udc570\ud835\udc35superscriptsubscript1\ud835\udc56\ud835\udc57objdelimited-[]superscriptsubscript\ud835\udc65\ud835\udc56subscript^\ud835\udc65\ud835\udc562superscriptsubscript\ud835\udc66\ud835\udc56subscript^\ud835\udc66\ud835\udc562subscript\ud835\udf06\ud835\udc1c\ud835\udc28\ud835\udc28\ud835\udc2b\ud835\udc1dsuperscriptsubscript\ud835\udc560superscript\ud835\udc462superscriptsubscript\ud835\udc570\ud835\udc35superscriptsubscript1\ud835\udc56\ud835\udc57objdelimited-[]superscriptsubscript\ud835\udc64\ud835\udc56subscript^\ud835\udc64\ud835\udc562superscriptsubscript\u210e\ud835\udc56subscript^\u210e\ud835\udc562superscriptsubscript\ud835\udc560superscript\ud835\udc462superscriptsubscript\ud835\udc570\ud835\udc35superscriptsubscript1\ud835\udc56\ud835\udc57objsuperscriptsubscript\ud835\udc36\ud835\udc56subscript^\ud835\udc36\ud835\udc562subscript\ud835\udf06noobjsuperscriptsubscript\ud835\udc560superscript\ud835\udc462superscriptsubscript\ud835\udc570\ud835\udc35superscriptsubscript1\ud835\udc56\ud835\udc57noobjsuperscriptsubscript\ud835\udc36\ud835\udc56subscript^\ud835\udc36\ud835\udc562superscriptsubscript\ud835\udc560superscript\ud835\udc462superscriptsubscript1\ud835\udc56objsubscript\ud835\udc50classessuperscriptsubscript\ud835\udc5d\ud835\udc56\ud835\udc50subscript^\ud835\udc5d\ud835\udc56\ud835\udc502\\lambda_{\\textbf{coord}}\\sum_{i=0}^{S^{2}}\\sum_{j=0}^{B}{\\mathbbm{1}}_{ij}^{\\text{obj}}\\left[\\left(x_{i}-\\hat{x}_{i}\\right)^{2}+\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\\right]\\\\+\\lambda_{\\textbf{coord}}\\sum_{i=0}^{S^{2}}\\sum_{j=0}^{B}{\\mathbbm{1}}_{ij}^{\\text{obj}}\\left[\\left(\\sqrt{w_{i}}-\\sqrt{\\hat{w}_{i}}\\right)^{2}+\\left(\\sqrt{h_{i}}-\\sqrt{\\hat{h}_{i}}\\right)^{2}\\right]\\\\+\\sum_{i=0}^{S^{2}}\\sum_{j=0}^{B}{\\mathbbm{1}}_{ij}^{\\text{obj}}\\left(C_{i}-\\hat{C}_{i}\\right)^{2}\\\\+\\lambda_{\\textrm{noobj}}\\sum_{i=0}^{S^{2}}\\sum_{j=0}^{B}{\\mathbbm{1}}_{ij}^{\\text{noobj}}\\left(C_{i}-\\hat{C}_{i}\\right)^{2}\\\\+\\sum_{i=0}^{S^{2}}{\\mathbbm{1}}_{i}^{\\text{obj}}\\sum_{c\\in\\textrm{classes}}\\left(p_{i}(c)-\\hat{p}_{i}(c)\\right)^{2}start_ROW start_CELL italic_\u03bb start_POSTSUBSCRIPT coord end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT blackboard_1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT obj end_POSTSUPERSCRIPT [ ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] end_CELL end_ROW start_ROW start_CELL + italic_\u03bb start_POSTSUBSCRIPT coord end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT blackboard_1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT obj end_POSTSUPERSCRIPT [ ( square-root start_ARG italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG - square-root start_ARG over^ start_ARG italic_w end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( square-root start_ARG italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG - square-root start_ARG over^ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] end_CELL end_ROW start_ROW start_CELL + \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT blackboard_1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT obj end_POSTSUPERSCRIPT ( italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG italic_C end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL + italic_\u03bb start_POSTSUBSCRIPT noobj end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT blackboard_1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT noobj end_POSTSUPERSCRIPT ( italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG italic_C end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL + \u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT blackboard_1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT obj end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_c \u2208 classes end_POSTSUBSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_c ) - over^ start_ARG italic_p end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_c ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_CELL end_ROW(3)where \ud835\udfd9iobjsuperscriptsubscript1\ud835\udc56obj\\mathbbm{1}_{i}^{\\text{obj}}blackboard_1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT obj end_POSTSUPERSCRIPT denotes if object appears in cell i\ud835\udc56iitalic_i and \ud835\udfd9ijobjsuperscriptsubscript1\ud835\udc56\ud835\udc57obj\\mathbbm{1}_{ij}^{\\text{obj}}blackboard_1 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT obj end_POSTSUPERSCRIPT denotes that the j\ud835\udc57jitalic_jth bounding box predictor in cell i\ud835\udc56iitalic_i is \u201cresponsible\u201d for that prediction.", "Note that the loss function only penalizes classification error if an object is present in that grid cell (hence the conditional class probability discussed earlier). It also only penalizes bounding box coordinate error if that predictor is \u201cresponsible\u201d for the ground truth box (i.e. has the highest IOU of any predictor in that grid cell).", "We train the network for about 135 epochs on the training and validation data sets from Pascal VOC 2007 and 2012. When testing on 2012 we also include the VOC 2007 test data for training. Throughout training we use a batch size of 64, a momentum of 0.90.90.90.9 and a decay of 0.00050.00050.00050.0005.", "Our learning rate schedule is as follows: For the first epochs we slowly raise the learning rate from 10\u22123superscript10310^{-3}10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT to 10\u22122superscript10210^{-2}10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT. If we start at a high learning rate our model often diverges due to unstable gradients. We continue training with 10\u22122superscript10210^{-2}10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT for 75 epochs, then 10\u22123superscript10310^{-3}10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT for 30 epochs, and finally 10\u22124superscript10410^{-4}10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT for 30 epochs.", "To avoid overfitting we use dropout and extensive data augmentation. A dropout layer with rate\u00a0=\u00a0.5 after the first connected layer prevents co-adaptation between layers [18]. For data augmentation we introduce random scaling and translations of up to 20% of the original image size. We also randomly adjust the exposure and saturation of the image by up to a factor of 1.51.51.51.5 in the HSV color space.", "Just like in training, predicting detections for a test image only requires one network evaluation. On Pascal VOC the network predicts 98 bounding boxes per image and class probabilities for each box. YOLO is extremely fast at test time since it only requires a single network evaluation, unlike classifier-based methods.", "The grid design enforces spatial diversity in the bounding box predictions. Often it is clear which grid cell an object falls in to and the network only predicts one box for each object. However, some large objects or objects near the border of multiple cells can be well localized by multiple cells. Non-maximal suppression can be used to fix these multiple detections. While not critical to performance as it is for R-CNN or DPM, non-maximal suppression adds 2-3% in mAP.", "YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict. Our model struggles with small objects that appear in groups, such as flocks of birds.", "Since our model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the input image.", "Finally, while we train on a loss function that approximates detection performance, our loss function treats errors the same in small bounding boxes versus large bounding boxes. A small error in a large box is generally benign but a small error in a small box has a much greater effect on IOU. Our main source of error is incorrect localizations.", "Object detection is a core problem in computer vision. Detection pipelines generally start by extracting a set of robust features from input images (Haar [25], SIFT [23], HOG [4], convolutional features [6]). Then, classifiers [36, 21, 13, 10] or localizers [1, 32] are used to identify objects in the feature space. These classifiers or localizers are run either in sliding window fashion over the whole image or on some subset of regions in the image [35, 15, 39]. We compare the YOLO detection system to several top detection frameworks, highlighting key similarities and differences.", "Deformable parts models. Deformable parts models (DPM) use a sliding window approach to object detection [10]. DPM uses a disjoint pipeline to extract static features, classify regions, predict bounding boxes for high scoring regions, etc. Our system replaces all of these disparate parts with a single convolutional neural network. The network performs feature extraction, bounding box prediction, non-maximal suppression, and contextual reasoning all concurrently. Instead of static features, the network trains the features in-line and optimizes them for the detection task. Our unified architecture leads to a faster, more accurate model than DPM.", "R-CNN. R-CNN and its variants use region proposals instead of sliding windows to find objects in images. Selective Search [35] generates potential bounding boxes, a convolutional network extracts features, an SVM scores the boxes, a linear model adjusts the bounding boxes, and non-max suppression eliminates duplicate detections. Each stage of this complex pipeline must be precisely tuned independently and the resulting system is very slow, taking more than 40 seconds per image at test time [14].", "YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. However, our system puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object. Our system also proposes far fewer bounding boxes, only 98 per image compared to about 2000 from Selective Search. Finally, our system combines these individual components into a single, jointly optimized model.", "Other Fast Detectors Fast and Faster R-CNN focus on speeding up the R-CNN framework by sharing computation and using neural networks to propose regions instead of Selective Search [14] [28]. While they offer speed and accuracy improvements over R-CNN, both still fall short of real-time performance.", "Many research efforts focus on speeding up the DPM pipeline [31] [38] [5]. They speed up HOG computation, use cascades, and push computation to GPUs. However, only 30Hz DPM [31] actually runs in real-time.", "Instead of trying to optimize individual components of a large detection pipeline, YOLO throws out the pipeline entirely and is fast by design.", "Detectors for single classes like faces or people can be highly optimized since they have to deal with much less variation [37]. YOLO is a general purpose detector that learns to detect a variety of objects simultaneously.", "Deep MultiBox. Unlike R-CNN, Szegedy et al. train a convolutional neural network to predict regions of interest [8] instead of using Selective Search. MultiBox can also perform single object detection by replacing the confidence prediction with a single class prediction. However, MultiBox cannot perform general object detection and is still just a piece in a larger detection pipeline, requiring further image patch classification. Both YOLO and MultiBox use a convolutional network to predict bounding boxes in an image but YOLO is a complete detection system.", "OverFeat. Sermanet et al. train a convolutional neural network to perform localization and adapt that localizer to perform detection [32]. OverFeat efficiently performs sliding window detection but it is still a disjoint system. OverFeat optimizes for localization, not detection performance. Like DPM, the localizer only sees local information when making a prediction. OverFeat cannot reason about global context and thus requires significant post-processing to produce coherent detections.", "MultiGrasp. Our work is similar in design to work on grasp detection by Redmon et al [27]. Our grid approach to bounding box prediction is based on the MultiGrasp system for regression to grasps. However, grasp detection is a much simpler task than object detection. MultiGrasp only needs to predict a single graspable region for an image containing one object. It doesn\u2019t have to estimate the size, location, or boundaries of the object or predict it\u2019s class, only find a region suitable for grasping. YOLO predicts both bounding boxes and class probabilities for multiple objects of multiple classes in an image.", "First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.", "Many research efforts in object detection focus on making standard detection pipelines fast. [5] [38] [31] [14] [17] [28] However, only Sadeghi et al. actually produce a detection system that runs in real-time (30 frames per second or better) [31]. We compare YOLO to their GPU implementation of DPM which runs either at 30Hz or 100Hz. While the other efforts don\u2019t reach the real-time milestone we also compare their relative mAP and speed to examine the accuracy-performance tradeoffs available in object detection systems.", "Fast YOLO is the fastest object detection method on Pascal; as far as we know, it is the fastest extant object detector. With 52.7%percent52.752.7\\%52.7 % mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to 63.4%percent63.463.4\\%63.4 % while still maintaining real-time performance.", "We also train YOLO using VGG-16. This model is more accurate but also significantly slower than YOLO. It is useful for comparison to other detection systems that rely on VGG-16 but since it is slower than real-time the rest of the paper focuses on our faster models.", "Fastest DPM effectively speeds up DPM without sacrificing much mAP but it still misses real-time performance by a factor of 2 [38]. It also is limited by DPM\u2019s relatively low accuracy on detection compared to neural network approaches.", "R-CNN minus R replaces Selective Search with static bounding box proposals [20]. While it is much faster than R-CNN, it still falls short of real-time and takes a significant accuracy hit from not having good proposals.", "Fast R-CNN speeds up the classification stage of R-CNN but it still relies on selective search which can take around 2 seconds per image to generate bounding box proposals. Thus it has high mAP but at 0.50.50.50.5 fps it is still far from real-time.", "The recent Faster R-CNN replaces selective search with a neural network to propose bounding boxes, similar to Szegedy et al. [8] In our tests, their most accurate model achieves 7 fps while a smaller, less accurate one runs at 18 fps. The VGG-16 version of Faster R-CNN is 10 mAP higher but is also 6 times slower than YOLO. The Zeiler-Fergus Faster R-CNN is only 2.5 times slower than YOLO but is also less accurate.", "To further examine the differences between YOLO and state-of-the-art detectors, we look at a detailed breakdown of results on VOC 2007. We compare YOLO to Fast R-CNN since Fast R-CNN is one of the highest performing detectors on Pascal and it\u2019s detections are publicly available.", "We use the methodology and tools of Hoiem et al. [19] For each category at test time we look at the top N predictions for that category. Each prediction is either correct or it is classified based on the type of error:", "\u2022Correct: correct class and IOU>.5IOU.5\\textrm{IOU}>.5IOU > .5\u2022Localization: correct class, .1<IOU<.5.1IOU.5.1<\\textrm{IOU}<.5.1 < IOU < .5\u2022Similar: class is similar, IOU>.1IOU.1\\textrm{IOU}>.1IOU > .1\u2022Other: class is wrong, IOU>.1IOU.1\\textrm{IOU}>.1IOU > .1\u2022Background: IOU<.1IOU.1\\textrm{IOU}<.1IOU < .1 for any object", "Figure 4 shows the breakdown of each error type averaged across all 20 classes.", "YOLO struggles to localize objects correctly. Localization errors account for more of YOLO\u2019s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it\u2019s top detections are false positives that don\u2019t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.", "YOLO makes far fewer background mistakes than Fast R-CNN. By using YOLO to eliminate background detections from Fast R-CNN we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes.", "The best Fast R-CNN model achieves a mAP of 71.8% on the VOC 2007 test set. When combined with YOLO, its mAP increases by 3.2% to 75.0%. We also tried combining the top Fast R-CNN model with several other versions of Fast R-CNN. Those ensembles produced small increases in mAP between .3 and .6%, see Table 2 for details.", "The boost from YOLO is not simply a byproduct of model ensembling since there is little benefit from combining different versions of Fast R-CNN. Rather, it is precisely because YOLO makes different kinds of mistakes at test time that it is so effective at boosting Fast R-CNN\u2019s performance.", "Unfortunately, this combination doesn\u2019t benefit from the speed of YOLO since we run each model seperately and then combine the results. However, since YOLO is so fast it doesn\u2019t add any significant computational time compared to Fast R-CNN.", "On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.", "Our combined Fast R-CNN + YOLO model is one of the highest performing detection methods. Fast R-CNN gets a 2.3% improvement from the combination with YOLO, boosting it 5 spots up on the public leaderboard.", "Academic datasets for object detection draw the training and testing data from the same distribution. In real-world applications it is hard to predict all possible use cases and the test data can diverge from what the system has seen before [3].We compare YOLO to other detection systems on the Picasso Dataset [12] and the People-Art Dataset [3], two datasets for testing person detection on artwork.", "Figure 5 shows comparative performance between YOLO and other detection methods. For reference, we give VOC 2007 detection AP on person where all models are trained only on VOC 2007 data. On Picasso models are trained on VOC 2012 while on People-Art they are trained on VOC 2010.", "R-CNN has high AP on VOC 2007. However, R-CNN drops off considerably when applied to artwork. R-CNN uses Selective Search for bounding box proposals which is tuned for natural images. The classifier step in R-CNN only sees small regions and needs good proposals.", "DPM maintains its AP well when applied to artwork. Prior work theorizes that DPM performs well because it has strong spatial models of the shape and layout of objects. Though DPM doesn\u2019t degrade as much as R-CNN, it starts from a lower AP.", "YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.", "YOLO is a fast, accurate object detector, making it ideal for computer vision applications. We connect YOLO to a webcam and verify that it maintains real-time performance, including the time to fetch images from the camera and display the detections.", "The resulting system is interactive and engaging. While YOLO processes images individually, when attached to a webcam it functions like a tracking system, detecting objects as they move around and change in appearance. A demo of the system and the source code can be found on our project website: http://pjreddie.com/yolo/.", "We introduce YOLO, a unified model for object detection. Our model is simple to construct and can be trained directly on full images. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.", "Fast YOLO is the fastest general-purpose object detector in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection.", "Acknowledgements: This work is partially supported by ONR N00014-13-1-0720, NSF IIS-1338054, and The Allen Distinguished Investigator Award."], "figure_types": {"f8e79ac0ea341056ef20f2616628b3e964764cfd/1-Figure1-1.png": "schematic", "f8e79ac0ea341056ef20f2616628b3e964764cfd/2-Figure2-1.png": "schematic", "f8e79ac0ea341056ef20f2616628b3e964764cfd/3-Figure3-1.png": "schematic", "f8e79ac0ea341056ef20f2616628b3e964764cfd/6-Figure4-1.png": "plot", "f8e79ac0ea341056ef20f2616628b3e964764cfd/6-Table1-1.png": "table", "f8e79ac0ea341056ef20f2616628b3e964764cfd/6-Table2-1.png": "table", "f8e79ac0ea341056ef20f2616628b3e964764cfd/7-Table3-1.png": "table", "f8e79ac0ea341056ef20f2616628b3e964764cfd/8-Figure5-1.png": "plot", "f8e79ac0ea341056ef20f2616628b3e964764cfd/8-Figure6-1.png": "photograph(s)"}}, "1511.07247": {"paper_id": "paper_12", "title": "NetVLAD: CNN Architecture for Weakly Supervised Place Recognition", "arxiv_url": "https://arxiv.org/abs/1511.07247", "s2orc_url": "https://www.semanticscholar.org/paper/f971a22287ead6aa23ecd84a4afd8efca57cee3c", "all_figures_tables": {"f971a22287ead6aa23ecd84a4afd8efca57cee3c/1-Figure1-1.png": "Figure 1. Our trained NetVLAD descriptor correctly recognizes the location (b) of the query photograph (a) despite the large amount of clutter (people, cars), changes in viewpoint and completely different illumination (night vs daytime). Please see the appendix [2] for more examples.", "f971a22287ead6aa23ecd84a4afd8efca57cee3c/4-Figure2-1.png": "Figure 2. CNN architecture with the NetVLAD layer. The layer can be implemented using standard CNN layers (convolutions, softmax, L2-normalization) and one easy-to-implement aggregation layer to perform aggregation in equation (4) (\u201cVLAD core\u201d), joined", "f971a22287ead6aa23ecd84a4afd8efca57cee3c/4-Figure3-1.png": "Figure 3. Benefits of supervised VLAD. Red and green circles are local descriptors from two different images, assigned to the same cluster (Voronoi cell). Under the VLAD encoding, their contribution to the similarity score between the two images is the scalar product (as final VLAD vectors are L2-normalized) between the corresponding residuals, where a residual vector is computed as the difference between the descriptor and the cluster\u2019s anchor point. The anchor point ck can be interpreted as the origin of a new coordinate system local to the the specific cluster k. In standard VLAD, the anchor is chosen as the cluster centre (\u00d7) in order to evenly distribute the residuals across the database. However, in a supervised setting where the two descriptors are known to belong to images which should not match, it is possible to learn a better anchor ( ) which causes the scalar product between the new residuals to be small.", "f971a22287ead6aa23ecd84a4afd8efca57cee3c/5-Figure4-1.png": "Figure 4. Google Street View Time Machine examples. Each column shows perspective images generated from panoramas from nearby locations, taken at different times. A well designed method can use this source of imagery to learn to be invariant to changes in viewpoint and lighting (a-c), and to moderate occlusions (b). It can also learn to suppress confusing visual information such as clouds (a), vehicles and people (b-c), and to chose to either ignore vegetation or to learn a season-invariant vegetation representation (a-c). More examples are given in [2].", "f971a22287ead6aa23ecd84a4afd8efca57cee3c/7-Figure5-1.png": "Figure 5. Comparison of our methods versus off-the-shelf networks and state-of-the-art. The base CNN architecture is denoted in brackets: (A)lexNet and (V)GG-16. Trained representations (red and magenta for AlexNet and VGG-16) outperform by a large margin offthe-shelf ones (blue, cyan, green for AlexNet, Places205, VGG-16), fV LAD (-o-) works better than fmax (-x-), and our fV LAD+whitening (-\u2217-) representation based on VGG-16 sets the state-of-the-art on all datasets. [79] only evaluated on Tokyo 24/7 as the method relies on depth data not available in other datasets. Additional results are shown in the appendix [2].", "f971a22287ead6aa23ecd84a4afd8efca57cee3c/8-Figure6-1.png": "Figure 6. What has been learnt? Each column corresponds to one image (top row) and the emphasis various networks (under fmax) give to different patches. Each pixel in the heatmap corresponds to the change in representation when a large gray occluding square (100\u00d7 100) is placed over the image in the same position; all heatmaps have the same colour scale. Note that the original image and the heatmaps are not in perfect alignment as nearby patches overlap 50% and patches touching an image edge are discarded to prevent border effects. All images are from Pitts250kval that the network hasn\u2019t seen at training. Further examples are given in the appendix [2].", "f971a22287ead6aa23ecd84a4afd8efca57cee3c/8-Table1-1.png": "Table 1. Partial training. Effects of performing backpropagation only down to a certain layer of AlexNet, e.g. \u2018conv4\u2019 means that weights of layers from conv4 and above are learnt, while weights of layers below conv4 are fixed to their pretrained state; r@N signifies recall@N. Results are shown on the Pitts30k-val dataset."}, "referred_figures_tables": [["f971a22287ead6aa23ecd84a4afd8efca57cee3c/7-Figure5-1.png"], ["f971a22287ead6aa23ecd84a4afd8efca57cee3c/4-Figure3-1.png"], ["f971a22287ead6aa23ecd84a4afd8efca57cee3c/7-Figure5-1.png"]], "question_id": [5, 12, 2], "question": ["To obtain the final compact descriptor of the image, why did the authors use PCA instead of other compression algorithms?.", "How does the NetVLAD layer differ from the original VLAD?", "What are the two place recognition benchmarks used by the authors?"], "question_section": ["Introduction", "Deep architecture for place recognition", "Abstract"], "question_trigger_sentence": ["The resulting aggregated representation is then compressed using Principal Component Analysis (PCA) to obtain the final compact descriptor of the image.", "Similarly to the original VLAD descriptor, the NetVLAD layer aggregates the first order statistics of residuals  in different parts of the descriptor space weighted by the soft-assignment  of descriptor  to cluster . ", "Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks."], "question_type": ["Deep/complex question", "Deep/complex question", "Shallow question"], "evidential_info": [[{"context": "Furthermore we compare our CNN representations trained for place recognitionagainst the state-of-the-art local feature based compact descriptor, which consists ofVLAD pooling [29] with intra-normalization [3]on top of densely extracted RootSIFTs [43, 2].The descriptor is optionally reduced to 4096 dimensions usingPCA (learnt on the training set) combined with whitening and L2-normalization [25];this setup together with view synthesis yields the state-of-the-art results on the challenging Tokyo 24/7 dataset(c.f. [80]).", "rationale": "The descriptor is optionally reduced to 4096 dimensions using PCA (learnt on the training set) combined with whitening and L2-normalization; this setup together with view synthesis yields the state-of-the-art results on the challenging Tokyo 24/7 dataset."}, {"context": "We follow the standard state-of-the-art procedure to perform dimensionalityreduction of VLAD, as described earlier,i.e. the reduction into 4096-D is performed usingPCA with whitening followed by L2-normalization [25, 80].Figure 5 shows that the lower dimensional f_{VLAD} (-\\ast-)performssimilarly to the full size vector (-o-).", "rationale": "We follow the standard state-of-the-art procedure to perform dimensionality reduction of VLAD, as described earlier,i.e. the reduction into 4096-D is performed using PCA with whitening followed by L2-normalization. Figure 5 shows that the lower dimensional fVLAD performs similarly to the full size vector."}], [{"context": "In order to profit from years of wisdom produced in image retrieval,we propose to mimic VLAD in a CNN frameworkand design a trainable generalized VLAD layer, NetVLAD. The result is a powerful image representationtrainable end-to-end on the target task (in our case place recognition).To construct a layer amenable to training via backpropagation,it is required that the layer\u2019s operation is differentiable withrespect to all its parameters and the input.Hence, the key challenge is to make the VLAD pooling differentiable, which we describe next.", "rationale": "we propose to mimic VLAD in a CNN framework and design a trainable generalized VLAD layer, NetVLAD. The result is a powerful image representation trainable end-to-end on the target task (in our case place recognition). To construct a layer amenable to training via backpropagation,it is required that the layer\u2019s operation is differentiable with respect to all its parameters and the input.Hence, the key challenge is to make the VLAD pooling differentiable"}, {"context": "By expanding the squares in\u00a0(2), it is easy to see that the terme^{-\\alpha\\lVert\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}\\rVert^{2}} cancels between the numerator and the denominatorresulting in a soft-assignment of the following form\\bar{a}_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i})=\\frac{e^{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k}^{T}\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}+b_{k}}}{\\sum_{k^{\\prime}}{e^{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k^{\\prime}}^{T}\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}+b_{k^{\\prime}}}}},(3)where vector \\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k}=2\\alpha\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k} and scalar b_{k}=-\\alpha\\lVert\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}\\rVert^{2}.The final form of the NetVLAD layer is obtained byplugging the soft-assignment\u00a0(3) into the VLAD descriptor\u00a0(1) resulting inV(j,k)=\\sum_{i=1}^{N}\\frac{e^{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k}^{T}\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}+b_{k}}}{\\sum_{k^{\\prime}}{e^{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k^{\\prime}}^{T}\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}+b_{k^{\\prime}}}}}\\left(x_{i}(j)-c_{k}(j)\\right),(4)where\\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k}\\}, \\{b_{k}\\} and \\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}\\} are sets of trainable parameters for each cluster k.Similarly to the original VLAD descriptor, the NetVLAD layer aggregates the first order statistics of residuals (\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}-\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k})in different parts of the descriptor space weighted by the soft-assignment \\bar{a}_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}) of descriptor \\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i} to cluster k.Note however, that the NetVLAD layer has three independentsets of parameters \\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k}\\}, \\{b_{k}\\} and \\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}\\}, compared to just\\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}\\} of the original VLAD. This enables greater flexibility than the original VLAD,as explained in figure 3.Decoupling \\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k},b_{k}\\} from \\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}\\} has been proposed in[3] as a means to adapt the VLAD to a new dataset.All parameters of NetVLAD are learnt for the specific task in an end-to-end manner.", "rationale": "Similarly to the original VLAD descriptor, the NetVLAD layer aggregates the first order statistics of residuals (xi \u2212 ck) in different parts of the descriptor"}], [{"context": "In the following we discuss figure 5,which compares place recognition performance of our method to the baselines outlined aboveon the Pittsburgh and Tokyo 24/7 benchmarks.", "rationale": "We discuss figure 5,which compares place recognition performance of our method to the baselines outlined above on the Pittsburgh and Tokyo 24/7 benchmarks"}]], "composition": ["Maybe authors found that PCA is  computationally less expensive and much memory and time saving in experiments than other methods. PCA is used to reduce the dimensions of the descriptor to 4096 learnt on the training set, which is discovered experimentally to help in achieving state-of-the-art results on the challenging Tokyo 24/7 dataset as comparisons show that the lower dimensional fVLAD performs similarly to the full size vector.", "The original VLAD method uses hand-crafted features and applies the VLAD technique to them by concatenating multiple VLADs. On the other hand, NetVLAD layer uses a CNN to extract features and applies the VLAD technique in a single layer by learning the aggregation weights of the residuals (xi \u2212 ck) in different parts of the descriptor space. The NetVLAD layer has three independent sets of parameters, {wk}, {bk} and {ck}, that enables greater flexibility and adaptability to the CNN features than the original VLAD method which uses only {ck}.", "Pittsburgh(Pitts250k) and Tokyo 24/7 benchmarks"], "Is_figure_in_evidence": [true, true, true], "Is_table_in_evidence": [false, false, false], "question_key": ["183", "188", "194"], "passages": ["Visual place recognition has received a significant amount of attention in the past yearsboth in computer vision\u00a0[66, 35, 10, 64, 65, 24, 9, 81, 4, 80, 63] and robotics communities\u00a0[15, 16, 46, 44, 75] motivated by, e.g., applications in autonomous driving\u00a0[46], augmented reality\u00a0[47] or geo-localizing archival imagery\u00a0[5].", "The place recognition problem, however, still remains extremely challenging. How can we recognize the same street-corner in the entire city or on the scale of the entire country despite the fact it can be captured in different illuminations or change its appearance over time? The fundamental scientific question is what is the appropriate representation of a place that is rich enough to distinguish similarly looking places yet compact to represent entire cities or countries.", "The place recognition problem has been traditionally cast as an instance retrieval task, where the query image location is estimated using the locations of the most visually similar images obtained byquerying a large geotagged database\u00a0[66, 10, 35, 81, 80, 4].Each database image is represented using local invariant features\u00a0[83] such as SIFT\u00a0[43] that are aggregated into a single vector representation for the entire image such as bag-of-visual-words\u00a0[74, 53], VLAD\u00a0[3, 29] or Fisher vector\u00a0[52, 31]. The resulting representation is then usually compressed and efficiently indexed\u00a0[74, 28]. The image database can be further augmented by 3D structure that enables recovery of accurate camera pose\u00a0[40, 63, 64].", "In the last few years convolutional neural networks (CNNs)\u00a0[38, 39] have emerged as powerful image representations for various category-level recognition tasks such as object classification\u00a0[37, 49, 73, 77], scene recognition\u00a0[91] or object detection\u00a0[21]. The basic principles of CNNs are known from 80\u2019s\u00a0[38, 39] and the recent successes are a combination of advances in GPU-based computation power together with large labelled image datasets\u00a0[37].While it has been shown that the trained representations are, to some extent, transferable between recognition tasks\u00a0[19, 21, 49, 69, 89], a direct application of CNN representations trained for object classification\u00a0[37] as black-box descriptor extractors has so far yielded limited improvements in performance on instance-level recognition tasks[6, 7, 22, 60, 62]. In this work we investigate whether this gap in performance can be bridged by CNN representations developed and trained directly for place recognition. This requires addressing the following three main challenges.First, what is a good CNN architecture for place recognition?Second, how to gather sufficient amount of annotated data for the training?Third, how can we train the developed architecture in an end-to-end manner tailored for the place recognition task?To address these challenges we bring the following three innovations.", "First, building on the lessons learnt from the current well performing hand-engineered object retrieval and place recognition pipelines\u00a0[2, 3, 25, 80] we develop a convolutional neural network architecture for place recognition that aggregates mid-level (conv5) convolutional features extracted from the entire imageinto a compact single vector representation amenable to efficient indexing. To achieve this, we design a new trainable generalized VLAD layer, NetVLAD, inspired by the Vector of Locally Aggregated Descriptors (VLAD) representation\u00a0[29] that has shown excellent performance in image retrieval and place recognition. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation.The resulting aggregated representation is then compressed usingPrincipal Component Analysis (PCA) to obtain the final compact descriptor of the image.", "Second, to train the architecture for place recognition, we gather a large dataset of multiple panoramic images depicting the same place from different viewpoints over time from the Google Street View Time Machine. Such data is available for vast areas of the world, but provides only weak form of supervision: we know the two panoramas are captured at approximately similar positions based on their (noisy) GPS but we don\u2019t know which parts of the panoramas depict the same parts of the scene.", "Third, we develop a learning procedure for place recognition that learns parameters of the architecture in an end-to-end manner tailored for the place recognition task from the weakly labelled Time Machine imagery. The resulting representation is robust tochanges in viewpoint and lighting conditions, while simultaneously learns to focus on the relevant parts of the image such as the building fa\u00e7ades and the skyline, while ignoring confusing elements such as cars and people that may occur at many different places.", "We show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks.", "While there have been many improvements in designing betterimage retrieval\u00a0[2, 3, 12, 11, 17, 26, 27, 29, 25, 32, 48, 51, 52, 53, 54, 71, 78, 79, 82] and place recognition\u00a0[4, 10, 15, 16, 24, 9, 35, 46, 44, 64, 65, 63, 75, 81, 80] systems, not many works have performedlearning for these tasks.All relevant learning-based approaches fall into one or both of the followingtwo categories:(i) learning for an auxiliary task (e.g. some form of distinctiveness of local features\u00a0[4, 15, 30, 35, 58, 59, 90]), and (ii) learning on top of shallow hand-engineered descriptors that cannot be fine-tuned for the target task\u00a0[2, 24, 9, 35, 57]. Both of these are in spirit opposite to the core idea behinddeep learning that has provided a major boost in performance in variousrecognition tasks: end-to-end learning. We will indeed show insection 5.2 that training representations directly for the end-task,place recognition, is crucial for obtaining good performance.", "Numerous works concentrate on learning better local descriptors ormetrics to compare them[88, 55, 45, 48, 71, 56, 70, 50],but even though some of them show results on image retrieval,the descriptors are learnt on the task of matching local image patches,and not directly with image retrieval in mind.Some of them also make use of hand-engineered features to bootstrap the learning,i.e. to provide noisy training data [55, 45, 48, 71, 50].", "Several works have investigated using CNN-based features forimage retrieval. These include treating activations from certain layersdirectly as descriptors by concatenating them [8, 60],or by pooling [6, 22, 7].However, none of these works actually train the CNNs for the task at hand,but use CNNs as black-box descriptor extractors.One exception is the work of Babenko et al. [8] in whichthe network is fine-tuned on an auxiliary task of classifying 700 landmarks. However,again the network is not trained directly on the target retrieval task.", "Finally, recently [34] and [41] performed end-to-endlearning for different but related tasks of ground-to-aerial matching [41] andcamera pose estimation [34].", "Building on the success of current place recognition systems (e.g.\u00a0[66, 35, 10, 64, 65, 81, 4, 80, 63]),we cast place recognition as image retrieval.The query image with unknown location is used to visually search a largegeotagged image database, and the locations of top ranked images are usedas suggestions for the location of the query. This is generally done by designinga function f\ud835\udc53fitalic_f which acts as the \u201cimage representation extractor\u201d,such that given an image Iisubscript\ud835\udc3c\ud835\udc56I_{i}italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT it produces a fixed size vector f(Ii)\ud835\udc53subscript\ud835\udc3c\ud835\udc56f(I_{i})italic_f ( italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).The function is used to extract the representations for the entiredatabase {Ii}subscript\ud835\udc3c\ud835\udc56\\{I_{i}\\}{ italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }, which can be done offline,and to extract the query image representation f(q)\ud835\udc53\ud835\udc5ef(q)italic_f ( italic_q ), done online.At test time, the visual search is performed by finding the nearest databaseimage to the query,either exactly or through fast approximate nearest neighbour search,by sorting images based on the Euclidean distance d(q,Ii)\ud835\udc51\ud835\udc5esubscript\ud835\udc3c\ud835\udc56d(q,I_{i})italic_d ( italic_q , italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) betweenf(q)\ud835\udc53\ud835\udc5ef(q)italic_f ( italic_q ) and f(Ii)\ud835\udc53subscript\ud835\udc3c\ud835\udc56f(I_{i})italic_f ( italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).", "While previous works have mainly used hand-engineered image representations(e.g. f(I)\ud835\udc53\ud835\udc3cf(I)italic_f ( italic_I ) corresponds to extracting SIFT descriptors [43],followed by pooling into a bag-of-words vector [74]or a VLAD vector [29]),here we propose to learn the representation f(I)\ud835\udc53\ud835\udc3cf(I)italic_f ( italic_I ) in an end-to-end manner,directly optimized for the task of place recognition.The representation is parametrized with a set of parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8and we emphasize this fact by referring to it as f\u03b8(I)subscript\ud835\udc53\ud835\udf03\ud835\udc3cf_{\\theta}(I)italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_I ).It follows that the Euclidean distanced\u03b8(Ii,Ij)=\u2225f\u03b8(Ii)\u2212f\u03b8(Ij)\u2225subscript\ud835\udc51\ud835\udf03subscript\ud835\udc3c\ud835\udc56subscript\ud835\udc3c\ud835\udc57delimited-\u2225\u2225subscript\ud835\udc53\ud835\udf03subscript\ud835\udc3c\ud835\udc56subscript\ud835\udc53\ud835\udf03subscript\ud835\udc3c\ud835\udc57d_{\\theta}(I_{i},I_{j})=\\lVert f_{\\theta}(I_{i})-f_{\\theta}(I_{j})\\rVertitalic_d start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) = \u2225 italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) \u2225also depends on the same parameters.An alternative setup would be to learn the distance function itself, but herewe choose to fix the distance function to be Euclidean distance, and to poseour problem as the search for the explicit feature map f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT which works wellunder the Euclidean distance.", "In section 3 we describe the proposed representation f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPTbased on a new deep convolutional neural network architecture inspired by thecompact aggregated image descriptors for instance retrieval.In section\u00a04 we describe a method to learn the parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 ofthe network in an end-to-end manner using weakly supervised training datafrom the Google Street View Time Machine.", "This section describes the proposed CNN architecture f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT,guided by the best practices from the image retrieval community.Most image retrieval pipelines are based on (i) extracting local descriptors,which are then (ii) pooled in an orderless manner. The motivation behind thischoice is that the procedure providessignificant robustness to translation and partial occlusion.Robustness to lighting and viewpoint changes is provided by the descriptorsthemselves, and scale invariance is ensured through extracting descriptorsat multiple scales.", "In order to learn the representation end-to-end, we designa CNN architecture that mimics this standard retrieval pipeline in an unifiedand principled manner with differentiable modules.For step (i), we crop the CNNat the last convolutional layer and view itas a dense descriptor extractor.This has been observed to work well for instance retrieval[6, 7, 62] and texture recognition\u00a0[13].Namely, the output of the last convolutional layer is aH\u00d7W\u00d7D\ud835\udc3b\ud835\udc4a\ud835\udc37H\\times W\\times Ditalic_H \u00d7 italic_W \u00d7 italic_D map which can be considered as a set of D-dimensionaldescriptors extracted at H\u00d7W\ud835\udc3b\ud835\udc4aH\\times Witalic_H \u00d7 italic_W spatial locations.For step (ii) we design a new pooling layer inspired by the Vector of Locally Aggregated Descriptors (VLAD)\u00a0[29]that pools extracted descriptors into a fixed image representation and its parameters are learnable via back-propagation.We call this new pooling layer \u201cNetVLAD\u201d layer and describe it in the next section.", "Vector of Locally Aggregated Descriptors (VLAD)\u00a0[29]is a popular descriptor pooling method for both instance level retrieval\u00a0[29] and image classification\u00a0[22].It captures information about the statisticsof local descriptors aggregated over the image. Whereas bag-of-visual-words\u00a0[14, 74] aggregation keeps countsof visual words, VLAD stores the sum of residuals (difference vector betweenthe descriptor and its corresponding cluster centre) for each visual word.", "Formally, given N\ud835\udc41Nitalic_N D-dimensional local image descriptors {\ud835\udc31i}subscript\ud835\udc31\ud835\udc56\\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}\\}{ bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } as input,and K\ud835\udc3eKitalic_K cluster centres (\u201cvisual words\u201d) {\ud835\udc1ck}subscript\ud835\udc1c\ud835\udc58\\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}\\}{ bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } as VLAD parameters,the output VLAD image representation V\ud835\udc49Vitalic_V is K\u00d7D\ud835\udc3e\ud835\udc37K\\times Ditalic_K \u00d7 italic_D-dimensional.For convenience we will write V\ud835\udc49Vitalic_V as a K\u00d7D\ud835\udc3e\ud835\udc37K\\times Ditalic_K \u00d7 italic_D matrix, but this matrixis converted into a vector and, after normalization, used asthe image representation. The (j,k)\ud835\udc57\ud835\udc58(j,k)( italic_j , italic_k ) element of V\ud835\udc49Vitalic_V is computedas follows:V(j,k)=\u2211i=1Nak(\ud835\udc31i)(xi(j)\u2212ck(j)),\ud835\udc49\ud835\udc57\ud835\udc58superscriptsubscript\ud835\udc561\ud835\udc41subscript\ud835\udc4e\ud835\udc58subscript\ud835\udc31\ud835\udc56subscript\ud835\udc65\ud835\udc56\ud835\udc57subscript\ud835\udc50\ud835\udc58\ud835\udc57V(j,k)=\\sum_{i=1}^{N}a_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i})\\left(x_{i}(j)-c_{k}(j)\\right),italic_V ( italic_j , italic_k ) = \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_j ) - italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_j ) ) ,(1)where xi(j)subscript\ud835\udc65\ud835\udc56\ud835\udc57x_{i}(j)italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_j ) and ck(j)subscript\ud835\udc50\ud835\udc58\ud835\udc57c_{k}(j)italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_j ) are the j\ud835\udc57jitalic_j-th dimensions of the i\ud835\udc56iitalic_i-th descriptor and k\ud835\udc58kitalic_k-th cluster centre, respectively.ak(\ud835\udc31i)subscript\ud835\udc4e\ud835\udc58subscript\ud835\udc31\ud835\udc56a_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i})italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) denotes the membership ofthe descriptor \ud835\udc31isubscript\ud835\udc31\ud835\udc56\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to k\ud835\udc58kitalic_k-th visual word, i.e. it is 1111 if cluster \ud835\udc1cksubscript\ud835\udc1c\ud835\udc58\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPTis the closest cluster to descriptor \ud835\udc31isubscript\ud835\udc31\ud835\udc56\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and 00 otherwise.Intuitively, each D-dimensional column k\ud835\udc58kitalic_k of V\ud835\udc49Vitalic_V records the sum ofresiduals (\ud835\udc31i\u2212\ud835\udc1ck)subscript\ud835\udc31\ud835\udc56subscript\ud835\udc1c\ud835\udc58(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}-\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k})( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) of descriptors which are assigned to cluster \ud835\udc1cksubscript\ud835\udc1c\ud835\udc58\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT.The matrix V\ud835\udc49Vitalic_V is then L2-normalized column-wise(intra-normalization\u00a0[3]),converted into a vector,and finally L2-normalized in its entirety [29].", "In order to profit from years of wisdom produced in image retrieval,we propose to mimic VLAD in a CNN frameworkand design a trainable generalized VLAD layer, NetVLAD. The result is a powerful image representationtrainable end-to-end on the target task (in our case place recognition).To construct a layer amenable to training via backpropagation,it is required that the layer\u2019s operation is differentiable withrespect to all its parameters and the input.Hence, the key challenge is to make the VLAD pooling differentiable, which we describe next.", "The source of discontinuities in VLAD is the hard assignment ak(\ud835\udc31i)subscript\ud835\udc4e\ud835\udc58subscript\ud835\udc31\ud835\udc56a_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i})italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) of descriptors \ud835\udc31isubscript\ud835\udc31\ud835\udc56\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to clusters centres \ud835\udc1cksubscript\ud835\udc1c\ud835\udc58\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT.To make this operation differentiable, we replace it with soft assignment of descriptors to multiple clustersa\u00afk(\ud835\udc31i)=e\u2212\u03b1\u2225\ud835\udc31i\u2212\ud835\udc1ck\u22252\u2211k\u2032e\u2212\u03b1\u2225\ud835\udc31i\u2212\ud835\udc1ck\u2032\u22252,subscript\u00af\ud835\udc4e\ud835\udc58subscript\ud835\udc31\ud835\udc56superscript\ud835\udc52\ud835\udefcsuperscriptdelimited-\u2225\u2225subscript\ud835\udc31\ud835\udc56subscript\ud835\udc1c\ud835\udc582subscriptsuperscript\ud835\udc58\u2032superscript\ud835\udc52\ud835\udefcsuperscriptdelimited-\u2225\u2225subscript\ud835\udc31\ud835\udc56subscript\ud835\udc1csuperscript\ud835\udc58\u20322\\bar{a}_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i})=\\frac{e^{-\\alpha\\lVert\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}-\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}\\rVert^{2}}}{\\sum_{k^{\\prime}}{e^{-\\alpha\\lVert\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}-\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k^{\\prime}}\\rVert^{2}}}},over\u00af start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = divide start_ARG italic_e start_POSTSUPERSCRIPT - italic_\u03b1 \u2225 bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT - italic_\u03b1 \u2225 bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_c start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT end_ARG ,(2)which assigns the weight of descriptor \ud835\udc31isubscript\ud835\udc31\ud835\udc56\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to cluster \ud835\udc1cksubscript\ud835\udc1c\ud835\udc58\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT proportional to their proximity, but relative to proximities to other cluster centres. a\u00afk(\ud835\udc31i)subscript\u00af\ud835\udc4e\ud835\udc58subscript\ud835\udc31\ud835\udc56\\bar{a}_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i})over\u00af start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ranges between 0 and 1, with the highest weight assigned to the closest cluster centre. \u03b1\ud835\udefc\\alphaitalic_\u03b1 is a parameter (positive constant) that controls the decay of the response with the magnitude of the distance.Note that for \u03b1\u2192+\u221e\u2192\ud835\udefc\\alpha\\to+\\inftyitalic_\u03b1 \u2192 + \u221e this setup replicates the original VLAD exactlyas a\u00afk(\ud835\udc31i)subscript\u00af\ud835\udc4e\ud835\udc58subscript\ud835\udc31\ud835\udc56\\bar{a}_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i})over\u00af start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) for the closest cluster would be 1111 and 00 otherwise.", "By expanding the squares in\u00a0(2), it is easy to see that the terme\u2212\u03b1\u2225\ud835\udc31i\u22252superscript\ud835\udc52\ud835\udefcsuperscriptdelimited-\u2225\u2225subscript\ud835\udc31\ud835\udc562e^{-\\alpha\\lVert\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}\\rVert^{2}}italic_e start_POSTSUPERSCRIPT - italic_\u03b1 \u2225 bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT cancels between the numerator and the denominatorresulting in a soft-assignment of the following forma\u00afk(\ud835\udc31i)=e\ud835\udc30kT\ud835\udc31i+bk\u2211k\u2032e\ud835\udc30k\u2032T\ud835\udc31i+bk\u2032,subscript\u00af\ud835\udc4e\ud835\udc58subscript\ud835\udc31\ud835\udc56superscript\ud835\udc52superscriptsubscript\ud835\udc30\ud835\udc58\ud835\udc47subscript\ud835\udc31\ud835\udc56subscript\ud835\udc4f\ud835\udc58subscriptsuperscript\ud835\udc58\u2032superscript\ud835\udc52superscriptsubscript\ud835\udc30superscript\ud835\udc58\u2032\ud835\udc47subscript\ud835\udc31\ud835\udc56subscript\ud835\udc4fsuperscript\ud835\udc58\u2032\\bar{a}_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i})=\\frac{e^{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k}^{T}\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}+b_{k}}}{\\sum_{k^{\\prime}}{e^{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k^{\\prime}}^{T}\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}+b_{k^{\\prime}}}}},over\u00af start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = divide start_ARG italic_e start_POSTSUPERSCRIPT bold_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT bold_w start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG ,(3)where vector \ud835\udc30k=2\u03b1\ud835\udc1cksubscript\ud835\udc30\ud835\udc582\ud835\udefcsubscript\ud835\udc1c\ud835\udc58\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k}=2\\alpha\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}bold_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 2 italic_\u03b1 bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and scalar bk=\u2212\u03b1\u2225\ud835\udc1ck\u22252subscript\ud835\udc4f\ud835\udc58\ud835\udefcsuperscriptdelimited-\u2225\u2225subscript\ud835\udc1c\ud835\udc582b_{k}=-\\alpha\\lVert\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}\\rVert^{2}italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = - italic_\u03b1 \u2225 bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT.The final form of the NetVLAD layer is obtained byplugging the soft-assignment\u00a0(3) into the VLAD descriptor\u00a0(1) resulting inV(j,k)=\u2211i=1Ne\ud835\udc30kT\ud835\udc31i+bk\u2211k\u2032e\ud835\udc30k\u2032T\ud835\udc31i+bk\u2032(xi(j)\u2212ck(j)),\ud835\udc49\ud835\udc57\ud835\udc58superscriptsubscript\ud835\udc561\ud835\udc41superscript\ud835\udc52superscriptsubscript\ud835\udc30\ud835\udc58\ud835\udc47subscript\ud835\udc31\ud835\udc56subscript\ud835\udc4f\ud835\udc58subscriptsuperscript\ud835\udc58\u2032superscript\ud835\udc52superscriptsubscript\ud835\udc30superscript\ud835\udc58\u2032\ud835\udc47subscript\ud835\udc31\ud835\udc56subscript\ud835\udc4fsuperscript\ud835\udc58\u2032subscript\ud835\udc65\ud835\udc56\ud835\udc57subscript\ud835\udc50\ud835\udc58\ud835\udc57V(j,k)=\\sum_{i=1}^{N}\\frac{e^{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k}^{T}\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}+b_{k}}}{\\sum_{k^{\\prime}}{e^{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k^{\\prime}}^{T}\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}+b_{k^{\\prime}}}}}\\left(x_{i}(j)-c_{k}(j)\\right),italic_V ( italic_j , italic_k ) = \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT divide start_ARG italic_e start_POSTSUPERSCRIPT bold_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT bold_w start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_j ) - italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_j ) ) ,(4)where{\ud835\udc30k}subscript\ud835\udc30\ud835\udc58\\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k}\\}{ bold_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }, {bk}subscript\ud835\udc4f\ud835\udc58\\{b_{k}\\}{ italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } and {\ud835\udc1ck}subscript\ud835\udc1c\ud835\udc58\\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}\\}{ bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } are sets of trainable parameters for each cluster k\ud835\udc58kitalic_k.Similarly to the original VLAD descriptor, the NetVLAD layer aggregates the first order statistics of residuals (\ud835\udc31i\u2212\ud835\udc1ck)subscript\ud835\udc31\ud835\udc56subscript\ud835\udc1c\ud835\udc58(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}-\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k})( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )in different parts of the descriptor space weighted by the soft-assignment a\u00afk(\ud835\udc31i)subscript\u00af\ud835\udc4e\ud835\udc58subscript\ud835\udc31\ud835\udc56\\bar{a}_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i})over\u00af start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) of descriptor \ud835\udc31isubscript\ud835\udc31\ud835\udc56\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to cluster k\ud835\udc58kitalic_k.Note however, that the NetVLAD layer has three independentsets of parameters {\ud835\udc30k}subscript\ud835\udc30\ud835\udc58\\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k}\\}{ bold_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }, {bk}subscript\ud835\udc4f\ud835\udc58\\{b_{k}\\}{ italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } and {\ud835\udc1ck}subscript\ud835\udc1c\ud835\udc58\\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}\\}{ bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }, compared to just{\ud835\udc1ck}subscript\ud835\udc1c\ud835\udc58\\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}\\}{ bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } of the original VLAD. This enables greater flexibility than the original VLAD,as explained in figure 3.Decoupling {\ud835\udc30k,bk}subscript\ud835\udc30\ud835\udc58subscript\ud835\udc4f\ud835\udc58\\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k},b_{k}\\}{ bold_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } from {\ud835\udc1ck}subscript\ud835\udc1c\ud835\udc58\\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf c$}}{\\mbox{\\boldmath$\\textstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptstyle\\bf c$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf c$}}_{k}\\}{ bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } has been proposed in[3] as a means to adapt the VLAD to a new dataset.All parameters of NetVLAD are learnt for the specific task in an end-to-end manner.", "As illustrated in figure 2 the NetVLAD layer can be visualized as a meta-layer that is further decomposed intobasic CNN layers connected up in a directed acyclic graph.First, note that the first term in eq.\u00a0(4) is asoft-max function \u03c3k(\ud835\udc33)=exp\u2061(zk)\u2211k\u2032exp\u2061(zk\u2032)subscript\ud835\udf0e\ud835\udc58\ud835\udc33subscript\ud835\udc67\ud835\udc58subscriptsuperscript\ud835\udc58\u2032subscript\ud835\udc67superscript\ud835\udc58\u2032\\sigma_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf z$}}{\\mbox{\\boldmath$\\textstyle\\bf z$}}{\\mbox{\\boldmath$\\scriptstyle\\bf z$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf z$}})=\\frac{\\exp(z_{k})}{\\sum_{k^{\\prime}}{\\exp(z_{k^{\\prime}})}}italic_\u03c3 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_z ) = divide start_ARG roman_exp ( italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_exp ( italic_z start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) end_ARG.Therefore, the soft-assignment of the input array of descriptors \ud835\udc31isubscript\ud835\udc31\ud835\udc56\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into K\ud835\udc3eKitalic_K clusters can be seen as a two step process:(i)a convolution with a set of K\ud835\udc3eKitalic_K filters {\ud835\udc30k}subscript\ud835\udc30\ud835\udc58\\{\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k}\\}{ bold_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } that have spatial support 1\u00d71111\\times 11 \u00d7 1 and biases {bk}subscript\ud835\udc4f\ud835\udc58\\{b_{k}\\}{ italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }, producing the outputsk(\ud835\udc31i)=\ud835\udc30kT\ud835\udc31i+bksubscript\ud835\udc60\ud835\udc58subscript\ud835\udc31\ud835\udc56superscriptsubscript\ud835\udc30\ud835\udc58\ud835\udc47subscript\ud835\udc31\ud835\udc56subscript\ud835\udc4f\ud835\udc58s_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i})=\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf w$}}{\\mbox{\\boldmath$\\textstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptstyle\\bf w$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf w$}}_{k}^{T}\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i}+b_{k}italic_s start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = bold_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT;(ii)the convolution output is then passed through the soft-max function \u03c3ksubscript\ud835\udf0e\ud835\udc58\\sigma_{k}italic_\u03c3 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT to obtain the final soft-assignment a\u00afk(\ud835\udc31i)subscript\u00af\ud835\udc4e\ud835\udc58subscript\ud835\udc31\ud835\udc56\\bar{a}_{k}(\\mathchoice{\\mbox{\\boldmath$\\displaystyle\\bf x$}}{\\mbox{\\boldmath$\\textstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptstyle\\bf x$}}{\\mbox{\\boldmath$\\scriptscriptstyle\\bf x$}}_{i})over\u00af start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) that weightsthe different terms in the aggregation layer that implements eq.\u00a0(4). The output after normalization is a (K\u00d7D)\u00d71\ud835\udc3e\ud835\udc371(K\\times D)\\times 1( italic_K \u00d7 italic_D ) \u00d7 1 descriptor.", "Other works have proposed to pool CNN activations using VLAD orFisher Vectors (FV) [22, 13], but do not learn the VLAD/FVparameters nor the input descriptors.The most related method to ours is the one of Sydorov et al. [76],which proposes to learnFV parameters jointly with an SVM for the end classificationobjective. However, in their work it is not possible to learn the input descriptorsas they are hand-engineered (SIFT), while our VLAD layer is easilypluggable into any CNN architecture as it is amenable to backpropagation.\u201cFisher Networks\u201d\u00a0[72] stack Fisher Vectorlayers on top of each other, but the system is not trainedend-to-end, only hand-crafted features are used, and the layersare trained greedily in a bottom-up fashion.Finally, our architecture is also related to bilinear networks\u00a0[42], recently developed for a different task of fine-grained category-level recognition.", "We also experiment with Max-poolingof the D-dimensional features across the H\u00d7W\ud835\udc3b\ud835\udc4aH\\times Witalic_H \u00d7 italic_W spatiallocations, thus producing a D-dimensional output vector,which is then L2-normalized.Both of these operations can be implemented using standard layers inpublic CNN packages.This setup mirrors the method of[6, 62], but a crucial difference is that we willlearn the representation (section 4) while[60, 6, 62] only use pretrained networks.Results will show (section 5.2) that simply using CNNsoff-the-shelf [60] results in poor performance, and thattraining for the end-task is crucial.Additionally, VLAD will prove itself to be superior to theMax-pooling baseline.", "In the previous section we have designed a new CNN architecture as an image representation for place recognition.Here we describe how to learn its parameters in an end-to-end manner for the place recognition task.The two main challenges are: (i) how to gather enough annotated training data and (ii) what is the appropriate lossfor the place recognition task. To address theses issues, we will first show that it is possible to obtain large amounts of weakly labelled imagery depicting the same places over time from the Google Street View Time Machine. Second, we will design a new weakly supervised triplet ranking loss that can deal with the incomplete and noisy position annotations of the Street View Time Machine imagery. The details are below.", "We propose to exploit a new source of data \u2013 Google Street View Time Machine \u2013which provides multiple street-level panoramic images taken at different times at close-by spatial locations on the map.As will be seen in section 5.2,this novel data source is precious for learning an image representation for place recognition.As shown in figure\u00a04, the same locations are depictedat different times and seasons, providing the learning algorithm with crucialinformation it can use to discover which features are useful or distracting,and what changes should the image representation be invariant to, in order to achievegood place recognition performance.", "The downside of the Time Machine imagery is that it provides only incomplete and noisy supervision.Each Time Machine panorama comes with a GPS tag giving only its approximate location on the map, which can be used toidentify close-by panoramas but does not provide correspondences between parts of the depicted scenes.In detail, as the test queries are perspective images from camera phones, each panorama is represented by a set of perspective images sampled evenlyin different orientations and two elevation angles\u00a0[35, 10, 24, 81]. Each perspective image is labelled with the GPS positionof the source panorama. As a result, two geographically close perspective images do not necessarily depict the same objectssince they could be facing different directions or occlusions could take place (e.g. the two images are around a corner from each other), etc.Therefore, for a given training query q\ud835\udc5eqitalic_q, the GPS informationcan only be used as a source of (i) potential positives {piq}subscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56\\{p^{q}_{i}\\}{ italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }, i.e. images that aregeographically close to the query, and (ii) definite negatives {njq}subscriptsuperscript\ud835\udc5b\ud835\udc5e\ud835\udc57\\{n^{q}_{j}\\}{ italic_n start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }, i.e. images that are geographically far from the query.111Note that even faraway images candepict the same object. For example, the Eiffel Tower can be visible from two faraway locations in Paris. But, for the purpose of localization we consider in this paper such image pairs as negative examples because they are not taken from the same place.", "We wish to learn a representation f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT that will optimize place recognition performance.That is, for a given test query image q\ud835\udc5eqitalic_q, the goal is to rank a database image Ii\u2063*subscript\ud835\udc3c\ud835\udc56I_{i*}italic_I start_POSTSUBSCRIPT italic_i * end_POSTSUBSCRIPT from a close-by locationhigher than all other far away images Iisubscript\ud835\udc3c\ud835\udc56I_{i}italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in the database. In other words, we wishthe Euclidean distance d\u03b8(q,I)subscript\ud835\udc51\ud835\udf03\ud835\udc5e\ud835\udc3cd_{\\theta}(q,I)italic_d start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_q , italic_I ) between the query q\ud835\udc5eqitalic_q and a close-by image Ii\u2063*subscript\ud835\udc3c\ud835\udc56I_{i*}italic_I start_POSTSUBSCRIPT italic_i * end_POSTSUBSCRIPT to be smaller than the distance to far away images in the database Iisubscript\ud835\udc3c\ud835\udc56I_{i}italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, i.e. d\u03b8(q,Ii\u2063*)<d\u03b8(q,Ii)subscript\ud835\udc51\ud835\udf03\ud835\udc5esubscript\ud835\udc3c\ud835\udc56subscript\ud835\udc51\ud835\udf03\ud835\udc5esubscript\ud835\udc3c\ud835\udc56d_{\\theta}(q,I_{i*})<d_{\\theta}(q,I_{i})italic_d start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_q , italic_I start_POSTSUBSCRIPT italic_i * end_POSTSUBSCRIPT ) < italic_d start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_q , italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), for all images Iisubscript\ud835\udc3c\ud835\udc56I_{i}italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT further than a certain distance from the query on the map.Next we show how this requirement can be translated into a ranking loss between training triplets {q,Ii\u2063*,Ii}\ud835\udc5esubscript\ud835\udc3c\ud835\udc56subscript\ud835\udc3c\ud835\udc56\\{q,I_{i*},I_{i}\\}{ italic_q , italic_I start_POSTSUBSCRIPT italic_i * end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }.", "From the Google Street View Time Machine data, we obtain a training dataset of tuples(q,{piq},{njq})\ud835\udc5esubscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56subscriptsuperscript\ud835\udc5b\ud835\udc5e\ud835\udc57(q,\\{p^{q}_{i}\\},\\{n^{q}_{j}\\})( italic_q , { italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } , { italic_n start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } ), where for each training query image q\ud835\udc5eqitalic_q we havea set of potential positives {piq}subscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56\\{p^{q}_{i}\\}{ italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } and the set of definite negatives {njq}subscriptsuperscript\ud835\udc5b\ud835\udc5e\ud835\udc57\\{n^{q}_{j}\\}{ italic_n start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }.The set of potential positives contains at least one positive image that should match the query, but we do not know which one.To address this ambiguity, we propose to identify the best matching potential positive image pi\u2063*qsubscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56p^{q}_{i*}italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i * end_POSTSUBSCRIPTpi\u2063*q=arg\u2061minpiq\u2061d\u03b8(q,piq)subscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56subscriptsubscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56subscript\ud835\udc51\ud835\udf03\ud835\udc5esubscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56p^{q}_{i*}=\\operatorname*{\\arg\\!\\min}_{p^{q}_{i}}d_{\\theta}(q,p^{q}_{i})italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i * end_POSTSUBSCRIPT = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_q , italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )(5)for each training tuple (q,{piq},{njq})\ud835\udc5esubscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56subscriptsuperscript\ud835\udc5b\ud835\udc5e\ud835\udc57(q,\\{p^{q}_{i}\\},\\{n^{q}_{j}\\})( italic_q , { italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } , { italic_n start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } ).The goal then becomes to learn an image representation f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT so that distance d\u03b8(q,pi\u2063*q)subscript\ud835\udc51\ud835\udf03\ud835\udc5esubscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56d_{\\theta}(q,p^{q}_{i*})italic_d start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_q , italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i * end_POSTSUBSCRIPT ) between the training query q\ud835\udc5eqitalic_q and the best matching potential positive pi\u2063*qsubscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56p^{q}_{i*}italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i * end_POSTSUBSCRIPTis smaller than the distance d\u03b8(q,njq)subscript\ud835\udc51\ud835\udf03\ud835\udc5esubscriptsuperscript\ud835\udc5b\ud835\udc5e\ud835\udc57d_{\\theta}(q,n^{q}_{j})italic_d start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_q , italic_n start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) between the query q\ud835\udc5eqitalic_q and all negative images qjsubscript\ud835\udc5e\ud835\udc57q_{j}italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT:d\u03b8(q,pi\u2063*q)<d\u03b8(q,njq),\u2200j.subscript\ud835\udc51\ud835\udf03\ud835\udc5esubscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56subscript\ud835\udc51\ud835\udf03\ud835\udc5esubscriptsuperscript\ud835\udc5b\ud835\udc5e\ud835\udc57for-all\ud835\udc57d_{\\theta}(q,p^{q}_{i*})<d_{\\theta}(q,n^{q}_{j}),~{}~{}~{}\\forall j.italic_d start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_q , italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i * end_POSTSUBSCRIPT ) < italic_d start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_q , italic_n start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , \u2200 italic_j .(6)Based on this intuition we define a weakly supervised ranking loss L\u03b8subscript\ud835\udc3f\ud835\udf03L_{\\theta}italic_L start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT for a training tuple (q,{piq},{njq})\ud835\udc5esubscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56subscriptsuperscript\ud835\udc5b\ud835\udc5e\ud835\udc57(q,\\{p^{q}_{i}\\},\\{n^{q}_{j}\\})( italic_q , { italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } , { italic_n start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } ) asL\u03b8=\u2211jl(mini\u2061d\u03b82(q,piq)+m\u2212d\u03b82(q,njq)),subscript\ud835\udc3f\ud835\udf03subscript\ud835\udc57\ud835\udc59subscript\ud835\udc56subscriptsuperscript\ud835\udc512\ud835\udf03\ud835\udc5esubscriptsuperscript\ud835\udc5d\ud835\udc5e\ud835\udc56\ud835\udc5asubscriptsuperscript\ud835\udc512\ud835\udf03\ud835\udc5esubscriptsuperscript\ud835\udc5b\ud835\udc5e\ud835\udc57L_{\\theta}=\\sum_{j}l\\left(\\min_{i}d^{2}_{\\theta}(q,p^{q}_{i})+m-d^{2}_{\\theta}(q,n^{q}_{j})\\right),italic_L start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_l ( roman_min start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_q , italic_p start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_m - italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_q , italic_n start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ) ,(7)where l\ud835\udc59litalic_l is the hinge loss l(x)=max\u2061(x,0)\ud835\udc59\ud835\udc65\ud835\udc650l(x)=\\max(x,0)italic_l ( italic_x ) = roman_max ( italic_x , 0 ), and m\ud835\udc5amitalic_m is a constant parameter giving the margin.Note that equation\u00a0(7) is a sum of individual losses for negative images njqsubscriptsuperscript\ud835\udc5b\ud835\udc5e\ud835\udc57n^{q}_{j}italic_n start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. For each negative, the loss l\ud835\udc59litalic_l is zero if the distance between the query and the negative is greater by a margin than the distance between the query and the best matching positive. Conversely,if the margin between the distance to the negative image and to the best matching positive is violated,the loss is proportional to the amount of violation.Note that the above loss is related to the commonly used triplet loss[68, 87, 86, 67], but adapted to our weaklysupervised scenario using a formulation (given by equation\u00a0(5)) similar to multiple instance learning\u00a0[20, 36, 85].", "We train the parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 of the representation f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT usingStochastic Gradient Descent (SGD) on a large set of training tuples from Time Machine data.Details of the training procedure are given inappendix A.", "In this section we describe the used datasets and evaluation methodology (section\u00a05.1),and give quantitative (section\u00a05.2) and qualitative (section\u00a05.3) results to validate our approach.Finally, we also test the method on the standard image retrieval benchmarks(section\u00a05.4).", "We report results on two publicly available datasets.", "contains 250kdatabase images downloaded from Google Street Viewand 24k test queries generated from Street View but taken at differenttimes, years apart.We divide this dataset into three roughly equal partsfor training, validation and testing,each containing around 83kdatabase images and 8k queries,where the division was done geographically to ensure the sets containindependent images.To facilitate faster training, for some experiments,a smaller subset (Pitts30k) is used, containing 10k database imagesin each of the train/val(idation)/test sets, which arealso geographically disjoint.", "contains 76k database images and315 query images taken using mobile phone cameras.This is an extremely challenging dataset where the queries were taken at daytime, sunset and night, while the databaseimages were only taken at daytime as they originate from Google Street Viewas described above.To form the train/val sets we collectedadditional Google Street View panoramas of Tokyo using theTime Machine feature, and name this set TokyoTM;Tokyo 24/7 (=test) andTokyoTM train/val are all geographically disjoint.Further details on the splits are given in appendix B.", "We follow the standard place recognition evaluation procedure[4, 24, 65, 81, 80].The query image is deemed correctly localized if at least one of the top N\ud835\udc41Nitalic_N retrieveddatabase images is within d=25\ud835\udc5125d=25italic_d = 25 meters from the ground truth position of the query.The percentage of correctly recognized queries (Recall) is then plotted for different values of N\ud835\udc41Nitalic_N.For Tokyo 24/7 we follow\u00a0[80] and perform spatial non-maximal suppression on ranked database images before evaluation.", "We use two base architectures which are extended with Max pooling (fmaxsubscript\ud835\udc53\ud835\udc5a\ud835\udc4e\ud835\udc65f_{max}italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT) andour NetVLAD (fVLADsubscript\ud835\udc53\ud835\udc49\ud835\udc3f\ud835\udc34\ud835\udc37f_{VLAD}italic_f start_POSTSUBSCRIPT italic_V italic_L italic_A italic_D end_POSTSUBSCRIPT) layers:AlexNet [37] andVGG-16 [73];both are cropped at the last convolutional layer (conv5), before ReLU.For NetVLAD we use K=64\ud835\udc3e64K=64italic_K = 64 resulting in 16k and 32k-D image representationsfor the two base architectures, respectively.The initialization procedure, parameters used for training, procedure for sampling training tuplesand other implementation details are given inappendix A.All training and evaluation code, as well as our trained networks,are online at [1].", "To assess benefits of our approach we compare our representations trained for place recognition against \u201coff-the-shelf\u201d networkspretrained on other tasks. Namely,given a base network cropped at conv5,the baselines either use Max pooling (fmaxsubscript\ud835\udc53\ud835\udc5a\ud835\udc4e\ud835\udc65f_{max}italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT),or aggregate the descriptors into VLAD (fVLADsubscript\ud835\udc53\ud835\udc49\ud835\udc3f\ud835\udc34\ud835\udc37f_{VLAD}italic_f start_POSTSUBSCRIPT italic_V italic_L italic_A italic_D end_POSTSUBSCRIPT),but perform no further task-specific training.The three base networks are:AlexNet [37],VGG-16 [73],both are pretrained for ImageNet classification [18],andPlaces205 [91],reusing the same architecture as AlexNet butpretrained for scene classification [91].Pretrained networks have been recently used as off-the-shelfdense descriptor extractors for instance retrieval\u00a0[6, 7, 22, 60, 62] andthe untrained fmaxsubscript\ud835\udc53\ud835\udc5a\ud835\udc4e\ud835\udc65f_{max}italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT network corresponds to the method of[6, 62].", "Furthermore we compare our CNN representations trained for place recognitionagainst the state-of-the-art local feature based compact descriptor, which consists ofVLAD pooling [29] with intra-normalization [3]on top of densely extracted RootSIFTs [43, 2].The descriptor is optionally reduced to 4096 dimensions usingPCA (learnt on the training set) combined with whitening and L2-normalization [25];this setup together with view synthesis yields the state-of-the-art results on the challenging Tokyo 24/7 dataset(c.f. [80]).", "In the following we discuss figure 5,which compares place recognition performance of our method to the baselines outlined aboveon the Pittsburgh and Tokyo 24/7 benchmarks.", "We follow the standard state-of-the-art procedure to perform dimensionalityreduction of VLAD, as described earlier,i.e. the reduction into 4096-D is performed usingPCA with whitening followed by L2-normalization [25, 80].Figure 5 shows that the lower dimensional fVLADsubscript\ud835\udc53\ud835\udc49\ud835\udc3f\ud835\udc34\ud835\udc37f_{VLAD}italic_f start_POSTSUBSCRIPT italic_V italic_L italic_A italic_D end_POSTSUBSCRIPT (-\u2217\u2217\\ast\u2217-)performssimilarly to the full size vector (-o-).", "Representations trained on the end-task of place recognitionconsistently outperform by a large margin off-the-shelf CNNs on both benchmarks.For example, on the Pitts250k-test our trained AlexNet with (trained) NetVLAD aggregation layer achieves recall@1of 81.0% compared to only 55.0% obtained by off-the-shelf AlexNet with standard VLAD aggregation, i.e. a relative improvement in recall of 47%.Similar improvements can be observed on all three datasets.This confirms two important premises of this work:(i) our approach can learn rich yet compact image representations for place recognition, and(ii) the popular idea of using pretrained networks \u201coff-the-shelf\u201d[60, 6, 22, 7, 62]is sub-optimal as the networks trained for object or scene classificationare not necessary suitable for the end-task of place recognition.We believe this could be attributed to the fact that\u201coff-the-shelf \u201d conv5 activations are not trained to be comparable using Euclidean distance.", "Figure 5 also shows that our trained fVLADsubscript\ud835\udc53\ud835\udc49\ud835\udc3f\ud835\udc34\ud835\udc37f_{VLAD}italic_f start_POSTSUBSCRIPT italic_V italic_L italic_A italic_D end_POSTSUBSCRIPTrepresentation with whitening based on VGG-16( magenta -\u2217\u2217\\ast\u2217-)convincingly outperforms RootSIFT+VLAD+whitening,as well as the method of Torii et al.\u00a0[80],and thereforesets the state-of-the-art for compact descriptors on all benchmarks.Note that these are strong baselines that outperform most off-the-shelf CNNdescriptors on the place recognition task.", "By comparing fVLADsubscript\ud835\udc53\ud835\udc49\ud835\udc3f\ud835\udc34\ud835\udc37f_{VLAD}italic_f start_POSTSUBSCRIPT italic_V italic_L italic_A italic_D end_POSTSUBSCRIPT (-o-) methods with their corresponding fmaxsubscript\ud835\udc53\ud835\udc5a\ud835\udc4e\ud835\udc65f_{max}italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT (-x-) counterpartsit is clear that VLAD pooling is much better than Max pooling for both off-the-shelf and trained representations.NetVLAD performance decreases gracefullywith dimensionality: 128-D NetVLAD performs similarly to 512-D Max(42.9% vs 38.4% recall@1 on Tokyo 24/7),resulting in four timesmore compact representation for the same performance.Furthermore, NetVLAD+whitening outperforms Max pooling convincingly whenreduced to the same dimensionality (60%).See appendix C for more details.", "In Table 1 we study the benefits of training different layers for the end-task of place recognition.The largest improvements are thanks to training the NetVLAD layer, but training other layersresults in further improvements, with some overfitting occurring below conv2.", "Here we examine whether the network can be trained without the Time Machine (TM) data.In detail, we have modified the training query set for Pitts30k-train to be sampled from the sameset as the training database images, i.e. the tuples of query and database images used in training were captured at the same time.Recall@1 with fmaxsubscript\ud835\udc53\ud835\udc5a\ud835\udc4e\ud835\udc65f_{max}italic_f start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT on Pitts30k-val for the off-the-shelf AlexNet is 33.5%, andtraining without TM improves this to 38.7%.However, training with TM obtains 68.5% showingthat Time Machine data is crucialfor good place recognition accuracy as without it the network does not generalize well.The network learns, for example,that recognizing cars is important for place recognition,as the same parked cars appear in all images of a place.", "To visualize what is being learnt by our place recognition architectures,we adapt the method of Zeiler and Fergus [89] for examiningocclusion sensitivity of classification networks.It can be seen in figure\u00a06that off-the-shelf AlexNet (pretrained on ImageNet) focuses very muchon categories it has been trained to recognize (e.g. cars)and certain shapes,such as circular blobsuseful for distinguishing 12 different ball types in the ImageNet categories.The Place205 network is fairly unresponsive to all occlusions as it does notaim to recognize specific places but scene-level categories,so even if an important part of the image is occluded, such asa characteristic part of a building fa\u00e7ade, it still provides a similaroutput feature which corresponds to an uninformative\u201ca building fa\u00e7ade\u201d image descriptor.In contrast to these two,our network trained for specific place recognition automatically learnsto ignore confusing features, such as cars and people, which are notdiscriminative for specific locations, and instead focuses on describingbuilding fa\u00e7ades and skylines.More qualitative examples are provided inappendix C.", "We use our best performing network (VGG-16, fVLADsubscript\ud835\udc53\ud835\udc49\ud835\udc3f\ud835\udc34\ud835\udc37f_{VLAD}italic_f start_POSTSUBSCRIPT italic_V italic_L italic_A italic_D end_POSTSUBSCRIPT with whitening down to 256-D)trained completely on Pittsburgh, to extract image representationsfor standard object and image retrieval benchmarks.Our representation sets the state-of-the-art for compact image representations (256-D)by a large margin on all three datasets, obtaining an mAP of63.5%, 73.5% and 79.9% onOxford 5k [53], Paris 6k [54], Holidays [26], respectively;for example, this is a +20% relative improvement on Oxford 5k.Appendix Ccontains more detailed results.", "We have designed a new convolutional neural network architecture thatis trained for place recognition in an end-to-end manner from weaklysupervised Street View Time Machine data. Our trained representationsignificantly outperforms off-the-shelf CNN models and significantlyimproves over the state-of-the-art on the challenging 24/7 Tokyodataset, as well as on the Oxford and Paris image retrieval benchmarks.The two main components of our architecture\u2013 (i) the NetVLAD pooling layer and (ii) weakly supervised rankingloss \u2013 are generic CNN building blocks applicable beyond the placerecognition task. The NetVLAD layer offers a powerful poolingmechanism with learnable parameters that can be easily plugged intoany other CNN architecture. The weakly supervised ranking lossopens up the possibility of end-to-end learning for other rankingtasks where large amounts of weakly labelled data are available, forexample, images described with natural language\u00a0[33].", "This work was partly supported byRVO13000 - Conceptual development of research organization,the ERC grant LEAP (no.\u00a0336845), ANR project Semapolis (ANR-13-CORD-0003),JSPS KAKENHI Grant Number 15H05313, the Inria CityLab IPL,and the Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory, contract FA8650-12-C-7212.The U.S.\u00a0Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, AFRL, or the U.S.\u00a0Government."], "figure_types": {"f971a22287ead6aa23ecd84a4afd8efca57cee3c/1-Figure1-1.png": "photograph(s)", "f971a22287ead6aa23ecd84a4afd8efca57cee3c/4-Figure2-1.png": "schematic", "f971a22287ead6aa23ecd84a4afd8efca57cee3c/4-Figure3-1.png": "schematic", "f971a22287ead6aa23ecd84a4afd8efca57cee3c/5-Figure4-1.png": "photograph(s)", "f971a22287ead6aa23ecd84a4afd8efca57cee3c/7-Figure5-1.png": "plot", "f971a22287ead6aa23ecd84a4afd8efca57cee3c/8-Figure6-1.png": "photograph(s)", "f971a22287ead6aa23ecd84a4afd8efca57cee3c/8-Table1-1.png": "table"}}, "1507.06228": {"paper_id": "paper_122", "title": "Training Very Deep Networks", "arxiv_url": "https://arxiv.org/abs/1507.06228", "s2orc_url": "https://www.semanticscholar.org/paper/b92aa7024b87f50737b372e5df31ef091ab54e62", "all_figures_tables": {"b92aa7024b87f50737b372e5df31ef091ab54e62/3-Figure1-1.png": "Figure 1: Comparison of optimization of plain networks and highway networks of various depths. Left: The training curves for the best hyperparameter settings obtained for each network depth. Right: Mean performance of top 10 (out of 100) hyperparameter settings. Plain networks become much harder to optimize with increasing depth, while highway networks with up to 100 layers can still be optimized well. Best viewed on screen (larger version included in Supplementary Material).", "b92aa7024b87f50737b372e5df31ef091ab54e62/4-Table1-1.png": "Table 1: Test set classification accuracy for pilot experiments on the MNIST dataset.", "b92aa7024b87f50737b372e5df31ef091ab54e62/4-Table2-1.png": "Table 2: CIFAR-10 test set accuracy of convolutional highway networks. Architectures tested were based on fitnets trained by Romero et. al. [25] using two-stage hint based training. Highway networks were trained in a single stage without hints, matching or exceeding the performance of fitnets.", "b92aa7024b87f50737b372e5df31ef091ab54e62/5-Table3-1.png": "Table 3: Test set accuracy of convolutional highway networks on the CIFAR-10 and CIFAR-100 object recognition datasets with typical data augmentation. For comparison, we list the accuracy reported by recent studies in similar experimental settings.", "b92aa7024b87f50737b372e5df31ef091ab54e62/6-Figure2-1.png": "Figure 2: Visualization of best 50 hidden-layer highway networks trained on MNIST (top row) and CIFAR-100 (bottom row). The first hidden layer is a plain layer which changes the dimensionality of the representation to 50. Each of the 49 highway layers (y-axis) consists of 50 blocks (x-axis). The first column shows the transform gate biases, which were initialized to -2 and -4 respectively. In the second column the mean output of the transform gate over all training examples is depicted. The third and fourth columns show the output of the transform gates and the block outputs (both networks using tanh) for a single random training sample. Best viewed in color.", "b92aa7024b87f50737b372e5df31ef091ab54e62/7-Figure3-1.png": "Figure 3: Visualization showing the extent to which the mean transform gate activity for certain classes differs from the mean activity over all training samples. Generated using the same 50-layer highway networks on MNIST on CIFAR-100 as Figure 2. Best viewed in color.", "b92aa7024b87f50737b372e5df31ef091ab54e62/8-Figure4-1.png": "Figure 4: Lesioned training set performance (y-axis) of the best 50-layer highway networks on MNIST (left) and CIFAR-100 (right), as a function of the lesioned layer (x-axis). Evaluated on the full training set while forcefully closing all the transform gates of a single layer at a time. The non-lesioned performance is indicated as a dashed line at the bottom."}, "referred_figures_tables": [["b92aa7024b87f50737b372e5df31ef091ab54e62/6-Figure2-1.png"], ["b92aa7024b87f50737b372e5df31ef091ab54e62/6-Figure2-1.png"], ["b92aa7024b87f50737b372e5df31ef091ab54e62/3-Figure1-1.png", "b92aa7024b87f50737b372e5df31ef091ab54e62/8-Figure4-1.png"], ["b92aa7024b87f50737b372e5df31ef091ab54e62/3-Figure1-1.png", "b92aa7024b87f50737b372e5df31ef091ab54e62/6-Figure2-1.png", "b92aa7024b87f50737b372e5df31ef091ab54e62/8-Figure4-1.png"]], "question_id": [1, 7, 8, 10], "question": ["What does \"information highways\" mean ?", "The authors claims that the LSTM networks systems allow the flow of information across many layers without attenuation, is that true?", "What are the difference between plain networks and deep highway networks ?", "From the left graph of Figure 1, we observe that even the deepest highway network has same/worse performance than the plain network, so what are the benefits of using the highway networks with deeper layers ?"], "question_section": ["Introduction & Previous work", "Highway networks", "Highway Networks", "Highway networks"], "question_trigger_sentence": ["Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.", "Our primary contribution is to show that extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases.", "We refer to T as the transform gate and C as the carry gate, since they express how much of the output is produced by transforming the input and carrying it, respectively. For simplicity, in this paper we set C = 1 - T,", "Plain networks become much harder to optimize with increasing depth, while highway networks with up to 100 layers can still be optimized well. "], "question_type": ["Testing question", "Shallow question", "Testing question", "Deep/complex question"], "evidential_info": [[{"context": "The last column of Figure 2 displays the block outputs and visualizes the concept of \u201cinformation highways\u201d.Most of the outputs stay constant over many layers forming a pattern of stripes.Most of the change in outputs happens in the early layers (\\approx 15 for MNIST and \\approx 40 for CIFAR-100).", "rationale": "We call such paths information highways."}, {"context": "To overcome this, we take inspiration from Long Short Term Memory (LSTM) recurrent networks [29, 30]. We propose to modify the architecture of very deep feedforward networks such that information flow across layers becomes much easier. This is accomplished through an LSTM-inspired adaptive gating mechanism that allows for computation paths along which information can flow across many layers without attenuation. We call such paths information highways. They yield highway networks, as opposed to traditional \u2018plain\u2019 networks.111This paper expands upon a shorter report on Highway Networks [31]. More recently, a similar LSTM-inspired model was also proposed [32].", "rationale": "Most of the outputs stay constant over many layers forming a pattern of stripes."}], [{"context": "The last column of Figure 2 displays the block outputs and visualizes the concept of \u201cinformation highways\u201d.Most of the outputs stay constant over many layers forming a pattern of stripes.Most of the change in outputs happens in the early layers (\\approx 15 for MNIST and \\approx 40 for CIFAR-100).", "rationale": "This is accomplished through an LSTM-inspired adaptive gating mechanism that allows for computation paths along which information can flow across many layers without attenuation."}, {"context": "To overcome this, we take inspiration from Long Short Term Memory (LSTM) recurrent networks [29, 30]. We propose to modify the architecture of very deep feedforward networks such that information flow across layers becomes much easier. This is accomplished through an LSTM-inspired adaptive gating mechanism that allows for computation paths along which information can flow across many layers without attenuation. We call such paths information highways. They yield highway networks, as opposed to traditional \u2018plain\u2019 networks.111This paper expands upon a shorter report on Highway Networks [31]. More recently, a similar LSTM-inspired model was also proposed [32].", "rationale": "Most of the outputs stay constant over many layers forming a pattern of stripes."}], [{"context": "Thus, depending on the output of the transform gates, a highway layer can smoothly vary its behavior between that of H and that of a layer which simply passes its inputs through. Just as a plain layer consists of multiple computing units such that the i^{th} unit computes y_{i}=H_{i}(\\mathbf{x}), a highway network consists of multiple blocks such that the i^{th} block computes a block state H_{i}(\\mathbf{x}) and transform gate output T_{i}(\\mathbf{x}). Finally, it produces the block output y_{i}=H_{i}(\\mathbf{x})*T_{i}(\\mathbf{x})+x_{i}*(1-T_{i}(\\mathbf{x})), which is connected to the next layer.222Our pilot experiments on training very deep networks were successful with a more complex block design closely resembling an LSTM block \u201cunrolled in time\u201d. Here we report results only for a much simplified form.", "rationale": "Highway networks do not suffer from an increase in depth, and 50/100 layer highway networks perform similar to 10/20 layer networks."}, {"context": "To support the hypothesis that highway networks do not suffer from increasing depth, we conducted a series of rigorous optimization experiments, comparing them to plain networks with normalized initialization [16, 17].", "rationale": "To support the hypothesis that highway networks do not suffer from increasing depth, we conducted a series of rigorous optimization experiments, comparing them to plain networks with normalized initialization"}, {"context": "The training curves for the best performing networks for each depth are shown in Figure 1. As expected, 10 and 20-layer plain networks exhibit very good performance (mean loss <1e^{-4}), which significantly degrades as depth increases, even though network capacity increases.Highway networks do not suffer from an increase in depth, and 50/100 layer highway networks perform similar to 10/20 layer networks. The 100-layer highway network performed more than 2 orders of magnitude better compared to a similarly-sized plain network.It was also observed that highway networks consistently converged significantly faster than plain ones.", "rationale": "Our primary contribution is to show that extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases (Section 3.1)."}, {"context": "A possible objection is that many layers might remain unused if the transform gates stay closed. Our experiments show that this possibility does not affect networks adversely\u2014deep and narrow highway networks can match/exceed the accuracy of wide and shallow maxout networks, which would not be possible if layers did not perform useful computations. Additionally, we can exploit the structure of highways to directly evaluate the contribution of each layer as shown in Figure 4. For the first time, highway networks allow us to examine how much computation depth is needed for a given problem, which can not be easily done with plain networks.", "rationale": "For the first time, highway networks allow us to examine how much computation depth is needed for a given problem, which can not be easily done with plain networks."}, {"context": "Our primary contribution is to show that extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases (Section 3.1).Deep networks with limited computational budget (for which a two-stage training procedure mentioned above was recently proposed [25]) can also be directly trained in a single stage when converted to highway networks. Their ease of training is supported by experimental results demonstrating that highway networks also generalize well to unseen data.", "rationale": "Just as a plain layer consists of multiple computing units such that the i^{th} unit computes y_{i}=H_{i}(\\mathbf{x}), a highway network consists of multiple blocks such that the i^{th} block computes a block state H_{i}(\\mathbf{x}) and transform gate output T_{i}(\\mathbf{x})."}], [{"context": "The training curves for the best performing networks for each depth are shown in Figure 1. As expected, 10 and 20-layer plain networks exhibit very good performance (mean loss <1e^{-4}), which significantly degrades as depth increases, even though network capacity increases.Highway networks do not suffer from an increase in depth, and 50/100 layer highway networks perform similar to 10/20 layer networks. The 100-layer highway network performed more than 2 orders of magnitude better compared to a similarly-sized plain network.It was also observed that highway networks consistently converged significantly faster than plain ones.", "rationale": "As expected, 10 and 20-layer plain networks exhibit very good performance (mean loss <1e^{-4}), which significantly degrades as depth increases, even though network capacity increases.Highway networks do not suffer from an increase in depth, and 50/100 layer highway networks perform similar to 10/20 layer networks."}, {"context": "One possible advantage of the highway architecture over hard-wired shortcut connections is that the network can learn to dynamically adjust the routing of the information based on the current input.This begs the question: does this behaviour manifest itself in trained networks or do they just learn a static routing that applies to all inputs similarly.A partial answer can be found by looking at the mean transform gate activity (second column) and the single example transform gate outputs (third column) in Figure\u00a02.Especially for the CIFAR-100 case, most transform gates are active on average, while they show very selective activity for the single example.This implies that for each sample only a few blocks perform transformation but different blocks are utilized by different samples.", "rationale": "Figure 4. For the first time, highway networks allow us to examine how much computation depth is needed for a given problem, which can not be easily done with plain networks."}, {"context": "We see a different picture for the CIFAR-100 dataset (right) with performance degrading noticeably when removing any of the first \\approx 40 layers.This suggests that for complex problems a highway network can learn to utilize all of its layers, while for simpler problems like MNIST it will keep many of the unneeded layers idle. Such behavior is desirable for deep networks in general, but appears difficult to obtain using plain networks.", "rationale": "extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases"}, {"context": "Very deep highway networks, on the other hand, can directly be trained with simple gradient descent methods due to their specific architecture. This property does not rely on specific non-linear transformations, which may be complex convolutional or recurrent transforms, and derivation of a suitable initialization scheme is not essential.The additional parameters required by the gating mechanism help in routing information through the use of multiplicative connections, responding differently to different inputs, unlike fixed \u201cskip\u201d connections.", "rationale": "This suggests that for complex problems a highway network can learn to utilize all of its layers"}, {"context": "A possible objection is that many layers might remain unused if the transform gates stay closed. Our experiments show that this possibility does not affect networks adversely\u2014deep and narrow highway networks can match/exceed the accuracy of wide and shallow maxout networks, which would not be possible if layers did not perform useful computations. Additionally, we can exploit the structure of highways to directly evaluate the contribution of each layer as shown in Figure 4. For the first time, highway networks allow us to examine how much computation depth is needed for a given problem, which can not be easily done with plain networks.", "rationale": "This property does not rely on specific non-linear transformations, which may be complex convolutional or recurrent transforms, and derivation of a suitable initialization scheme is not essential."}, {"context": "Our primary contribution is to show that extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases (Section 3.1).Deep networks with limited computational budget (for which a two-stage training procedure mentioned above was recently proposed [25]) can also be directly trained in a single stage when converted to highway networks. Their ease of training is supported by experimental results demonstrating that highway networks also generalize well to unseen data.", "rationale": "One possible advantage of the highway architecture over hard-wired shortcut connections is that the network can learn to dynamically adjust the routing of the information based on the current input."}]], "composition": ["'information highways' means that some information is not lost while passing through the layer.", "Inspired by LSTM, the authors designed an information highway that adaptively passes information back, which is effective when there are many layers, so LSTM is also effective for many layers.", "A highway network is a layer that uses an information highway layer, and a plain network is a general layer. In highway networks, increasing layer depth does not affect performance, but in plain networks, it can. One layer of the plain network is made up of normal computation units, whereas the highway network is made up of block units.", "Although highway networks do not perform well at best, they do not break down significantly when stacked deeply. Also, there is freedom in setting the number of depths, and it can be learned well with vanilla SGD. In addition, meaningful outputs come out from all layers and information can be handed over dynamically."], "Is_figure_in_evidence": [true, true, true, true], "Is_table_in_evidence": [false, false, false, true], "question_key": ["215", "217", "218", "220"], "passages": ["Many recent empirical breakthroughs in supervised machine learning have been achieved through large and deep neural networks. Network depth (the number of successive computational layers) has played perhaps the most important role in these successes. For instance, within just a few years, the top-5 image classification accuracy on the 1000-class ImageNet dataset has increased from \u223csimilar-to\\sim\u223c84% [1] to \u223csimilar-to\\sim\u223c95% [2, 3] using deeper networks with rather small receptive fields [4, 5].Other results on practical machine learning problems have also underscored the superiority of deeper networks [6] in terms of accuracy and/or performance.", "In fact, deep networks can represent certain function classes far more efficiently than shallow ones. This is perhaps most obvious for recurrent nets, the deepest of them all. For example, the n\ud835\udc5bnitalic_n bit parity problem can in principle be learned by a large feedforward net with n\ud835\udc5bnitalic_n binary input units, 1 output unit, and a single but large hidden layer. But the natural solution for arbitrary n\ud835\udc5bnitalic_n is a recurrent net with only 3 units and 5 weights, reading the input bit string one bit at a time, making a single recurrent hidden unit flip its state whenever a new 1 is observed [7].Related observations hold for Boolean circuits [8, 9] and modern neural networks [10, 11, 12].", "To deal with the difficulties of training deep networks, some researchers have focused on developing better optimizers (e.g. [13, 14, 15]). Well-designed initialization strategies, in particular the normalized variance-preserving initialization for certain activation functions [16, 17], have been widely adopted for training moderately deep networks. Other similarly motivated strategies have shown promising results in preliminary experiments [18, 19].Experiments showed that certain activation functions based on local competition [20, 21] may help to train deeper networks.Skip connections between layers or to output layers (where error is \u201cinjected\u201d) have long been used in neural networks, more recently with the explicit aim to improve the flow of information [22, 23, 2, 24].A related recent technique is based on using soft targets from a shallow teacher network to aid in training deeper student networks in multiple stages [25], similar to the neural history compressor for sequences, where a slowly ticking teacher recurrent net is \u201cdistilled\u201d into a quickly ticking student recurrent net by forcing the latter to predict the hidden units of the former [26]. Finally, deep networks can be trained layer-wise to help in credit assignment [26, 27], but this approach is less attractive compared to direct training.", "Very deep network training still faces problems, albeit perhaps less fundamental ones than the problem of vanishing gradients in standard recurrent networks [28]. The stacking of several non-linear transformations in conventional feed-forward network architectures typically results in poor propagation of activations and gradients. Hence it remains hard to investigate the benefits of very deep networks for a variety of problems.", "To overcome this, we take inspiration from Long Short Term Memory (LSTM) recurrent networks [29, 30]. We propose to modify the architecture of very deep feedforward networks such that information flow across layers becomes much easier. This is accomplished through an LSTM-inspired adaptive gating mechanism that allows for computation paths along which information can flow across many layers without attenuation. We call such paths information highways. They yield highway networks, as opposed to traditional \u2018plain\u2019 networks.111This paper expands upon a shorter report on Highway Networks [31]. More recently, a similar LSTM-inspired model was also proposed [32].", "Our primary contribution is to show that extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases (Section 3.1).Deep networks with limited computational budget (for which a two-stage training procedure mentioned above was recently proposed [25]) can also be directly trained in a single stage when converted to highway networks. Their ease of training is supported by experimental results demonstrating that highway networks also generalize well to unseen data.", "We use boldface letters for vectors and matrices, and italicized capital letters to denote transformation functions. \ud835\udfce0\\mathbf{0}bold_0 and \ud835\udfcf1\\mathbf{1}bold_1 denote vectors of zeros and ones respectively, and \ud835\udc08\ud835\udc08\\mathbf{I}bold_I denotes an identity matrix. The function \u03c3(x)\ud835\udf0e\ud835\udc65\\sigma(x)italic_\u03c3 ( italic_x ) is defined as \u03c3(x)=11+e\u2212x,x\u2208\u211dformulae-sequence\ud835\udf0e\ud835\udc6511superscript\ud835\udc52\ud835\udc65\ud835\udc65\u211d\\sigma(x)=\\frac{1}{1+e^{-x}},x\\in\\mathbb{R}italic_\u03c3 ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT end_ARG , italic_x \u2208 blackboard_R. The dot operator (\u22c5\u22c5\\cdotp\u22c5) is used to denote element-wise multiplication.", "A plain feedforward neural network typically consists of L\ud835\udc3fLitalic_L layers where the lthsuperscript\ud835\udc59\ud835\udc61\u210el^{th}italic_l start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer (l\u2208{1,2,\u2026,L}\ud835\udc5912\u2026\ud835\udc3fl\\in\\{1,2,...,L\\}italic_l \u2208 { 1 , 2 , \u2026 , italic_L }) applies a non-linear transformation H\ud835\udc3bHitalic_H (parameterized by \ud835\udc16\ud835\udc07,\ud835\udc25subscript\ud835\udc16\ud835\udc07\ud835\udc25\\mathbf{W_{H,l}}bold_W start_POSTSUBSCRIPT bold_H , bold_l end_POSTSUBSCRIPT) on its input \ud835\udc31\ud835\udc25subscript\ud835\udc31\ud835\udc25\\mathbf{x_{l}}bold_x start_POSTSUBSCRIPT bold_l end_POSTSUBSCRIPT to produce its output \ud835\udc32\ud835\udc25subscript\ud835\udc32\ud835\udc25\\mathbf{y_{l}}bold_y start_POSTSUBSCRIPT bold_l end_POSTSUBSCRIPT. Thus, \ud835\udc31\ud835\udfcfsubscript\ud835\udc311\\mathbf{x_{1}}bold_x start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT is the input to the network and \ud835\udc32\ud835\udc0bsubscript\ud835\udc32\ud835\udc0b\\mathbf{y_{L}}bold_y start_POSTSUBSCRIPT bold_L end_POSTSUBSCRIPT is the network\u2019s output. Omitting the layer index and biases for clarity,", "\ud835\udc32=H(\ud835\udc31,\ud835\udc16\ud835\udc07).\ud835\udc32\ud835\udc3b\ud835\udc31subscript\ud835\udc16\ud835\udc07\\mathbf{y}=H(\\mathbf{x},\\mathbf{W_{H}}).bold_y = italic_H ( bold_x , bold_W start_POSTSUBSCRIPT bold_H end_POSTSUBSCRIPT ) .(1)", "H\ud835\udc3bHitalic_H is usually an affine transform followed by a non-linear activation function, but in general it may take other forms, possibly convolutional or recurrent. For a highway network, we additionally define two non-linear transforms T(\ud835\udc31,\ud835\udc16\ud835\udc13)\ud835\udc47\ud835\udc31subscript\ud835\udc16\ud835\udc13T(\\mathbf{x},\\mathbf{W_{T}})italic_T ( bold_x , bold_W start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT ) and C(\ud835\udc31,\ud835\udc16\ud835\udc02)\ud835\udc36\ud835\udc31subscript\ud835\udc16\ud835\udc02C(\\mathbf{x},\\mathbf{W_{C}})italic_C ( bold_x , bold_W start_POSTSUBSCRIPT bold_C end_POSTSUBSCRIPT ) such that", "\ud835\udc32=H(\ud835\udc31,\ud835\udc16\ud835\udc07)\u22c5T(\ud835\udc31,\ud835\udc16\ud835\udc13)+\ud835\udc31\u22c5C(\ud835\udc31,\ud835\udc16\ud835\udc02).\ud835\udc32\u22c5\ud835\udc3b\ud835\udc31subscript\ud835\udc16\ud835\udc07\ud835\udc47\ud835\udc31subscript\ud835\udc16\ud835\udc13\u22c5\ud835\udc31\ud835\udc36\ud835\udc31subscript\ud835\udc16\ud835\udc02\\mathbf{y}=H(\\mathbf{x},\\mathbf{W_{H}})\\cdotp T(\\mathbf{x},\\mathbf{W_{T}})+\\mathbf{x}\\cdot C(\\mathbf{x},\\mathbf{W_{C}}).bold_y = italic_H ( bold_x , bold_W start_POSTSUBSCRIPT bold_H end_POSTSUBSCRIPT ) \u22c5 italic_T ( bold_x , bold_W start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT ) + bold_x \u22c5 italic_C ( bold_x , bold_W start_POSTSUBSCRIPT bold_C end_POSTSUBSCRIPT ) .(2)", "We refer to T\ud835\udc47Titalic_T as the transform gate and C\ud835\udc36Citalic_C as the carry gate, since they express how much of the output is produced by transforming the input and carrying it, respectively. For simplicity, in this paper we set C=1\u2212T\ud835\udc361\ud835\udc47C=1-Titalic_C = 1 - italic_T, giving", "\ud835\udc32=H(\ud835\udc31,\ud835\udc16\ud835\udc07)\u22c5T(\ud835\udc31,\ud835\udc16\ud835\udc13)+\ud835\udc31\u22c5(1\u2212T(\ud835\udc31,\ud835\udc16\ud835\udc13)).\ud835\udc32\u22c5\ud835\udc3b\ud835\udc31subscript\ud835\udc16\ud835\udc07\ud835\udc47\ud835\udc31subscript\ud835\udc16\ud835\udc13\u22c5\ud835\udc311\ud835\udc47\ud835\udc31subscript\ud835\udc16\ud835\udc13\\mathbf{y}=H(\\mathbf{x},\\mathbf{W_{H}})\\cdotp T(\\mathbf{x},\\mathbf{W_{T}})+\\mathbf{x}\\cdot(1-T(\\mathbf{x},\\mathbf{W_{T}})).bold_y = italic_H ( bold_x , bold_W start_POSTSUBSCRIPT bold_H end_POSTSUBSCRIPT ) \u22c5 italic_T ( bold_x , bold_W start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT ) + bold_x \u22c5 ( 1 - italic_T ( bold_x , bold_W start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT ) ) .(3)", "The dimensionality of \ud835\udc31,\ud835\udc32,H(\ud835\udc31,\ud835\udc16\ud835\udc07)\ud835\udc31\ud835\udc32\ud835\udc3b\ud835\udc31subscript\ud835\udc16\ud835\udc07\\mathbf{x},\\mathbf{y},H(\\mathbf{x},\\mathbf{W_{H}})bold_x , bold_y , italic_H ( bold_x , bold_W start_POSTSUBSCRIPT bold_H end_POSTSUBSCRIPT ) and T(\ud835\udc31,\ud835\udc16\ud835\udc13)\ud835\udc47\ud835\udc31subscript\ud835\udc16\ud835\udc13T(\\mathbf{x},\\mathbf{W_{T}})italic_T ( bold_x , bold_W start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT ) must be the same for Equation 3 to be valid.Note that this layer transformation is much more flexible than Equation 1.In particular, observe that for particular values of T\ud835\udc47Titalic_T,", "\ud835\udc32={\ud835\udc31,if\u00a0T(\ud835\udc31,\ud835\udc16\ud835\udc13)=\ud835\udfce,H(\ud835\udc31,\ud835\udc16\ud835\udc07),if\u00a0T(\ud835\udc31,\ud835\udc16\ud835\udc13)=\ud835\udfcf.\ud835\udc32cases\ud835\udc31if\u00a0\ud835\udc47\ud835\udc31subscript\ud835\udc16\ud835\udc130\ud835\udc3b\ud835\udc31subscript\ud835\udc16\ud835\udc07if\u00a0\ud835\udc47\ud835\udc31subscript\ud835\udc16\ud835\udc131\\mathbf{y}=\\begin{cases}\\mathbf{x},&\\text{if }T(\\mathbf{x},\\mathbf{W_{T}})=\\mathbf{0},\\\\H(\\mathbf{x},\\mathbf{W_{H}}),&\\text{if }T(\\mathbf{x},\\mathbf{W_{T}})=\\mathbf{1}.\\end{cases}bold_y = { start_ROW start_CELL bold_x , end_CELL start_CELL if italic_T ( bold_x , bold_W start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT ) = bold_0 , end_CELL end_ROW start_ROW start_CELL italic_H ( bold_x , bold_W start_POSTSUBSCRIPT bold_H end_POSTSUBSCRIPT ) , end_CELL start_CELL if italic_T ( bold_x , bold_W start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT ) = bold_1 . end_CELL end_ROW(4)", "Similarly, for the Jacobian of the layer transform,", "d\ud835\udc32d\ud835\udc31={\ud835\udc08,if\u00a0T(\ud835\udc31,\ud835\udc16\ud835\udc13)=\ud835\udfce,H\u2032(\ud835\udc31,\ud835\udc16\ud835\udc07),if\u00a0T(\ud835\udc31,\ud835\udc16\ud835\udc13)=\ud835\udfcf.\ud835\udc51\ud835\udc32\ud835\udc51\ud835\udc31cases\ud835\udc08if\u00a0\ud835\udc47\ud835\udc31subscript\ud835\udc16\ud835\udc130superscript\ud835\udc3b\u2032\ud835\udc31subscript\ud835\udc16\ud835\udc07if\u00a0\ud835\udc47\ud835\udc31subscript\ud835\udc16\ud835\udc131\\frac{d\\mathbf{y}}{d\\mathbf{x}}=\\begin{cases}\\mathbf{I},&\\text{if }T(\\mathbf{x},\\mathbf{W_{T}})=\\mathbf{0},\\\\H^{\\prime}(\\mathbf{x},\\mathbf{W_{H}}),&\\text{if }T(\\mathbf{x},\\mathbf{W_{T}})=\\mathbf{1}.\\end{cases}divide start_ARG italic_d bold_y end_ARG start_ARG italic_d bold_x end_ARG = { start_ROW start_CELL bold_I , end_CELL start_CELL if italic_T ( bold_x , bold_W start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT ) = bold_0 , end_CELL end_ROW start_ROW start_CELL italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( bold_x , bold_W start_POSTSUBSCRIPT bold_H end_POSTSUBSCRIPT ) , end_CELL start_CELL if italic_T ( bold_x , bold_W start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT ) = bold_1 . end_CELL end_ROW(5)", "Thus, depending on the output of the transform gates, a highway layer can smoothly vary its behavior between that of H\ud835\udc3bHitalic_H and that of a layer which simply passes its inputs through. Just as a plain layer consists of multiple computing units such that the ithsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT unit computes yi=Hi(\ud835\udc31)subscript\ud835\udc66\ud835\udc56subscript\ud835\udc3b\ud835\udc56\ud835\udc31y_{i}=H_{i}(\\mathbf{x})italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ), a highway network consists of multiple blocks such that the ithsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT block computes a block state Hi(\ud835\udc31)subscript\ud835\udc3b\ud835\udc56\ud835\udc31H_{i}(\\mathbf{x})italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) and transform gate output Ti(\ud835\udc31)subscript\ud835\udc47\ud835\udc56\ud835\udc31T_{i}(\\mathbf{x})italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ). Finally, it produces the block output yi=Hi(\ud835\udc31)*Ti(\ud835\udc31)+xi*(1\u2212Ti(\ud835\udc31))subscript\ud835\udc66\ud835\udc56subscript\ud835\udc3b\ud835\udc56\ud835\udc31subscript\ud835\udc47\ud835\udc56\ud835\udc31subscript\ud835\udc65\ud835\udc561subscript\ud835\udc47\ud835\udc56\ud835\udc31y_{i}=H_{i}(\\mathbf{x})*T_{i}(\\mathbf{x})+x_{i}*(1-T_{i}(\\mathbf{x}))italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) * italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) + italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT * ( 1 - italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) ), which is connected to the next layer.222Our pilot experiments on training very deep networks were successful with a more complex block design closely resembling an LSTM block \u201cunrolled in time\u201d. Here we report results only for a much simplified form.", "As mentioned earlier, Equation 3 requires that the dimensionality of \ud835\udc31,\ud835\udc32,H(\ud835\udc31,\ud835\udc16\ud835\udc07)\ud835\udc31\ud835\udc32\ud835\udc3b\ud835\udc31subscript\ud835\udc16\ud835\udc07\\mathbf{x},\\mathbf{y},H(\\mathbf{x},\\mathbf{W_{H}})bold_x , bold_y , italic_H ( bold_x , bold_W start_POSTSUBSCRIPT bold_H end_POSTSUBSCRIPT ) and T(\ud835\udc31,\ud835\udc16\ud835\udc13)\ud835\udc47\ud835\udc31subscript\ud835\udc16\ud835\udc13T(\\mathbf{x},\\mathbf{W_{T}})italic_T ( bold_x , bold_W start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT ) be the same. To change the size of the intermediate representation, one can replace \ud835\udc31\ud835\udc31\\mathbf{x}bold_x with \ud835\udc31^^\ud835\udc31\\mathbf{\\hat{x}}over^ start_ARG bold_x end_ARG obtained by suitably sub-sampling or zero-padding \ud835\udc31\ud835\udc31\\mathbf{x}bold_x. Another alternative is to use a plain layer (without highways) to change dimensionality, which is the strategy we use in this study.", "Convolutional highway layers utilize weight-sharing and local receptive fields for both H\ud835\udc3bHitalic_H and T\ud835\udc47Titalic_T transforms. We used the same sized receptive fields for both, and zero-padding to ensure that the block state and transform gate feature maps match the input size.", "We use the transform gate defined as T(\ud835\udc31)=\u03c3(\ud835\udc16\ud835\udc13T\ud835\udc31+\ud835\udc1b\ud835\udc13)\ud835\udc47\ud835\udc31\ud835\udf0esuperscriptsubscript\ud835\udc16\ud835\udc13\ud835\udc47\ud835\udc31subscript\ud835\udc1b\ud835\udc13T(\\mathbf{x})=\\sigma(\\mathbf{W_{T}}^{T}\\mathbf{x}+\\mathbf{b_{T}})italic_T ( bold_x ) = italic_\u03c3 ( bold_W start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_x + bold_b start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT ), where \ud835\udc16\ud835\udc13subscript\ud835\udc16\ud835\udc13\\mathbf{W_{T}}bold_W start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT is the weight matrix and \ud835\udc1b\ud835\udc13subscript\ud835\udc1b\ud835\udc13\\mathbf{b_{T}}bold_b start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT the bias vector for the transform gates. This suggests a simple initialization scheme which is independent of the nature of H\ud835\udc3bHitalic_H: bTsubscript\ud835\udc4f\ud835\udc47b_{T}italic_b start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT can be initialized with a negative value (e.g. -1, -3 etc.) such that the network is initially biased towards carry behavior. This scheme is strongly inspired by the proposal [30] to initially bias the gates in an LSTM network, to help bridge long-term temporal dependencies early in learning. Note that \u03c3(x)\u2208(0,1),\u2200x\u2208\u211dformulae-sequence\ud835\udf0e\ud835\udc6501for-all\ud835\udc65\u211d\\sigma(x)\\in(0,1),\\forall x\\in\\mathbb{R}italic_\u03c3 ( italic_x ) \u2208 ( 0 , 1 ) , \u2200 italic_x \u2208 blackboard_R, so the conditions in Equation 4 can never be met exactly.", "In our experiments, we found that a negative bias initialization for the transform gates was sufficient for training to proceed in very deep networks for various zero-mean initial distributions of WHsubscript\ud835\udc4a\ud835\udc3bW_{H}italic_W start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT and different activation functions used by H\ud835\udc3bHitalic_H. In pilot experiments, SGD did not stall for networks with more than 1000 layers. Although the initial bias is best treated as a hyperparameter, as a general guideline we suggest values of -1, -2 and -3 for convolutional highway networks of depth approximately 10, 20 and 30.", "All networks were trained using SGD with momentum. An exponentially decaying learning rate was used in Section 3.1. For the rest of the experiments, a simpler commonly used strategy was employed where the learning rate starts at a value \u03bb\ud835\udf06\\lambdaitalic_\u03bb and decays according to a fixed schedule by a factor \u03b3\ud835\udefe\\gammaitalic_\u03b3. \u03bb\ud835\udf06\\lambdaitalic_\u03bb, \u03b3\ud835\udefe\\gammaitalic_\u03b3 and the schedule were selected once based on validation set performance on the CIFAR-10 dataset, and kept fixed for all experiments.All convolutional highway networks utilize the rectified linear activation function [16] to compute the block state H\ud835\udc3bHitalic_H. To provide a better estimate of the variability of classification results due to random initialization, we report our results in the format Best (mean \u00b1plus-or-minus\\pm\u00b1 std.dev.) based on 5 runs wherever available. Experiments were conducted using Caffe [33] and Brainstorm (https://github.com/IDSIA/brainstorm) frameworks. Source code, hyperparameter search results and related scripts are publicly available at http://people.idsia.ch/~rupesh/very_deep_learning/.", "To support the hypothesis that highway networks do not suffer from increasing depth, we conducted a series of rigorous optimization experiments, comparing them to plain networks with normalized initialization [16, 17].", "We trained both plain and highway networks of varying varying depths on the MNIST digit classification dataset.All networks are thin: each layer has 50 blocks for highway networks and 71 units for plain networks, yielding roughly identical numbers of parameters (\u2248\\approx\u22485000) per layer.In all networks, the first layer is a fully connected plain layer followed by 9, 19, 49, or 99 fully connected plain or highway layers. Finally, the network output is produced by a softmax layer.We performed a random search of 100 runs for both plain and highway networks to find good settings for the following hyperparameters: initial learning rate, momentum, learning rate exponential decay factor & activation function (either rectified linear or tanh). For highway networks, an additional hyperparameter was the initial value for the transform gate bias (between -1 and -10). Other weights were initialized using the same normalized initialization as plain networks.", "The training curves for the best performing networks for each depth are shown in Figure 1. As expected, 10 and 20-layer plain networks exhibit very good performance (mean loss <1e\u22124absent1superscript\ud835\udc524<1e^{-4}< 1 italic_e start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT), which significantly degrades as depth increases, even though network capacity increases.Highway networks do not suffer from an increase in depth, and 50/100 layer highway networks perform similar to 10/20 layer networks. The 100-layer highway network performed more than 2 orders of magnitude better compared to a similarly-sized plain network.It was also observed that highway networks consistently converged significantly faster than plain ones.", "As a sanity check for the generalization capability of highway networks, we trained 10-layer convolutional highway networks on MNIST, using two architectures, each with 9 convolutional layers followed by a softmax output. The number of filter maps (width) was set to 16 and 32 for all the layers. We obtained test set performance competitive with state-of-the-art methods with much fewer parameters, as show in Table\u00a01.", "Maxout networks can cope much better with increased depth than those with traditional activation functions [20]. However, Romero et. al. [25] recently reported that training on CIFAR-10 through plain backpropogation was only possible for maxout networks with a depth up to 5 layers when the number of parameters was limited to \u223csimilar-to\\sim\u223c250K and the number of multiplications to \u223csimilar-to\\sim\u223c30M. Similar limitations were observed for higher computational budgets. Training of deeper networks was only possible through the use of a two-stage training procedure and addition of soft targets produced from a pre-trained shallow teacher network (hint-based training).", "We found that it was easy to train highway networks with numbers of parameters and operations comparable to those of fitnets in a single stage using SGD. As shown in Table 2, Highway A and Highway B, which are based on the architectures of Fitnet A and Fitnet B, respectively, obtain similar or higher accuracy on the test set. We were also able to train thinner and deeper networks: for example a 32-layer highway network consisting of alternating receptive fields of size 3x3 and 1x1 with \u223csimilar-to\\sim\u223c1.25M parameters performs better than the earlier teacher network [20].", "It is possible to obtain high performance on the CIFAR-10 and CIFAR-100 datasets by utilizing very large networks and extensive data augmentation. This approach was popularized by Ciresan et. al. [5] and recently extended by Graham [34]. Since our aim is only to demonstrate that deeper networks can be trained without sacrificing ease of training or generalization ability, we only performed experiments in the more common setting of global contrast normalization, small translations and mirroring of images. Following Lin et. al. [35], we replaced the fully connected layer used in the networks in the previous section with a convolutional layer with a receptive field of size one and a global average pooling layer. The hyperparameters from the last section were re-used for both CIFAR-10 and CIFAR-100, therefore it is quite possible to obtain much better results with better architectures/hyperparameters. The results are tabulated in Table 3.", "Figure 2 illustrates the inner workings of the best333obtained via random search over hyperparameters to minimize the best training set error achieved using each configuration 50 hidden layer fully-connected highway networks trained on MNIST (top row) and CIFAR-100 (bottom row). The first three columns show the bias, the mean activity over all training samples, and the activity for a single random sample for each transform gate respectively. Block outputs for the same single sample are displayed in the last column.", "The transform gate biases of the two networks were initialized to -2 and -4 respectively.It is interesting to note that contrary to our expectations most biases decreased further during training.For the CIFAR-100 network the biases increase with depth forming a gradient.Curiously this gradient is inversely correlated with the average activity of the transform gates, as seen in the second column.This indicates that the strong negative biases at low depths are not used to shut down the gates, but to make them more selective.This behavior is also suggested by the fact that the transform gate activity for a single example (column 3) is very sparse.The effect is more pronounced for the CIFAR-100 network, but can also be observed to a lesser extent in the MNIST network.", "The last column of Figure 2 displays the block outputs and visualizes the concept of \u201cinformation highways\u201d.Most of the outputs stay constant over many layers forming a pattern of stripes.Most of the change in outputs happens in the early layers (\u224815absent15\\approx 15\u2248 15 for MNIST and \u224840absent40\\approx 40\u2248 40 for CIFAR-100).", "One possible advantage of the highway architecture over hard-wired shortcut connections is that the network can learn to dynamically adjust the routing of the information based on the current input.This begs the question: does this behaviour manifest itself in trained networks or do they just learn a static routing that applies to all inputs similarly.A partial answer can be found by looking at the mean transform gate activity (second column) and the single example transform gate outputs (third column) in Figure\u00a02.Especially for the CIFAR-100 case, most transform gates are active on average, while they show very selective activity for the single example.This implies that for each sample only a few blocks perform transformation but different blocks are utilized by different samples.", "This data-dependent routing mechanism is further investigated in Figure 3.In each of the columns we show how the average over all samples of one specific class differs from the total average shown in the second column of Figure 2.For MNIST digits 0 and 7 substantial differences can be seen within the first 15 layers, while for CIFAR class numbers 0 and 1 the differences are sparser and spread out over all layers.In both cases it is clear that the mean activity pattern differs between classes.The gating system acts not just as a mechanism to ease training, but also as an important part of the computation in a trained network.", "Since we bias all the transform gates towards being closed, in the beginning every layer mostly copies the activations of the previous layer.Does training indeed change this behaviour, or is the final network still essentially equivalent to a network with a much fewer layers?To shed light on this issue, we investigated the extent to which lesioning a single layer affects the total performance of trained networks from Section 3.1. By lesioning, we mean manually setting all the transform gates of a layer to 0 forcing it to simply copy its inputs.For each layer, we evaluated the network on the full training set with the gates of that layer closed. The resulting performance as a function of the lesioned layer is shown in Figure 4.", "For MNIST (left) it can be seen that the error rises significantly if any one of the early layers is removed, but layers 15\u221245154515-4515 - 45 seem to have close to no effect on the final performance. About 60% of the layers don\u2019t learn to contribute to the final result, likely because MNIST is a simple dataset that doesn\u2019t require much depth.", "We see a different picture for the CIFAR-100 dataset (right) with performance degrading noticeably when removing any of the first \u224840absent40\\approx 40\u2248 40 layers.This suggests that for complex problems a highway network can learn to utilize all of its layers, while for simpler problems like MNIST it will keep many of the unneeded layers idle. Such behavior is desirable for deep networks in general, but appears difficult to obtain using plain networks.", "Alternative approaches to counter the difficulties posed by depth mentioned in Section 1 often have several limitations. Learning to route information through neural networks with the help of competitive interactions has helped to scale up their application to challenging problems by improving credit assignment [38], but they still suffer when depth increases beyond \u2248\\approx\u224820 even with careful initialization [17]. Effective initialization methods can be difficult to derive for a variety of activation functions. Deep supervision [24] has been shown to hurt performance of thin deep networks [25].", "Very deep highway networks, on the other hand, can directly be trained with simple gradient descent methods due to their specific architecture. This property does not rely on specific non-linear transformations, which may be complex convolutional or recurrent transforms, and derivation of a suitable initialization scheme is not essential.The additional parameters required by the gating mechanism help in routing information through the use of multiplicative connections, responding differently to different inputs, unlike fixed \u201cskip\u201d connections.", "A possible objection is that many layers might remain unused if the transform gates stay closed. Our experiments show that this possibility does not affect networks adversely\u2014deep and narrow highway networks can match/exceed the accuracy of wide and shallow maxout networks, which would not be possible if layers did not perform useful computations. Additionally, we can exploit the structure of highways to directly evaluate the contribution of each layer as shown in Figure 4. For the first time, highway networks allow us to examine how much computation depth is needed for a given problem, which can not be easily done with plain networks.", "We thank NVIDIA Corporation for their donation of GPUs and acknowledge funding from the EU project NASCENCE (FP7-ICT-317662). We are grateful to Sepp Hochreiter and Thomas Unterthiner for helpful comments and Jan Koutn\u00edk for help in conducting experiments."], "figure_types": {"b92aa7024b87f50737b372e5df31ef091ab54e62/3-Figure1-1.png": "plot", "b92aa7024b87f50737b372e5df31ef091ab54e62/4-Table1-1.png": "table", "b92aa7024b87f50737b372e5df31ef091ab54e62/4-Table2-1.png": "table", "b92aa7024b87f50737b372e5df31ef091ab54e62/5-Table3-1.png": "table", "b92aa7024b87f50737b372e5df31ef091ab54e62/6-Figure2-1.png": "plot", "b92aa7024b87f50737b372e5df31ef091ab54e62/7-Figure3-1.png": "plot", "b92aa7024b87f50737b372e5df31ef091ab54e62/8-Figure4-1.png": "plot"}}, "1704.06904": {"paper_id": "paper_123", "title": "Residual Attention Network for Image Classification", "arxiv_url": "https://arxiv.org/abs/1704.06904", "s2orc_url": "https://www.semanticscholar.org/paper/77d30cf9a34fb6b50979c6a68863099da9a060ad", "all_figures_tables": {"77d30cf9a34fb6b50979c6a68863099da9a060ad/2-Figure1-1.png": "Figure 1: Left: an example shows the interaction between features and attention masks. Right: example images illustrating that different features have different corresponding attention masks in our network. The sky mask diminishes low-level background blue color features. The balloon instance mask highlights high-level balloon bottom part features.", "77d30cf9a34fb6b50979c6a68863099da9a060ad/4-Figure2-1.png": "Figure 2: Example architecture of the proposed network for ImageNet. We use three hyper-parameters for the design of Attention Module: p, t and r. The hyper-parameter p denotes the number of pre-processing Residual Units before splitting into trunk branch and mask branch. t denotes the number of Residual Units in trunk branch. r denotes the number of Residual Units between adjacent pooling layer in the mask branch. In our experiments, we use the following hyper-parameters setting: {p = 1, t = 2, r = 1}. The number of channels in the soft mask Residual Unit and corresponding trunk branches is the same.", "77d30cf9a34fb6b50979c6a68863099da9a060ad/5-Figure3-1.png": "Figure 3: The receptive field comparison between mask branch and trunk branch.", "77d30cf9a34fb6b50979c6a68863099da9a060ad/5-Table1-1.png": "Table 1: The test error (%) on CIFAR-10 of Attention-56 network with different activation functions.", "77d30cf9a34fb6b50979c6a68863099da9a060ad/5-Table2-1.png": "Table 2: Residual Attention Network architecture details for ImageNet. Attention structure is described in Fig. 2. We make the size of the smallest output map in each mask branch 7\u00d77 to be consistent with the smallest trunk output map size. Thus 3,2,1 max-pooling layers are used in mask branch with input size 56\u00d756, 28\u00d728, 14\u00d714 respectively. The Attention Module is built by pre-activation Residual Unit [11] with the number of channels in each stage is the same as ResNet [10].", "77d30cf9a34fb6b50979c6a68863099da9a060ad/6-Figure4-1.png": "Figure 4: The mean absolute response of output features in each stage.", "77d30cf9a34fb6b50979c6a68863099da9a060ad/6-Table3-1.png": "Table 3: Classification error (%) on CIAFR-10.", "77d30cf9a34fb6b50979c6a68863099da9a060ad/7-Table4-1.png": "Table 4: Test error (%) on CIFAR-10 using different mask structures.", "77d30cf9a34fb6b50979c6a68863099da9a060ad/7-Table5-1.png": "Table 5: Test error (%) on CIFAR-10 with label noises.", "77d30cf9a34fb6b50979c6a68863099da9a060ad/7-Table6-1.png": "Table 6: Comparisons with state-of-the-art methods on CIFAR-10/100. \u2020: the Attention-452 consists of Attention Module with hyper-parameters setting: {p = 2, t = 4, r = 3} and 6 Attention Modules per stage.", "77d30cf9a34fb6b50979c6a68863099da9a060ad/8-Table7-1.png": "Table 7: Single crop validation error on ImageNet."}, "referred_figures_tables": [["77d30cf9a34fb6b50979c6a68863099da9a060ad/7-Table6-1.png"]], "question_id": [12], "question": ["The authors claims that the  performance increase with the number of attention module, is that true, knowing that they tried only m = {1,2,3,4} ?"], "question_section": ["Experiments"], "question_trigger_sentence": ["To understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.\n"], "question_type": ["Shallow question"], "evidential_info": [[{"context": "Table 6: Comparisons with state-of-the-art methods on CIFAR-10/100. \u2020: the Attention-452 consists of Attention Module with hyper-parameters setting: {p = 2, t = 4, r = 3} and 6 Attention Modules per stage.", "rationale": "They also used m > 4 for Table 6, as seen by the usage of Attention-236 and Attention-452 (which used 6 modules per stage). The results are consistent with their claim."}]], "composition": ["It seems true as they also tried m = 5 and 6 and performance still improved, as seen in Table 6."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["234"], "passages": ["Not only a friendly face but also red color will draw our attention. The mixed nature of attention has been studied extensively in the previous literatures\u00a0[34, 16, 23, 40]. Attention not only serves to select a focused location but also enhances different representations of objects at that location. Previous works formulate attention drift as a sequential process to capture different attended aspects. However, as far as we know, no attention mechanism has been applied to feedforward network structure to achieve state-of-art results in image classification task. Recent advances of image classification focus on training feedforward convolutional neural networks using \u201cvery deep\u201d structure\u00a0[27, 33, 10].", "Inspired by the attention mechanism and recent advances in the deep neural network, we propose Residual Attention Network, a convolutional network that adopts mixed attention mechanism in \u201cvery deep\u201d structure. The Residual Attention Network is composed of multiple Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper.", "Apart from more discriminative feature representation brought by the attention mechanism, our model also exhibits following appealing properties:", "(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.1 shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.", "(2) It is able to incorporate with state-of-the-art deep network structures in an end-to-end training fashion. Specifically, the depth of our network can be easily extended to hundreds of layers. Our Residual Attention Network outperforms state-of-the-art residual networks on CIFAR-10, CIFAR-100 and challenging ImageNet\u00a0[5] image classification dataset with significant reduction of computation (69% forward FLOPs).", "All of the aforementioned properties, which are challenging to achieve with previous approaches, are made possible with following contributions:", "(1) Stacked network structure: Our Residual Attention Network is constructed by stacking multiple Attention Modules. The stacked structure is the basic application of mixed attention mechanism. Thus, different types of attention are able to be captured in different Attention Modules.", "(2) Attention Residual Learning: Stacking Attention Modules directly would lead to the obvious performance drop. Therefore, we propose attention residual learning mechanism to optimize very deep Residual Attention Network with hundreds of layers. ", "(3) Bottom-up top-down feedforward attention: Bottom-up top-down feedforward structure has been successfully applied to human pose estimation\u00a0[24] and image segmentation\u00a0[22, 25, 1]. We use such structure as part of Attention Module to add soft weights on features. This structure can mimic bottom-up fast feedforward process and top-down attention feedback in a single feedforward process which allows us to develop an end-to-end trainable network with top-down attention. The bottom-up top-down structure in our work differs from stacked hourglass network\u00a0[24] in its intention of guiding feature learning.", "Evidence from human perception process\u00a0[23] shows the importance of attention mechanism, which uses top information to guide bottom-up feedforward process. Recently, tentative efforts have been made towards applying attention into deep neural network. Deep Boltzmann Machine (DBM)\u00a0[21] contains top-down attention by its reconstruction process in the training stage. Attention mechanism has also been widely applied to recurrent neural networks (RNN) and long short term memory (LSTM) \u00a0[13] to tackle sequential decision tasks\u00a0[25, 29, 21, 18]. Top information is gathered sequentially and decides where to attend for the next feature learning steps.", "Residual learning\u00a0[10] is proposed to learn residual of identity mapping. This technique greatly increases the depth of feedforward neuron network. Similar to our work, \u00a0[25, 29, 21, 18] use residual learning with attention mechanism to benefit from residual learning. Two information sources (query and query context) are captured using attention mechanism to assist each other in their work. While in our work, a single information source (image) is split into two different ones and combined repeatedly. And residual learning is applied to alleviate the problem brought by repeated splitting and combining.", "In image classification, top-down attention mechanism has been applied using different methods: sequential process, region proposal and control gates. Sequential process \u00a0[23, 12, 37, 7] models image classification as a sequential decision. Thus attention can be applied similarly with above. This formulation allows end-to-end optimization using RNN and LSTM and can capture different kinds of attention in a goal-driven way.", "Region proposal\u00a0[26, 4, 8, 38] has been successfully adopted in image detection task. In image classification, an additional region proposal stage is added before feedforward classification. The proposed regions contain top information and are used for feature learning in the second stage. Unlike image detection whose region proposals rely on large amount of supervision, e.g. the ground truth bounding boxes or detailed segmentation masks\u00a0[6], unsupervised learning\u00a0[35] is usually used to generate region proposals for image classification.", "Control gates have been extensively used in LSTM. In image classification with attention, control gates for neurones are updated with top information and have influence on the feedforward process during training\u00a0[2, 30]. However, a new process, reinforcement learning\u00a0[30] or optimization\u00a0[2] is involved during the training step. Highway Network\u00a0[29] extends control gate to solve gradient degradation problem for deep convolutional neural network.", "However, recent advances of image classification focus on training feedforward convolutional neural networks using \u201cvery deep\u201d structure\u00a0[27, 33, 10]. The feedforward convolutional network mimics the bottom-up paths of human cortex. Various approaches have been proposed to further improve the discriminative ability of deep convolutional neural network. VGG\u00a0[27], Inception\u00a0[33] and residual learning\u00a0[10] are proposed to train very deep neural networks. Stochastic depth\u00a0[14], Batch Normalization\u00a0[15] and Dropout\u00a0[28] exploit regularization for convergence and avoiding overfitting and degradation.", "Soft attention developed in recent work\u00a0[3, 17] can be trained end-to-end for convolutional network. Our Residual Attention Network incorporates the soft attention in fast developing feedforward network structure in an innovative way. Recent proposed spatial transformer module\u00a0[17] achieves state-of-the-art results on house number recognition task. A deep network module capturing top information is used to generate affine transformation. The affine transformation is applied to the input image to get attended region and then feed to another deep network module. The whole process can be trained end-to-end by using differentiable network layer which performs spatial transformation. Attention to scale\u00a0[3] uses soft attention as a scale selection mechanism and gets state-of-the-art results in image segmentation task.", "The design of soft attention structure in our Residual Attention Network is inspired by recent development of localization oriented task, i.e. segmentation\u00a0[22, 25, 1] and human pose estimation\u00a0[24]. These tasks motivate researchers to explore structure with fined-grained feature maps. The frameworks tend to cascade a bottom-up and a top-down structure. The bottom-up feedforward structure produces low resolution feature maps with strong semantic information. After that, a top-down network produces dense features to inference on each pixel. Skip connection\u00a0[22] is employed between bottom and top feature maps and achieved state-of-the-art result on image segmentation. The recent stacked hourglass network\u00a0[24] fuses information from multiple scales to predict human pose, and benefits from encoding both global and local information.", "Our Residual Attention Network is constructed by stacking multiple Attention Modules. Each Attention Module is divided into two branches: mask branch and trunk branch. The trunk branch performs feature processing and can be adapted to any state-of-the-art network structures.In this work, we use pre-activation Residual Unit\u00a0[11], ResNeXt\u00a0[36] and Inception\u00a0[32] as our Residual Attention Networks basic unit to construct Attention Module. Given trunk branch output T(x)\ud835\udc47\ud835\udc65T(x)italic_T ( italic_x ) with input x\ud835\udc65xitalic_x, the mask branch uses bottom-up top-down structure\u00a0[22, 25, 1, 24] to learn same size mask M(x)\ud835\udc40\ud835\udc65M(x)italic_M ( italic_x ) that softly weight output features T(x)\ud835\udc47\ud835\udc65T(x)italic_T ( italic_x ). The bottom-up top-down structure mimics the fast feedforward and feedback attention process. The output mask is used as control gates for neurons of trunk branch similar to Highway Network\u00a0[29]. The output of Attention Module H\ud835\udc3bHitalic_H is:Hi,c(x)=Mi,c(x)*Ti,c(x)subscript\ud835\udc3b\ud835\udc56\ud835\udc50\ud835\udc65subscript\ud835\udc40\ud835\udc56\ud835\udc50\ud835\udc65subscript\ud835\udc47\ud835\udc56\ud835\udc50\ud835\udc65H_{i,c}(x)=M_{i,c}(x)*T_{i,c}(x)italic_H start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ( italic_x ) = italic_M start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ( italic_x ) * italic_T start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ( italic_x )(1)where i ranges over all spatial positions and c\u2208{1,\u2026,C}\ud835\udc501\u2026\ud835\udc36c\\in\\{1,...,C\\}italic_c \u2208 { 1 , \u2026 , italic_C } is the index of the channel. The whole structure can be trained end-to-end.", "In Attention Modules, the attention mask can not only serve as a feature selector during forward inference, but also as a gradient update filter during back propagation. In the soft mask branch, the gradient of mask for input feature is:\u2202M(x,\u03b8)T(x,\u03d5)\u2202\u03d5=M(x,\u03b8)\u2202T(x,\u03d5)\u2202\u03d5\ud835\udc40\ud835\udc65\ud835\udf03\ud835\udc47\ud835\udc65italic-\u03d5italic-\u03d5\ud835\udc40\ud835\udc65\ud835\udf03\ud835\udc47\ud835\udc65italic-\u03d5italic-\u03d5\\frac{\\partial M(x,\\theta)T(x,\\phi)}{\\partial\\phi}=M(x,\\theta)\\frac{\\partial T(x,\\phi)}{\\partial\\phi}divide start_ARG \u2202 italic_M ( italic_x , italic_\u03b8 ) italic_T ( italic_x , italic_\u03d5 ) end_ARG start_ARG \u2202 italic_\u03d5 end_ARG = italic_M ( italic_x , italic_\u03b8 ) divide start_ARG \u2202 italic_T ( italic_x , italic_\u03d5 ) end_ARG start_ARG \u2202 italic_\u03d5 end_ARG(2)where the \u03b8\ud835\udf03\\thetaitalic_\u03b8 are the mask branch parameters and the \u03d5italic-\u03d5\\phiitalic_\u03d5 are the trunk branch parameters. This property makes Attention Modules robust to noisy labels. Mask branches can prevent wrong gradients (from noisy labels) to update trunk parameters. Experiment in Sec.4.1 shows the robustness of our Residual Attention Network against noisy labels.", "Instead of stacking Attention Modules in our design, a simple approach would be using a single network branch to generate soft weight mask, similar to spatial transformer layer\u00a0[17]. However, these methods have several drawbacks on challenging datasets such as ImageNet. First, images with clutter background, complex scenes, and large appearance variations need to be modeled by different types of attentions. In this case, features from different layers need to be modeled by different attention masks. Using a single mask branch would require exponential number of channels to capture all combinations of different factors. Second, a single Attention Module only modify the features once. If the modification fails on some parts of the image, the following network modules do not get a second chance.", "The Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.1, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.", "However, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.", "We propose attention residual learning to ease the above problems. Similar to ideas in residual learning, if soft mask unit can be constructed as identical mapping, the performances should be no worse than its counterpart without attention. Thus we modify output H\ud835\udc3bHitalic_H of Attention Module asHi,c(x)=(1+Mi,c(x))*Fi,c(x)subscript\ud835\udc3b\ud835\udc56\ud835\udc50\ud835\udc651subscript\ud835\udc40\ud835\udc56\ud835\udc50\ud835\udc65subscript\ud835\udc39\ud835\udc56\ud835\udc50\ud835\udc65H_{i,c}(x)=(1+M_{i,c}(x))*F_{i,c}(x)italic_H start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ( italic_x ) = ( 1 + italic_M start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ( italic_x ) ) * italic_F start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ( italic_x )(3)M(x)\ud835\udc40\ud835\udc65M(x)italic_M ( italic_x ) ranges from [0,1]01[0,1][ 0 , 1 ], with M(x)\ud835\udc40\ud835\udc65M(x)italic_M ( italic_x ) approximating 0, H(x)\ud835\udc3b\ud835\udc65H(x)italic_H ( italic_x ) will approximate original features F(x)\ud835\udc39\ud835\udc65F(x)italic_F ( italic_x ). We call this method attention residual learning.Our stacked attention residual learning is different from residual learning. In the origin ResNet, residual learning is formulated as Hi,c(x)=x+Fi,c(x)subscript\ud835\udc3b\ud835\udc56\ud835\udc50\ud835\udc65\ud835\udc65subscript\ud835\udc39\ud835\udc56\ud835\udc50\ud835\udc65H_{i,c}(x)=x+F_{i,c}(x)italic_H start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ( italic_x ) = italic_x + italic_F start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ( italic_x ), where Fi,c(x)subscript\ud835\udc39\ud835\udc56\ud835\udc50\ud835\udc65F_{i,c}(x)italic_F start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ( italic_x ) approximates the residual function. In our formulation, Fi,c(x)subscript\ud835\udc39\ud835\udc56\ud835\udc50\ud835\udc65F_{i,c}(x)italic_F start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ( italic_x ) indicates the features generated by deep convolutional networks. The key lies on our mask branches M(x)\ud835\udc40\ud835\udc65M(x)italic_M ( italic_x ). They work as feature selectors which enhance good features and suppress noises from trunk features.In addition, stacking Attention Modules backs up attention residual learning by its incremental nature. Attention residual learning can keep good properties of original features, but also gives them the ability to bypass soft mask branch and forward to top layers to weaken mask branch\u2019s feature selection ability. Stacked Attention Modules can gradually refine the feature maps. As show in Fig.1, features become much clearer as depth going deeper. By using attention residual learning, increasing depth of the proposed Residual Attention Network can improve performance consistently. As shown in the experiment section, the depth of Residual Attention Network is increased up to 452 whose performance surpasses ResNet-1001 by a large margin on CIFAR dataset.", "Following previous attention mechanism idea in DBN\u00a0[21], our mask branch contains fast feed-forward sweep and top-down feedback steps. The former operation quickly collects global information of the whole image, the latter operation combines global information with original feature maps. In convolutional neural network, the two steps unfold into bottom-up top-down fully convolutional structure.", "From input, max pooling are performed several times to increase the receptive field rapidly after a small number of Residual Units. After reaching the lowest resolution, the global information is then expanded by a symmetrical top-down architecture to guide input features in each position. Linear interpolation up sample the output after some Residual Units. The number of bilinear interpolation is the same as max pooling to keep the output size the same as the input feature map. Then a sigmoid layer normalizes the output range to [0,1]01[0,1][ 0 , 1 ] after two consecutive 1\u00d71111\\times 11 \u00d7 1 convolution layers. We also added skip connections between bottom-up and top-down parts to capture information from different scales. The full module is illustrated in Fig.2.", "The bottom-up top-down structure has been applied to image segmentation and human pose estimation. However, the difference between our structure and the previous one lies in its intention. Our mask branch aims at improving trunk branch features rather than solving a complex problem directly. Experiment in Sec.4.1 is conducted to verify above arguments.", "In our work, attention provided by mask branch changes adaptably with trunk branch features. However, constrains to attention can still be added to mask branch by changing normalization step in activation function before soft mask output. We use three types of activation functions corresponding to mixed attention, channel attention and spatial attention. Mixed attention f1subscript\ud835\udc531f_{1}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT without additional restriction use simple sigmoid for each channel and spatial position. Channel attention f2subscript\ud835\udc532f_{2}italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT performs L2\ud835\udc3f2L2italic_L 2 normalization within all channels for each spatial position to remove spatial information. Spatial attention f3subscript\ud835\udc533f_{3}italic_f start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT performs normalization within feature map from each channel and then sigmoid to get soft mask related to spatial information only.f1(xi,c)=11+exp(\u2212xi,c)subscript\ud835\udc531subscript\ud835\udc65\ud835\udc56\ud835\udc5011\ud835\udc52\ud835\udc65\ud835\udc5dsubscript\ud835\udc65\ud835\udc56\ud835\udc50\\displaystyle f_{1}(x_{i,c})=\\frac{1}{1+exp(-x_{i,c})}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e italic_x italic_p ( - italic_x start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ) end_ARG(4)f2(xi,c)=xi,c\u2016xi\u2016subscript\ud835\udc532subscript\ud835\udc65\ud835\udc56\ud835\udc50subscript\ud835\udc65\ud835\udc56\ud835\udc50normsubscript\ud835\udc65\ud835\udc56\\displaystyle f_{2}(x_{i,c})=\\frac{x_{i,c}}{\\|x_{i}\\|}italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ) = divide start_ARG italic_x start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT end_ARG start_ARG \u2225 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2225 end_ARG(5)f3(xi,c)=11+exp(\u2212(xi,c\u2212meanc)/stdc)subscript\ud835\udc533subscript\ud835\udc65\ud835\udc56\ud835\udc5011\ud835\udc52\ud835\udc65\ud835\udc5dsubscript\ud835\udc65\ud835\udc56\ud835\udc50subscriptmean\ud835\udc50subscriptstd\ud835\udc50\\displaystyle f_{3}(x_{i,c})=\\frac{1}{1+exp(-(x_{i,c}-\\text{mean}_{c})/\\text{std}_{c})}italic_f start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e italic_x italic_p ( - ( italic_x start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT - mean start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) / std start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) end_ARG(6)Where i\ud835\udc56iitalic_i ranges over all spatial positions and c\ud835\udc50citalic_c ranges over all channels. meancsubscriptmean\ud835\udc50\\text{mean}_{c}mean start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT and stdcsubscriptstd\ud835\udc50\\text{std}_{c}std start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT denotes the mean value and standard deviation of feature map from c\ud835\udc50citalic_c-th channel. xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT denotes the feature vector at the i\ud835\udc56iitalic_ith spatial position.", "The experiment results are shown in Table\u00a01, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention\u00a0[3] or spatial attention\u00a0[17], which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.", "In this section, we evaluate the performance of proposed Residual Attention Network on a series of benchmark datasets including CIFAR-10, CIFAR-100\u00a0[19], and ImageNet\u00a0[5].Our experiments contain two parts. In the first part, we analyze the effectiveness of each component in the Residual Attention Network including attention residual learning mechanism and different architectures of soft mask branch in the Attention Module.After that, we explore the noise resistance property. Given limited computation resources, we choose CIFAR-10 and CIFAR-100 dataset to conduct these experiments. Finally, we compare our network with state-of-the-art results in CIFAR dataset.In the second part, we replace the Residual Unit with Inception Module and ResNeXt to demonstrate our Residual Attention Network surpasses origin networks both in parameter efficiency and final performance.We also compare image classification performance with state-of-the-art ResNet and Inception on ImageNet dataset.", "The CIFAR-10 and CIFAR-100 datasets consist of 60,0006000060,00060 , 000 32\u00d732323232\\times 3232 \u00d7 32 color images of 10101010 and 100100100100 classes respectively, with 50,0005000050,00050 , 000 training images and 10,0001000010,00010 , 000 test images.The broadly applied state-of-the-art network structure ResNet is used as baseline method.To conduct fair comparison, we keep most of the settings same as ResNet paper\u00a0[10].The image is padded by 4 pixels on each side, filled with 00 value resulting in 40\u00d740404040\\times 4040 \u00d7 40 image. A 32\u00d732323232\\times 3232 \u00d7 32 crop is randomly sampled from an image or its horizontal flip, with the per-pixel RGB mean value subtracted.We adopt the same weight initialization method following previous study\u00a0[9] and train Residual Attention Network using nesterov SGD with a mini-batch size of 64.We use a weight decay of 0.00010.00010.00010.0001 with a momentum of 0.90.90.90.9 and set the initial learning rate to 0.1. The learning rate is divided by 10 at 64646464k and 96969696k iterations. We terminate training at 160160160160k iterations.", "The overall network architecture and the hyper parameters setting are described in Fig.2.The network consists of 3 stages and similar to ResNet\u00a0[10], equal number of Attention Modules are stacked in each stage.Additionally, we add two Residual Units at each stage. The number of weighted layers in trunk branch is 36m\ud835\udc5amitalic_m+20 where m\ud835\udc5amitalic_m is the number of Attention Module in one stage.We use original 32\u00d732323232\\times 3232 \u00d7 32 image for testing.", "In this experiment, we evaluate the effectiveness of attention residual learning mechanism.Since the notion of attention residual learning (ARL) is new, no suitable previous methods are comparable therefore we use \u201cnaive attention learning\u201d (NAL) as baseline.Specifically, \u201cnaive attention learning\u201d uses Attention Module where features are directly dot product by soft mask without attention residual learning.We set the number of Attention Module in each stage m\ud835\udc5amitalic_m = {1, 2, 3, 4}. For Attention Module, this leads to Attention-56 (named by trunk layer depth), Attention-92, Attention-128 and Attention-164 respectively.", "We train these networks using different mechanisms and summarize the results in the Table\u00a03.As shown in Table\u00a03, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method.The performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with \u201cnaive attention learning\u201d method suffers obvious degradation with increased number of Attention Module.", "To understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.As shown in the Fig.\u00a04, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.The Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.The attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.Therefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.In the rest of the experiments, we apply this technique to train our networks.", "We conduct experiments to validate the effectiveness of encoder-decoder structure by comparing with local convolutions without any down sampling or up sampling. The local convolutions soft mask consists of three Residual Units using the same number of FLOPs.The Attention-56 is used to construct Attention-Encoder-Decoder-56 and Attention-Local-Conv-56 respectively.Results are shown in Table\u00a04.The Attention-Encoder-Decoder-56 network achieves lower test error 5.52%percent5.525.52\\%5.52 % compared with Attention-Local-Conv-56 network 6.48%percent6.486.48\\%6.48 % with a considerable margin 0.94%percent0.940.94\\%0.94 %. The result suggests that the soft attention optimization process will benefit from multi-scale information.", "In this experiment, we show our Residual Attention Network enjoys noise resistant property on CIFAR-10 dataset following the setting of paper\u00a0[31].The confusion matrix Q\ud835\udc44Qitalic_Q in our experiment is set as follows:Q=(r1\u2212r9\u22ef1\u2212r91\u2212r9r\u22ef1\u2212r9\u22ee\u22ee\u22f1\u22ee1\u2212r91\u2212r9\u22efr)10\u00d710\ud835\udc44subscriptmatrix\ud835\udc5f1\ud835\udc5f9\u22ef1\ud835\udc5f91\ud835\udc5f9\ud835\udc5f\u22ef1\ud835\udc5f9\u22ee\u22ee\u22f1\u22ee1\ud835\udc5f91\ud835\udc5f9\u22ef\ud835\udc5f1010Q=\\left(\\begin{matrix}r&\\frac{1-r}{9}&\\cdots&\\frac{1-r}{9}\\\\\\frac{1-r}{9}&r&\\cdots&\\frac{1-r}{9}\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\\\frac{1-r}{9}&\\frac{1-r}{9}&\\cdots&r\\\\\\end{matrix}\\right)_{10\\times 10}italic_Q = ( start_ARG start_ROW start_CELL italic_r end_CELL start_CELL divide start_ARG 1 - italic_r end_ARG start_ARG 9 end_ARG end_CELL start_CELL \u22ef end_CELL start_CELL divide start_ARG 1 - italic_r end_ARG start_ARG 9 end_ARG end_CELL end_ROW start_ROW start_CELL divide start_ARG 1 - italic_r end_ARG start_ARG 9 end_ARG end_CELL start_CELL italic_r end_CELL start_CELL \u22ef end_CELL start_CELL divide start_ARG 1 - italic_r end_ARG start_ARG 9 end_ARG end_CELL end_ROW start_ROW start_CELL \u22ee end_CELL start_CELL \u22ee end_CELL start_CELL \u22f1 end_CELL start_CELL \u22ee end_CELL end_ROW start_ROW start_CELL divide start_ARG 1 - italic_r end_ARG start_ARG 9 end_ARG end_CELL start_CELL divide start_ARG 1 - italic_r end_ARG start_ARG 9 end_ARG end_CELL start_CELL \u22ef end_CELL start_CELL italic_r end_CELL end_ROW end_ARG ) start_POSTSUBSCRIPT 10 \u00d7 10 end_POSTSUBSCRIPT(7)", "where r\ud835\udc5fritalic_r denotes the clean label ratio for the whole dataset.", "We compare ResNet-164 network with Attention-92 network under different noise levels.The Table\u00a05 shows the results.The test error of Attention-92 network is significantly lower than ResNet-164 network with the same noise level.In addition, when we increase the ratio of noise, test error of Attenion-92 declines slowly compared with ResNet-164 network.These results suggest that our Residual Attention Network can perform well even trained with high level noise data.When the label is noisy, the corresponding mask can prevent gradient caused by label error to update trunk branch parameters in the network.In this way, only the trunk branch is learning the wrong supervision information and soft mask branch masks the wrong label.", "We compare our Residual Attention Network with state-of-the-art methods including ResNet\u00a0[11] and Wide ResNet\u00a0[39] on CIFAR-10 and CIFAR-100 datasets.The results are shown in Table\u00a06.Our Attention-452 outperforms all the baseline methods on CIFAR-10 and CIFAR-100 datasets.Note that Attention-92 network achieves 4.99%percent4.994.99\\%4.99 % test error on CIFAR-10 and 21.71%percent21.7121.71\\%21.71 % test error on CIFAR-100 compared with 5.46%percent5.465.46\\%5.46 % and 24.33%percent24.3324.33\\%24.33 % test error on CIFAR-10 and CIFAR-100 for ResNet-164 network under similar parameter size.In addition, Attention-236 outperforms ResNet-1001 using only half of the parameters. It suggests that our Attention Module and attention residual learning scheme can effectively reduce the number of parameters in the network while improving the classification performance.", "In this section, we conduct experiments using ImageNet LSVRC 2012201220122012 dataset\u00a0[5], which contains 1,00010001,0001 , 000 classes with 1.21.21.21.2 million training images, 50,0005000050,00050 , 000 validation images, and 100,000100000100,000100 , 000 test images.The evaluation is measured on the non-blacklist images of the ImageNet LSVRC 2012201220122012 validation set.We use Attention-56 and Attention-92 to conduct the experiments. The network structures and hyper parameters can be found in the Table\u00a02.", "Our implementation generally follows the practice in the previous study\u00a0[20].We apply scale and aspect ratio augmentation\u00a0[33] to the original image.A 224\u00d7224224224224\\times 224224 \u00d7 224 crop is randomly sampled from an augment image or its horizontal flip, with the per-pixel RGB scale to [0,1]01[0,1][ 0 , 1 ] and mean value subtracted and standard variance divided. We adopt standard color augmentation\u00a0[20].The network is trained using SGD with a momentum of 0.90.90.90.9.We set initial learning rate to 0.1. The learning rate is divided by 10 at 200200200200k, 400400400400k, 500500500500k iterations. We terminate training at 530530530530k iterations.", "In this experiment, we explore the efficiency of proposed Residual Attention Network.We compare Attention-56 with ResNet-152\u00a0[10].The ResNet-152 has 50 trunk Residual Units and 60.2\u00d7106absentsuperscript106\\times 10^{6}\u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT parameters compared with 18 trunk Residual Units and 31.9\u00d7106absentsuperscript106\\times 10^{6}\u00d7 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT parameters in Attention-56.We evaluate our model using single crop scheme on the ImageNet validation set and show results in Table\u00a07.The Attention-56 network outperforms ResNet-152 by a large margin with a 0.4%percent0.40.4\\%0.4 % reduction on top-1 error and a 0.26%percent0.260.26\\%0.26 % reduction on top-5 error.More importantly, Attention-56 network achieves better performance with only 52% parameters and 56% FLOPs compared with ResNet-152, which suggests that the proposed attention mechanism can significantly improve network performance while reducing the model complexity.", "In this experiment, we show Residual Attention Network can generalize well using different basic unit. We apply three popular basic units: Residual Unit, ResNeXt\u00a0[36], and Inception\u00a0[32] to construct our Residual Attention Networks. To keep the number of parameters and FLOPs in the same scale, we simplify the Inception. Results are shown in Table\u00a07.", "When the basic unit is ResNeXt, the AttentionNeXt-56 network performance is the same as ResNeXt-101 while the parameters and FLOPs are significantly fewer than ResNeXt-101.For Inception, The AttentionIncepiton-56 outperforms Inception-ResNet-v1\u00a0[32] by a margin with a 0.94% reduction on top-1 error and a 0.21% reduction on top-5 error.The results show that our method can be applied on different network structures.", "We compare our Attention-92 evaluated using single crop on the ILSVRC 2012 validation set with state-of-the-art algorithms.Table\u00a07 shows the results.Our Attention-92 outperforms ResNet-200 with a large margin. The reduction on top-1 error is 0.6%percent0.60.6\\%0.6 %.Note that the ResNet-200 network contains 32%percent3232\\%32 % more parameters than Attention-92.The computational complexity of Attention-92 shown in the Table\u00a07 suggests that our network reduces nearly half training time comparing with ResNet-200 by adding attention mechanism and reducing trunk depth.Above results suggest that our model enjoys high efficiency and good performance.", "We propose a Residual Attention Network which stacks multiple Attention Modules. The benefits of our network are in two folds: it can capture mixed attention and is an extensible convolutional neural network. The first benefit lies in that different Attention Modules capture different types of attention to guide feature learning. Our experiments on the forms of activation function also validate this point: free form mixed attention will have better performance than constrained (including single) attention. The second benefit comes from encoding top-down attention mechanism into bottom-up top-down feedforward convolutional structure in each Attention Module. Thus, the basic Attention Modules can be combined to form larger network structure. Moreover, residual attention learning allows training very deep Residual Attention Network. The performance of our model surpasses state-of-the-art image classification methods, i.e. ResNet on CIFAR-10 (3.90% error), CIFAR-100 (20.67% error), and challenging ImageNet dataset (0.6% top-1 accuracy improvement) with only 46%percent4646\\%46 % trunk depth and 69%percent6969\\%69 % forward FLOPs (comparing with ResNet-200). In the future, we will exploit different applications of deep Residual Attention Network such as detection and segmentation to better explore mixed attention mechanism for specific tasks."], "figure_types": {"77d30cf9a34fb6b50979c6a68863099da9a060ad/2-Figure1-1.png": "schematic", "77d30cf9a34fb6b50979c6a68863099da9a060ad/4-Figure2-1.png": "schematic", "77d30cf9a34fb6b50979c6a68863099da9a060ad/5-Figure3-1.png": "schematic", "77d30cf9a34fb6b50979c6a68863099da9a060ad/5-Table1-1.png": "table", "77d30cf9a34fb6b50979c6a68863099da9a060ad/5-Table2-1.png": "table", "77d30cf9a34fb6b50979c6a68863099da9a060ad/6-Figure4-1.png": "plot", "77d30cf9a34fb6b50979c6a68863099da9a060ad/6-Table3-1.png": "table", "77d30cf9a34fb6b50979c6a68863099da9a060ad/7-Table4-1.png": "table", "77d30cf9a34fb6b50979c6a68863099da9a060ad/7-Table5-1.png": "table", "77d30cf9a34fb6b50979c6a68863099da9a060ad/7-Table6-1.png": "table", "77d30cf9a34fb6b50979c6a68863099da9a060ad/8-Table7-1.png": "table"}}, "1603.05959": {"paper_id": "paper_124", "title": "Efficient multi\u2010scale 3D CNN with fully connected CRF for accurate brain lesion segmentation", "arxiv_url": "https://arxiv.org/abs/1603.05959", "s2orc_url": "https://www.semanticscholar.org/paper/7c2bcf6f32b05a04cd3444c030db743e5666af88", "all_figures_tables": {"7c2bcf6f32b05a04cd3444c030db743e5666af88/10-Table1-1.png": "Table 1 Performance of DeepMedic and an ensemble of three networks on the TBI database. For comparison, we provide results for a Random Forest baseline. Values correspond to the mean (and standard deviation). Numbers in bold indicate significant improvement by the CRF post-processing, according to a two-sided, paired t -test on the DSC metric ( \u2217p &lt; 5 \u00b7 10 \u22122 , \u2217\u2217p &lt; 10 \u22124 ).", "7c2bcf6f32b05a04cd3444c030db743e5666af88/11-Figure10-1.png": "Fig. 10. (Top) DSC achieved by our ensemble of three networks on each of the 61 TBI datasets. (Bottom) Manually segmented (black) and predicted lesion volumes (red). Note here the logarithmic scale. Continuous lines represent mean values. The outlying subject 12 presents small TBI lesions, which are successfully segmented, but also vascular ischemia. Because it is the only case in the database with the latter pathology, the networks fail to segment it as such lesion was not seen during training. (For interpretation of the references to colour, the reader is referred to the web version of this article.)", "7c2bcf6f32b05a04cd3444c030db743e5666af88/11-Figure11-1.png": "Fig. 11. Three examples from the application of our system on the TBI database. It is capable of precise segmentation of both small and large lesions. Second row depicts one of the common mistakes observed. A contusion near the edge of the brain is under-segmented, possibly mistaken for background. Bottom row shows one of the worst cases, representative of the challenges in segmenting TBI. Post-surgical sub-dural debris is mistakenly captured by the brain mask. The network partly segments the abnormality, which is not a celebral lesion of interest.", "7c2bcf6f32b05a04cd3444c030db743e5666af88/12-Table2-1.png": "Table 2 Average performance of our system on the training data of BRATS 2015 as computed on the online evaluation platform and comparison to other submissions visible at the time of manuscript submission. Presenting only teams that submitted more than half of the 274 cases. Numbers in bold indicate significant improvement by the CRF, according to a two-sided, paired t -test on the DSC metric ( \u2217p &lt; 5 \u00b7 10 \u22122 , \u2217\u2217p &lt; 10 \u22123 ).", "7c2bcf6f32b05a04cd3444c030db743e5666af88/13-Table3-1.png": "Table 3 Average performance of our system on the 110 test cases of BRATS 2015, as computed on the online evaluation platform. Numbers in bold indicate significant improvement by the CRF, according to a two-sided, paired t -test on the DSC metric ( \u2217p &lt; 5 \u00b7 10 \u22122 , \u2217\u2217p &lt; 10 \u22123 ). The decrease of the mean DSC by the CRF and the ensemble for the \u201cCore\u201d class was not found significant.", "7c2bcf6f32b05a04cd3444c030db743e5666af88/13-Table4-1.png": "Table 4 Performance of our system on the training data of the ISLES-SISS 2015 competition. Values correspond to the mean (and standard deviation). Numbers in bold indicate significant improvement by the CRF, according to a two-sided, paired t -test on the DSC metric ( p &lt; 10 \u22122 ).", "7c2bcf6f32b05a04cd3444c030db743e5666af88/14-Figure13-1.png": "Fig. 13. Examples of segmentations performed by our system on the training datasets of (SISS) ISLES 2015. (top and middle) The system is capable of satisfying segmentation of both large and smaller lesions. (bottom) Common mistakes are performed due to the challenge of differentiating stroke lesions from White Matter lesions.", "7c2bcf6f32b05a04cd3444c030db743e5666af88/14-Table5-1.png": "Table 5 Our ensemble of three networks, coupled with the fully connected CRF obtained overall best performance among all participants in the testing stage of the ISLES-SISS 2015 challenge. Shown is the performance of our pipeline along with the second and third entries. Values correspond to the mean (and standard deviation).", "7c2bcf6f32b05a04cd3444c030db743e5666af88/15-Figure14-1.png": "Fig. 14. (First row) GE scan and DeepMedic\u2019s segmentation. (Second row) FMs of earlier and (third row) deeper layers of the first convolutional pathway. (Fourth row) Features learnt in the low-resolution pathway. (Last row) FMs of the two last hidden layers, which combine multi-resolution features towards the final segmentation.", "7c2bcf6f32b05a04cd3444c030db743e5666af88/17-TableB.1-1.png": "Table B.1 Network architectures investigated in Section 3 and final validation accuracy achieved in the corresponding experiments. (top half) 3D and (lower half) 2D architectures. Columns from left to right: model\u2019s name, number of parallel identical pathways and number of feature maps at each of their convolutional layers, number of feature maps at each hidden layer that follows the concatenation of the pathways, dimensions of input segment to the normal and low resolution pathways, batch size and, finally, average DSC achieved on the validation fold. Further configuration details provided in Appendix B .", "7c2bcf6f32b05a04cd3444c030db743e5666af88/17-TableC.1-1.png": "Table C.1 Real distribution of the classes in the training data of BRATS 2015, along with the distribution captured by our proposed training scheme, when segments of size 25 3 are extracted centred on the tumour and healthy tissue with equal probability. Relative distribution of the foreground classes is closely preserved and the imbalance in comparison to the healthy tissue is automatically alleviated.", "7c2bcf6f32b05a04cd3444c030db743e5666af88/2-Figure1-1.png": "Fig. 1. Heterogeneous appearance of TBI lesions poses challenges in devising discriminative models. Lesion size varies significantly with both large, focal and small, diffused lesions (a,b). Alignment of manual lesion segmentations reveals the wide spatial distribution of lesions in (c,d) with some areas being more likely than others. (e) shows the average of the normalized intensity histograms of different MR channels over all the TBI cases in our database, for healthy (green) and injured (red) tissue. One can observe a large overlap between the distributions of healthy and non-healthy brain tissue. (For interpretation of the references to colour, the reader is referred to the web version of this article.)", "7c2bcf6f32b05a04cd3444c030db743e5666af88/4-Figure2-1.png": "Fig. 2. Our baseline CNN consists of four layers with 5 3 kernels for feature extraction, leading to a receptive field of size 17 3 . The classification layer is implemented as convolutional with 1 3 kernels, which enables efficient dense-inference . When the network segments an input it predicts multiple voxels simultaneously, one for each shift of its receptive field over the input. Number of FMs and their size depicted as ( Number \u00d7 Size ).", "7c2bcf6f32b05a04cd3444c030db743e5666af88/5-Figure3-1.png": "Fig. 3. Consider a network with a 2D receptive field of 3 2 (for illustration) denselyapplied on the depicted lesion-centred image segments of size 7 2 or 9 2 . Relatively more background (green) is captured by larger segments and around smaller lesions. (For interpretation of the references to colour, the reader is referred to the web version of this article.)", "7c2bcf6f32b05a04cd3444c030db743e5666af88/5-Figure4-1.png": "Fig. 4. The replacement of the depicted layer with 5 5 kernels (left) with two successive layers using 3 3 kernels (right) introduces an additional non-linearity without altering the CNN\u2019s receptive field. Additionally, the number of weights is reduced from 200k to 86.4k and the required convolutions are cheaper (see text). Number of FMs and their size depicted as ( Number \u00d7 Size ).", "7c2bcf6f32b05a04cd3444c030db743e5666af88/6-Figure5-1.png": "Fig. 5. Multi-scale 3D CNN with two convolutional pathways. The kernels of the two pathways are here of size 5 3 (for illustration only to reduce the number of layers in the figure). The neurons of the last layers of the two pathways thus have receptive fields of size 17 3 voxels. The inputs of the two pathways are centred at the same image location, but the second segment is extracted from a down-sampled version of the image by a factor of 3. The second pathway processes context in an actual area of size 51 3 voxels. DeepMedic , our proposed 11-layers architecture, results by replacing each layer of the depicted pathways with two that use 3 3 kernels (see Section 2.3 ). Number of FMs and their size depicted as ( Number \u00d7 Size ).", "7c2bcf6f32b05a04cd3444c030db743e5666af88/8-Figure6-1.png": "Fig. 6. Comparison of the commonly used methods for training on patches uniformly sampled from the brain region (P uni ) and equally sampled from lesion and background (P eq ) against our proposed scheme (S- d ) on cubic segments of side length d , also equally sampled from lesion and background. We varied d to observe its effect. From left to right: percentage of training samples extracted from the lesion class, mean accuracy, sensitivity, specificity calculated on uniformly sampled validation patches and, finally, mean DSC of the segmentation of the validation datasets. Progress throughout training is plotted. Because lesions are small, P uni achieves very high voxel-wise accuracy by being very specific but not sensitive, with the opposite being the case for P eq . Our method achieves an effective balance between the two, resulting in better segmentation as reflected by higher DSC.", "7c2bcf6f32b05a04cd3444c030db743e5666af88/8-Figure7-1.png": "Fig. 7. Mean accuracy over validation samples and DSC for the segmentations of the validation images, as obtained from the \u201cShallow\u201d baseline and \u201cDeep\u201d variant with smaller kernels. Training of the plain deeper model fails (cf. Section 3.3 ). This is overcome by adopting the initialization scheme of He et al. (2015) , which further combined with Batch Normalization leads to the enhanced ( + ) variants. Deep + performs significantly better than Shallow + with similar computation time, thanks to the use of small kernels.", "7c2bcf6f32b05a04cd3444c030db743e5666af88/8-Figure8-1.png": "Fig. 8. Mean accuracy over validation samples and DSC for the segmentation of the validation images, as obtained by a single-scale model (Deep + ) and our dual pathway architecture (DeepMedic). We also trained a single-scale model with larger capacity (BigDeep + ), similar to the capacity of DeepMedic. DeepMedic yields best performance by capturing greater context, while BigDeep + seems to suffer from over-fitting.", "7c2bcf6f32b05a04cd3444c030db743e5666af88/9-Figure9-1.png": "Fig. 9. (Rows) Two cases from the severe TBI dataset, showing representative improvements when using the multi-scale CNN approach. (Columns) From left to right: the MRI FLAIR sequence with the manually labelled lesions, predicted soft segmentation map obtained from a single-scale model (Deep + ) and the prediction of the multi-scale DeepMedic model. The incorporation of greater context enables DeepMedic to identify when it processes an area within larger lesions (top). Spurious false positives are significantly reduced across the image on the bottom."}, "referred_figures_tables": [["7c2bcf6f32b05a04cd3444c030db743e5666af88/14-Figure13-1.png", "7c2bcf6f32b05a04cd3444c030db743e5666af88/13-Table3-1.png", "7c2bcf6f32b05a04cd3444c030db743e5666af88/14-Table5-1.png", "7c2bcf6f32b05a04cd3444c030db743e5666af88/12-Table2-1.png", "7c2bcf6f32b05a04cd3444c030db743e5666af88/13-Table4-1.png"], ["7c2bcf6f32b05a04cd3444c030db743e5666af88/12-Table2-1.png"]], "question_id": [17, 19], "question": ["How did the authors showed that the methods performed worse on the data coming from the second clinical center? Using which metrics ?", "The authors claims that DeepMedic  behaves very well in preserving the hierarchical structure tumours, is that true ? Have they tried it across different types of varying cases?"], "question_section": ["Evaluation on clinical data", "Abstract"], "question_trigger_sentence": ["All methods performed worse on the data com- ing from the second clinical centre, including the method of Feng et al. (2015) that is not machine-learning based. ", "Opposite to the experiments on TBI, we do not employ the intensity perturbation and dropout on convolutional layers, because the network should not require as much regularisation with this large database."], "question_type": ["Deep/complex question", "Shallow question"], "evidential_info": [[{"context": "Table\u00a03 shows the results of our method on the BRATS test data. Results of other submissions are not accessible. The decrease in performance is possibly due to the the inclusion of test images that vary significantly from the training data, such as cases acquired in clinical centers that did not provide any of the training images, something that was confirmed by the organisers. Note that performance gains obtained with the CRF are larger in this case. This indicates not only that its configuration has not overfitted to the training database but also that the CRF is robust to factors of variation between acquisition sites, which complements nicely the more sensitive CNN.", "rationale": "Table 3 shows results from BRATS test data. DSC, precision, and sensitivity are used for metrics."}, {"context": "For the testing phase of the challenge we formed an ensemble of three networks, coupled with the fully connected CRF. Our submission ranked first, indicating superior performance on this challenging task among 14 submissions. Table\u00a05 shows our results, along with the other two top entries (Feng et\u00a0al. (2015); Halme et\u00a0al. (2015)). Among the other participating methods was the CNN of Havaei et\u00a0al. (2015) with 3 layers of 2D convolutions. That method perfomed less well on this challenging task (Maier et\u00a0al. (2017)). This points out the advantage offered by 3D context, the large field of view of DeepMedic thanks to multi-scale processing and the representational power of deeper networks. It is important to note the decrease of performance in comparison to the training set. All methods performed worse on the data coming from the second clinical center, including the method of Feng et\u00a0al. (2015) that is not machine-learning based. This highlights a general difficulty with current approaches when applied on multi-center data.", "rationale": "Table 5 shows results from ISLES test data. DSC, precision, sensitivity, ASSD, and Haussdorf are used for metrics."}, {"context": "Quantitative results from the application of the DeepMedic, the CRF and an ensemble of three similar networks on the training data are presented in Table 2. The latter two offer an improvement, albeit fairly small since the performance of DeepMedic is already rather high in this task. Also shown are results from previous works, as reported on the online evaluation platform. Various settings may vary among submissions, such as the pre-processing pipeline or the number of folds used for cross-validation. Still it appears that our system performs favourably compared to previous state-of-the-art, including the semi-automatic system of Bakas et al. (2015) (bakas1) who won the latest challenge and the method of Pereira et al. (2015) (peres1), which is based on grade-specific 2D CNNs and requires visual inspection of the tumor and identification of the grade by the user prior to segmentation. Examples of segmentations obtained with our method are shown in Fig. 12. DeepMedic behaves very well in preserving the hierarchical structure of the tumor, which we account to the large context processed by our multi-scale network.", "rationale": "Table 2 shows results from BRATS training data. Performance is much better for training data for most metrics compared to the data shown in Table 3."}, {"context": "The performance of our system on the training data is shown in Table 4. Significant improvement is achieved by the structural regularisation offered by the CRF, although it could be partially accounted for by overfitting the training data during the CRF\u2019s configuration. Examples for visual inspection are shown in Fig. 13.", "rationale": "Table 4 shows results from ISLES training data. Performance is much better for training data for most metrics compared to the data shown in Table 5."}], [{"context": "Quantitative results from the application of the DeepMedic, the CRF and an ensemble of three similar networks on the training data are presented in Table 2. The latter two offer an improvement, albeit fairly small since the performance of DeepMedic is already rather high in this task. Also shown are results from previous works, as reported on the online evaluation platform. Various settings may vary among submissions, such as the pre-processing pipeline or the number of folds used for cross-validation. Still it appears that our system performs favourably compared to previous state-of-the-art, including the semi-automatic system of Bakas et\u00a0al. (2015) (bakas1) who won the latest challenge and the method of Pereira et\u00a0al. (2015) (peres1), which is based on grade-specific 2D CNNs and requires visual inspection of the tumor and identification of the grade by the user prior to segmentation. Examples of segmentations obtained with our method are shown in Fig.\u00a012. DeepMedic behaves very well in preserving the hierarchical structure of the tumor, which we account to the large context processed by our multi-scale network.", "rationale": "Figure 12 shows successful cases of segmentation for brain tumors. As seen in Figure 12, the model understands that the sequence of layers goes from oedema to non-enhancing core to enhancing core to necrotic core, preserving the hierarchical structure of tumors. They also show a relatively unsuccessful case where oversegmentation occurs, but even in this example, the hierarchy of the tumor is preserved."}]], "composition": ["Through Tables 2 to 5, the authors have shown that the performance of DeepMedic in terms of DSC, precision, sensitivity, ASSD, and Haussdorf for the BRATS and ISLES test datasets are worse than the performance of DeepMedic when trained with the BRATS and ISLES training datasets.", "Figure 12 shows successful cases of segmentation for the hierarchy of brain tumors. As seen in Figure 12, the model understands that the sequence of layers goes from oedema to non-enhancing core to enhancing core to necrotic core, preserving the hierarchical structure of tumors. They also show a relatively unsuccessful case where oversegmentation occurs, but even in this example, the hierarchy of the tumor is preserved."], "Is_figure_in_evidence": [true, false], "Is_table_in_evidence": [false, true], "question_key": ["246", "247"], "passages": ["Segmentation and the subsequent quantitative assessment of lesions in medical images provide valuable information for the analysis of neuropathologies and are important for planning of treatment strategies, monitoring of disease progression and prediction of patient outcome. For a better understanding of the pathophysiology of diseases, quantitative imaging can reveal clues about the disease characteristics and effects on particular anatomical structures. For example, the associations of different lesion types, their spatial distribution and extent with acute and chronic sequelae after traumatic brain injury (TBI) are still poorly understood (Maas et\u00a0al. (2015)). However, there is growing evidence that quantification of lesion burden may add insight into the functional outcome of patients (Ding et\u00a0al. (2008); Moen et\u00a0al. (2012)). Additionally, exact locations of injuries relate to particular deficits depending on the brain structure that is affected (Lehtonen et\u00a0al. (2005); Warner et\u00a0al. (2010); Sharp et\u00a0al. (2011)). This is in line with estimates that functional deficits caused by stroke are associated with the extent of damage to particular parts of the brain (Carey et\u00a0al. (2013)). Lesion burden is commonly quantified by means of volume and number of lesions, biomarkers that have been shown to be related to cognitive deficits. For example, volume of white matter lesions (WML) correlates with cognitive decline and increased risk of dementia (Ikram et\u00a0al. (2010)). In clinical research on multiple sclerosis (MS), lesion count and volume are used to analyse disease progression and effectiveness of pharmaceutical treatment (Rovira and Le\u00f3n (2008); Kappos et\u00a0al. (2007)). Finally, accurate delineation of the pathology is important in the case of brain tumors, where estimation of the relative volume of a tumor\u2019s sub-components is required for planning radiotherapy and treatment follow-up (Wen et\u00a0al. (2010)).", "The quantitative analysis of lesions requires accurate lesion segmentation in multi-modal, three-dimensional images which is a challenging task for a number of reasons. The heterogeneous appearance of lesions including the large variability in location, size, shape and frequency make it difficult to devise effective segmentation rules.It is thus highly non-trivial to delineate contusions, edema and haemorrhages in TBI (Irimia et\u00a0al. (2012)), or sub-components of brain tumors such as proliferating cells and necrotic core (Menze et\u00a0al. (2015)). The arguably most accurate segmentation results can be obtained through manual delineation by a human expert which is tedious, expensive, time-consuming, impractical in larger studies, and introduces inter-observer variability. Additionally, for deciding whether a particular region is part of a lesion multiple image sequences with varying contrasts need to be considered, and the level of expert knowledge and experience are important factors that impact segmentation accuracy. Hence, in clinical routine often only qualitative, visual inspection, or at best crude measures like approximate lesion volume and number of lesions are used (Yuh et\u00a0al. (2012); Wen et\u00a0al. (2010)). In order to capture and better understand the complexity of brain pathologies it is important to conduct large studies with many subjects to gain the statistical power for drawing conclusions across a whole patient population. The development of accurate, automatic segmentation algorithms has therefore become a major research focus in medical image computing with the potential to offer objective, reproducible, and scalable approaches to quantitative assessment of brain lesions.", "Figure\u00a01 illustrates some of the challenges that arise when devising a computational approach for the task of automatic lesion segmentation. The figure summarizes statistics and shows examples of brain lesions in the case of TBI, but is representative of other pathologies such as brain tumors and ischemic stroke. Lesions can occur at multiple sites, with varying shapes and sizes, and their image intensity profiles largely overlap with non-affected, healthy parts of the brain or lesions which are not in the focus of interest. For example, stroke and MS lesions have a similar hyper-intense appearance in FLAIR sequences as other WMLs (Mitra et\u00a0al. (2014); Schmidt et\u00a0al. (2012)). It is generally difficult to derive statistical prior information about lesion shape and appearance. On the other hand, in some applications there is an expectation on the spatial configuration of segmentation labels, for example there is a hierarchical layout of sub-components in brain tumors. Ideally, a computational approach is able to adjust itself to application specific characteristics by learning from a set of a few example images.", "A multitude of automatic lesion segmentation methods have been proposed over the last decade, and several main categories of approaches can be identified. One group of methods poses the lesion segmentation task as an abnormality detection problem, for example by employing image registration. The early work of Prastawa et\u00a0al. (2004) and more recent ones by Schmidt et\u00a0al. (2012) and Doyle et\u00a0al. (2013) align the pathological scan to a healthy atlas and lesions are detected based on deviations in tissue appearance between the patient and the atlas image. Lesions, however, may cause large structural deformations that may lead to incorrect segmentation due to incorrect registration. Gooya et\u00a0al. (2011); Parisot et\u00a0al. (2012) alleviate this problem by jointly solving the segmentation and registration tasks. Liu et\u00a0al. (2014) showed that registration together with a low-rank decomposition gives as a by-product the abnormal structures in the sparse components, although, this may not be precise enough for detection of small lesions. Abnormality detection has also been proposed within image synthesis works. Representative approaches are those of Weiss et\u00a0al. (2013) using dictionary learning and Ye et\u00a0al. (2013) using a patch-based approach. The idea is to synthesize pseudo-healthy images that when compared to the patient scan allow to highlight abnormal regions. In this context, Cardoso et\u00a0al. (2015) present a generative model for image synthesis that yields a probabilistic segmentation of abnormalities. Another unsupervised technique is proposed by Erihov et\u00a0al. (2015), a saliency-based method that exploits brain asymmetry in pathological cases. A common advantage of the above methods is that they do not require a training dataset with corresponding manual annotations. In general, these approaches are more suitable for detecting lesions rather than accurately segmenting them.", "Some of the most successful, supervised segmentation methods for brain lesions are based on voxel-wise classifiers, such as Random Forests. Representative work is that of Geremia et\u00a0al. (2010) on MS lesions, employing intensity features to capture the appearance of the region around each voxel. Zikic et\u00a0al. (2012) combine this with a generative Gaussian Mixture Model (GMM) to obtain tissue-specific probabilistic priors (Van\u00a0Leemput et\u00a0al. (1999)). This framework was adopted in multiple works, with representative pipelines for brain tumors by Tustison et\u00a0al. (2013) and TBI by Rao et\u00a0al. (2014). Both works incorporate morphological and contextual features to better capture the heterogeneity of lesions. Rao et\u00a0al. (2014) also incorporate brain structure segmentation results obtained from a multi-atlas label propagation approach (Ledig et\u00a0al. (2015)) to provide strong tissue-class priors to the Random Forests. Tustison et\u00a0al. (2013) additionally use a Markov Random Field (MRF) to incorporate spatial regularization. MRFs are commonly used to encourage spatial continuity of the segmentation (Schmidt et\u00a0al. (2012); Mitra et\u00a0al. (2014)). Although those methods have been very successful, it appears that their modeling capabilities still have significant limitations. This is confirmed by the results of the most recent challenges 111links: http://braintumorsegmentation.org/, www.isles-challenge.org, and also by our own experience and experimentation with such approaches.", "At the same time, deep learning techniques have emerged as a powerful alternative for supervised learning with great model capacity and the ability to learn highly discriminative features for the task at hand. These features often outperform hand-crafted and pre-defined feature sets. In particular, Convolutional Neural Networks (CNNs) (LeCun et\u00a0al. (1998); Krizhevsky et\u00a0al. (2012)) have been applied with promising results on a variety of biomedical imaging problems. Ciresan et\u00a0al. (2012) presented the first GPU implementation of a two-dimensional CNN for the segmentation of neural membranes. From the CNN based work that followed, related to our approach are the methods of Zikic et\u00a0al. (2014); Havaei et\u00a0al. (2015); Pereira et\u00a0al. (2015), with the latter being the best performing automatic approach in the BRATS 2015 challenge (Menze et\u00a0al. (2015)). These methods are based on 2D CNNs, which have been used extensively in computer vision applications on natural images. Here, the segmentation of a 3D brain scan is achieved by processing each 2D slice independently, which is arguably a non-optimal use of the volumetric medical image data. Despite the simplicity in the architecture, the promising results obtained by these methods indicate the potential of CNNs.", "Fully 3D CNNs come with an increased number of parameters and significant memory and computational requirements. Previous work discusses problems and apparent limitations when employing a 3D CNN on medical imaging data (Prasoon et\u00a0al. (2013); Li et\u00a0al. (2014); Roth et\u00a0al. (2014)). To incorporate 3D contextual information, multiple works used 2D CNNs on three orthogonal 2D patches (Prasoon et\u00a0al. (2013); Roth et\u00a0al. (2014); Lyksborg et\u00a0al. (2015)). In their work for structural brain segmentation, Brebisson and Montana (2015) extracted large 2D patches from multiple scales of the image and combined them with small single-scale 3D patches, in order to avoid the memory requirements of fully 3D networks.", "One of the reasons that discouraged the use of 3D CNNs is the slow inference due to the computationally expensive 3D convolutions. In contrast to the 2D/3D hybrid variants (Roth et\u00a0al. (2014); Brebisson and Montana (2015)), 3D CNNs can fully exploit dense-inference (LeCun et\u00a0al. (1998); Sermanet et\u00a0al. (2014)), a technique that greatly decreases inference times and which we will further discuss in section 2.1. By employing dense-inference with 3D CNNs, Brosch et\u00a0al. (2015) and Urban et\u00a0al. (2014) reported computation times of a few seconds and approximately a minute respectively for the processing of a single brain scan. Even though the size of their developed networks was limited, a factor that is directly related to a network\u2019s representational power, their results on MS and brain tumor segmentation respectively were very promising.", "Performance of CNNs is significantly influenced by the strategy for extracting training samples. A commonly adopted approach is training on image patches that are equally sampled from each class. This, however, biases the classifier towards rare classes and may result in over-segmentation. To counter this, Cire\u015fan et\u00a0al. (2013) proposes to train a second CNN on samples with a class distribution close to the real one, but oversample pixels that were incorrectly classified in the first stage. A secondary training stage was also suggested by Havaei et\u00a0al. (2015), who retrain the classification layer on patches extracted uniformly from the image. In practice, two stage training schemes can be prone to overfitting and sensitive to the state of the first classifier. Alternatively, dense training (Long et\u00a0al. (2015)) has been used to train a network on multiple or all voxels of a single image per optimisation step (Urban et\u00a0al. (2014); Brosch et\u00a0al. (2015); Ronneberger et\u00a0al. (2015)). This can introduce severe class imbalance, similarly to uniform sampling. Weighted cost functions have been proposed in the two latter works to alleviate this problem. Brosch et\u00a0al. (2015) manually adjusted the sensitivity of the network, but the method can become difficult to calibrate for multi-class problems. Ronneberger et\u00a0al. (2015) first balance the cost from each class, which has an effect similar to equal sampling, and further adjust it for the specific task by estimating the difficulty of segmenting each pixel.", "We present a fully automatic approach for lesion segmentation in multi-modal brain MRI based on an 11-layers deep, multi-scale, 3D CNN with the following main contributions:", "1.We propose an efficient hybrid training scheme, utilizing dense training (Long et\u00a0al. (2015)) on sampled image segments, and analyze its behaviour in adapting to class imbalance of the segmentation problem at hand.2.We analyze in depth the development of deeper, thus more discriminative, yet computationally efficient 3D CNNs. We exploit the utilization of small kernels, a design approach previously found beneficial in 2D networks (Simonyan and Zisserman (2014)) that impacts 3D CNNs even more, and present adopted solutions that enable training deeper networks.3.We employ parallel convolutional pathways for multi-scale processing, a solution to efficiently incorporate both local and contextual information which greatly improves segmentation results.4.We demonstrate the generalization capabilities of our system, which without significant modifications outperforms the state-of-the-art on a variety of challenging segmentation tasks, with top ranking results in two MICCAI challenges, ISLES and BRATS.", "Furthermore, a detailed analysis of the network reveals valuable insights into the powerful black box of deep learning with CNNs. For example, we have found that our network is capable of learning very complex, high level features that separate gray matter (GM), cerebrospinal fluid (CSF) and other anatomical structures to identify the image regions corresponding to lesions.", "Additionally, we have extended the fully-connected Conditional Random Field (CRF) model by Kr\u00e4henb\u00fchl and Koltun (2011) to 3D which we use for final post-processing of the CNN\u2019s soft segmentation maps. This CRF overcomes limitations of previous models as it can handle arbitrarily large neighborhoods while preserving fast inference times. To the best of our knowledge, this is the first use of a fully connected CRF on medical data.", "To facilitate further research and encourage other researchers to build upon our results, the source code of our lesion segmentation method including the CNN and the 3D fully connected CRF is made publicly available on https://biomedia.doc.ic.ac.uk/software/deepmedic/.", "Our proposed lesion segmentation method consists of two main components, a 3D CNN that produces highly accurate, soft segmentation maps, and a fully connected 3D CRF that imposes regularization constraints on the CNN output and produces the final hard segmentation labels. The main contributions of our work are within the CNN component which we describe first in the following.", "CNNs produce estimates for the voxel-wise segmentation labels by classifying each voxel in an image independently taking the neighborhood, i.e. local and contextual image information, into account. This is achieved by sequential convolutions of the input with multiple filters at the cascaded layers of the network. Each layer l\u2208[1,L]\ud835\udc591\ud835\udc3fl\\in[1,L]italic_l \u2208 [ 1 , italic_L ] consists of Clsubscript\ud835\udc36\ud835\udc59C_{l}italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT feature maps (FMs), also referred to as channels. Every FM is a group of neurons that detects a particular pattern, i.e. a feature, in the channels of the previous layer. The pattern is defined by the kernel weights associated with the FM. If the neurons of the m\ud835\udc5amitalic_m-th FM in the l\ud835\udc59litalic_l-th layer are arranged in a 3D grid, their activations constitute the image \ud835\udc32lm=f(\u2211n=1Cl\u22121\ud835\udc24lm,n\u22c6\ud835\udc32l\u22121n+blm)subscriptsuperscript\ud835\udc32\ud835\udc5a\ud835\udc59\ud835\udc53superscriptsubscript\ud835\udc5b1subscript\ud835\udc36\ud835\udc591\u22c6subscriptsuperscript\ud835\udc24\ud835\udc5a\ud835\udc5b\ud835\udc59subscriptsuperscript\ud835\udc32\ud835\udc5b\ud835\udc591subscriptsuperscript\ud835\udc4f\ud835\udc5a\ud835\udc59\\mathbf{y}^{m}_{l}=f(\\sum_{n=1}^{C_{l-1}}{\\mathbf{k}^{m,n}_{l}\\star\\mathbf{y}^{n}_{l-1}}+b^{m}_{l})bold_y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = italic_f ( \u2211 start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT bold_k start_POSTSUPERSCRIPT italic_m , italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT \u22c6 bold_y start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT + italic_b start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ). This is the result of convolving each of the previous layer\u2019s channels with a 3-dimensional kernel \ud835\udc24lm,nsubscriptsuperscript\ud835\udc24\ud835\udc5a\ud835\udc5b\ud835\udc59\\mathbf{k}^{m,n}_{l}bold_k start_POSTSUPERSCRIPT italic_m , italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT, adding a learned bias blmsubscriptsuperscript\ud835\udc4f\ud835\udc5a\ud835\udc59b^{m}_{l}italic_b start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and applying a non-linearity f\ud835\udc53fitalic_f. Each kernel is a matrix of learned hidden weights \ud835\udc16lm,nsubscriptsuperscript\ud835\udc16\ud835\udc5a\ud835\udc5b\ud835\udc59\\mathbf{W}^{m,n}_{l}bold_W start_POSTSUPERSCRIPT italic_m , italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT. The images \ud835\udc320nsubscriptsuperscript\ud835\udc32\ud835\udc5b0\\mathbf{y}^{n}_{0}bold_y start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, input to the first layer, correspond to the channels of the original input image, for instance a multi-sequence 3D MRI scan of the brain. The concatenation of the kernels \ud835\udc24l=(\ud835\udc24lm,1,\u2026,\ud835\udc24lm,Cl\u22121)subscript\ud835\udc24\ud835\udc59subscriptsuperscript\ud835\udc24\ud835\udc5a1\ud835\udc59\u2026subscriptsuperscript\ud835\udc24\ud835\udc5asubscript\ud835\udc36\ud835\udc591\ud835\udc59\\mathbf{k}_{l}=(\\mathbf{k}^{m,1}_{l},...,\\mathbf{k}^{m,C_{l-1}}_{l})bold_k start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = ( bold_k start_POSTSUPERSCRIPT italic_m , 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , \u2026 , bold_k start_POSTSUPERSCRIPT italic_m , italic_C start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) can be viewed as a 4-dimensional kernel convolving the concatenated channels \ud835\udc32l\u22121=(\ud835\udc32l\u221211,\u2026,\ud835\udc32l\u22121Cl\u22121)subscript\ud835\udc32\ud835\udc591subscriptsuperscript\ud835\udc321\ud835\udc591\u2026subscriptsuperscript\ud835\udc32subscript\ud835\udc36\ud835\udc591\ud835\udc591\\mathbf{y}_{l-1}=(\\mathbf{y}^{1}_{l-1},...,\\mathbf{y}^{C_{l-1}}_{l-1})bold_y start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT = ( bold_y start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT , \u2026 , bold_y start_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT ), which then intuitively expresses that the neurons of higher layers combine the patterns extracted in previous layers, which results in the detection of increasingly more complex patterns. The activations of the neurons in the last layer L\ud835\udc3fLitalic_L correspond to particular segmentation class labels, hence this layer is also referred to as the classification layer. The neurons are thus grouped in CLsubscript\ud835\udc36\ud835\udc3fC_{L}italic_C start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT FMs, one for each of the segmentation classes. Their activations are fed into a position-wise softmax function that produces the predicted posterior pc(\ud835\udc31)=exp\u2061(\ud835\udc32Lc(\ud835\udc31))/\u2211c=1CLexp\u2061(\ud835\udc32Lc(\ud835\udc31))subscript\ud835\udc5d\ud835\udc50\ud835\udc31superscriptsubscript\ud835\udc32\ud835\udc3f\ud835\udc50\ud835\udc31superscriptsubscript\ud835\udc501subscript\ud835\udc36\ud835\udc3fsuperscriptsubscript\ud835\udc32\ud835\udc3f\ud835\udc50\ud835\udc31p_{c}(\\mathbf{x})=\\exp(\\mathbf{y}_{L}^{c}(\\mathbf{x}))/\\sum_{c=1}^{C_{L}}\\exp(\\mathbf{y}_{L}^{c}(\\mathbf{x}))italic_p start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_x ) = roman_exp ( bold_y start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_x ) ) / \u2211 start_POSTSUBSCRIPT italic_c = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUPERSCRIPT roman_exp ( bold_y start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_x ) ) for each class c\ud835\udc50citalic_c, which form soft segmentation maps with (pseudo-)probabilities. \ud835\udc32Lc(\ud835\udc31)superscriptsubscript\ud835\udc32\ud835\udc3f\ud835\udc50\ud835\udc31\\mathbf{y}_{L}^{c}(\\mathbf{x})bold_y start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_x ) is the activation of the c\ud835\udc50citalic_c-th classification FM at position \ud835\udc31\u2208\u21153\ud835\udc31superscript\u21153\\mathbf{x}\\in\\mathbb{N}^{3}bold_x \u2208 blackboard_N start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT. This baseline network is depicted in Fig.\u00a02.", "The neighborhood of voxels in the input that influence the activation of a neuron is its receptive field. Its size, \ud835\udf4blsubscript\ud835\udf4b\ud835\udc59\\bm{\\varphi}_{l}bold_italic_\u03c6 start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT, increases at each subsequent layer l\ud835\udc59litalic_l and is given by the 3-dimensional vector:\ud835\udf4bl{x,y,z}=\ud835\udf4bl\u22121{x,y,z}+(\ud835\udf3fl{x,y,z}\u22121)\ud835\udf49l{x,y,z}\u00a0,superscriptsubscript\ud835\udf4b\ud835\udc59\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udf4b\ud835\udc591\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udf3f\ud835\udc59\ud835\udc65\ud835\udc66\ud835\udc671superscriptsubscript\ud835\udf49\ud835\udc59\ud835\udc65\ud835\udc66\ud835\udc67\u00a0,\\bm{\\varphi}_{l}^{\\{x,y,z\\}}=\\bm{\\varphi}_{l-1}^{\\{x,y,z\\}}+(\\bm{\\kappa}_{l}^{\\{x,y,z\\}}-1)\\bm{\\tau}_{l}^{\\{x,y,z\\}}\\textrm{ ,}bold_italic_\u03c6 start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT = bold_italic_\u03c6 start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT + ( bold_italic_\u03ba start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT - 1 ) bold_italic_\u03c4 start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT ,(1)where \ud835\udf3fl,\ud835\udf49l\u2208\u21153subscript\ud835\udf3f\ud835\udc59subscript\ud835\udf49\ud835\udc59superscript\u21153\\bm{\\kappa}_{l},\\bm{\\tau}_{l}\\in\\mathbb{N}^{3}bold_italic_\u03ba start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , bold_italic_\u03c4 start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT \u2208 blackboard_N start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT are vectors expressing the size of the kernels and stride of the receptive field at layer l\ud835\udc59litalic_l. \ud835\udf49lsubscript\ud835\udf49\ud835\udc59\\bm{\\tau}_{l}bold_italic_\u03c4 start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT is given by the product of the strides of kernels in layers preceding l\ud835\udc59litalic_l. In this work only unary strides are used, as larger strides downsample the FMs (Springenberg et\u00a0al. (2014)), which is unwanted behaviour for accurate segmentation. Thus in our system \ud835\udf49l=(1,1,1)subscript\ud835\udf49\ud835\udc59111\\bm{\\tau}_{l}=(1,1,1)bold_italic_\u03c4 start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = ( 1 , 1 , 1 ). The receptive field of a neuron in the classification layer corresponds to the image patch that influences the prediction for its central voxel. This is called the CNN\u2019s receptive field, with \ud835\udf4bCNN=\ud835\udf4bLsubscript\ud835\udf4b\ud835\udc36\ud835\udc41\ud835\udc41subscript\ud835\udf4b\ud835\udc3f\\bm{\\varphi}_{CNN}=\\bm{\\varphi}_{L}bold_italic_\u03c6 start_POSTSUBSCRIPT italic_C italic_N italic_N end_POSTSUBSCRIPT = bold_italic_\u03c6 start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT.", "If input of size \ud835\udf39insubscript\ud835\udf39\ud835\udc56\ud835\udc5b\\bm{\\delta}_{in}bold_italic_\u03b4 start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT is provided, the dimensions of the FMs in layer l\ud835\udc59litalic_l are given by:\ud835\udf39l{x,y,z}=\u230a(\ud835\udf39in{x,y,z}\u2212\ud835\udf4bl{x,y,z})/\ud835\udf49l{x,y,z}+1\u230bsuperscriptsubscript\ud835\udf39\ud835\udc59\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udf39\ud835\udc56\ud835\udc5b\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udf4b\ud835\udc59\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udf49\ud835\udc59\ud835\udc65\ud835\udc66\ud835\udc671\\bm{\\delta}_{l}^{\\{x,y,z\\}}=\\lfloor(\\bm{\\delta}_{in}^{\\{x,y,z\\}}-\\bm{\\varphi}_{l}^{\\{x,y,z\\}})/\\bm{\\tau}_{l}^{\\{x,y,z\\}}+1\\rfloorbold_italic_\u03b4 start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT = \u230a ( bold_italic_\u03b4 start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT - bold_italic_\u03c6 start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT ) / bold_italic_\u03c4 start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT + 1 \u230b(2)", "In the common patch-wise classification setting, an input patch of size \ud835\udf39in=\ud835\udf4bCNNsubscript\ud835\udf39\ud835\udc56\ud835\udc5bsubscript\ud835\udf4b\ud835\udc36\ud835\udc41\ud835\udc41\\bm{\\delta}_{in}=\\bm{\\varphi}_{CNN}bold_italic_\u03b4 start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT = bold_italic_\u03c6 start_POSTSUBSCRIPT italic_C italic_N italic_N end_POSTSUBSCRIPT is provided and the network outputs a single prediction for its central voxel. In this case the classification layer consists of FMs with size 13superscript131^{3}1 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT. Networks that are implemented as fully-convolutionals are capable of dense-inference, which is performed when input of size greater than \ud835\udf4bCNNsubscript\ud835\udf4b\ud835\udc36\ud835\udc41\ud835\udc41\\bm{\\varphi}_{CNN}bold_italic_\u03c6 start_POSTSUBSCRIPT italic_C italic_N italic_N end_POSTSUBSCRIPT is provided (Sermanet et\u00a0al. (2014)). In this case, the dimensions of FMs increase according to Eq.\u00a0(2). This includes the classification FMs which then output multiple predictions simultaneously, one for each stride of the CNN\u2019s receptive field on the input (Fig.\u00a02). All predictions are equally trustworthy, as long as the receptive field is fully contained within the input and captures only original content, i.e. no padding is used. This strategy significantly reduces the computational costs and memory loads since the otherwise repeated computations of convolutions on the same voxels in overlapping patches are avoided. Optimal performance is achieved if the whole image is scanned in one forward pass. If GPU memory constraints do not allow it, such as in the case of large 3D networks where a large number of FMs need to be cached, the volume is tiled in multiple image-segments, which are larger than individual patches, but small enough to fit into memory.", "Before analyzing how we exploit the above dense-inference technique for training, which is the first main contribution of our work, we present the commonly used setting in which CNNs are trained patch-by-patch. Random patches of size \ud835\udf4bCNNsubscript\ud835\udf4b\ud835\udc36\ud835\udc41\ud835\udc41\\bm{\\varphi}_{CNN}bold_italic_\u03c6 start_POSTSUBSCRIPT italic_C italic_N italic_N end_POSTSUBSCRIPT are extracted from the training images. A batch is formed out of B\ud835\udc35Bitalic_B of these samples, which is then processed by the network for one training iteration of Stochastic Gradient Descent (SGD). This step aims to alter the network\u2019s parameters \ud835\udeaf\ud835\udeaf\\mathbf{\\Theta}bold_\u0398, such as weights and biases, in order to maximize the log likelihood of the data or, equally, minimize the Cross Entropy via the cost function:", "J(\ud835\udeaf;\ud835\udc08i,ci)=\u22121B\u2211i=1Blog\u2061(P(Y=ci|\ud835\udc08i,\ud835\udeaf))=\u22121B\u2211i=1Blog\u2061(pci)\u00a0,\ud835\udc3d\ud835\udeafsuperscript\ud835\udc08\ud835\udc56superscript\ud835\udc50\ud835\udc561\ud835\udc35superscriptsubscript\ud835\udc561\ud835\udc35\ud835\udc43\ud835\udc4cconditionalsuperscript\ud835\udc50\ud835\udc56superscript\ud835\udc08\ud835\udc56\ud835\udeaf1\ud835\udc35superscriptsubscript\ud835\udc561\ud835\udc35subscript\ud835\udc5dsuperscript\ud835\udc50\ud835\udc56\u00a0,J(\\mathbf{\\Theta};\\mathbf{I}^{i},c^{i})=-\\frac{1}{B}\\sum_{i=1}^{B}\\log\\left(P(Y=c^{i}|\\mathbf{I}^{i},\\mathbf{\\Theta})\\right)=-\\frac{1}{B}\\sum_{i=1}^{B}\\log(p_{c^{i}})\\textrm{ ,}italic_J ( bold_\u0398 ; bold_I start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_c start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) = - divide start_ARG 1 end_ARG start_ARG italic_B end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT roman_log ( italic_P ( italic_Y = italic_c start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | bold_I start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_\u0398 ) ) = - divide start_ARG 1 end_ARG start_ARG italic_B end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) ,(3)where the pair (\ud835\udc08i,ci),\u2200i\u2208[1,B]superscript\ud835\udc08\ud835\udc56superscript\ud835\udc50\ud835\udc56for-all\ud835\udc561\ud835\udc35(\\mathbf{I}^{i},c^{i}),\\forall{i}\\in{[1,B]}( bold_I start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_c start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) , \u2200 italic_i \u2208 [ 1 , italic_B ] is the i\ud835\udc56iitalic_i-th patch in the batch and the true label of its central voxel, while the scalar value pcisubscript\ud835\udc5dsuperscript\ud835\udc50\ud835\udc56p_{c^{i}}italic_p start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT is the predicted posterior for class cisuperscript\ud835\udc50\ud835\udc56c^{i}italic_c start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT. Regularization terms were omitted for simplicity. Multiple sequential optimization steps over different batches gradually lead to convergence.", "Larger training batch sizes B\ud835\udc35Bitalic_B are preferred as they approximate the overall data more accurately and lead to better estimation of the true gradient by SGD. However, the memory requirement and computation time increase with the batch size. This limitation is especially relevant for 3D CNNs, where only a few dozens of patches can be processed within reasonable time on modern GPUs.", "To overcome this problem, we devise a training strategy that exploits the dense inference technique on image segments. Following from Eq.\u00a0(2), if an image segment of size greater than \ud835\udf4bCNNsubscript\ud835\udf4b\ud835\udc36\ud835\udc41\ud835\udc41\\bm{\\varphi}_{CNN}bold_italic_\u03c6 start_POSTSUBSCRIPT italic_C italic_N italic_N end_POSTSUBSCRIPT is given as input to our network, the output is a posterior probability for multiple voxels V=\u220fi={x,y,z}\ud835\udf39L(i)\ud835\udc49subscriptproduct\ud835\udc56\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udf39\ud835\udc3f\ud835\udc56V=\\prod_{i=\\{x,y,z\\}}{\\bm{\\delta}_{L}^{(i)}}italic_V = \u220f start_POSTSUBSCRIPT italic_i = { italic_x , italic_y , italic_z } end_POSTSUBSCRIPT bold_italic_\u03b4 start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT. If the training batches are formed of B\ud835\udc35Bitalic_B segments extracted from the training images, the cost function (3) in the case of dense-training becomes:", "JD(\ud835\udeaf;\ud835\udc08s,\ud835\udc1cs)=\u22121B\u22c5V\u2211s=1B\u2211v=1Vlog\u2061(pcsv(\ud835\udc31v))\u00a0,subscript\ud835\udc3d\ud835\udc37\ud835\udeafsubscript\ud835\udc08\ud835\udc60subscript\ud835\udc1c\ud835\udc601\u22c5\ud835\udc35\ud835\udc49superscriptsubscript\ud835\udc601\ud835\udc35superscriptsubscript\ud835\udc631\ud835\udc49subscript\ud835\udc5dsuperscriptsubscript\ud835\udc50\ud835\udc60\ud835\udc63superscript\ud835\udc31\ud835\udc63\u00a0,J_{D}(\\mathbf{\\Theta};\\mathbf{I}_{s},\\mathbf{c}_{s})=-\\frac{1}{B\\cdot V}\\sum_{s=1}^{B}\\sum_{v=1}^{V}\\log(p_{c_{s}^{v}}(\\mathbf{x}^{v}))\\textrm{ ,}italic_J start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( bold_\u0398 ; bold_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , bold_c start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = - divide start_ARG 1 end_ARG start_ARG italic_B \u22c5 italic_V end_ARG \u2211 start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_v = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ) ) ,(4)", "where \ud835\udc08ssubscript\ud835\udc08\ud835\udc60\\mathbf{I}_{s}bold_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and \ud835\udc1cssubscript\ud835\udc1c\ud835\udc60\\mathbf{c}_{s}bold_c start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT are the s\ud835\udc60sitalic_s-th segment of the batch and the true labels of its V\ud835\udc49Vitalic_V predicted voxels respectively. csvsuperscriptsubscript\ud835\udc50\ud835\udc60\ud835\udc63c_{s}^{v}italic_c start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT is the true label of the v\ud835\udc63vitalic_v-th voxel, \ud835\udc31vsuperscript\ud835\udc31\ud835\udc63\\mathbf{x}^{v}bold_x start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT the corresponding position in the classification FMs and pcsvsubscript\ud835\udc5dsuperscriptsubscript\ud835\udc50\ud835\udc60\ud835\udc63p_{c_{s}^{v}}italic_p start_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT end_POSTSUBSCRIPT the output of the softmax function. The effective batch size is increased by a factor of V\ud835\udc49Vitalic_V without a corresponding increase in computational and memory requirements, as earlier discussed in Sec.\u00a02.1. Notice that this is a hybrid scheme between the commonly used training on individual patches and the dense training scheme on a whole image (Long et\u00a0al. (2015)), with the latter being problematic to apply for training large 3D CNNs on volumes of high resolution due to memory limitations.", "An appealing consequence of this scheme is that the sampling of input segments provides a flexible and automatic way to balance the distribution of training samples from different segmentation classes which is an important issue that directly impacts the segmentation accuracy. Specifically, we build the training batches by extracting segments from the training images with 50% probability being centred on a foreground or background voxel, alleviating class-imbalance. Note that the predicted voxels V\ud835\udc49Vitalic_V in a segment do not have to be of the same class, something that occurs when a segment is sampled from a region near class boundaries (Fig.\u00a03). Hence, the sampling rate of the proposed hybrid method adjusts to the true distribution of the segmentation task\u2019s classes. Specifically, the smaller a labelled object, the more background voxels will be captured within segments centred on the foreground voxel. Implicitly, this yields a balance between sensitivity and specificity in the case of binary segmentation tasks. In multi-class problems, the rate at which different classes are captured within a segment centred on foreground reflects the real relative distribution of the foreground classes, while adjusting their frequency relatively to the background.", "Deeper networks have greater discriminative power due to the additional non-linearities and better quality of local optima (Choromanska et\u00a0al. (2015)). However, convolutions with 3D kernels are computationally expensive in comparison to the 2D variants, which hampers the addition of more layers. Additionally, 3D architectures have a larger number of trainable parameters, with each layer adding ClCl\u22121\u220fi={x,y,z}\ud835\udf3fl(i)subscript\ud835\udc36\ud835\udc59subscript\ud835\udc36\ud835\udc591subscriptproduct\ud835\udc56\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udf3f\ud835\udc59\ud835\udc56C_{l}C_{l-1}\\prod_{i=\\{x,y,z\\}}{\\bm{\\kappa}_{l}^{(i)}}italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT \u220f start_POSTSUBSCRIPT italic_i = { italic_x , italic_y , italic_z } end_POSTSUBSCRIPT bold_italic_\u03ba start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT weights to the model. Clsubscript\ud835\udc36\ud835\udc59C_{l}italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT is the number of FMs in layer l\ud835\udc59litalic_l and \ud835\udf3fl{x,y,z}superscriptsubscript\ud835\udf3f\ud835\udc59\ud835\udc65\ud835\udc66\ud835\udc67\\bm{\\kappa}_{l}^{\\{x,y,z\\}}bold_italic_\u03ba start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT the size of its kernel in the respective spatial dimension. Overall this makes the network increasingly prone to over-fitting.", "In order to build a deeper 3D architecture, we adopt the sole use of small 33superscript333^{3}3 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT kernels that are faster to convolve with and contain less weights. This design approach was previously found beneficial for classification of natural images (Simonyan and Zisserman (2014)) but its effect is even more drastic on 3D networks. When compared to common kernel choices of 53superscript535^{3}5 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT (Zikic et\u00a0al. (2014); Urban et\u00a0al. (2014); Prasoon et\u00a0al. (2013)) and in our baseline CNN, the smaller 33superscript333^{3}3 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT kernels reduce the element-wise multiplications by a factor of approximately 53/33\u22484.6superscript53superscript334.65^{3}/3^{3}\\approx 4.65 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT / 3 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT \u2248 4.6 while reducing the number of trainable parameters by the same factor. Thus deeper network variants that are implicitly regularised and more efficient can be designed by simply replacing each layer of common architectures with more layers that use smaller kernels (Fig.\u00a04).", "However, deeper networks are more difficult to train. It has been shown that the forward (neuron activations) and backwards (gradients) propagated signal may explode or vanish if care is not given to retain its variance (Glorot and Bengio (2010)). This occurs because at every successive layer l\ud835\udc59litalic_l, the variance of the signal is multiplied by nlin\u22c5var(\ud835\udc16l)\u22c5subscriptsuperscript\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc59\ud835\udc63\ud835\udc4e\ud835\udc5fsubscript\ud835\udc16\ud835\udc59n^{in}_{l}\\cdot var(\\mathbf{W}_{l})italic_n start_POSTSUPERSCRIPT italic_i italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT \u22c5 italic_v italic_a italic_r ( bold_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ), where nlin=Cl\u22121\u220fi={x,y,z}\ud835\udf3fl(i)subscriptsuperscript\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc59subscript\ud835\udc36\ud835\udc591subscriptproduct\ud835\udc56\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udf3f\ud835\udc59\ud835\udc56n^{in}_{l}=C_{l-1}\\prod_{i=\\{x,y,z\\}}{\\bm{\\kappa}_{l}^{(i)}}italic_n start_POSTSUPERSCRIPT italic_i italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT \u220f start_POSTSUBSCRIPT italic_i = { italic_x , italic_y , italic_z } end_POSTSUBSCRIPT bold_italic_\u03ba start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT is the number of weights through which a neuron of layer l\ud835\udc59litalic_l is connected to its input and var(\ud835\udc16l)\ud835\udc63\ud835\udc4e\ud835\udc5fsubscript\ud835\udc16\ud835\udc59var(\\mathbf{W}_{l})italic_v italic_a italic_r ( bold_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) is the variance of the layer\u2019s weights. To better preserve the signal in the initial training stage we adopt a scheme recently derived for ReLu-based networks by He et\u00a0al. (2015) and initialize the kernel weights of our system by sampling from the normal distribution \ud835\udca9(0,2/nlin)\ud835\udca902subscriptsuperscript\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc59\\mathcal{N}(0,\\sqrt{2/n^{in}_{l}})caligraphic_N ( 0 , square-root start_ARG 2 / italic_n start_POSTSUPERSCRIPT italic_i italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG ).", "A phenomenon of similar nature that hinders the network\u2019s performance is the \u201cinternal covariate shift\u201d (Ioffe and Szegedy (2015)). It occurs throughout training, because the weight updates to deeper layers result in a continuously changing distribution of signal at higher layers, which hinders the convergence of their weights. Specifically, at training iteration t\ud835\udc61titalic_t the weight updates may cause deviation \u03f5l,tsubscriptitalic-\u03f5\ud835\udc59\ud835\udc61\\epsilon_{l,t}italic_\u03f5 start_POSTSUBSCRIPT italic_l , italic_t end_POSTSUBSCRIPT to the variance of the weights. At the next iteration the signal will be amplified by nlin\u22c5var(\ud835\udc16l,t+1)=nlin\u22c5(var(\ud835\udc16l,t)+\u03f5l,t)\u22c5subscriptsuperscript\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc59\ud835\udc63\ud835\udc4e\ud835\udc5fsubscript\ud835\udc16\ud835\udc59\ud835\udc611\u22c5subscriptsuperscript\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc59\ud835\udc63\ud835\udc4e\ud835\udc5fsubscript\ud835\udc16\ud835\udc59\ud835\udc61subscriptitalic-\u03f5\ud835\udc59\ud835\udc61n^{in}_{l}\\cdot var(\\mathbf{W}_{l,t+1})=n^{in}_{l}\\cdot(var(\\mathbf{W}_{l,t})+\\epsilon_{l,t})italic_n start_POSTSUPERSCRIPT italic_i italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT \u22c5 italic_v italic_a italic_r ( bold_W start_POSTSUBSCRIPT italic_l , italic_t + 1 end_POSTSUBSCRIPT ) = italic_n start_POSTSUPERSCRIPT italic_i italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT \u22c5 ( italic_v italic_a italic_r ( bold_W start_POSTSUBSCRIPT italic_l , italic_t end_POSTSUBSCRIPT ) + italic_\u03f5 start_POSTSUBSCRIPT italic_l , italic_t end_POSTSUBSCRIPT ). Thus before influencing the signal, any deviation \u03f5l,tsubscriptitalic-\u03f5\ud835\udc59\ud835\udc61\\epsilon_{l,t}italic_\u03f5 start_POSTSUBSCRIPT italic_l , italic_t end_POSTSUBSCRIPT is amplified by nlinsubscriptsuperscript\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc59n^{in}_{l}italic_n start_POSTSUPERSCRIPT italic_i italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT which is exponential in the number of dimensions. For this reason the problem affects training of 3D CNNs more severely than conventional 2D systems. For countering it, we adopt the recently proposed Batch Normalisation (BN) technique to all hidden layers (Ioffe and Szegedy (2015)), which allows normalization of the FM activations at every optimization step in order to better preserve the signal.", "The segmentation of each voxel is performed by taking into account the contextual information that is captured by the receptive field of the CNN when it is centred on the voxel. The spatial context is providing important information for being able to discriminate voxels that otherwise appear very similar when considering only local appearance. From Eq.\u00a0(1) follows that an increase of the CNN\u2019s receptive field requires bigger kernels or more convolutional layers, which increases computation and memory requirements. An alternative would be the use of pooling (LeCun et\u00a0al. (1998)), which however leads to loss of the exact position of the segmented voxel and thus can negatively impact accuracy.", "In order to incorporate both local and larger contextual information into our 3D CNN, we add a second pathway that operates on down-sampled images. Thus, our dual pathway 3D CNN simultaneously processes the input image at multiple scales (Fig.\u00a05). Higher level features such as the location within the brain are learned in the second pathway, while the detailed local appearance of structures is captured in the first. As the two pathways are decoupled in this architecture, arbitrarily large context can be processed by the second pathway by simply adjusting the down-sampling factor FDsubscript\ud835\udc39\ud835\udc37F_{D}italic_F start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT. The size of the pathways can be independently adjusted according to the computational capacity and the task at hand, which may require relatively more or less filters focused on the down-sampled context.", "To preserve the capability of dense inference, spatial correspondence of the activations in the FMs of the last convolutional layers of the two pathways, L1\ud835\udc3f1L1italic_L 1 and L2\ud835\udc3f2L2italic_L 2, should be ensured. In networks where only unary kernel strides are used, such as the proposed architecture, this requires that for every FDsubscript\ud835\udc39\ud835\udc37F_{D}italic_F start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT shifts of the receptive field \ud835\udf4bL1subscript\ud835\udf4b\ud835\udc3f1\\bm{\\varphi}_{L1}bold_italic_\u03c6 start_POSTSUBSCRIPT italic_L 1 end_POSTSUBSCRIPT over the normal resolution input, only one shift is performed by \ud835\udf4bL2subscript\ud835\udf4b\ud835\udc3f2\\bm{\\varphi}_{L2}bold_italic_\u03c6 start_POSTSUBSCRIPT italic_L 2 end_POSTSUBSCRIPT over the down-sampled input. Hence it is required that the dimensions of the FMs in L2\ud835\udc3f2L2italic_L 2 are \ud835\udf39L2{x,y,z}=\u2308\ud835\udf39L1{x,y,z}/FD\u2309superscriptsubscript\ud835\udf39\ud835\udc3f2\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udf39\ud835\udc3f1\ud835\udc65\ud835\udc66\ud835\udc67subscript\ud835\udc39\ud835\udc37\\bm{\\delta}_{L2}^{\\{x,y,z\\}}=\\lceil\\bm{\\delta}_{L1}^{\\{x,y,z\\}}/F_{D}\\rceilbold_italic_\u03b4 start_POSTSUBSCRIPT italic_L 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT = \u2308 bold_italic_\u03b4 start_POSTSUBSCRIPT italic_L 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT / italic_F start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT \u2309. From Eq.\u00a0(2), the size of the input to the second pathway is \ud835\udf39in2{x,y,z}=\ud835\udf4bL2{x,y,z}+\ud835\udf39L2{x,y,z}\u22121superscriptsubscript\ud835\udf39\ud835\udc56\ud835\udc5b2\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udf4b\ud835\udc3f2\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udf39\ud835\udc3f2\ud835\udc65\ud835\udc66\ud835\udc671\\bm{\\delta}_{in2}^{\\{x,y,z\\}}=\\bm{\\varphi}_{L2}^{\\{x,y,z\\}}+\\bm{\\delta}_{L2}^{\\{x,y,z\\}}-1bold_italic_\u03b4 start_POSTSUBSCRIPT italic_i italic_n 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT = bold_italic_\u03c6 start_POSTSUBSCRIPT italic_L 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT + bold_italic_\u03b4 start_POSTSUBSCRIPT italic_L 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT { italic_x , italic_y , italic_z } end_POSTSUPERSCRIPT - 1 and similar is the relation between \ud835\udf39in1subscript\ud835\udf39\ud835\udc56\ud835\udc5b1\\bm{\\delta}_{in1}bold_italic_\u03b4 start_POSTSUBSCRIPT italic_i italic_n 1 end_POSTSUBSCRIPT and \ud835\udf39L1subscript\ud835\udf39\ud835\udc3f1\\bm{\\delta}_{L1}bold_italic_\u03b4 start_POSTSUBSCRIPT italic_L 1 end_POSTSUBSCRIPT. These establish the relation between the required dimensions of the input segments from the two resolutions, which can then be extracted centered on the same image location. The FMs of L2\ud835\udc3f2L2italic_L 2 are up-sampled to match the dimensions of L1\ud835\udc3f1L1italic_L 1\u2019s FMs and are then concatenated together. We add two more hidden layers for combining the multi-scale features before the final classification, as shown in Fig.\u00a05. Integration of the multi-scale parallel pathways in architectures with non-unary strides is discussed in A.", "Combining multi-scale features has been found beneficial in other recent works (Long et\u00a0al. (2015); Ronneberger et\u00a0al. (2015)), in which whole 2D images are processed in the network by applying a few number of convolutions and then down-sampling the FMs for further processing at various scales. Our decoupled pathways allow arbitrarily large context to be provided while avoiding the need to load large parts of the 3D volume into memory. Additionally, our architecture extracts features completely independently from the multiple resolutions. This way, the features learned by the first pathway retain finest details, as they are not involved in processing low resolution context.", "Because neighboring voxels share substantial spatial context, the soft segmentation maps produced by the CNN tend to be smooth, even though neighborhood dependencies are not modeled directly. However, local minima in training and noise in the input images can still result in some spurious outputs, with small isolated regions or holes in the predictions. We employ a fully connected CRF (Kr\u00e4henb\u00fchl and Koltun (2011)) as a post-processing step to achieve more structured predictions. As we describe below, this CRF is capable of modeling arbitrarily large voxel-neighborhoods but is also computationally efficient, making it ideal for processing 3D multi-modal medical scans.", "For an input image \ud835\udc08\ud835\udc08\\mathbf{I}bold_I and the label configuration (segmentation) \ud835\udc33\ud835\udc33\\mathbf{z}bold_z, the Gibbs energy in a CRF model is given by", "E(\ud835\udc33)=\u2211i\u03c8u(zi)+\u2211ij,i\u2260j\u03c8p(zi,zj)\u00a0.\ud835\udc38\ud835\udc33subscript\ud835\udc56subscript\ud835\udf13\ud835\udc62subscript\ud835\udc67\ud835\udc56subscript\ud835\udc56\ud835\udc57\ud835\udc56\ud835\udc57subscript\ud835\udf13\ud835\udc5dsubscript\ud835\udc67\ud835\udc56subscript\ud835\udc67\ud835\udc57\u00a0.E(\\mathbf{z})=\\sum_{i}{\\psi_{u}(z_{i})}+\\sum_{ij,i\\neq j}{\\psi_{p}(z_{i},z_{j})}\\textrm{ .}italic_E ( bold_z ) = \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_\u03c8 start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + \u2211 start_POSTSUBSCRIPT italic_i italic_j , italic_i \u2260 italic_j end_POSTSUBSCRIPT italic_\u03c8 start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) .(5)", "The unary potential is the negative log-likelihood \u03c8u(zi)=\u2212logP(zi|\ud835\udc08)subscript\ud835\udf13\ud835\udc62subscript\ud835\udc67\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc43conditionalsubscript\ud835\udc67\ud835\udc56\ud835\udc08\\psi_{u}(z_{i})=-logP(z_{i}|\\mathbf{I})italic_\u03c8 start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = - italic_l italic_o italic_g italic_P ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | bold_I ), where in our case P(zi|\ud835\udc08)\ud835\udc43conditionalsubscript\ud835\udc67\ud835\udc56\ud835\udc08P(z_{i}|\\mathbf{I})italic_P ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | bold_I ) is the CNN\u2019s output for voxel i\ud835\udc56iitalic_i. In a fully connected CRF, the pairwise potential is of form \u03c8p(zi,zj)=\u03bc(zi,zj)k(\ud835\udc1f\ud835\udc22,\ud835\udc1f\ud835\udc23)subscript\ud835\udf13\ud835\udc5dsubscript\ud835\udc67\ud835\udc56subscript\ud835\udc67\ud835\udc57\ud835\udf07subscript\ud835\udc67\ud835\udc56subscript\ud835\udc67\ud835\udc57\ud835\udc58subscript\ud835\udc1f\ud835\udc22subscript\ud835\udc1f\ud835\udc23\\psi_{p}(z_{i},z_{j})=\\mu(z_{i},z_{j})k(\\mathbf{f_{i}},\\mathbf{f_{j}})italic_\u03c8 start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) = italic_\u03bc ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) italic_k ( bold_f start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT , bold_f start_POSTSUBSCRIPT bold_j end_POSTSUBSCRIPT ) between any pair of voxels, regardless of their spatial distance. The Pott\u2019s Model is commonly used as the label compatibility function, giving \u03bc(zi,zj)=[zi\u2260zj]\ud835\udf07subscript\ud835\udc67\ud835\udc56subscript\ud835\udc67\ud835\udc57delimited-[]subscript\ud835\udc67\ud835\udc56subscript\ud835\udc67\ud835\udc57\\mu(z_{i},z_{j})=[z_{i}\\neq z_{j}]italic_\u03bc ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) = [ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2260 italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ]. The corresponding energy penalty is given by the function k\ud835\udc58kitalic_k, which is defined over an arbitrary feature space, with \ud835\udc1f\ud835\udc22,\ud835\udc1f\ud835\udc23subscript\ud835\udc1f\ud835\udc22subscript\ud835\udc1f\ud835\udc23\\mathbf{f_{i}},\\mathbf{f_{j}}bold_f start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT , bold_f start_POSTSUBSCRIPT bold_j end_POSTSUBSCRIPT being the feature vectors of the pair of voxels. Kr\u00e4henb\u00fchl and Koltun (2011) observed that if the penalty function is defined as a linear combination of Gaussian kernels, k(\ud835\udc1f\ud835\udc22,\ud835\udc1f\ud835\udc23)=\u2211m=1Mw(m)k(m)(\ud835\udc1f\ud835\udc22,\ud835\udc1f\ud835\udc23)\ud835\udc58subscript\ud835\udc1f\ud835\udc22subscript\ud835\udc1f\ud835\udc23superscriptsubscript\ud835\udc5a1\ud835\udc40superscript\ud835\udc64\ud835\udc5asuperscript\ud835\udc58\ud835\udc5asubscript\ud835\udc1f\ud835\udc22subscript\ud835\udc1f\ud835\udc23k(\\mathbf{f_{i}},\\mathbf{f_{j}})=\\sum_{m=1}^{M}{w^{(m)}k^{(m)}(\\mathbf{f_{i}},\\mathbf{f_{j}})}italic_k ( bold_f start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT , bold_f start_POSTSUBSCRIPT bold_j end_POSTSUBSCRIPT ) = \u2211 start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_w start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT ( bold_f start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT , bold_f start_POSTSUBSCRIPT bold_j end_POSTSUBSCRIPT ), the model lends itself for very efficient inference with mean field approximation, after expressing message passing as convolutions with the Gaussian kernels in the space of the feature vectors \ud835\udc1f\ud835\udc22,\ud835\udc1f\ud835\udc23subscript\ud835\udc1f\ud835\udc22subscript\ud835\udc1f\ud835\udc23\\mathbf{f_{i}},\\mathbf{f_{j}}bold_f start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT , bold_f start_POSTSUBSCRIPT bold_j end_POSTSUBSCRIPT.", "We extended the work of the original authors and implemented a 3D version of the CRF for processing multi-modal scans. We make use of two Gaussian kernels, which operate in the feature space defined by the voxel coordinates pi,dsubscript\ud835\udc5d\ud835\udc56\ud835\udc51p_{i,d}italic_p start_POSTSUBSCRIPT italic_i , italic_d end_POSTSUBSCRIPT and the intensities of the c\ud835\udc50citalic_c-th modality-channel Ii,csubscript\ud835\udc3c\ud835\udc56\ud835\udc50I_{i,c}italic_I start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT for voxel i\ud835\udc56iitalic_i. The smoothness kernel, k(1)(\ud835\udc1f\ud835\udc22,\ud835\udc1f\ud835\udc23)=exp(\u2212\u2211d={x,y,z}|pi,d\u2212pj,d|22\u03c3\u03b1,d2)superscript\ud835\udc581subscript\ud835\udc1f\ud835\udc22subscript\ud835\udc1f\ud835\udc23\ud835\udc52\ud835\udc65\ud835\udc5dsubscript\ud835\udc51\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udc5d\ud835\udc56\ud835\udc51subscript\ud835\udc5d\ud835\udc57\ud835\udc5122superscriptsubscript\ud835\udf0e\ud835\udefc\ud835\udc512k^{(1)}(\\mathbf{f_{i}},\\mathbf{f_{j}})=exp\\Big{(}-\\sum_{d=\\{x,y,z\\}}{\\frac{|p_{i,d}-p_{j,d}|^{2}}{2\\sigma_{\\alpha,d}^{2}}}\\Big{)}italic_k start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT ( bold_f start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT , bold_f start_POSTSUBSCRIPT bold_j end_POSTSUBSCRIPT ) = italic_e italic_x italic_p ( - \u2211 start_POSTSUBSCRIPT italic_d = { italic_x , italic_y , italic_z } end_POSTSUBSCRIPT divide start_ARG | italic_p start_POSTSUBSCRIPT italic_i , italic_d end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT italic_j , italic_d end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_\u03c3 start_POSTSUBSCRIPT italic_\u03b1 , italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ), is defined by a diagonal covariance matrix with elements the configurable parameters \u03c3\u03b1,dsubscript\ud835\udf0e\ud835\udefc\ud835\udc51\\sigma_{\\alpha,d}italic_\u03c3 start_POSTSUBSCRIPT italic_\u03b1 , italic_d end_POSTSUBSCRIPT, one for each axis. These parameters express the size and shape of neighborhoods that homogeneous labels are encouraged. The appearance kernel k(2)(\ud835\udc1f\ud835\udc22,\ud835\udc1f\ud835\udc23)=exp(\u2212\u2211d={x,y,z}|pi,d\u2212pj,d|22\u03c3\u03b2,d2\u2212\u2211c=1C|Ii,c\u2212Ij,c|22\u03c3\u03b3,c2)superscript\ud835\udc582subscript\ud835\udc1f\ud835\udc22subscript\ud835\udc1f\ud835\udc23\ud835\udc52\ud835\udc65\ud835\udc5dsubscript\ud835\udc51\ud835\udc65\ud835\udc66\ud835\udc67superscriptsubscript\ud835\udc5d\ud835\udc56\ud835\udc51subscript\ud835\udc5d\ud835\udc57\ud835\udc5122superscriptsubscript\ud835\udf0e\ud835\udefd\ud835\udc512superscriptsubscript\ud835\udc501\ud835\udc36superscriptsubscript\ud835\udc3c\ud835\udc56\ud835\udc50subscript\ud835\udc3c\ud835\udc57\ud835\udc5022superscriptsubscript\ud835\udf0e\ud835\udefe\ud835\udc502k^{(2)}(\\mathbf{f_{i}},\\mathbf{f_{j}})=exp\\Big{(}-\\sum_{d=\\{x,y,z\\}}{\\frac{|p_{i,d}-p_{j,d}|^{2}}{2\\sigma_{\\beta,d}^{2}}}-\\sum_{c=1}^{C}{\\frac{|I_{i,c}-I_{j,c}|^{2}}{2\\sigma_{\\gamma,c}^{2}}}\\Big{)}italic_k start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT ( bold_f start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT , bold_f start_POSTSUBSCRIPT bold_j end_POSTSUBSCRIPT ) = italic_e italic_x italic_p ( - \u2211 start_POSTSUBSCRIPT italic_d = { italic_x , italic_y , italic_z } end_POSTSUBSCRIPT divide start_ARG | italic_p start_POSTSUBSCRIPT italic_i , italic_d end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT italic_j , italic_d end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_\u03c3 start_POSTSUBSCRIPT italic_\u03b2 , italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG - \u2211 start_POSTSUBSCRIPT italic_c = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT divide start_ARG | italic_I start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT - italic_I start_POSTSUBSCRIPT italic_j , italic_c end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_\u03c3 start_POSTSUBSCRIPT italic_\u03b3 , italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) is defined similarly. The additional parameters \u03c3\u03b3,csubscript\ud835\udf0e\ud835\udefe\ud835\udc50\\sigma_{\\gamma,c}italic_\u03c3 start_POSTSUBSCRIPT italic_\u03b3 , italic_c end_POSTSUBSCRIPT can be interpreted as how strongly to enforce homogeneous appearance in the C\ud835\udc36Citalic_C input channels, when voxels in an area spatially defined by \u03c3\u03b2,dsubscript\ud835\udf0e\ud835\udefd\ud835\udc51\\sigma_{\\beta,d}italic_\u03c3 start_POSTSUBSCRIPT italic_\u03b2 , italic_d end_POSTSUBSCRIPT are identically labelled. Finally, the configurable weights w(1),w(2)superscript\ud835\udc641superscript\ud835\udc642w^{(1)},w^{(2)}italic_w start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT define the relative strength of the two factors.", "In this section we present a series of experiments in order to analyze the impact of each of the main contributions and to justify the choices made in the design of the proposed 11-layers, multi-scale 3D CNN architecture, referred to as the DeepMedic. Starting from the CNN baseline as discussed in Sec.\u00a02.1, we first explore the benefit of our proposed dense training scheme (cf. Sec.\u00a02.2), then investigate the use of deeper models (cf. Sec.\u00a02.3) and then evaluate the influence of the multi-scale dual pathway (cf. Sec.\u00a02.4). Finally, we compare our method with corresponding 2D variants to assess the benefit of processing 3D context.", "The following experiments are conducted using the TBI dataset with 61 multi-channel MRIs which is described in more detail later in Sec.\u00a04.1. Here, the images are randomly split into a validation and training set, with 15 and 46 images each. The same sets are used in all analyses. To monitor the progress of segmentation accuracy during training, we extract 10k random patches at regular intervals, with equal numbers extracted from each of the validation images. The patches are uniformly sampled from the brain region in order to approximate the true distribution of lesions and healthy tissue. Full segmentation of the validation datasets is performed every five epochs and the mean Dice similarity coefficient (DSC) is determined. Details on the configuration of the networks are provided in B.", "We compare our proposed dense training method with two other commonly used training schemes on the 5-layers baseline CNN (see Fig.\u00a02). The first common scheme trains on 173superscript17317^{3}17 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT patches extracted uniformly from the brain region, and the second scheme samples patches equally from the lesion and background class. We refer to these schemes as Puniuni{}_{\\text{uni}}start_FLOATSUBSCRIPT uni end_FLOATSUBSCRIPT and Peqeq{}_{\\text{eq}}start_FLOATSUBSCRIPT eq end_FLOATSUBSCRIPT. The results shown in Fig.\u00a06 show a correlation of sensitivity and specificity with the percentage of training samples that come from the lesion class. Peqeq{}_{\\text{eq}}start_FLOATSUBSCRIPT eq end_FLOATSUBSCRIPT performs poorly because of over-segmentation (high sensitivity, low specificity). Puniuni{}_{\\text{uni}}start_FLOATSUBSCRIPT uni end_FLOATSUBSCRIPT has better classification on the background class (high specificity), which leads to high mean voxel-wise accuracy since the majority corresponds to background, but not particularly high DSC scores due to under-segmentation (low sensitivity).", "To evaluate our dense training scheme, we train multiple models with varying sized image segments, equally sampled from lesions and background. The tested sizes of the segments go from 193superscript19319^{3}19 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT upwards to 293superscript29329^{3}29 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT. The models are referred to as \u201cS-d\ud835\udc51ditalic_d\u201d, where d\ud835\udc51ditalic_d is the side length of the cubic segments. For fair comparison, the batch sizes in all the experiments are adjusted to have a similar memory footprint and lead to similar training times as compared to training on Puni\ud835\udc62\ud835\udc5b\ud835\udc56{}_{uni}start_FLOATSUBSCRIPT italic_u italic_n italic_i end_FLOATSUBSCRIPT and Peq\ud835\udc52\ud835\udc5e{}_{eq}start_FLOATSUBSCRIPT italic_e italic_q end_FLOATSUBSCRIPT222Dense training on a whole volume was inapplicable in these experimental settings due to memory limitations but was previously shown to give similar results as training on uniformly sampled patches (Long et\u00a0al. (2015)).. We observe a great performance increase for model S-1919{19}19 over Peqeq{}_{\\text{eq}}start_FLOATSUBSCRIPT eq end_FLOATSUBSCRIPT. We account this partly to the efficient increase of the effective batch size (B\u22c5V\u22c5\ud835\udc35\ud835\udc49B\\cdot Vitalic_B \u22c5 italic_V in Eq.\u00a0(4)), but also to the altered distribution of training samples. As we increase the size of the training segments further, we quickly reach a balance between the sensitivity of Peq\ud835\udc52\ud835\udc5e{}_{eq}start_FLOATSUBSCRIPT italic_e italic_q end_FLOATSUBSCRIPT and the specificity of Puni\ud835\udc62\ud835\udc5b\ud835\udc56{}_{uni}start_FLOATSUBSCRIPT italic_u italic_n italic_i end_FLOATSUBSCRIPT, which results in improved segmentation as expressed by the DSC.", "The segment size is a hyper-parameter in our model. We observe that the increase in performance with increasing segment size quickly levels off, and similar performance is obtained for a wide range of segment sizes, which allows for easy configuration. For the remaining experiments, all models were trained on segments of size 253superscript25325^{3}25 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT.", "The 5-layers baseline CNN (Fig.\u00a02), here referred to as the \u201cShallow\u201d model, is extended to 9-layers by replacing each convolutional layer that uses 53superscript535^{3}5 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT kernels with two layers that use 33superscript333^{3}3 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT kernels (Fig.\u00a04). This model is referred to as \u201cDeep\u201d. Training the latter, however, utterly fails with the model making only predictions corresponding to the background class. This problem is related to the challenge of preserving the signal as it propagates through deep networks and its variance gets multiplied with the variance of the weights, as previously discussed in Sec.\u00a02.3. One of the causes is that the weights of both models have been initialized with the commonly used scheme of sampling from the normal distribution \ud835\udca9(0,0.01)\ud835\udca900.01\\mathcal{N}(0,0.01)caligraphic_N ( 0 , 0.01 ) (cf. Krizhevsky et\u00a0al. (2012)). In comparison, the initialization scheme by He et\u00a0al. (2015), derived for preserving the signal in the initial stage of training, results in higher values and overcomes this problem. Further preservation of the signal is obtained by employing Batch Normalization. This results in an enhanced 9-layers model which we refer to as \u201cDeep+\u201d, and using the same enhancements on the Shallow model yields \u201cShallow+\u201d. The significant performance improvement of Deep+ over Shallow+, as shown in Fig.\u00a07, is the result of the greater representational power of the deeper network. The two models need similar computational times, which highlights the benefits of utilizing small kernels in the design of 3D CNNs. Although the deeper model requires more sequential (layer by layer) computations on the GPU, those are faster due to the smaller kernel size.", "The final version of the proposed network architecture, referred to as \u201cDeepMedic\u201d, is built by extending the Deep+ model with a second convolutional pathway that is identical to the first one. Two hidden layers are added for combining the multi-scale features before the classification layer, resulting in a deep network of 11-layers (cf. Fig.\u00a05). The input segments to the second pathway are extracted from the images down-sampled by a factor of three. Thus, the network is capable of capturing context in a 513superscript51351^{3}51 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT area of the original image through the 173superscript17317^{3}17 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT receptive field of the lower-resolution pathway, while only doubling the computational and memory requirements over the single pathway CNN. In comparison, the most recent 2D CNN systems proposed for lesion segmentation (Havaei et\u00a0al. (2015); Pereira et\u00a0al. (2015)) have a receptive field limited to 332superscript33233^{2}33 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT voxels.", "Figure\u00a08 shows the improvement DeepMedic achieves over the single pathway model Deep+. In Fig.\u00a09 we show two representative visual examples of this improvement when using the multi-scale CNN. Finally, we confirm that the performance increase can be accounted to the additional context and not the additional capacity of DeepMedic. To this end, we build a big single-scale model by doubling the FMs at each of the 9-layers of Deep+ and adding two hidden layers. This 11-layers deep and wide model, referred to as \u201cBigDeep+\u201d, has the same number of parameters as DeepMedic. The performance of the model is not improved, while showing signs of over-fitting.", "Acquired brain MRI scans are often anisotropic. Such is the case for most sequences in our TBI dataset, which have been acquired with lower axial resolution, except for the isotropic MPRAGE. We perform a series of experiments to investigate the behaviour of 2D networks and assess the benefit of processing 3D context in this setting.", "DeepMedic can be converted to 2D by setting the third dimension of each kernel to one. This way only information from the surrounding context on the axial plane influences the classification of each voxel. If 2D segments are given as input, the dimensionality of the feature maps decreases and so does the memory required. This allows developing 2D variants with increased width, depth and size of training batch with similar requirements as the 3D version, which are valid candidates for model selection in practical scenarios. We assess various configurations and present some representatives in Table 1(b) along with their performance. Best segmentation among investigated 2D variants is achieved by a 19-layers, multi-scale network, reaching 61.5% average DSC on the validation fold. The decline from the 66.6% DSC achieved by the 3D version of DeepMedic indicates the importance of processing 3D context even in settings where most acquired sequences have low resolution along a certain axis.", "The proposed system consisting of the DeepMedic CNN architecture, optionally coupled with a fully connected CRF, is evaluated on three lesion segmentation tasks including challenging clinical data from patients with traumatic brain injuries, brain tumors, and ischemic stroke. Quantitative evaluation and comparisons with state-of-the-art are reported for each of the tasks.", "Sixty-six patients with moderate-to-severe TBI who required admission to the Neurosciences Critical Care Unit at Addenbrooke\u2019s Hospital, Cambridge, UK, underwent imaging using a 3-Tesla Siemens Magnetom TIM Trio within the first week of injury. Ethical approval was obtained from the Local Research Ethics Committee (LREC 97/290) and written assent via consultee agreement was obtained for all patients. The structural MRI sequences that are used in this work are isotropic MPRAGE (1mm\u00d7mm\\timesitalic_m italic_m \u00d71mm\u00d7mm\\timesitalic_m italic_m \u00d71mm\ud835\udc5a\ud835\udc5ammitalic_m italic_m), axial FLAIR, T2 and Proton Density (PD) (0.7mm\u00d7mm\\timesitalic_m italic_m \u00d70.7mm\u00d7mm\\timesitalic_m italic_m \u00d75mm\ud835\udc5a\ud835\udc5ammitalic_m italic_m), and Gradient-Echo (GE) (0.86mm\u00d7mm\\timesitalic_m italic_m \u00d70.86mm\u00d7mm\\timesitalic_m italic_m \u00d75mm\ud835\udc5a\ud835\udc5ammitalic_m italic_m). All visible lesions were manually annotated on the FLAIR and GE sequences with separate labeling for each lesion type. In nine patients the presence of hyperintense white matter lesions that were felt to be chronic in nature were also annotated. Artifacts, for example, signal loss secondary to intraparenchymal pressure probes, were also noted. For the purpose of this study we focus on binary segmentation of all abnormalities within the brain tissue. Thus, we merged all classes that correspond to intra-cerebral abnormalities into a single \u201clesion\u201d label. Extra-cerebral pathologies such as epidural and subdural hematoma were treated as background. We excluded two datasets because of corrupted FLAIR images, two cases because no lesions were found and one case  because of a major scanning artifact corrupting the images. This results in a total of 61 cases used for quantitative evaluation. Brain masks were obtained using the ROBEX tool (Iglesias et\u00a0al. (2011)). All images were resampled to an isotropic 1mm31\ud835\udc5asuperscript\ud835\udc5a31mm^{3}1 italic_m italic_m start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT resolution, with dimensions 193\u00d7\\times\u00d7229\u00d7\\times\u00d7193 and affinely registered (Studholme et\u00a0al. (1999)) to MNI space using the atlas by Grabner et\u00a0al. (2006). No bias field correction was used as preliminary results showed that this can negatively affect lesion appearance. Image intensities were normalized to have zero-mean and unit variance, as it has been reported that this improves CNN results (Jarrett et\u00a0al. (2009)).", "Network configuration and training: The network architecture corresponds to the one described in Sec.\u00a03.4, i.e. a dual-pathway, 11-layers deep CNN. The training data is augmented by adding images reflected along the sagittal axis. To make the network invariant to absolute intensities we also shift the intensities of each MR channel c\ud835\udc50citalic_c of every training segment by ic=rc\u03c3csubscript\ud835\udc56\ud835\udc50subscript\ud835\udc5f\ud835\udc50subscript\ud835\udf0e\ud835\udc50i_{c}=r_{c}\\sigma_{c}italic_i start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = italic_r start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT italic_\u03c3 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT. rcsubscript\ud835\udc5f\ud835\udc50r_{c}italic_r start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT is sampled for every segment from \ud835\udca9(0,0.1)\ud835\udca900.1\\mathcal{N}(0,0.1)caligraphic_N ( 0 , 0.1 ) and \u03c3csubscript\ud835\udf0e\ud835\udc50\\sigma_{c}italic_\u03c3 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT is the standard deviation of intensities under the brain mask in the corresponding image. The network is regularized using dropout (Hinton et\u00a0al. (2012)) with a rate of 2% on all convolutional layers, which is in addition to a 50% rate used on the last two layers. The network is evaluated with 5-fold cross-validation on the 61 subjects.", "CRF configuration: The parameters of the fully connected CRF are determined in a configuration experiment using random-search and 15 randomly selected subjects from the TBI database with predictions from a preliminary version of the corresponding model. The 15 subjects are reshuffled into the 5-folds used for subsequent evaluation.", "Random Forest baseline: We have done our best to set up a competitive baseline for comparison. We employ a context-sensitive Random Forest, similar to the model presented by Zikic et\u00a0al. (2012) for brain tumors except that we apply the forest to the MR images without additional tissue specific priors. We train a forest with 50 trees and maximum depth of 30. Larger size did not improve results. Training data points are approximately equally sampled from lesion and background classes, with the optimal balance empirically chosen. Two hundred randomized cross-channel box features are evaluated at each split node with maximum offsets and box sizes of 20mm. The same folds of training and test sets are used as for our CNN approach.", "Table 1 summarizes the results on TBI. Our CNN significantly outperforms the Random Forest baseline, while the relatively overall low DSC values indicate the difficulty of the task. Due to randomness during training the local minima where a network converges are different between training sessions and some errors they produce differ (Choromanska et\u00a0al. (2015)). To clear the unbiased errors of the network we form an ensemble of three similar networks, aggregating their output by averaging. This ensemble yields better performance in all metrics but also allows us to investigate the behaviour of our network focusing only on the biased errors. Fig.\u00a010 shows the DSC obtained by the ensemble on each subject in relation to the manually segmented and predicted lesion volume. The network is capable of segmenting cases with very small lesions, although, performance is less robust in these cases as even small errors have large influence on the DSC metric. Investigation of the predicted lesion volume, which is an important biomarker for prognostication, shows that the network is neither biased towards the lesion nor background class, with promising results even on cases with very small lesions. Furthermore, we separately evaluate the influence of the post-processing with the fully connected CRF. As shown in Table 1, the CRF yields improvements over all classifiers. Effects are more prominent when the performance of the primary segmenter degrades, which shows the robustness of this regulariser. Fig.\u00a011 shows three representative cases.", "For brain tumors, we evaluate our system on the data from the 2015 Brain Tumor Segmentation Challenge (BRATS) (Menze et\u00a0al. (2015)). The training set consists of 220 cases with high grade (HG) and 54 cases with low grade (LG) glioma for which corresponding reference segmentations are provided. The segmentations include the following tumor tissue classes: 1) necrotic core, 2) edema, 3) non-enhancing and 4) enhancing core. The test set consists of 110 cases of both HG and LG but the grade is not revealed. Reference segmentations for the test set are hidden and evaluation is carried out via an online system. For evaluation, the four predicted labels are merged into different sets of whole tumor (all four classes), the core (classes 1,3,4), and the enhancing tumor (class 4)333For interpretation of the results note that, to the best of our knowledge, cases where the \u201cenhancing tumor\u201d class is not present in the manual segmentation are considered as zeros for the calculation of average performance by the evaluation platform, lowering the upper bound for this class.. For each subject, four MRI sequences are available, FLAIR, T1, T1-contrast and T2. The datasets are pre-processed by the organizers and provided as skull-stripped, registered to a common space and resampled to isotropic 1mm31\ud835\udc5asuperscript\ud835\udc5a31mm^{3}1 italic_m italic_m start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT resolution. Dimensions of each volume are 240\u00d7\\times\u00d7240\u00d7\\times\u00d7155. We add minimal pre-processing of normalizing the brain-tissue intensities of each sequence to have zero-mean and unit variance.", "Network configuration and training: We modify the DeepMedic architecture to handle multi-class problems by extending the classification layer to five feature maps (four tumor classes plus background). The rest of the configuration remains unchanged. We enrich the dataset with sagittal reflections. Opposite to the experiments on TBI, we do not employ the intensity perturbation and dropout on convolutional layers, because the network should not require as much regularisation with this large database. The network is trained on image segments extracted with equal probability centred on the whole tumor and healthy tissue. The distribution of the classes captured by our training scheme is provided in C.", "To examine our network\u2019s behaviour, we first evaluate it on the training data of the challenge. For this, we run a 5-fold cross validation where each fold contains both HG and LG images. We then retrain the network using all training images, before applying it on the test data.", "CRF configuration: For the multi-class problem it is challenging to find a global set of parameters for the CRF which can consistently improve the segmentation of all classes. So instead we merge the four predicted probability maps into a single \u201cwhole tumor\u201d map for CRF post-processing. The CRF then only refines the boundaries between tumor and background and additionally removes isolated false positives. Similarly to the experiments on TBI, the CRF is configured on a random subset of 44 HG and 18 LG training images, which are then reshuffled into the subsequent 5-fold cross validation.", "Quantitative results from the application of the DeepMedic, the CRF and an ensemble of three similar networks on the training data are presented in Table 2. The latter two offer an improvement, albeit fairly small since the performance of DeepMedic is already rather high in this task. Also shown are results from previous works, as reported on the online evaluation platform. Various settings may vary among submissions, such as the pre-processing pipeline or the number of folds used for cross-validation. Still it appears that our system performs favourably compared to previous state-of-the-art, including the semi-automatic system of Bakas et\u00a0al. (2015) (bakas1) who won the latest challenge and the method of Pereira et\u00a0al. (2015) (peres1), which is based on grade-specific 2D CNNs and requires visual inspection of the tumor and identification of the grade by the user prior to segmentation. Examples of segmentations obtained with our method are shown in Fig.\u00a012. DeepMedic behaves very well in preserving the hierarchical structure of the tumor, which we account to the large context processed by our multi-scale network.", "Table\u00a03 shows the results of our method on the BRATS test data. Results of other submissions are not accessible. The decrease in performance is possibly due to the the inclusion of test images that vary significantly from the training data, such as cases acquired in clinical centers that did not provide any of the training images, something that was confirmed by the organisers. Note that performance gains obtained with the CRF are larger in this case. This indicates not only that its configuration has not overfitted to the training database but also that the CRF is robust to factors of variation between acquisition sites, which complements nicely the more sensitive CNN.", "We participated in the 2015 Ischemic Stroke Lesion Segmentation (ISLES) challenge, where our system achieved the best results among all participants on sub-acute ischemic stroke lesions (Maier et\u00a0al. (2017)). In the training phase of the challenge, 28 datasets have been made available, along with manual segmentations. Each dataset included T1, T1-contrast, FLAIR and DWI sequences. All images were provided as skull-stripped and resampled to isotropic 1mm31\ud835\udc5asuperscript\ud835\udc5a31mm^{3}1 italic_m italic_m start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT voxel resolution. Each volume is of size 230\u00d7\\times\u00d7230\u00d7\\times\u00d7154. In the testing stage, teams were provided with 36 datasets for evaluation. The test data were acquired in two clinical centers, with one of them being the same that provided all training images. Corresponding expert segmentations were hidden and results had to be submitted to an online evaluation platform. Similar to BRATS, the only pre-processing that we applied is the normalization of each image to the zero-mean and unit variance.", "Network Configuration and Training: The configuration of the network employed is described in Kamnitsas et\u00a0al. (2015). The main difference with the configuration used for TBI and tumors as employed above is the relatively smaller number of FMs in the low-resolution pathway. This choice should not significantly influence accuracy on the generally small SISS lesions but it allowed us to lower the computational cost.", "Similar to the other experiments, we evaluate our network with a 5-fold cross validation on the training datasets. We use data augmentation with sagittal reflections. For the testing phase of the challenge, we trained an ensemble of three networks on all training cases and aggregate their predictions by averaging.", "CRF configuration: The parameters of the CRF were configured via a random search on the whole training dataset.", "The performance of our system on the training data is shown in Table\u00a04. Significant improvement is achieved by the structural regularisation offered by the CRF, although it could be partially accounted for by overfitting the training data during the CRF\u2019s configuration. Examples for visual inspection are shown in Fig.\u00a013.", "For the testing phase of the challenge we formed an ensemble of three networks, coupled with the fully connected CRF. Our submission ranked first, indicating superior performance on this challenging task among 14 submissions. Table\u00a05 shows our results, along with the other two top entries (Feng et\u00a0al. (2015); Halme et\u00a0al. (2015)). Among the other participating methods was the CNN of Havaei et\u00a0al. (2015) with 3 layers of 2D convolutions. That method perfomed less well on this challenging task (Maier et\u00a0al. (2017)). This points out the advantage offered by 3D context, the large field of view of DeepMedic thanks to multi-scale processing and the representational power of deeper networks. It is important to note the decrease of performance in comparison to the training set. All methods performed worse on the data coming from the second clinical center, including the method of Feng et\u00a0al. (2015) that is not machine-learning based. This highlights a general difficulty with current approaches when applied on multi-center data.", "Our CNN is implemented using the Theano library (Bastien et\u00a0al. (2012)). Each training session requires approximately one day on an NVIDIA GTX Titan X GPU using cuDNN v5.0. The efficient architecture of DeepMedic also allows models to be trained on GPUs with only 3GB of memory. Note that although dimensions of the volumes in the processed databases do not allow dense training on whole volumes for this size of network, dense inference on a whole volume is still possible, as it requires only a forward-pass and thus less memory. In this fashion segmentation of a volume takes less than 30 seconds but requires 12 GB of GPU memory. Tiling the volume into multiple segments of size 353superscript35335^{3}35 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT allows inference on 3 GB GPUs in less than three minutes.", "Our 3D fully connected CRF is implemented by extending the original source code by Kr\u00e4henb\u00fchl and Koltun (2011). A CPU implementation is fast, capable of processing a five-channel brain scan in under three minutes. Further speed-up could be achieved with a GPU implementation, but was not found necessary in the scope of this work.", "We have presented DeepMedic, a 3D CNN architecture for automatic lesion segmentation that surpasses state-of-the-art on challenging data. The proposed novel training scheme is not only computationally efficient but also offers an adaptive way of partially alleviating the inherent class-imbalance of segmentation problems. We analyzed the benefits of using small convolutional kernels in 3D CNNs, which allowed us to develop a deeper and thus more discriminative network, without increasing the computational cost and number of trainable parameters. We discussed the challenges of training deep neural networks and the adopted solutions from the latest advances in deep learning. Furthermore, we proposed an efficient solution for processing large image context by the use of parallel convolutional pathways for multi-scale processing, alleviating one of the main computational limitations of previous 3D CNNs. Finally, we presented the first application of a 3D fully connected CRF on medical data, employed as a post-processing step to refine the network\u2019s output, a method that has also been shown promising for processing 2D natural images (Chen et\u00a0al. (2014)). The design of the proposed system is well suited for processing medical volumes thanks to its generic 3D nature. The capabilities of DeepMedic and the employed CRF for capturing 3D patterns exceed those of 2D networks and locally connected random fields, models that have been commonly used in previous work. At the same time, our system is very efficient at inference time, which allows its adoption in a variety of research and clinical settings.", "The generic nature of our system allows its straightforward application for different lesion segmentation tasks without major adaptations. To the best of our knowledge, our system achieved the highest reported accuracy on a cohort of patients with severe TBI. As a comparison, we improved over the reported performance of the pipeline in Rao et\u00a0al. (2014). Important to note is that the latter work focused only on segmentation of contusions, while our system has been shown capable of segmenting even small and diffused pathologies. Additionally, our pipeline achieved state-of-the-art performance on both public benchmarks of brain tumors (BRATS 2015) and stroke lesions (SISS ISLES 2015). We believe performance can be further improved with task- and data-specific adjustments, for instance in the pre-processing, but our results show the potential of this generically designed segmentation system.", "When applying our pipeline to new tasks, a laborious process is the reconfiguration of the CRF. The model improved our system\u2019s performance with statistical significance in all investigated tasks, most profoundly when the performance of the underlying classifier degrades, proving its flexibility and robustness. Finding optimal parameters for each task, however, can be challenging. This became most obvious on the task of multi-class tumor segmentation. Because the tumor\u2019s substructures vary significantly in appearance, finding a global set of parameters that yields improvements on all classes proved difficult. Instead, we applied the CRF in a binary fashion. This CRF model can be configured with a separate set of parameters for each class. However the larger parameter space would complicate its configuration further. Recent work from Zheng et\u00a0al. (2015) showed that this particular CRF can be casted as a neural network and its parameters can be learned with regular gradient descent. Training it in an end-to-end fashion on top of a neural network would alleviate the discussed problems. This will be explored as part of future work.", "The discriminative power of the learned features is indicated by the success of recent CNN-based systems in matching human performance in domains where it was previously considered too ambitious (He et\u00a0al. (2015); Silver et\u00a0al. (2016)). Analysis of the automatically extracted information could potentially provide novel insights and facilitate research on pathologies for which little prior knowledge is currently available. In an attempt to illustrate this, we explore what patterns have been learned automatically for the lesion segmentation tasks. We visualize the activations of DeepMedic\u2019s FMs when processing a subject from our TBI database. Many appearing patterns are difficult to interpret, especially in deeper layers. In Fig.\u00a014 we provide some examples that have an intuitive explanation. One of the most interesting findings is that the network learns to identify the ventricles, CSF, white and gray matter. This reveals that differentiation of tissue type is beneficial for lesion segmentation. This is in line with findings in the literature, where segmentation performance of traditional classifiers was significantly improved by incorporation of tissue priors (Van\u00a0Leemput et\u00a0al. (1999); Zikic et\u00a0al. (2012)). It is intuitive that different types of lesions affect different parts of the brain depending on the underlying mechanisms of the pathology. A rigorous analysis of spatial cues extracted by the network may reveal correlations that are not well defined yet.", "Similarly intriguing is the information extracted in the low-resolution pathway. As they process greater context, these neurons gain additional localization capabilities. The activations of certain FMs form fields in the surrounding areas of the brain. These patterns are preserved in the deepest hidden layers, which indicates they are beneficial for the final segmentation (see two last rows of Fig.\u00a014). We believe these cues provide a spatial bias to the system, for instance that large TBI contusions tend to occur towards the front and sides of the brain (see Fig.\u00a00(c)). Furthermore, the interaction of the multi-resolution features can be observed in FMs of the hidden layer that follows the concatenation of the pathways. The network learns to weight the output of the two pathways, preserving low resolution in certain parts and show fine details in others (bottom row of Fig.\u00a014, first three FMs). Our assumption is that the low-resolution pathway provides a rough localization of large pathologies and brain areas that are challenging to segment, which reserves the rest of the network\u2019s capacity for learning detailed patterns associated with the detection of smaller lesions, fine structures and ambiguous areas.", "The findings of the above exploration lead us to believe that great potential lies into fusing the discriminative power of the \u201cdeep black box\u201d with the knowledge acquired over years of targeted biomedical research. Clinical knowledge is available for certain pathologies, such as spatial priors for white matter lesions. Previously engineered models have been proven effective in tackling fundamental imaging problems, such as brain extraction, tissue segmentation and bias field correction. We show that a network is capable of automatically extracting some of this information. It would be interesting, however, to investigate structured ways for incorporating such existing information as priors into the network\u2019s feature space, which should simplify the optimization problem while letting a specialist guide the network towards an optimal solution.", "Although neural networks seem promising for medical image analysis, making the inference process more interpretable is required. This would allow understanding when the network fails, an important aspect in biomedical applications. Although the output is bounded in the [0,1]01[0,1][ 0 , 1 ] range and commonly referred to as probability for convenience, it is not a true probability in a Bayesian sense. Research towards Bayesian networks aims to alleviate this limitation. An example is the recent work of Gal and Ghahramani (2015) who show that model confidence can be estimated via sampling the dropout mask.", "A general point should be made about the performance drop observed when our system is applied on test datasets of BRATS and ISLES in comparison to its cross-validated performance on the training data. In both cases, subsets of the test images were acquired in clinical centers different from the ones of training datasets. Differences in scanner type and acquisition protocols have significant impact on the appearance of the images. The issue of multi-center data heterogeneity is considered a major bottleneck for enabling large-scale imaging studies. This is not specific to our approach, but a general problem in medical image analysis. One possible way of making the CNN invariant to the data heterogeneity is to learn a generative model for the data acquisition process, and use this model in the data augmentation step. This is a direction we explore as part of future work.", "In order to facilitate further research in this area and to provide a baseline for future evaluations, we make the source code of the entire system publicly available."], "figure_types": {"7c2bcf6f32b05a04cd3444c030db743e5666af88/10-Table1-1.png": "table", "7c2bcf6f32b05a04cd3444c030db743e5666af88/11-Figure10-1.png": "plot", "7c2bcf6f32b05a04cd3444c030db743e5666af88/11-Figure11-1.png": "photograph(s)", "7c2bcf6f32b05a04cd3444c030db743e5666af88/12-Table2-1.png": "photograph(s)", "7c2bcf6f32b05a04cd3444c030db743e5666af88/13-Table3-1.png": "table", "7c2bcf6f32b05a04cd3444c030db743e5666af88/13-Table4-1.png": "table", "7c2bcf6f32b05a04cd3444c030db743e5666af88/14-Figure13-1.png": "photograph(s)", "7c2bcf6f32b05a04cd3444c030db743e5666af88/14-Table5-1.png": "table", "7c2bcf6f32b05a04cd3444c030db743e5666af88/15-Figure14-1.png": "photograph(s)", "7c2bcf6f32b05a04cd3444c030db743e5666af88/17-TableB.1-1.png": "table", "7c2bcf6f32b05a04cd3444c030db743e5666af88/17-TableC.1-1.png": "table", "7c2bcf6f32b05a04cd3444c030db743e5666af88/2-Figure1-1.png": "photograph(s)", "7c2bcf6f32b05a04cd3444c030db743e5666af88/4-Figure2-1.png": "schematic", "7c2bcf6f32b05a04cd3444c030db743e5666af88/5-Figure3-1.png": "schematic", "7c2bcf6f32b05a04cd3444c030db743e5666af88/5-Figure4-1.png": "schematic", "7c2bcf6f32b05a04cd3444c030db743e5666af88/6-Figure5-1.png": "schematic", "7c2bcf6f32b05a04cd3444c030db743e5666af88/8-Figure6-1.png": "plot", "7c2bcf6f32b05a04cd3444c030db743e5666af88/8-Figure7-1.png": "plot", "7c2bcf6f32b05a04cd3444c030db743e5666af88/8-Figure8-1.png": "plot", "7c2bcf6f32b05a04cd3444c030db743e5666af88/9-Figure9-1.png": "photograph(s)"}}, "2208.01626": {"paper_id": "paper_125", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "arxiv_url": "https://arxiv.org/abs/2208.01626", "s2orc_url": "https://www.semanticscholar.org/paper/04e541391e8dce14d099d00fb2c21dbbd8afe87f", "all_figures_tables": {"04e541391e8dce14d099d00fb2c21dbbd8afe87f/10-Figure9-1.png": "Figure 9: Text-based image editing with fader control. By reducing (top rows) or increasing (bottom) the cross-attention of the specified words (marked with an arrow), we can control the extent to which it influences the generated image.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/11-Figure10-1.png": "Figure 10: Editing of real images. On the left, inversion results using DDIM [40] sampling. We reverse the diffusion process initialized on a given real image and text prompt. This results in a latent noise that produces an approximation to the input image when fed to the diffusion process. Afterward, on the right, we apply our Prompt-to-Prompt technique to edit the images.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/11-Figure11-1.png": "Figure 11: Inversion Failure Cases. Current DDIM-based inversion of real images might result in unsatisfied reconstructions.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/12-Figure12-1.png": "Figure 12: Mask-based editing. Using the attention maps, we preserve the unedited parts of the image when the inversion distortion is significant. This does not require any user-provided masks, as we extract the spatial information from the model using our method. Note how the cat\u2019s identity is retained after the editing process.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/17-Figure13-1.png": "Figure 13: Additional results for Prompt-to-Prompt editing by word swap.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/18-Figure14-1.png": "Figure 14: Additional results for Prompt-to-Prompt editing by adding a specification.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/19-Figure15-1.png": "Figure 15: Additional results for Prompt-to-Prompt editing by attention re-weighting.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/2-Figure1-1.png": "Figure 1: Our method provides variety of Prompt-to-Prompt editing capabilities. The user can tune the level of influence of an adjective word (top-left), replace items in the image (top-right), specify a style for an image (bottom-left), or make further refinements over the generated image (bottom-right). The manipulations are infiltrated through the cross-attention mechanism of the diffusion model without the need for any specifications over the image pixel space.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/3-Figure2-1.png": "Figure 2: Content modification through attention injection. We start from an original image generated from the prompt \u201dlemon cake\u201d, and modify the text prompt to a variety of other cakes. On the top rows, we inject the attention weights of the original image during the diffusion process. On the bottom, we only use the same random seeds as the original image, without injecting the attention weights. The latter leads to a completely new structure that is hardly related to the original image.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/4-Figure3-1.png": "Figure 3: Method overview. Top: visual and textual embedding are fused using cross-attention layers that produce spatial attention maps for each textual token. Bottom: we control the spatial layout and geometry of the generated image using the attention maps of a source image. This enables various editing tasks through editing the textual prompt only. When swapping a word in the prompt, we inject the source image maps Mt, overriding the target image maps M\u2217t , to preserve the spatial layout. Where in the case of adding a new phrase, we inject only the maps that correspond to the unchanged part of the prompt. Amplify or attenuate the semantic effect of a word achieved by re-weighting the corresponding attention map.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/5-Figure4-1.png": "Figure 4: Cross-attention maps of a text-conditioned diffusion image generation. The top row displays the average attention masks for each word in the prompt that synthesized the image on the left. The bottom rows display the attention maps from different diffusion steps with respect to the words \u201cbear\u201d and \u201cbird\u201d.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/6-Figure5-1.png": "Figure 5: Object preservation. By injecting only the attention weights of the word \u201cbutterfly\u201d, taken from the top-left image, we can preserve the structure and appearance of a single item while replacing its context. Note how the butterfly sits on top of all objects in a very plausible manner.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/7-Figure6-1.png": "Figure 6: Attention injection through a varied number of diffusion steps. On the top, we show the source image and prompt. In each row, we modify the content of the image by replacing a single word in the text and injecting the cross-attention maps of the source image ranging from 0% (on the left) to 100% (on the right) of the diffusion steps. Notice that on one hand, without our method, none of the source image content is guaranteed to be preserved. On the other hand, injecting the cross-attention throughout all the diffusion steps may over-constrain the geometry, resulting in low fidelity to the text prompt, e.g., the car (3rd row) becomes a bicycle with full cross-attention injection.", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/8-Figure7-1.png": "Figure 7: Editing by prompt refinement. By extending the description of the initial prompt, we can make local edits to the car (top rows) or global modifications (bottom rows).", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/9-Figure8-1.png": "Figure 8: Image stylization. By adding a style description to the prompt while injecting the source attention maps, we can create various images in the new desired styles that preserve the structure of the original image."}, "referred_figures_tables": [["04e541391e8dce14d099d00fb2c21dbbd8afe87f/3-Figure2-1.png", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/6-Figure5-1.png", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/17-Figure13-1.png"]], "question_id": [25], "question": ["Did the method proposed in this paper perform on par with or better than the state-of-the-art methods that require users to provide spatial masks for editing?"], "question_section": ["abstract"], "question_trigger_sentence": ["State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region."], "question_type": ["Shallow question"], "evidential_info": [[{"context": "To circumvent this, LLI-based methods nichol2021glide ; avrahami2022blendedlatent ; ramesh2022hierarchical require the user to explicitly mask a part of the image to be inpainted, and drive the edited image to change in the masked area only, while matching the background of the original image. This approach has provided appealing results, however, the masking procedure is cumbersome, hampering quick and intuitive text-driven editing. Moreover, masking the image content removes important structural information, which is completely ignored in the inpainting process. Therefore, some editing capabilities are out of the inpainting scope, such as modifying the texture of a specific object.", "rationale": "LLI-based methods requires to explicitly mask a part of the image to be in painted to edit images in the masked area only. However, the masking procedure is cumbersome, hampering quick and intuitive text-driven editing."}, {"context": "In this paper, we introduce an intuitive and powerful textual editingmethod to semantically edit images in pre-trained text-conditioned diffusion models via Prompt-to-Prompt manipulations. To do so, we dive deep into the cross-attention layers and explore their semantic strength as a handle to control the generated image. Specifically, we consider the internal cross-attention maps, which are high-dimensional tensors that bind pixels and tokens extracted from the prompt text. We find that these maps contain rich semantic relations which critically affect the generated image.", "rationale": "Most works that require only text are limited to global editing, and despite that, the proposed other localized editing techniques without any masks showed impressive results, but their work allowed changing textures but not modifying complex structures such as changing a bicycle to a car. and their approach requires training a network for each input. and the author's works don't require a training network for each input."}, {"context": "Our method, described in section\u00a03, enables intuitive text-only editing by controlling the spatial layout corresponding to each word in the user-provided prompt. In this section, we show several applications using this technique.", "rationale": "Author's work requires only a textual input by using the spatial features from the internal layers. which offers users a much more intuitive editing experience of modifying local or global details by modifying the text prompt."}, {"context": "Text-Only Localized Editing.We first demonstrate localized editing by modifying the user-provided prompt without requiring any user-provided mask. In fig.\u00a02, we depict an example where we generate an image using the prompt \u201clemon cake\u201d. Our method allows us to retain the spatial layout, geometry, and semantics when replacing the word \u201clemon\u201d with \u201cpumpkin\u201d (top row). Observe that the background is well-preserved, including the top-left lemons transforming into pumpkins. On the other hand, naively feeding the synthesis model with the prompt \u201cpumpkin cake\u201d results in a completely different geometry (3rd row), even when using the same random seed in a deterministic setting (i.e., DDIM song2020denoising ). Our method succeeds even for a challenging prompt such as \u201cpasta cake.\u201d (2nd row) \u2014 the generated cake consists of pasta layers with tomato sauce on top. Another example is provided in fig.\u00a05 where we do not inject the attention of the entire prompt but only the attention of a specific word \u2013 \u201cbutterfly\u201d. This enables the preservation of the original butterfly while changing the rest of the content. Additional results are provided in the appendix (fig.\u00a013).", "rationale": "Authors approach is more intuitive image editing by using textual prompts only. and enables various editing tasks and doesn't require model training, fine-tuning, extra data, or optimization. Authors even demonstrated that their methods can be applied to real images by using an existing inversion process. Authors' experiments and numerous results show that their method enables seamless editing in an intuitive text-based manner over extremely diverse images."}, {"context": "In this work, we uncovered the powerful capabilities of the cross-attention layers within text-to-image diffusion models.We showed that these high-dimensional layers have an interpretable representation of spatial maps that play a key role in tying the words in the text prompt to the spatial layout of the synthesized image.With this observation, we showed how various manipulations of the prompt can directly control attributes in the synthesized image, paving the way to various applications including local and global editing.This work is a first step towards providing users with simple and intuitive means to edit images, leveraging textual semantic power. It enables users to navigate through a semantic, textual, space, which exhibits incremental changes after each step, rather than producing the desired image from scratch after each text manipulation.", "rationale": "In this work, authors enable users to navigate through semantic, textual space which exhibits incremental changes after each step, rather than producing the desired images from scratch after each text manipulation."}, {"context": "Our approach constitutes an intuitive image editing interface through editing only the textual prompt, therefore called Prompt-to-Prompt. This method enables various editing tasks, which are challenging otherwise, and does not requires model training, fine-tuning, extra data, or optimization. Throughout our analysis, we discover even more control over the generation process, recognizing a trade-off between the fidelity to the edited prompt and the source image. We even demonstrate that our method can be applied to real images by using an existing inversion process. Our experiments and numerous results show that our method enables seamless editing in an intuitive text-based manner over extremely diverse images.", "rationale": "Authors introduced an intuitive and powerful textual editing method to semantically edit images in pre-trained text-conditioned diffusion models via Prompt-to-Prompt manipulations."}, {"context": "While most works that require only text (i.e., no masks) are limited to global editing crowson2022vqgan ; kwon2021clipstyler ,Bar-Tal et al.\u00a0bar2022text2live  proposed a text-based localized editing technique without using any mask, showing impressive results.Yet, their techniques mainly allow changing textures, but not modifying complex structures, such as changing a bicycle to a car.Moreover, unlike our method, their approach requires training a network for each input.", "rationale": "Authors demonstrated localized editing by modifying the user-provided mask. and demonstrated in Figure.2 and Figure 5, their method. for more results see appendix Figure 13."}, {"context": "Unlike previous works, our method requires textual input only, by using the spatial information from the internal layers of the generative model itself. This offers the user a much more intuitive editing experience of modifying local or global details by merely modifying the text prompt.", "rationale": "Authors method enabled intuitive text-only editing by controlling the spatial layout corresponding to each word in the user-provided prompt."}]], "composition": ["Yes their method did perform better than mask editing methods, as authors demonstrated by examples that their method is more intuitive for users using only prompt, and doesn't require to explicitly mask parts of the image which results to remove important structural information and doesn't modify complex structures information. And their work enables local or global modifications as well and besides their method doesn't require a training network."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["267"], "passages": ["Recently, large-scale language-image (LLI) models, such as Imagen\u00a0saharia2022photorealistic , DALL\u00b7E 2\u00a0ramesh2022hierarchical  and Parti\u00a0yu2022scaling , have shown phenomenal generative semantic and compositional power, and gained unprecedented attention from the research community and the public eye.These LLI models are trained on extremely large language-image datasets and use state-of-the-art image generative models including auto-regressive and diffusion models.However, these models do not provide simple editing means, and generally lack control over specific semantic regions of a given image. In particular, even the slightest change in the textual prompt may lead to a completely different output image.", "To circumvent this, LLI-based methods nichol2021glide ; avrahami2022blendedlatent ; ramesh2022hierarchical require the user to explicitly mask a part of the image to be inpainted, and drive the edited image to change in the masked area only, while matching the background of the original image. This approach has provided appealing results, however, the masking procedure is cumbersome, hampering quick and intuitive text-driven editing. Moreover, masking the image content removes important structural information, which is completely ignored in the inpainting process. Therefore, some editing capabilities are out of the inpainting scope, such as modifying the texture of a specific object.", "In this paper, we introduce an intuitive and powerful textual editingmethod to semantically edit images in pre-trained text-conditioned diffusion models via Prompt-to-Prompt manipulations. To do so, we dive deep into the cross-attention layers and explore their semantic strength as a handle to control the generated image. Specifically, we consider the internal cross-attention maps, which are high-dimensional tensors that bind pixels and tokens extracted from the prompt text. We find that these maps contain rich semantic relations which critically affect the generated image.", "Our key idea is that we can edit images by injecting the cross-attention maps during the diffusion process, controlling which pixels attend to which tokens of the prompt text during which diffusion steps. To apply our method to various creative editing applications, we show several methods to control the cross-attention maps through a simple and semantic interface (see fig.\u00a01). The first is to change a single token\u2019s value in the prompt (e.g., \u201cdog\u201d to \u201ccat\u201d), while fixing the cross-attention maps, to preserve the scene composition. The second is to globally edit an image, e.g., change the style, by adding new words to the prompt and freezing the attention on previous tokens, while allowing new attention to flow to the new tokens. The third is to amplify or attenuate the semantic effect of a word in the generated image.", "Our approach constitutes an intuitive image editing interface through editing only the textual prompt, therefore called Prompt-to-Prompt. This method enables various editing tasks, which are challenging otherwise, and does not requires model training, fine-tuning, extra data, or optimization. Throughout our analysis, we discover even more control over the generation process, recognizing a trade-off between the fidelity to the edited prompt and the source image. We even demonstrate that our method can be applied to real images by using an existing inversion process. Our experiments and numerous results show that our method enables seamless editing in an intuitive text-based manner over extremely diverse images.", "Image editing is one of the most fundamental tasks in computer graphics, encompassing the process of modifying an input image through the use of an auxiliary input, such as a label, scribble, mask, or reference image.A specifically intuitive way to edit an image is through textual prompts provided by the user.Recently, text-driven image manipulation has achieved significant progress using GANs\u00a0goodfellow2014generative ; brock2018large ; karras2021alias ; karras2019style ; karras2020analyzing , which are known for their high-quality generation, in tandem with CLIP\u00a0radford2021learning , which consists of a semantically rich joint image-text representation, trained over millions of text-image pairs.Seminal works patashnik2021styleclip ; gal2021stylegan ; xia2021tedigan ; abdal2021clip2stylegan  which combined these components were revolutionary, since they did not require extra manual labor, and produced highly realistic manipulations using text only.Bau et al. bau2021paint  further demonstrated how to use masks provided by the user, to localize the text-based editing and restrict the change to a specific spatial region.However, while GAN-based image editing approaches succeed on highly-curated datasets mokady2022self , e.g., human faces, they struggle over large and diverse datasets.", "To obtain more expressive generation capabilities, Crowson et al. crowson2022vqgan  use VQ-GAN esser2021taming , trained over diverse data, as a backbone.Other works\u00a0avrahami2022blended ; kim2022diffusionclip  exploit the recent Diffusion models ho2020denoising ; sohl2015deep ; song2019generative ; ho2020denoising ; song2020denoising ; rombach2021highresolution , which achieve state-of-the-art generation quality over highly diverse datasets, often surpassing GANs\u00a0dhariwal2021diffusion .Kim et al.\u00a0kim2022diffusionclip  show how to perform global changes, whereas Avrahami et al.\u00a0avrahami2022blended  successfully perform local manipulations using user-provided masks for guidance.", "While most works that require only text (i.e., no masks) are limited to global editing crowson2022vqgan ; kwon2021clipstyler ,Bar-Tal et al.\u00a0bar2022text2live  proposed a text-based localized editing technique without using any mask, showing impressive results.Yet, their techniques mainly allow changing textures, but not modifying complex structures, such as changing a bicycle to a car.Moreover, unlike our method, their approach requires training a network for each input.", "Numerous works ding2021cogview ; hinz2020semantic ; tao2020df ; li2019controllable ; li2019object ; qiao2019learn ; qiao2019mirrorgan ; ramesh2021zero ; zhang2018photographic ; crowson2022vqgan ; gafni2022make ; rombach2021highresolution  significantly advanced the generation of images conditioned on plain text, known as text-to-image synthesis. Several large-scale text-image models have recently emerged, such as Imagen\u00a0saharia2022photorealistic , DALL-E2\u00a0ramesh2022hierarchical , and Parti\u00a0yu2022scaling , demonstrating unprecedented semantic generation. However, these models do not provide control over a generated image, specifically using text guidance only.Changing a single word in the original prompt associated with the image often leads to a completely different outcome. For instance, adding the adjective \u201cwhite\u201d to \u201cdog\u201d often changes the dog\u2019s shape.To overcome this, several works\u00a0nichol2021glide ; avrahami2022blendedlatent  assume that the user provides a mask to restrict the area in which the changes are applied.", "Unlike previous works, our method requires textual input only, by using the spatial information from the internal layers of the generative model itself. This offers the user a much more intuitive editing experience of modifying local or global details by merely modifying the text prompt.", "Let \u2110\u2110\\mathcal{I}caligraphic_I be an image which was generated by a text-guided diffusion model saharia2022photorealistic  using the text prompt \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P and a random seed s\ud835\udc60sitalic_s. Our goal is editing the input image guided only by the edited prompt \ud835\udcab*superscript\ud835\udcab\\mathcal{P}^{*}caligraphic_P start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT, resulting in an edited image \u2110*superscript\u2110\\mathcal{I}^{*}caligraphic_I start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT.For example, consider an image generated from the prompt \u201cmy new bicycle\u201d, and assume that the user wants to edit the color of the bicycle, its material, or even replace it with a scooter while preserving the appearance and structure of the original image.An intuitive interface for the user is to directly change the text prompt by further describing the appearance of the bikes, or replacing it with another word. As opposed to previous works, we wish to avoid relying on any user-defined mask to assist or signify where the edit should occur. A simple, but an unsuccessful attempt is to fix the internal randomness and regenerate using the edited text prompt. Unfortunately, as fig.\u00a02 shows, this results in a completely different image with a different structure and composition.", "Our key observation is that the structure and appearances of the generated image depend not only on the random seed, but also on the interaction between the pixels to the text embedding through the diffusion process. By modifying the pixel-to-text interaction that occurs in cross-attention layers, we provide Prompt-to-Prompt image editing capabilities. More specifically, injecting the cross-attention maps of the input image \u2110\u2110\\mathcal{I}caligraphic_I enables us to preserve the original composition and structure. In section\u00a03.1, we review how cross-attention is used, and in section\u00a03.2 we describe how to exploit the cross-attention for editing. For additional background on diffusion models, please refer to appendix\u00a0A.", "We use the Imagen\u00a0saharia2022photorealistic  text-guided synthesis model as a backbone. Since the composition and geometry are mostly determined at the 64\u00d764646464\\times 6464 \u00d7 64 resolution, we only adapt the text-to-image diffusion model, using the super-resolution process as is.Recall that each diffusion step t\ud835\udc61titalic_t consists of predicting the noise \u03f5italic-\u03f5\\epsilonitalic_\u03f5 from a noisy image ztsubscript\ud835\udc67\ud835\udc61z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and text embedding \u03c8(\ud835\udcab)\ud835\udf13\ud835\udcab\\psi(\\mathcal{P})italic_\u03c8 ( caligraphic_P ) using a U-shaped network ronneberger2015u . At the final step, this process yields the generated image \u2110=z0\u2110subscript\ud835\udc670\\mathcal{I}=z_{0}caligraphic_I = italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT.Most importantly, the interaction between the two modalities occurs during the noise prediction, where the embeddings of the visual and textual features are fused using Cross-attention layers that produce spatial attention maps for each textual token.", "More formally, as illustrated in fig.\u00a03(Top), the deep spatial features of the noisy image \u03d5(zt)italic-\u03d5subscript\ud835\udc67\ud835\udc61\\phi(z_{t})italic_\u03d5 ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) are projected to a query matrix Q=\u2113Q(\u03d5(zt))\ud835\udc44subscript\u2113\ud835\udc44italic-\u03d5subscript\ud835\udc67\ud835\udc61Q=\\ell_{Q}(\\phi(z_{t}))italic_Q = roman_\u2113 start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT ( italic_\u03d5 ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ), and the textual embedding is projected to a key matrix K=\u2113K(\u03c8(\ud835\udcab))\ud835\udc3esubscript\u2113\ud835\udc3e\ud835\udf13\ud835\udcabK=\\ell_{K}(\\psi(\\mathcal{P}))italic_K = roman_\u2113 start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ( italic_\u03c8 ( caligraphic_P ) ) and a value matrix V=\u2113V(\u03c8(\ud835\udcab))\ud835\udc49subscript\u2113\ud835\udc49\ud835\udf13\ud835\udcabV=\\ell_{V}(\\psi(\\mathcal{P}))italic_V = roman_\u2113 start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ( italic_\u03c8 ( caligraphic_P ) ), via learned linear projections \u2113Q,\u2113K,\u2113Vsubscript\u2113\ud835\udc44subscript\u2113\ud835\udc3esubscript\u2113\ud835\udc49\\ell_{Q},\\ell_{K},\\ell_{V}roman_\u2113 start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT , roman_\u2113 start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , roman_\u2113 start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT.The attention maps are thenM=Softmax(QKTd),\ud835\udc40Softmax\ud835\udc44superscript\ud835\udc3e\ud835\udc47\ud835\udc51M=\\text{Softmax}\\left(\\frac{QK^{T}}{\\sqrt{d}}\\right),italic_M = Softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d end_ARG end_ARG ) ,(1)where the cell Mijsubscript\ud835\udc40\ud835\udc56\ud835\udc57M_{ij}italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT defines the weight of the value of the j\ud835\udc57jitalic_j-th token on the pixel i\ud835\udc56iitalic_i, and where d\ud835\udc51ditalic_d is the latent projection dimension of the keys and queries. Finally, the cross-attention output is defined to be \u03d5^(zt)=MV^italic-\u03d5subscript\ud835\udc67\ud835\udc61\ud835\udc40\ud835\udc49\\widehat{\\phi}\\left(z_{t}\\right)=MVover^ start_ARG italic_\u03d5 end_ARG ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_M italic_V, which is then used to update the spatial features \u03d5(zt)italic-\u03d5subscript\ud835\udc67\ud835\udc61\\phi(z_{t})italic_\u03d5 ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ).", "Intuitively, the cross-attention output MV\ud835\udc40\ud835\udc49MVitalic_M italic_V is a weighted average of the values V\ud835\udc49Vitalic_V where the weights are the attention maps M\ud835\udc40Mitalic_M, which are correlated to the similarity between Q\ud835\udc44Qitalic_Q and K\ud835\udc3eKitalic_K.In practice, to increase their expressiveness, multi-head attention NIPS2017_3f5ee243  is used in parallel, and then the results are concatenated and passed through a learned linear layer to get the final output.", "Imagen\u00a0saharia2022photorealistic , similar to GLIDE \u00a0nichol2021glide , conditions on the text prompt in the noise prediction of each diffusion step (see section\u00a0A.2) through two types of attention layers: i) cross-attention layers. ii) hybrid attention that acts both as self-attention and cross-attention by simply concatenating the text embedding sequence to the key-value pairs of each self-attention layer. Throughout the rest of the paper, we refer to both of them as cross-attention since our method only intervenes in the cross-attention part of the hybrid attention. That is, only the last channels, which refer to text tokens, are modified in the hybrid attention modules.", "We return to our key observation \u2014 the spatial layout and geometry of the generated image depend on the cross-attention maps. This interaction between pixels and text is illustrated in fig.\u00a04, where the average attention maps are plotted. As can be seen, pixels are more attracted to the words that describe them, e.g., pixels of the bear are correlated with the word \u201cbear\u201d. Note that averaging is done for visualization purposes, and attention maps are kept separate for each head in our method.Interestingly, we can see that the structure of the image is already determined in the early steps of the diffusion process.", "Since the attention reflects the overall composition, we can inject the attention maps M\ud835\udc40Mitalic_M that were obtained from the generation with the original prompt \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P, into a second generation with the modified prompt \ud835\udcab*superscript\ud835\udcab\\mathcal{P}^{*}caligraphic_P start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT. This allows the synthesis of an edited image \u2110*superscript\u2110\\mathcal{I}^{*}caligraphic_I start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT that is not only manipulated according to the edited prompt, but also preserves the structure of the input image \u2110\u2110\\mathcal{I}caligraphic_I. This example is a specific instance of a broader set of attention-based manipulations leading to different types of intuitive editing. We, therefore, start by proposing a general framework, followed by the details of the specific editing operations.", "Let DM(zt,\ud835\udcab,t,s)\ud835\udc37\ud835\udc40subscript\ud835\udc67\ud835\udc61\ud835\udcab\ud835\udc61\ud835\udc60DM(z_{t},\\mathcal{P},t,s)italic_D italic_M ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , caligraphic_P , italic_t , italic_s ) be the computation of a single step t\ud835\udc61titalic_t of the diffusion process, which outputs the noisy image zt\u22121subscript\ud835\udc67\ud835\udc611z_{t-1}italic_z start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT, and the attention map Mtsubscript\ud835\udc40\ud835\udc61M_{t}italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT (omitted if not used). We denote by DM(zt,\ud835\udcab,t,s){M\u2190M^}\ud835\udc37\ud835\udc40subscript\ud835\udc67\ud835\udc61\ud835\udcab\ud835\udc61\ud835\udc60\u2190\ud835\udc40^\ud835\udc40DM(z_{t},\\mathcal{P},t,s)\\{M\\leftarrow\\widehat{M}\\}italic_D italic_M ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , caligraphic_P , italic_t , italic_s ) { italic_M \u2190 over^ start_ARG italic_M end_ARG } the diffusion step where we override the attention map M\ud835\udc40Mitalic_M with an additional given map M^^\ud835\udc40\\widehat{M}over^ start_ARG italic_M end_ARG, but keep the values V\ud835\udc49Vitalic_V from the supplied prompt. We also denote by Mt*superscriptsubscript\ud835\udc40\ud835\udc61M_{t}^{*}italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT the produced attention map using the edited prompt \ud835\udcab*superscript\ud835\udcab\\mathcal{P}^{*}caligraphic_P start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT.Lastly, we define Edit(Mt,Mt*,t)\ud835\udc38\ud835\udc51\ud835\udc56\ud835\udc61subscript\ud835\udc40\ud835\udc61superscriptsubscript\ud835\udc40\ud835\udc61\ud835\udc61Edit(M_{t},M_{t}^{*},t)italic_E italic_d italic_i italic_t ( italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_t ) to be a general edit function, receiving as input the t\ud835\udc61titalic_t\u2019th attention maps of the original and edited images during their generation.", "Our general algorithm for controlled image generation consists of performing the iterative diffusion process for both prompts simultaneously, where an attention-based manipulation is applied in each step according to the desired editing task.We note that for the method above to work, we must fix the internal randomness. This is due to the nature of diffusion models, where even for the same prompt, two random seeds produce drastically different outputs. Formally, our general algorithm is:", "Notice that we can also define image \u2110\u2110\\mathcal{I}caligraphic_I, which is generated by prompt \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P and random seed s\ud835\udc60sitalic_s, as an additional input. Yet, the algorithm would remain the same. For editing real images, see section\u00a04.Also, note that we can skip the forward call in line 7777 by applying the edit function inside the diffusion forward function. Moreover, a diffusion step can be applied on both zt\u22121subscript\ud835\udc67\ud835\udc611z_{t-1}italic_z start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT and zt*superscriptsubscript\ud835\udc67\ud835\udc61z_{t}^{*}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT in the same batch (i.e., in parallel), and so there is only one step overhead with respect to the original inference of the diffusion model.", "We now turn to address specific editing operations, filling the missing definition of the Edit(Mt,Mt*,t)\ud835\udc38\ud835\udc51\ud835\udc56\ud835\udc61subscript\ud835\udc40\ud835\udc61superscriptsubscript\ud835\udc40\ud835\udc61\ud835\udc61Edit(M_{t},M_{t}^{*},t)italic_E italic_d italic_i italic_t ( italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_t ) function.An overview is presented in fig.\u00a03(Bottom).", "In this case, the user swaps tokens of the original prompt with others, e.g., \ud835\udcab=\ud835\udcababsent\\mathcal{P}=caligraphic_P =\u201ca big red bicycle\u201d to \ud835\udcab*=superscript\ud835\udcababsent\\mathcal{P}^{*}=caligraphic_P start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT =\u201ca big red car\u201d. The main challenge is to preserve the original composition while also addressing the content of the new prompt. To this end, we inject the attention maps of the source image into the generation with the modified prompt. However, the proposed attention injection may over constrain the geometry, especially when a large structural modification, such as \u201ccar\u201d to \u201cbicycle\u201d, is involved. We address this by suggesting a softer attention constrain:", "Edit(Mt,Mt*,t):={Mt*ift<\u03c4Mtotherwise.assign\ud835\udc38\ud835\udc51\ud835\udc56\ud835\udc61subscript\ud835\udc40\ud835\udc61superscriptsubscript\ud835\udc40\ud835\udc61\ud835\udc61casessuperscriptsubscript\ud835\udc40\ud835\udc61if\ud835\udc61\ud835\udf0fsubscript\ud835\udc40\ud835\udc61otherwise.Edit(M_{t},M_{t}^{*},t):=\\begin{cases}M_{t}^{*}&\\quad\\text{if}\\;t<\\tau\\\\M_{t}&\\quad\\text{otherwise.}\\\\\\end{cases}italic_E italic_d italic_i italic_t ( italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_t ) := { start_ROW start_CELL italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_CELL start_CELL if italic_t < italic_\u03c4 end_CELL end_ROW start_ROW start_CELL italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL otherwise. end_CELL end_ROWwhere \u03c4\ud835\udf0f\\tauitalic_\u03c4 is a timestamp parameter that determines until which step the injection is applied.Note that the composition is determined in the early steps of the diffusion process. Therefore, by limiting the number of injection steps, we can guide the composition of the newly generated image while allowing the necessary geometry freedom for adapting to the new prompt. An illustration is provided in section\u00a04. Another natural relaxation for our algorithm is to assign a different number of injection timestamps for the different tokens in the prompt. In case the two words are represented using a different number of tokens, the maps can be duplicated/averaged as necessary using an alignment function as described in the next paragraph.", "In another setting, the user adds new tokens to the prompt, e.g., \ud835\udcab=\ud835\udcababsent\\mathcal{P}=caligraphic_P =\u201ca castle next to a river\u201d to \ud835\udcab*=superscript\ud835\udcababsent\\mathcal{P}^{*}=caligraphic_P start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT =\u201cchildren drawing of a castle next to a river\u201d. To preserve the common details, we apply the attention injection only over the common tokens from both prompts.Formally, we use an alignment function A\ud835\udc34Aitalic_A that receives a token index from target prompt \ud835\udcab*superscript\ud835\udcab\\mathcal{P}^{*}caligraphic_P start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT and outputs the corresponding token index in \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P or None if there isn\u2019t a match. Then, the editing function is given by:", "(Edit(Mt,Mt*,t))i,j:={(Mt*)i,jifA(j)=None(Mt)i,A(j)otherwise.assignsubscript\ud835\udc38\ud835\udc51\ud835\udc56\ud835\udc61subscript\ud835\udc40\ud835\udc61superscriptsubscript\ud835\udc40\ud835\udc61\ud835\udc61\ud835\udc56\ud835\udc57casessubscriptsuperscriptsubscript\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc57if\ud835\udc34\ud835\udc57\ud835\udc41\ud835\udc5c\ud835\udc5b\ud835\udc52subscriptsubscript\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc34\ud835\udc57otherwise.\\left(Edit\\left(M_{t},M_{t}^{*},t\\right)\\right)_{i,j}:=\\begin{cases}(M_{t}^{*})_{i,j}&\\quad\\text{if}\\;A(j)=None\\\\(M_{t})_{i,A(j)}&\\quad\\text{otherwise.}\\\\\\end{cases}( italic_E italic_d italic_i italic_t ( italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_t ) ) start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT := { start_ROW start_CELL ( italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT end_CELL start_CELL if italic_A ( italic_j ) = italic_N italic_o italic_n italic_e end_CELL end_ROW start_ROW start_CELL ( italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i , italic_A ( italic_j ) end_POSTSUBSCRIPT end_CELL start_CELL otherwise. end_CELL end_ROWRecall that index i\ud835\udc56iitalic_i corresponds to a pixel value, where j\ud835\udc57jitalic_j corresponds to a text token.Again, we may set a timestamp \u03c4\ud835\udf0f\\tauitalic_\u03c4 to control the number of diffusion steps in which the injection is applied.This kind of editing enables diverse Prompt-to-Prompt capabilities such as stylization, specification of object attributes, or global manipulations as demonstrated in section\u00a04.", "Lastly, the user may wish to strengthen or weakens the extent to which each token is affecting the resulting image. For example, consider the prompt \ud835\udcab=\ud835\udcababsent\\mathcal{P}=caligraphic_P = \u201ca fluffy red ball\u201d, and assume we want to make the ball more or less fluffy. To achieve such manipulation, we scale the attention map of the assigned token j*superscript\ud835\udc57j^{*}italic_j start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT with parameter c\u2208[\u22122,2]\ud835\udc5022c\\in[-2,2]italic_c \u2208 [ - 2 , 2 ], resulting in a stronger/weaker effect. The rest of the attention maps remain unchanged. That is:(Edit(Mt,Mt*,t))i,j:={c\u22c5(Mt)i,jif\u00a0j=j*(Mt)i,jotherwise.assignsubscript\ud835\udc38\ud835\udc51\ud835\udc56\ud835\udc61subscript\ud835\udc40\ud835\udc61superscriptsubscript\ud835\udc40\ud835\udc61\ud835\udc61\ud835\udc56\ud835\udc57cases\u22c5\ud835\udc50subscriptsubscript\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc57if\u00a0\ud835\udc57superscript\ud835\udc57subscriptsubscript\ud835\udc40\ud835\udc61\ud835\udc56\ud835\udc57otherwise.\\left(Edit\\left(M_{t},M_{t}^{*},t\\right)\\right)_{i,j}:=\\begin{cases}c\\cdot(M_{t})_{i,j}&\\quad\\text{if }j=j^{*}\\\\(M_{t})_{i,j}&\\quad\\text{otherwise.}\\\\\\end{cases}( italic_E italic_d italic_i italic_t ( italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_t ) ) start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT := { start_ROW start_CELL italic_c \u22c5 ( italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT end_CELL start_CELL if italic_j = italic_j start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL ( italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT end_CELL start_CELL otherwise. end_CELL end_ROWAs described in section\u00a04, the parameter c\ud835\udc50citalic_c allows fine and intuitive control over the induced effect.", "Our method, described in section\u00a03, enables intuitive text-only editing by controlling the spatial layout corresponding to each word in the user-provided prompt. In this section, we show several applications using this technique.", "Text-Only Localized Editing.We first demonstrate localized editing by modifying the user-provided prompt without requiring any user-provided mask. In fig.\u00a02, we depict an example where we generate an image using the prompt \u201clemon cake\u201d. Our method allows us to retain the spatial layout, geometry, and semantics when replacing the word \u201clemon\u201d with \u201cpumpkin\u201d (top row). Observe that the background is well-preserved, including the top-left lemons transforming into pumpkins. On the other hand, naively feeding the synthesis model with the prompt \u201cpumpkin cake\u201d results in a completely different geometry (3333rd row), even when using the same random seed in a deterministic setting (i.e., DDIM song2020denoising ). Our method succeeds even for a challenging prompt such as \u201cpasta cake.\u201d (2222nd row) \u2014 the generated cake consists of pasta layers with tomato sauce on top. Another example is provided in fig.\u00a05 where we do not inject the attention of the entire prompt but only the attention of a specific word \u2013 \u201cbutterfly\u201d. This enables the preservation of the original butterfly while changing the rest of the content. Additional results are provided in the appendix (fig.\u00a013).", "As can be seen in fig.\u00a06, our method is not confined to modifying only textures, and it can perform structural modifications, e.g., change a \u201cbicycle\u201d to a \u201ccar\u201d. To analyze our attention injection, in the left column we show the results without cross-attention injection, where changing a single word leads to an entirely different outcome. From left to right, we then show the resulting generated image by injecting attention to an increasing number of diffusion steps. Note that the more diffusion steps in which we apply cross-attention injection, the higher the fidelity to the original image.However, the optimal result is not necessarily achieved by applying the injection throughout all diffusion steps. Therefore, we can provide the user with even better control over the fidelity to the original image by changing the number of injection steps.", "Instead of replacing one word with another, the user may wish to add a new specification to the generated image. In this case, we keep the attention maps of the original prompt, while allowing the generator to address the newly added words. For example, see fig.\u00a07 (top), where we add \u201ccrushed\u201d to the \u201ccar\u201d, resulting in the generation of additional details over the original image while the background is still preserved. See the appendix (fig.\u00a014) for more examples.", "Global editing.Preserving the image composition is not only valuable for localized editing, but also an important aspect of global editing. In this setting, the editing should affect all parts of the image, but still retain the original composition, such as the location and identity of the objects. As shown in fig.\u00a07 (bottom), we retain the image content while adding \u201csnow\u201d or changing the lightning. Additional examples appear in fig.\u00a08, including translating a sketch into a photo-realistic image and inducing an artistic style.", "Fader Control using Attention Re-weighting.While controlling the image by editing the prompt is very effective, we find that it still does not allow full control over the generated image. Consider the prompt \u201csnowy mountain\u201d. A user may want to control the amount of snow on the mountain. However, it is quite difficult to describe the desired amount of snow through text. Instead, we suggest a fader control lample2017fader , where the user controls the magnitude of the effect induced by a specific word, as depicted in fig.\u00a09. As described in section\u00a03, we achieve such control by re-scaling the attention of the specified word. Additional results are in the appendix (fig.\u00a015).", "Real Image Editing.Editing a real image requires finding an initial noise vector that produces the given input image when fed into the diffusion process. This process, known as inversion, has recently drawn considerable attention for GANs, e.g., zhu2016generative ; abdal2019image2stylegan ; alaluf2022hyperstyle ; roich2021pivotal ; zhu2020domain ; tov2021designing ; Wang2021HighFidelityGI ; xia2021gan , but has not yet been fully addressed for text-guided diffusion models.", "In the following, we show preliminary editing results on real images, based on common inversion techniques for diffusion models. First, a rather na\u00efve approach is to add Gaussian noise to the input image, and then perform a predefined number of diffusion steps. Since this approach results in significant distortions,we adopt an improved inversion approach dhariwal2021diffusion ; song2020denoising , which is based on the deterministic DDIM model rather than the DDPM model. We perform the diffusion process in the reverse direction, that is x0\u27f6xT\u27f6subscript\ud835\udc650subscript\ud835\udc65\ud835\udc47x_{0}\\longrightarrow x_{T}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u27f6 italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT instead of xT\u27f6x0\u27f6subscript\ud835\udc65\ud835\udc47subscript\ud835\udc650x_{T}\\longrightarrow x_{0}italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT \u27f6 italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, where x0subscript\ud835\udc650x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is set to be the given real image.", "This inversion process often produces satisfying results, as presented in fig.\u00a010.However, the inversion is not sufficiently accurate in many other cases, as in fig.\u00a011.This is partially due to a distortion-editability tradeoff tov2021designing , where we recognize that reducing the classifier-free guidance ho2021classifier  parameter (i.e., reducing the prompt influence) improves reconstruction but constrains our ability to perform significant manipulations.", "To alleviate this limitation, we propose to restore the unedited regions of the original image using a mask, directly extracted from the attention maps. Note that here the mask is generated with no guidance from the user. As presented in fig.\u00a012, this approach works well even using the na\u00efve DDPM inversion scheme (adding noise followed by denoising). Note that the cat\u2019s identity is well-preserved under various editing operations, while the mask is produced only from the prompt itself.", "In this work, we uncovered the powerful capabilities of the cross-attention layers within text-to-image diffusion models.We showed that these high-dimensional layers have an interpretable representation of spatial maps that play a key role in tying the words in the text prompt to the spatial layout of the synthesized image.With this observation, we showed how various manipulations of the prompt can directly control attributes in the synthesized image, paving the way to various applications including local and global editing.This work is a first step towards providing users with simple and intuitive means to edit images, leveraging textual semantic power. It enables users to navigate through a semantic, textual, space, which exhibits incremental changes after each step, rather than producing the desired image from scratch after each text manipulation.", "While we have demonstrated semantic control by changing only textual prompts, our technique is still subject to a few limitations to be addressed in follow-up work. First, the current inversion process results in a visible distortion over some of the test images. In addition, the inversion requires the user to come up with a suitable prompt.This could be challenging for complicated compositions. Note that the challenge of inversion for text-guided diffusion models is an orthogonal endeavor to our work, which will be thoroughly studied in the future. Second, the current attention maps are of low resolution, as the cross-attention is placed in the network\u2019s bottleneck. This bounds our ability to perform even more precise localized editing. To alleviate this, we suggest incorporating cross-attention also in higher-resolution layers. We leave this for future works since it requires analyzing the training procedure which is out of our current scope. Finally, we recognize that our current method cannot be used to spatially move existing objects across the image and also leave this kind of control for future work.", "We thank Noa Glaser, Adi Zicher, Yaron Brodsky and Shlomi Fruchterfor their valuable inputs that helped improve this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan for providing us with their support and the pretrained models of Imagen saharia2022photorealistic .Special thanks to Yossi Matias for early inspiring discussion on the problem and for motivating and encouraging us to develop technologies along the avenue of intuitive interaction."], "figure_types": {"04e541391e8dce14d099d00fb2c21dbbd8afe87f/10-Figure9-1.png": "photograph(s)", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/11-Figure10-1.png": "photograph(s)", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/11-Figure11-1.png": "photograph(s)", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/12-Figure12-1.png": "photograph(s)", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/17-Figure13-1.png": "photograph(s)", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/18-Figure14-1.png": "photograph(s)", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/19-Figure15-1.png": "photograph(s)", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/2-Figure1-1.png": "photograph(s)", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/3-Figure2-1.png": "photograph(s)", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/4-Figure3-1.png": "schematic", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/5-Figure4-1.png": "schematic", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/6-Figure5-1.png": "photograph(s)", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/7-Figure6-1.png": "photograph(s)", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/8-Figure7-1.png": "photograph(s)", "04e541391e8dce14d099d00fb2c21dbbd8afe87f/9-Figure8-1.png": "photograph(s)"}}, "2103.12204": {"paper_id": "paper_126", "title": "Human-Like Controllable Image Captioning With Verb-Specific Semantic Roles", "arxiv_url": "https://arxiv.org/abs/2103.12204", "s2orc_url": "https://www.semanticscholar.org/paper/ada35e2c099fbde9d07a279311f4abe698341cd8", "all_figures_tables": {"ada35e2c099fbde9d07a279311f4abe698341cd8/12-Table3-1.png": "Table 3: List of the main arguments in the PropBank.", "ada35e2c099fbde9d07a279311f4abe698341cd8/13-Figure8-1.png": "Figure 8: Additional examples of generated image captions using the VSR corresponding to the ground truth caption. SS denotes the learned semantic structures. Different colors show a correspondence between image regions and semantic roles. Best viewed in color.", "ada35e2c099fbde9d07a279311f4abe698341cd8/13-Figure9-1.png": "Figure 9: Additional examples of diverse image caption generation conditioned on different VSRs. The correspondences between image regions and noun phrases are indicated by different colors. Best viewed in color.", "ada35e2c099fbde9d07a279311f4abe698341cd8/15-Table4-1.png": "Table 4: Performance comparisons between Transformer (TF) and Sinkhorn Network (SN) in S-level SSP on dataset COCO Entities and Flickr30K Entities.", "ada35e2c099fbde9d07a279311f4abe698341cd8/2-Figure2-1.png": "Figure 2: Two image examples of a verb and its semantic roles. The verb eating captures the scope of the activity, and agent, food, container, tool are all reasonable semantic roles for this activity.", "ada35e2c099fbde9d07a279311f4abe698341cd8/4-Figure3-1.png": "Figure 3: The whole architecture of our proposed VSR-guided CIC model. This framework consists of three components: 1) a GSRL model to ground the entities for each role; 2) an SSP to learn a semantic structure; 3) a role-shift captioning model to generate the caption.", "ada35e2c099fbde9d07a279311f4abe698341cd8/6-Figure4-1.png": "Figure 4: A toy example of merging two different semantic structures Sa and Sb into a single sub-role sequence.", "ada35e2c099fbde9d07a279311f4abe698341cd8/7-Figure5-1.png": "Figure 5: Examples of generated image captions using the VSR corresponding to the ground truth caption. SS denotes the learned semantic structures.", "ada35e2c099fbde9d07a279311f4abe698341cd8/7-Figure6-1.png": "Figure 6: Examples of the learned verb-specific semantic structures. The first row of each sample is the verb and all reasonable semantic roles. The second or third row is a sampled role set with two top-ranking structures. The green (blue) tick denotes that this structure is (not) in the dataset.", "ada35e2c099fbde9d07a279311f4abe698341cd8/7-Table1-1.png": "Table 1: Performance (%) compared with SOTA methods for controllable image captioning. The upper part denotes that all grounded proposal sets come from the GSRL model, and the below part denotes that all grounded proposal sets come from ground truth annotations.", "ada35e2c099fbde9d07a279311f4abe698341cd8/8-Figure7-1.png": "Figure 7: Examples of diverse image caption generation conditioned on different VSRs. Best viewed in color.", "ada35e2c099fbde9d07a279311f4abe698341cd8/8-Table2-1.png": "Table 2: Performance compared with two strong baselines for diverse image captioning on dataset COCO Entities."}, "referred_figures_tables": [["ada35e2c099fbde9d07a279311f4abe698341cd8/4-Figure3-1.png"], ["ada35e2c099fbde9d07a279311f4abe698341cd8/7-Figure5-1.png", "ada35e2c099fbde9d07a279311f4abe698341cd8/7-Figure6-1.png", "ada35e2c099fbde9d07a279311f4abe698341cd8/7-Table1-1.png", "ada35e2c099fbde9d07a279311f4abe698341cd8/7-Table1-1.png"]], "question_id": [0, 2], "question": ["Who is responsible for designating the control signal?", "How do the authors verify that the two characteristics mentioned in the sentence are indispensable for the ideal control signal?  "], "question_section": ["Abstract", "Abstract"], "question_trigger_sentence": ["Controllable Image Captioning (CIC) \u2014 generating image descriptions following designated control signals \u2014 has received unprecedented attention over the last few years. ", "However, we argue that almost all existing objective control signals have overlooked two indispensable characteristics of an ideal control signal: 1) Event-compatible: all visual contents referred to in a single sentence should be compatible with the described activity. 2) Sample-suitable: the control signals should be suitable for a specific image sample."], "question_type": ["Shallow question", "Deep/complex question"], "evidential_info": [[{"context": "Image captioning, \\ie, generating fluent and meaningful descriptions to summarize the salient contents of an image, is a classic proxy task for comprehensive scene understanding\u00a0[21]. With the release of several large scale datasets and advanced encoder-decoder frameworks, current captioning models plausibly have already achieved \u201csuper-human\u201d performance in all accuracy-based evaluation metrics. However, many studies have indicated that these models tend to produce generic descriptions, and fail to control the caption generation process as humans, \\eg, referring to different contents of interest or descriptive patterns. In order to endow the captioning models with human-like controllability, a recent surge of efforts\u00a0[16, 10, 19, 78, 48, 77, 27, 20] resort to introducing extra control signals as constraints of the generated captions, called Controllable Image Captioning (CIC). As a byproduct, the CIC models can easily generate diverse descriptions by feeding different control signals.", "rationale": "Authors made 3 contributions, 1- they proposed a new control signal for CIC which is the first control signal to consider both event-compatible and sample-suitable requirements. 2- they can learn human-like verb-specific semantic structures automatically. 3- authors achieved state of the art controllability on two challenging benchmarks and generate diverse captions by using different verbs."}, {"context": "For human-like controllable image captioning, we first propose the Verb-specific Semantic Roles (VSR) as the control signal for generating customized captions. As shown in Figure\u00a03, we formally represent a control signal VSR as:\ud835\udcb1\ud835\udcae\u211b={v,<s1,n1>,\u2026,<sm,nm>},\\displaystyle\\begin{aligned} \\mathcal{VSR}=\\{v,<s_{1},n_{1}>,...,<s_{m},n_{m}>\\},\\\\\\end{aligned}start_ROW start_CELL caligraphic_V caligraphic_S caligraphic_R = { italic_v , < italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > , \u2026 , < italic_s start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , italic_n start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT > } , end_CELL end_ROW(1)where v is a verb capturing the scope of a salient activity in the image (\\eg, ride), s_{i} is a semantic role of verb v (\\eg, LOC), and n_{i} is the number of interested entities in the role s_{i}. For example, for \ud835\udcb1\ud835\udcae\u211b={\ud835\ude9b\ud835\ude92\ud835\ude8d\ud835\ude8e,<\ud835\ude70\ud835\ude9b\ud835\ude90\ud835\udff6,\ud835\udff7>,<\ud835\ude70\ud835\ude9b\ud835\ude90\ud835\udff7,\ud835\udff7>,<\ud835\ude7b\ud835\ude98\ud835\ude8c,\ud835\udff8>}\\mathcal{VSR}=\\{\\texttt{ride},<\\texttt{Arg0},\\texttt{1}>,<\\texttt{Arg1},\\texttt{1}>,<\\texttt{Loc},\\texttt{2}>\\}caligraphic_V caligraphic_S caligraphic_R = { ride , < Arg0 , 1 > , < Arg1 , 1 > , < Loc , 2 > }, we hope to generate a caption which not only focuses on describing the ride activity, but also contains one entity respectively in the role Arg0{}_{\\text{rider}} and Arg1{}_{\\text{steed}}, and two entities in the role LOC. Thus, VSR can effectively control the amount of information carried in the whole sentence and each role, \\ie, the level of details.", "rationale": "Authors proposed a novel control signal called VSR."}, {"context": "In this paper, we argued that all existing objective control signals for CIC have overlooked two indispensable characteristics: event-compatible and sample-suitable. To this end, we proposed a novel control signal called VSR. VSR consists of a verb and several semantic roles, \\ie, all components are guaranteed to be event-compatible. Meanwhile, VSR only restricts the involved semantic roles, which is also sample-suitable for all the images containing the activity. We have validated the effectiveness of VSR through extensive experiments. Moving forward, we will plan to 1) design a more effective captioning model to benefit more from the VSR signals; 2) extend VSR to other controllable text generation tasks, \\eg, video captioning\u00a0[69]; 3) design a more general framework to cover the images without verbs.", "rationale": "Authors proposed in this paper the first control signal that considers both the event-compatible and sample-suitable requirements."}, {"context": "In summary, we make three contributions in this paper:1.We propose a new control signal for CIC: Verb-specific Semantic Roles (VSR). To the best of our knowledge, VSR is the first control signal to consider both event-compatible and sample-suitable requirements222When using control signals extracted from GT captions, existing control signals can always meet both requirements and generate reasonable captions. However, in more general settings (\\eg, construct control signals without GT captions), the form of VSR is more human-friendly, and it is easier to construct signals which meet both requirements compared with all existing forms of control signals, which is the main advantage of VSR..2.We can learn human-like verb-specific semantic structures automatically, and abundant visualization examples demonstrate that these patterns are reasonable.3.We achieve state-of-the-art controllability on two challenging benchmarks, and generate diverse captions by using different verbs, semantic roles, or structures.", "rationale": "Authors proposed a verb-specific semantic role \"VSR\" as the control signal for customized captions."}, {"context": "Controllable Image Captioning.Compared with conventional image captioning\u00a0[63, 68, 9, 25, 13], CIC is a more challenging task, which needs to consider extra constraints. Early CIC works are mostly about stylized image captioning, \\ie, constraints are the linguistic styles of sentences. According to the requirements of parallel training samples, existing solutions can be divided into two types: models using parallel stylized image-caption data\u00a0[41, 11, 54, 1] or not\u00a0[22, 42]. Subsequently, the community gradually shifts the emphasis to controlling described contents\u00a0[16, 77, 27, 10, 78, 48, 35] or structures\u00a0[20, 19, 75, 76] of the sentences. In this paper, we propose a novel control signal VSR, which is the first control signal to consider both the event-compatible and sample-suitable requirements.", "rationale": "A recent surge of efforts introduced extra control signals as constraints of the generated captions [16, 10, 19, 78, 48, 77, 27, 20]."}], [{"context": "Settings. To evaluate the controllability of proposed framework, we followed the conventions of prior CIC works\u00a0[16, 10, 78], and utilized the VSR aligned with ground truth captions as the control signals. Specifically, we compared the proposed framework with several carefully designed baselines666All baselines use the same visual regions as models with VSRs.: 1) C-LSTM: It is a Controllable LSTM model\u00a0[63]. Given the features of all grounded visual regions, it first averages all region features, and then uses an LSTM to generate the captions. 2) C-UpDn: It is a Controllable UpDn model\u00a0[3], which uses an adaptive attention to generate the captions. 3) SCT\u00a0[16]: It regards the set of visual regions as a control signal, and utilizes a chunk-shift captioning model to generate the captions. 4) Ours w/o verb: We ablate our model by removing the verb information in both the SSP and captioning model. 5) Ours (oracle verb): It is an ideal situation, where the captioning model directly outputs the oracle format of the verb when the attending role is the verb.", "rationale": "Authors evaluated the controallability of their proposed framework as they followed conventions of prior CIC works and utilized the VSR aligned with ground truth captions as the control signals."}, {"context": "Quantitative Results. The quantitative results are reported in Table 1. From Table 1, we can observe that our framework can achieve the best performance over almost all metrics and benchmarks. By comparing the two different proposal settings (i.e., GSRL and GT), we can find that the accuracy of GSRL is a major bottleneck of the whole framework. Meanwhile, the ablative model (Ours w/o verb) can only achieve slightly better performance than baseline SCT and much worse performance than our full model, which reflects the importance of the verb in semantic structure learning and caption generation.", "rationale": "As in Quantitative results reported in Table 1. we can observe that author's framework can achieve the best performance over almost all metrics and benchmarks."}, {"context": "Visualizations. In Figure 5, we illustrate some examples of the generated captions. We can observe that our framework always learns a human-like semantic structure based on the VSR and grounded visual regions (e.g., Arg1thing \u2013 sit \u2013 Arg2position \u2013 LOC \u2013 MNR). According to the semantic structures, the captioning model can generate near-perfect descriptions. As a by-product, a well-trained SSP can automatically produce several verb-specific semantic structures for a set of user-interested roles, and we show some examples in Figure 6. For each verb and role set, we illustrate the top two structures by using beam search. Particularly, we are surprised to find that we can even learn some structures that never appear in original datasets (the blue tick ones).", "rationale": "As for visualized evaluation, in Figure 5, we can observe that author's framework always learns a human-like semantic structure based on the VSR and grounded visual regions. and according to the semantic structures, the captioning model can generate near-perfect descriptions."}]], "composition": ["control signal is designated by each author in their work. as the authors of this paper proposed a \"verb-specific semantic role\" \"VSR\" as control signal for customized captions. while a recent surge of efforts by other works introduced extra control signals as constrains of the generated captions [16, 10, 19, 78, 48, 77, 27, 20].", "Authors verify their work using a conventions evaluation metrics in prior CIC works. As their quantitative results report in Table 1, you can observe that author's framework can achieve the best performance over almost all metrics and benchmarks. and as for the visualized evaluation, you can observe in Figure 5 that the author's framework always learns a human-like semantic structure based on the VSR and grounded visual regions. and according to the semantic structures, the captioning model can generate near-perfect descriptions."], "Is_figure_in_evidence": [false, true], "Is_table_in_evidence": [true, true], "question_key": ["270", "273"], "passages": ["Image captioning, \\ie, generating fluent and meaningful descriptions to summarize the salient contents of an image, is a classic proxy task for comprehensive scene understanding\u00a0[21]. With the release of several large scale datasets and advanced encoder-decoder frameworks, current captioning models plausibly have already achieved \u201csuper-human\u201d performance in all accuracy-based evaluation metrics. However, many studies have indicated that these models tend to produce generic descriptions, and fail to control the caption generation process as humans, \\eg, referring to different contents of interest or descriptive patterns. In order to endow the captioning models with human-like controllability, a recent surge of efforts\u00a0[16, 10, 19, 78, 48, 77, 27, 20] resort to introducing extra control signals as constraints of the generated captions, called Controllable Image Captioning (CIC). As a byproduct, the CIC models can easily generate diverse descriptions by feeding different control signals.", "Early CIC works mainly focus on subjective control signals, such as sentiments\u00a0[41], emotions\u00a0[42, 22], and personality\u00a0[14, 54], \\ie, the linguistic styles of sentences. Although these stylized captioning models can eventually produce style-related captions, they remain hard to control the generation process effectively and precisely. To further improve the controllability, recent CIC works gradually put a more emphasis on objective control signals. More specifically, they can be coarsely classified into two categories: 1) Content-controlled: the control signals are about the contents of interest which need to be described. As the example shown in Figure\u00a01 (a), given the region set () as a control signal, we hope that the generated caption can cover all regions (\\ie, man, wave, and surfboard). So far, various types of content-controlled signals have been proposed, such as visual relations\u00a0[27], object regions\u00a0[16, 35], scene graphs\u00a0[10, 78], and mouse trace\u00a0[48]. 2) Structure-controlled: the control signals are about the semantic structures of sentences. For instance, the length-level\u00a0[19], part-of-speech tags\u00a0[20], or attributes\u00a0[79] of the sentence (cf. Figure\u00a01 (b)) are some typical structure-controlled signals.", "Nevertheless, all existing objective control signals (\\ie, both content-controlled and structure-controlled) have overlooked two indispensable characteristics of an ideal control signal towards \u201chuman-like\u201d controllable image captioning: 1) Event-compatible: all visual contents referred to in a single sentence should be compatible with the described activity. Imaging how humans describe images \u2014 our brains always quickly structure a descriptive pattern like \u201csth do sth at someplace\u201d first, and then fill in the detailed description\u00a0[56, 46, 30, 71], \\ie, we have subconsciously made sure that all the mentioned entities are event-compatible (\\eg, man, wave, surfboard are all involved in activity riding in Figure\u00a01 (a)). To further see the negative impact of dissatisfying this requirement, suppose that we deliberately utilize two more objects (hand and sky, \\ie, ) as part of the control signal, and the model generates an incoherent and illogical caption. 2) Sample-suitable: the control signals should be suitable for the specific image sample. By \u201csuitable\u201d, we mean that there do exist reasonable descriptions satisfying the control signals, \\eg, a large length-level may not be suitable for an image with a very simple scene. Unfortunately, it is always very difficult to decide whether a control signal is sample-suitable in advance. For example in Figure\u00a01 (b), although the two control signals (\\ie, length-levels 3 and 4) are quite close, the quality of respectively generated captions varies greatly.", "In this paper, we propose a new event-oriented objective control signal, Verb-specific Semantic Roles (VSR), to meet both event-compatible and sample-suitable requirements simultaneously. VSR consists of a verb (\\ie, predicate\u00a0[8]) and some user-interested semantic roles\u00a0[31]. As shown in Figure\u00a02, the verb captures the scope of a salient activity in the image (\\eg, eating), and the corresponding semantic roles111We use PropBank-style annotations of semantic roles (\\eg, Arg0, Arg1) in all experiments (cf. Figure\u00a01). The FrameNet-style annotations of semantic roles (\\eg, Agent) here are just for a more intuitive illustration. In the PropBank-style annotations, Arg denotes \u201cargument\u201d, MNR denotes \u201cmanner\u201d, DIR denotes \u201cdirectional\u201d, and LOC denotes \u201clocation\u201d. We leave more details in the supplementary material.  (\\eg, agent, food, container, and tool) categorize how objects participate in this activity, \\ie, a child (agent) is eating (activity) a pancake (food) from a plate (container) with a fork (tool). Thus, VSR is designed to guarantee that all the mentioned entities are event-compatible. Meanwhile, unlike the existing structure-controlled signals which directly impose constraints on the generated captions, VSR only restricts the involved semantic roles, which is theoretically suitable for all the images with the activity, \\ie, sample-suitable.", "In order to generate sentences with respect to the designated VSRs, we first train a grounded semantic role labeling (GSRL) model to identify and ground all entities for each role. Then, we propose a semantic structure planner (SSP) to rank the given verb and semantic roles, and output some human-like descriptive semantic structures, \\eg, Arg0readerreader{}_{\\text{reader}}start_FLOATSUBSCRIPT reader end_FLOATSUBSCRIPT \u2013 read \u2013 Arg1thingthing{}_{\\text{thing}}start_FLOATSUBSCRIPT thing end_FLOATSUBSCRIPT \u2013 LOC in Figure\u00a01 (c). Finally, we combine the grounded entities and semantic structures, and use an RNN-based role-shift captioning model to generate the captions by sequentially focusing on different roles.", "Although these are no available captioning datasets with the VSR annotations, they can be easily obtained by off-the-shelf semantic role parsing toolkits\u00a0[53]. Extensive experiments on two challenging CIC benchmarks (\\ie, COCO Entities\u00a0[16] and Flickr30K Entities\u00a0[47]) demonstrate that our framework can achieve better controllability given designated VSRs than several strong baselines. Moreover, our framework can also realize diverse image captioning and achieve a better trade-off between quality and diversity.", "In summary, we make three contributions in this paper:1.We propose a new control signal for CIC: Verb-specific Semantic Roles (VSR). To the best of our knowledge, VSR is the first control signal to consider both event-compatible and sample-suitable requirements222When using control signals extracted from GT captions, existing control signals can always meet both requirements and generate reasonable captions. However, in more general settings (\\eg, construct control signals without GT captions), the form of VSR is more human-friendly, and it is easier to construct signals which meet both requirements compared with all existing forms of control signals, which is the main advantage of VSR..2.We can learn human-like verb-specific semantic structures automatically, and abundant visualization examples demonstrate that these patterns are reasonable.3.We achieve state-of-the-art controllability on two challenging benchmarks, and generate diverse captions by using different verbs, semantic roles, or structures.", "Controllable Image Captioning.Compared with conventional image captioning\u00a0[63, 68, 9, 25, 13], CIC is a more challenging task, which needs to consider extra constraints. Early CIC works are mostly about stylized image captioning, \\ie, constraints are the linguistic styles of sentences. According to the requirements of parallel training samples, existing solutions can be divided into two types: models using parallel stylized image-caption data\u00a0[41, 11, 54, 1] or not\u00a0[22, 42]. Subsequently, the community gradually shifts the emphasis to controlling described contents\u00a0[16, 77, 27, 10, 78, 48, 35] or structures\u00a0[20, 19, 75, 76] of the sentences. In this paper, we propose a novel control signal VSR, which is the first control signal to consider both the event-compatible and sample-suitable requirements.", "Diverse and Distinctive Image Captioning.Diverse image captioning, \\ie, describing the image contents with diverse wordings and rich expressions, is an essential property of human-like captioning models. Except from feeding different control signals to the CIC models, other diverse captioning methods can be coarsely grouped into four types:1) GAN-based\u00a0[17, 52, 32]: they use a discriminator to force the generator to generate human-indistinguishable captions. 2) VAE-based\u00a0[65, 7]: the diversity obtained with them is by sampling from a learned latent space. 3) RL-based\u00a0[39]: they regard diversity as an extra reward in the RL training stage. 4) BS-based\u00a0[62]: they decode a list of diverse captions by optimizing a diversity-augmented objective.", "Meanwhile, distinctive image captioning is another close research direction\u00a0[18, 60, 37, 36, 64], which aims to generate discriminative and unique captions for individual images. Unfortunately, due to the subjective nature of diverse and distinctive captions, effective evaluation remains as an open problem, and several new metrics are proposed, such as SPICE-U\u00a0[67], CIDErBtw\u00a0[64], self-CIDEr\u00a0[66], word recall\u00a0[58], mBLEU\u00a0[52]. In this paper, we can easily generate diverse captions in both lexical-level and syntactic-level.", "Semantic Roles in Images. Inspired from the semantic role labeling task\u00a0[6] in NLP, several tasks have been proposed to label the roles of each object in an activity in an image:", "Visual Semantic Role Labeling (VSRL), also called situation recognition, is a generalization of action recognition and human-object interaction, which aims to label an image with a set of verb-specific action frames\u00a0[73]. Specifically, each action frame describes details of the activity captured by the verb, and it consists of a fixed set of verb-specific semantic roles and their corresponding values. The values are the entities or objects involved in the activity and the semantic roles categorize how objects participate in the activity. The current VSRL methods\u00a0[23, 73, 40, 33, 72, 57, 15] usually learn an independent action classifier first, and then model the role inter-dependency by RNNs or GNNs.", "Grounded Semantic Role Labeling (GSRL), also called grounded situation recognition, builds upon the VSRL task, which requires the models not only to label a set of frames, but also to localize each role-value pair in the image\u00a0[49, 55, 70, 23]. In this paper, we use the GSRL model as a bridge to connect the control signals (VSR) and related regions. To the best of our knowledge, we are the first captioning work to benefit from the verb lexicon developed by linguists.", "For human-like controllable image captioning, we first propose the Verb-specific Semantic Roles (VSR) as the control signal for generating customized captions. As shown in Figure\u00a03, we formally represent a control signal VSR as:\ud835\udcb1\ud835\udcae\u211b={v,<s1,n1>,\u2026,<sm,nm>},\\displaystyle\\begin{aligned} \\mathcal{VSR}=\\{v,<s_{1},n_{1}>,...,<s_{m},n_{m}>\\},\\\\\\end{aligned}start_ROW start_CELL caligraphic_V caligraphic_S caligraphic_R = { italic_v , < italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > , \u2026 , < italic_s start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , italic_n start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT > } , end_CELL end_ROW(1)where v\ud835\udc63vitalic_v is a verb capturing the scope of a salient activity in the image (\\eg, ride), sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is a semantic role of verb v\ud835\udc63vitalic_v (\\eg, LOC), and nisubscript\ud835\udc5b\ud835\udc56n_{i}italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the number of interested entities in the role sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. For example, for \ud835\udcb1\ud835\udcae\u211b={\ud835\ude9b\ud835\ude92\ud835\ude8d\ud835\ude8e,<\ud835\ude70\ud835\ude9b\ud835\ude90\ud835\udff6,\ud835\udff7>,<\ud835\ude70\ud835\ude9b\ud835\ude90\ud835\udff7,\ud835\udff7>,<\ud835\ude7b\ud835\ude98\ud835\ude8c,\ud835\udff8>}\\mathcal{VSR}=\\{\\texttt{ride},<\\texttt{Arg0},\\texttt{1}>,<\\texttt{Arg1},\\texttt{1}>,<\\texttt{Loc},\\texttt{2}>\\}caligraphic_V caligraphic_S caligraphic_R = { ride , < Arg0 , 1 > , < Arg1 , 1 > , < Loc , 2 > }, we hope to generate a caption which not only focuses on describing the ride activity, but also contains one entity respectively in the role Arg0riderrider{}_{\\text{rider}}start_FLOATSUBSCRIPT rider end_FLOATSUBSCRIPT and Arg1steedsteed{}_{\\text{steed}}start_FLOATSUBSCRIPT steed end_FLOATSUBSCRIPT, and two entities in the role LOC. Thus, VSR can effectively control the amount of information carried in the whole sentence and each role, \\ie, the level of details.", "It is convenient to construct VSRs automatically or manually. For the verbs, they can be accurately predicted by an off-the-shelf action recognition network with a predefined verb vocabulary. For the verb-specific semantic roles, they can be easily retrieved from the verb lexicon such as PropBank or FrameNet. Then, the users can easily select a subset of roles or an automatic sampling to generate a subset of roles, and randomly assign the entity number for each role.", "Given an image \ud835\udc70\ud835\udc70\\bm{I}bold_italic_I and a control signal \ud835\udcb1\ud835\udcae\u211b\ud835\udcb1\ud835\udcae\u211b\\mathcal{VSR}caligraphic_V caligraphic_S caligraphic_R, the controllable image captioning model aims to describe \ud835\udc70\ud835\udc70\\bm{I}bold_italic_I by a textual sentence \ud835\udc9a={y1,\u2026,yT}\ud835\udc9asubscript\ud835\udc661\u2026subscript\ud835\udc66\ud835\udc47\\bm{y}=\\{y_{1},...,y_{T}\\}bold_italic_y = { italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_y start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT }, \\ie, modeling the probability p(\ud835\udc9a|\ud835\udc70,\ud835\udcb1\ud835\udcae\u211b)\ud835\udc5dconditional\ud835\udc9a\ud835\udc70\ud835\udcb1\ud835\udcae\u211bp(\\bm{y}|\\bm{I},\\mathcal{VSR})italic_p ( bold_italic_y | bold_italic_I , caligraphic_V caligraphic_S caligraphic_R ). Inspired from the human habit of describing images, we decompose this task into two steps: structuring a descriptive pattern and filling in detailed captions:p(\ud835\udc9a|\ud835\udc70,\ud835\udcb1\ud835\udcae\u211b)=p(\ud835\udc9a|pattern)p(pattern|\ud835\udc70,\ud835\udcb1\ud835\udcae\u211b).\ud835\udc5dconditional\ud835\udc9a\ud835\udc70\ud835\udcb1\ud835\udcae\u211b\ud835\udc5dconditional\ud835\udc9apattern\ud835\udc5dconditionalpattern\ud835\udc70\ud835\udcb1\ud835\udcae\u211b\\displaystyle p(\\bm{y}|\\bm{I},\\mathcal{VSR})=p(\\bm{y}|\\text{pattern})p(\\text{pattern}|\\bm{I},\\mathcal{VSR}).italic_p ( bold_italic_y | bold_italic_I , caligraphic_V caligraphic_S caligraphic_R ) = italic_p ( bold_italic_y | pattern ) italic_p ( pattern | bold_italic_I , caligraphic_V caligraphic_S caligraphic_R ) .(2)", "Further, we utilize two sequences \ud835\udcae=(s1b,\u2026,sKb)\ud835\udcaesubscriptsuperscript\ud835\udc60\ud835\udc4f1\u2026subscriptsuperscript\ud835\udc60\ud835\udc4f\ud835\udc3e\\mathcal{S}=(s^{b}_{1},...,s^{b}_{K})caligraphic_S = ( italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ) and \u211b=(\ud835\udc931,\u2026,\ud835\udc93K)\u211bsubscript\ud835\udc931\u2026subscript\ud835\udc93\ud835\udc3e\\mathcal{R}=(\\bm{r}_{1},...,\\bm{r}_{K})caligraphic_R = ( bold_italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , bold_italic_r start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ) to model the descriptive patterns. Specifically, \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S is a semantic structure of the sentence and each sib\u2208\ud835\udcaesubscriptsuperscript\ud835\udc60\ud835\udc4f\ud835\udc56\ud835\udcaes^{b}_{i}\\in\\mathcal{S}italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_S is a sub-role. By \u201csub-role\u201d, we mean that each role si\u2208\ud835\udcb1\ud835\udcae\u211bsubscript\ud835\udc60\ud835\udc56\ud835\udcb1\ud835\udcae\u211bs_{i}\\in\\mathcal{VSR}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_V caligraphic_S caligraphic_R can be divided into nisubscript\ud835\udc5b\ud835\udc56n_{i}italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT sub-roles, and when ni=1subscript\ud835\udc5b\ud835\udc561n_{i}=1italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1, role sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT itself is a sub-role. Thus, VSR in Figure\u00a03 can be rewritten as Arg0, Arg1, LOC-1, and LOC-2. \u211b\u211b\\mathcal{R}caligraphic_R is a sequence of visual features of the corresponding grounded entities for each sub-role in \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S (\\eg, \ud835\udc93isubscript\ud835\udc93\ud835\udc56\\bm{r}_{i}bold_italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the features of visual regions referring to sibsubscriptsuperscript\ud835\udc60\ud835\udc4f\ud835\udc56s^{b}_{i}italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT). Particularly, for presentation conciseness, we regard the verb in \ud835\udcb1\ud835\udcae\u211b\ud835\udcb1\ud835\udcae\u211b\\mathcal{VSR}caligraphic_V caligraphic_S caligraphic_R as a special type of sub-role, and since there are no grounded visual regions referring to the verb, we use the global image feature as the grounded region feature in \u211b\u211b\\mathcal{R}caligraphic_R. Meanwhile, we use \u211b~~\u211b\\mathcal{\\tilde{R}}over~ start_ARG caligraphic_R end_ARG to denote a set of all elements in the sequence \u211b\u211b\\mathcal{R}caligraphic_R. Thus, we further decompose this task into three components:p(\ud835\udc9a|\ud835\udc70,\ud835\udcb1\ud835\udcae\u211b)=p(\ud835\udc9a|\ud835\udcae,\u211b)\u23dfCaptionerp(\ud835\udcae,\u211b|\u211b~,\ud835\udcb1\ud835\udcae\u211b)\u23dfSSPp(\u211b~|\ud835\udc70,\ud835\udcb1\ud835\udcae\u211b)\u23dfGSRL.\ud835\udc5dconditional\ud835\udc9a\ud835\udc70\ud835\udcb1\ud835\udcae\u211bsubscript\u23df\ud835\udc5dconditional\ud835\udc9a\ud835\udcae\u211bCaptionersubscript\u23df\ud835\udc5d\ud835\udcaeconditional\u211b~\u211b\ud835\udcb1\ud835\udcae\u211bSSPsubscript\u23df\ud835\udc5dconditional~\u211b\ud835\udc70\ud835\udcb1\ud835\udcae\u211bGSRL\\displaystyle p(\\bm{y}|\\bm{I},\\mathcal{VSR})=\\underbrace{p(\\bm{y}|\\mathcal{S},\\mathcal{R})}_{\\text{Captioner}}\\underbrace{p(\\mathcal{S},\\mathcal{R}|\\mathcal{\\tilde{R}},\\mathcal{VSR})}_{\\text{SSP}}\\underbrace{p(\\mathcal{\\tilde{R}}|\\bm{I},\\mathcal{VSR})}_{\\text{GSRL}}.italic_p ( bold_italic_y | bold_italic_I , caligraphic_V caligraphic_S caligraphic_R ) = under\u23df start_ARG italic_p ( bold_italic_y | caligraphic_S , caligraphic_R ) end_ARG start_POSTSUBSCRIPT Captioner end_POSTSUBSCRIPT under\u23df start_ARG italic_p ( caligraphic_S , caligraphic_R | over~ start_ARG caligraphic_R end_ARG , caligraphic_V caligraphic_S caligraphic_R ) end_ARG start_POSTSUBSCRIPT SSP end_POSTSUBSCRIPT under\u23df start_ARG italic_p ( over~ start_ARG caligraphic_R end_ARG | bold_italic_I , caligraphic_V caligraphic_S caligraphic_R ) end_ARG start_POSTSUBSCRIPT GSRL end_POSTSUBSCRIPT .(3)", "In this section, we first introduce each component of the whole framework of the VSR-guided controllable image captioning model sequentially in Section\u00a03.1 (cf. Figure\u00a03), including a grounded semantic role labeling (GSRL) model, a semantic structure planner (SSP), and a role-shift captioning model. Then, we demonstrate the details about all training objectives and the inference stage in Section\u00a03.2, including extending from a single VSR to multiple VSRs.", "Given an image \ud835\udc70\ud835\udc70\\bm{I}bold_italic_I, we first utilize an object detector\u00a0[50] to extract a set of object proposals \u212c\u212c\\mathcal{B}caligraphic_B. Each proposal \ud835\udc83i\u2208\u212csubscript\ud835\udc83\ud835\udc56\u212c\\bm{b}_{i}\\in\\mathcal{B}bold_italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_B is associated with a visual feature \ud835\udc87isubscript\ud835\udc87\ud835\udc56\\bm{f}_{i}bold_italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and a class label ci\u2208\ud835\udc9esubscript\ud835\udc50\ud835\udc56\ud835\udc9ec_{i}\\in\\mathcal{C}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_C. Then, we group all these proposals into N\ud835\udc41Nitalic_N disjoint sets, \\ie, \u212c={\u212c1,\u2026,\u212cN}\u212csubscript\u212c1\u2026subscript\u212c\ud835\udc41\\mathcal{B}=\\{\\mathcal{B}_{1},...,\\mathcal{B}_{N}\\}caligraphic_B = { caligraphic_B start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , caligraphic_B start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }333Due to different annotation natures of specific CIC datasets, we group proposals by different principles. Details are shown in Section\u00a04.2., and each proposal set \u212cisubscript\u212c\ud835\udc56\\mathcal{B}_{i}caligraphic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT consists of one or more proposals. In this GSRL step, we need to refer each sub-role in the \ud835\udcb1\ud835\udcae\u211b\ud835\udcb1\ud835\udcae\u211b\\mathcal{VSR}caligraphic_V caligraphic_S caligraphic_R to a proposal set in \u212c\u212c\\mathcal{B}caligraphic_B. Specifically, we calculate the similarity score aijsubscript\ud835\udc4e\ud835\udc56\ud835\udc57a_{ij}italic_a start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT between semantic role sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and proposal set \u212cjsubscript\u212c\ud835\udc57\\mathcal{B}_{j}caligraphic_B start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT by:\ud835\udc92i=[\ud835\udc86vg;\ud835\udc86sig;\ud835\udc87\u00af],aij=Fa(\ud835\udc92i,\ud835\udc87\u00af\ud835\udc8b),formulae-sequencesubscript\ud835\udc92\ud835\udc56subscriptsuperscript\ud835\udc86\ud835\udc54\ud835\udc63subscriptsuperscript\ud835\udc86\ud835\udc54subscript\ud835\udc60\ud835\udc56bold-\u00af\ud835\udc87subscript\ud835\udc4e\ud835\udc56\ud835\udc57subscript\ud835\udc39\ud835\udc4esubscript\ud835\udc92\ud835\udc56subscriptbold-\u00af\ud835\udc87\ud835\udc8b\\displaystyle\\bm{q}_{i}=\\left[\\bm{e}^{g}_{v};\\bm{e}^{g}_{s_{i}};\\bm{\\bar{f}}\\right],\\quad a_{ij}=F_{a}(\\bm{q}_{i},\\bm{\\bar{f}_{j}}),bold_italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ bold_italic_e start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ; bold_italic_e start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ; overbold_\u00af start_ARG bold_italic_f end_ARG ] , italic_a start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = italic_F start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( bold_italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , overbold_\u00af start_ARG bold_italic_f end_ARG start_POSTSUBSCRIPT bold_italic_j end_POSTSUBSCRIPT ) ,(4)where \ud835\udc86vgsubscriptsuperscript\ud835\udc86\ud835\udc54\ud835\udc63\\bm{e}^{g}_{v}bold_italic_e start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and \ud835\udc86sigsubscriptsuperscript\ud835\udc86\ud835\udc54subscript\ud835\udc60\ud835\udc56\\bm{e}^{g}_{s_{i}}bold_italic_e start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT are the word embedding features of verb v\ud835\udc63vitalic_v and semantic role sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, \ud835\udc87\u00afbold-\u00af\ud835\udc87\\bm{\\bar{f}}overbold_\u00af start_ARG bold_italic_f end_ARG and \ud835\udc87\u00af\ud835\udc8bsubscriptbold-\u00af\ud835\udc87\ud835\udc8b\\bm{\\bar{f}_{j}}overbold_\u00af start_ARG bold_italic_f end_ARG start_POSTSUBSCRIPT bold_italic_j end_POSTSUBSCRIPT represent the average-pooled visual features of proposal set \u212c\u212c\\mathcal{B}caligraphic_B and \u212cjsubscript\u212c\ud835\udc57\\mathcal{B}_{j}caligraphic_B start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, [;] is a concatenation operation, and Fasubscript\ud835\udc39\ud835\udc4eF_{a}italic_F start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT is a learnable similarity function444For conciseness, we leave the details in the supplementary material. .", "After obtaining the grounding similarity scores {aij}subscript\ud835\udc4e\ud835\udc56\ud835\udc57\\{a_{ij}\\}{ italic_a start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT } between semantic role sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and all proposal sets {\u212cj}subscript\u212c\ud835\udc57\\{\\mathcal{B}_{j}\\}{ caligraphic_B start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }, we then select the top nisubscript\ud835\udc5b\ud835\udc56n_{i}italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT proposal sets with the highest scores as the grounding results for all sub-roles of sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. \u211b~~\u211b\\mathcal{\\tilde{R}}over~ start_ARG caligraphic_R end_ARG in Eq.\u00a0(3) is the set of visual features of all grounded proposal sets.", "Semantic structure planner (SSP) is a hierarchical semantic structure learning model, which aims to learn a reasonable sequence of sub-roles \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S. As shown in Figure\u00a03, it consists of two subnets: an S-level SSP and an R-level SSP.", "S-level SSP. The sentence-level (S-level) SSP is a coarse-grained structure learning model, which only learns a sequence of all involved general semantic roles (including the verb) in \ud835\udcb1\ud835\udcae\u211b\ud835\udcb1\ud835\udcae\u211b\\mathcal{VSR}caligraphic_V caligraphic_S caligraphic_R (\\eg, ride, Arg0riderrider{}_{\\text{rider}}start_FLOATSUBSCRIPT rider end_FLOATSUBSCRIPT, Arg1steedsteed{}_{\\text{steed}}start_FLOATSUBSCRIPT steed end_FLOATSUBSCRIPT and LOC in Figure\u00a03). To this end, we formulate this sentence-level structure learning as a role sequence generation task, as long as we constrain that each output role token belongs to the given role set and each role can only appear once. Specifically, we utilize a three-layer Transformer\u00a0[59]555More comparison results between Transformer and Sinkhorn networks\u00a0[43, 16] are left in supplementary material. to calucate the probability of roles p(st|\ud835\udcb1\ud835\udcae\u211b)\ud835\udc5dconditionalsubscript\ud835\udc60\ud835\udc61\ud835\udcb1\ud835\udcae\u211bp(s_{t}|\\mathcal{VSR})italic_p ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | caligraphic_V caligraphic_S caligraphic_R ) at each time step t\ud835\udc61titalic_t4:\ud835\udc6f\ud835\udc6f\\displaystyle\\bm{H}bold_italic_H=Transformerenc({FCa(\ud835\udc86vi+\ud835\udc86sii)}),absentsubscriptTransformerencsubscriptFC\ud835\udc4esubscriptsuperscript\ud835\udc86\ud835\udc56\ud835\udc63subscriptsuperscript\ud835\udc86\ud835\udc56subscript\ud835\udc60\ud835\udc56\\displaystyle=\\text{Transformer}_{\\text{enc}}\\left(\\{\\text{FC}_{a}(\\bm{e}^{i}_{v}+\\bm{e}^{i}_{s_{i}})\\}\\right),= Transformer start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT ( { FC start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( bold_italic_e start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT + bold_italic_e start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) } ) ,(5)p(st|\ud835\udcb1\ud835\udcae\u211b)\ud835\udc5dconditionalsubscript\ud835\udc60\ud835\udc61\ud835\udcb1\ud835\udcae\u211b\\displaystyle p(s_{t}|\\mathcal{VSR})italic_p ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | caligraphic_V caligraphic_S caligraphic_R )=Transformerdec(\ud835\udc6f,\ud835\udc86s<to),absentsubscriptTransformerdec\ud835\udc6fsubscriptsuperscript\ud835\udc86\ud835\udc5csubscript\ud835\udc60absent\ud835\udc61\\displaystyle=\\text{Transformer}_{\\text{dec}}\\left(\\bm{H},\\bm{e}^{o}_{s_{<t}}\\right),= Transformer start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT ( bold_italic_H , bold_italic_e start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ,where Transformer*{}_{*}start_FLOATSUBSCRIPT * end_FLOATSUBSCRIPT are the encoder (enc) and decoder (dec) of the standard multi-head transformer. \ud835\udc86visubscriptsuperscript\ud835\udc86\ud835\udc56\ud835\udc63\\bm{e}^{i}_{v}bold_italic_e start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and \ud835\udc86siisubscriptsuperscript\ud835\udc86\ud835\udc56subscript\ud835\udc60\ud835\udc56\\bm{e}^{i}_{s_{i}}bold_italic_e start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT are the word embedding features of verb v\ud835\udc63vitalic_v and semantic role sjsubscript\ud835\udc60\ud835\udc57s_{j}italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, respectively. FCasubscriptFC\ud835\udc4e\\text{FC}_{a}FC start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT is a learnable fc-layer to obtain the embedding of each input token. \ud835\udc86s<tosubscriptsuperscript\ud835\udc86\ud835\udc5csubscript\ud835\udc60absent\ud835\udc61\\bm{e}^{o}_{s_{<t}}bold_italic_e start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT is the sequence of embeddings of previous roles. Based on p(st|\ud835\udcb1\ud835\udcae\u211b)\ud835\udc5dconditionalsubscript\ud835\udc60\ud835\udc61\ud835\udcb1\ud835\udcae\u211bp(s_{t}|\\mathcal{VSR})italic_p ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | caligraphic_V caligraphic_S caligraphic_R ), we can predict a role at time step t\ud835\udc61titalic_t and obtain an initial role sequence, \\eg, Arg0riderrider{}_{\\text{rider}}start_FLOATSUBSCRIPT rider end_FLOATSUBSCRIPT \u2013 ride \u2013 Arg1steedsteed{}_{\\text{steed}}start_FLOATSUBSCRIPT steed end_FLOATSUBSCRIPT \u2013 LOC in Figure\u00a03.", "R-level SSP. The role-level (R-level) SSP is a fine-grained structure model which aims to rank all sub-roles within the same semantic role (\\eg, LOC-1 and LOC-2 are two sub-roles of role Loc in Figure\u00a03). Since the only differences among these sub-roles are the grounded visual regions, we borrow ideas from the Sinkhorn networks\u00a0[43, 16], which use a differentiable Sinkhorn operation to learn a soft permutation matrix \ud835\udc77\ud835\udc77\\bm{P}bold_italic_P. Specifically, for each role sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT with multiple sub-roles (\\ie, ni>1subscript\ud835\udc5b\ud835\udc561n_{i}>1italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > 1), we first select all the corresponding grounded proposal sets for these sub-roles, denoted as \u212c^={\u212c^1,\u2026,\u212c^ni}^\u212csubscript^\u212c1\u2026subscript^\u212csubscript\ud835\udc5b\ud835\udc56\\mathcal{\\hat{B}}=\\{\\mathcal{\\hat{B}}_{1},...,\\mathcal{\\hat{B}}_{n_{i}}\\}over^ start_ARG caligraphic_B end_ARG = { over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT }. And for each proposal \ud835\udc83*\u2208\u212c^subscript\ud835\udc83^\u212c\\bm{b}_{*}\\in\\mathcal{\\hat{B}}bold_italic_b start_POSTSUBSCRIPT * end_POSTSUBSCRIPT \u2208 over^ start_ARG caligraphic_B end_ARG, we encode a feature vector \ud835\udc9b*=[\ud835\udc9b*v;\ud835\udc9b*si;\ud835\udc9b*l]subscript\ud835\udc9bsubscriptsuperscript\ud835\udc9b\ud835\udc63subscriptsuperscript\ud835\udc9bsubscript\ud835\udc60\ud835\udc56subscriptsuperscript\ud835\udc9b\ud835\udc59\\bm{z}_{*}=[\\bm{z}^{v}_{*};\\bm{z}^{s_{i}}_{*};\\bm{z}^{l}_{*}]bold_italic_z start_POSTSUBSCRIPT * end_POSTSUBSCRIPT = [ bold_italic_z start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ; bold_italic_z start_POSTSUPERSCRIPT italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ; bold_italic_z start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ], where \ud835\udc9b*vsubscriptsuperscript\ud835\udc9b\ud835\udc63\\bm{z}^{v}_{*}bold_italic_z start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT * end_POSTSUBSCRIPT is a transformation of its visual feature \ud835\udc87*subscript\ud835\udc87\\bm{f}_{*}bold_italic_f start_POSTSUBSCRIPT * end_POSTSUBSCRIPT, \ud835\udc9b*sisubscriptsuperscript\ud835\udc9bsubscript\ud835\udc60\ud835\udc56\\bm{z}^{s_{i}}_{*}bold_italic_z start_POSTSUPERSCRIPT italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT * end_POSTSUBSCRIPT is the word embedding feature of the semantic role sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and \ud835\udc9b*lsubscriptsuperscript\ud835\udc9b\ud835\udc59\\bm{z}^{l}_{*}bold_italic_z start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT * end_POSTSUBSCRIPT is a 4-d encoding of the spatial position of proposal \ud835\udc83*subscript\ud835\udc83\\bm{b}_{*}bold_italic_b start_POSTSUBSCRIPT * end_POSTSUBSCRIPT. Then, we transform each feature \ud835\udc9b*subscript\ud835\udc9b\\bm{z}_{*}bold_italic_z start_POSTSUBSCRIPT * end_POSTSUBSCRIPT into nisubscript\ud835\udc5b\ud835\udc56n_{i}italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT-d, and average-pooled all features among the same proposal set, \\ie, we can obtain an nisubscript\ud835\udc5b\ud835\udc56n_{i}italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT-d feature for each \u212c^isubscript^\u212c\ud835\udc56\\mathcal{\\hat{B}}_{i}over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. We concatenate all these features to get an ni\u00d7nisubscript\ud835\udc5b\ud835\udc56subscript\ud835\udc5b\ud835\udc56n_{i}\\times n_{i}italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u00d7 italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT matrix \ud835\udc81\ud835\udc81\\bm{Z}bold_italic_Z. Finally, we use the Sinkhorn operation to obtain the soft permutation matrix \ud835\udc77\ud835\udc77\\bm{P}bold_italic_P4:\ud835\udc77=Sinkhorn(\ud835\udc81).\ud835\udc77Sinkhorn\ud835\udc81\\displaystyle\\bm{P}=\\text{Sinkhorn}(\\bm{Z}).bold_italic_P = Sinkhorn ( bold_italic_Z ) .(6)", "After the two SSP subnets (\\ie, S-level and R-level), we can obtain the semantic structure \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S (cf. Eq.\u00a0(3)). Based on the sequence of \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S and the set of proposal featurs \u211b~~\u211b\\mathcal{\\tilde{R}}over~ start_ARG caligraphic_R end_ARG from the GSRL model, we re-rank \u211b~~\u211b\\mathcal{\\tilde{R}}over~ start_ARG caligraphic_R end_ARG based on \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S and obtain \u211b\u211b\\mathcal{R}caligraphic_R.", "Given the semantic structure sequence \ud835\udcae=(s1b,\u2026,sKb)\ud835\udcaesubscriptsuperscript\ud835\udc60\ud835\udc4f1\u2026subscriptsuperscript\ud835\udc60\ud835\udc4f\ud835\udc3e\\mathcal{S}=(s^{b}_{1},...,s^{b}_{K})caligraphic_S = ( italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ) and corresponding proposal feature sequence \u211b=(\ud835\udc931,\u2026,\ud835\udc93K)\u211bsubscript\ud835\udc931\u2026subscript\ud835\udc93\ud835\udc3e\\mathcal{R}=(\\bm{r}_{1},...,\\bm{r}_{K})caligraphic_R = ( bold_italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , bold_italic_r start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ), we utilize a two-layer LSTM to generate the final caption \ud835\udc9a\ud835\udc9a\\bm{y}bold_italic_y. At each time step, the model fouces on one specific sub-role \ud835\udc94tbsubscriptsuperscript\ud835\udc94\ud835\udc4f\ud835\udc61\\bm{s}^{b}_{t}bold_italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and its grounded region set \ud835\udc93tsubscript\ud835\udc93\ud835\udc61\\bm{r}_{t}bold_italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and then generates the word ytsubscript\ud835\udc66\ud835\udc61y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Therefore, we take inspirations from previous CIC methods\u00a0[16, 10], and predict two distributions simultaneously: p(gt|\ud835\udcae,\u211b)\ud835\udc5dconditionalsubscript\ud835\udc54\ud835\udc61\ud835\udcae\u211bp(g_{t}|\\mathcal{S},\\mathcal{R})italic_p ( italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | caligraphic_S , caligraphic_R ) for controlling the shift of sub-roles, and p(yt|\ud835\udcae,\u211b)\ud835\udc5dconditionalsubscript\ud835\udc66\ud835\udc61\ud835\udcae\u211bp(y_{t}|\\mathcal{S},\\mathcal{R})italic_p ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | caligraphic_S , caligraphic_R ) to predict the distribution of a word.", "As for the role-shift, we use an adaptive attention mechanism\u00a0[38] to predict the probability of shifting4:\u03b1tg,\ud835\udf36tr,\ud835\udc94\ud835\udc93tgsubscriptsuperscript\ud835\udefc\ud835\udc54\ud835\udc61subscriptsuperscript\ud835\udf36\ud835\udc5f\ud835\udc61\ud835\udc94subscriptsuperscript\ud835\udc93\ud835\udc54\ud835\udc61\\displaystyle\\alpha^{g}_{t},\\bm{\\alpha}^{r}_{t},\\bm{sr}^{g}_{t}italic_\u03b1 start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_\u03b1 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_s bold_italic_r start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT=AdaptiveAttna(\ud835\udc99t,\ud835\udc93t),absentsubscriptAdaptiveAttn\ud835\udc4esubscript\ud835\udc99\ud835\udc61subscript\ud835\udc93\ud835\udc61\\displaystyle=\\text{AdaptiveAttn}_{a}(\\bm{x}_{t},\\bm{r}_{t}),= AdaptiveAttn start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,(7)where AdaptiveAttnasubscriptAdaptiveAttn\ud835\udc4e\\text{AdaptiveAttn}_{a}AdaptiveAttn start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT is an adaptive attention network, \ud835\udc99tsubscript\ud835\udc99\ud835\udc61\\bm{x}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the input query for attention, \ud835\udc94\ud835\udc93tg\ud835\udc94subscriptsuperscript\ud835\udc93\ud835\udc54\ud835\udc61\\bm{sr}^{g}_{t}bold_italic_s bold_italic_r start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is a sential vector, \u03b1tgsubscriptsuperscript\ud835\udefc\ud835\udc54\ud835\udc61\\alpha^{g}_{t}italic_\u03b1 start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and \ud835\udc82trsubscriptsuperscript\ud835\udc82\ud835\udc5f\ud835\udc61\\bm{a}^{r}_{t}bold_italic_a start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are the attention weights for the sential vector and region features, respectively. We directly use attention weight \u03b1tgsubscriptsuperscript\ud835\udefc\ud835\udc54\ud835\udc61\\alpha^{g}_{t}italic_\u03b1 start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as the probability of shifting sub-roles, \\ie, p(gt|\ud835\udcae,\u211b)=\u03b1tg\ud835\udc5dconditionalsubscript\ud835\udc54\ud835\udc61\ud835\udcae\u211bsubscriptsuperscript\ud835\udefc\ud835\udc54\ud835\udc61p(g_{t}|\\mathcal{S},\\mathcal{R})=\\alpha^{g}_{t}italic_p ( italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | caligraphic_S , caligraphic_R ) = italic_\u03b1 start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Based on probability p(gt|\ud835\udcae,\u211b)\ud835\udc5dconditionalsubscript\ud835\udc54\ud835\udc61\ud835\udcae\u211bp(g_{t}|\\mathcal{S},\\mathcal{R})italic_p ( italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | caligraphic_S , caligraphic_R ), we can sample a gate value gj\u2208{0,1}subscript\ud835\udc54\ud835\udc5701g_{j}\\in\\{0,1\\}italic_g start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u2208 { 0 , 1 }, and the focused sub-role at time step t\ud835\udc61titalic_t is:stb\u2190\ud835\udcae[i],wherei=min\u2061(1+\u2211j=1t\u22121gj,K).formulae-sequence\u2190subscriptsuperscript\ud835\udc60\ud835\udc4f\ud835\udc61\ud835\udcaedelimited-[]\ud835\udc56where\ud835\udc561subscriptsuperscript\ud835\udc611\ud835\udc571subscript\ud835\udc54\ud835\udc57\ud835\udc3e\\displaystyle s^{b}_{t}\\leftarrow\\mathcal{S}[i],\\text{where}\\;i=\\min\\left(1+\\textstyle{\\sum}^{t-1}_{j=1}g_{j},K\\right).italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2190 caligraphic_S [ italic_i ] , where italic_i = roman_min ( 1 + \u2211 start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_K ) .(8)Due to the special nature of sub-role \u201cverb\u201d, we fix gt+1=1subscript\ud835\udc54\ud835\udc6111g_{t+1}=1italic_g start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = 1 when stbsubscriptsuperscript\ud835\udc60\ud835\udc4f\ud835\udc61s^{b}_{t}italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the verb.", "For each sub-role stbsubscriptsuperscript\ud835\udc60\ud835\udc4f\ud835\udc61s^{b}_{t}italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, we use the corresponding proposal set features \ud835\udc93tsubscript\ud835\udc93\ud835\udc61\\bm{r}_{t}bold_italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and a two-layer LSTM to generate word ytsubscript\ud835\udc66\ud835\udc61y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT:\ud835\udc89t1subscriptsuperscript\ud835\udc891\ud835\udc61\\displaystyle\\bm{h}^{1}_{t}bold_italic_h start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT=LSTM1(\ud835\udc89t\u221211,{yt\u22121,\ud835\udc87\u00af,\ud835\udc89t\u221212}),absentsubscriptLSTM1subscriptsuperscript\ud835\udc891\ud835\udc611subscript\ud835\udc66\ud835\udc611bold-\u00af\ud835\udc87subscriptsuperscript\ud835\udc892\ud835\udc611\\displaystyle=\\text{LSTM}_{1}\\left(\\bm{h}^{1}_{t-1},\\{y_{t-1},\\bm{\\bar{f}},\\bm{h}^{2}_{t-1}\\}\\right),= LSTM start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_h start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , { italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , overbold_\u00af start_ARG bold_italic_f end_ARG , bold_italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT } ) ,(9)\ud835\udc89t2subscriptsuperscript\ud835\udc892\ud835\udc61\\displaystyle\\bm{h}^{2}_{t}bold_italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT=LSTM2(\ud835\udc89t\u221212,{\ud835\udc89t1,\ud835\udc84t}),absentsubscriptLSTM2subscriptsuperscript\ud835\udc892\ud835\udc611subscriptsuperscript\ud835\udc891\ud835\udc61subscript\ud835\udc84\ud835\udc61\\displaystyle=\\text{LSTM}_{2}\\left(\\bm{h}^{2}_{t-1},\\{\\bm{h}^{1}_{t},\\bm{c}_{t}\\}\\right),= LSTM start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , { bold_italic_h start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } ) ,ytsubscript\ud835\udc66\ud835\udc61\\displaystyle y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT\u223cp(yt|\ud835\udcae,\u211b)=FCb(\ud835\udc89t2),similar-toabsent\ud835\udc5dconditionalsubscript\ud835\udc66\ud835\udc61\ud835\udcae\u211bsubscriptFC\ud835\udc4fsubscriptsuperscript\ud835\udc892\ud835\udc61\\displaystyle\\sim p(y_{t}|\\mathcal{S},\\mathcal{R})=\\text{FC}_{b}(\\bm{h}^{2}_{t}),\u223c italic_p ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | caligraphic_S , caligraphic_R ) = FC start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( bold_italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,where \ud835\udc89t1subscriptsuperscript\ud835\udc891\ud835\udc61\\bm{h}^{1}_{t}bold_italic_h start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and \ud835\udc89t2subscriptsuperscript\ud835\udc892\ud835\udc61\\bm{h}^{2}_{t}bold_italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are hidden states of the first- and second-layer LSTM (\\ie, LSTM11{}_{1}start_FLOATSUBSCRIPT 1 end_FLOATSUBSCRIPT and LSTM22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT), FCbsubscriptFC\ud835\udc4f\\text{FC}_{b}FC start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT is a learnable fc-layer, and \ud835\udc84tsubscript\ud835\udc84\ud835\udc61\\bm{c}_{t}bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is a context vector. To further distinguish the textual and visual words, we use another adaptive attention network to obtain the context vector \ud835\udc84tsubscript\ud835\udc84\ud835\udc61\\bm{c}_{t}bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT4:\u03b1tv,\ud835\udf36tr,\ud835\udc94\ud835\udc93tvsubscriptsuperscript\ud835\udefc\ud835\udc63\ud835\udc61subscriptsuperscript\ud835\udf36\ud835\udc5f\ud835\udc61\ud835\udc94subscriptsuperscript\ud835\udc93\ud835\udc63\ud835\udc61\\displaystyle\\alpha^{v}_{t},\\bm{\\alpha}^{r}_{t},\\bm{sr}^{v}_{t}italic_\u03b1 start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_\u03b1 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_s bold_italic_r start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT=AdaptiveAttnb(\ud835\udc99t,\ud835\udc93t),absentsubscriptAdaptiveAttn\ud835\udc4fsubscript\ud835\udc99\ud835\udc61subscript\ud835\udc93\ud835\udc61\\displaystyle=\\text{AdaptiveAttn}_{b}(\\bm{x}_{t},\\bm{r}_{t}),= AdaptiveAttn start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,(10)\ud835\udc84tsubscript\ud835\udc84\ud835\udc61\\displaystyle\\bm{c}_{t}bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT=\u03b1tv\u22c5\ud835\udc94\ud835\udc93tv+\u2211i\ud835\udf36t,ir\u22c5\ud835\udc93t,i,absent\u22c5subscriptsuperscript\ud835\udefc\ud835\udc63\ud835\udc61\ud835\udc94subscriptsuperscript\ud835\udc93\ud835\udc63\ud835\udc61subscript\ud835\udc56\u22c5subscriptsuperscript\ud835\udf36\ud835\udc5f\ud835\udc61\ud835\udc56subscript\ud835\udc93\ud835\udc61\ud835\udc56\\displaystyle=\\alpha^{v}_{t}\\cdot\\bm{sr}^{v}_{t}+\\textstyle{\\sum}_{i}\\bm{\\alpha}^{r}_{t,i}\\cdot\\bm{r}_{t,i},= italic_\u03b1 start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u22c5 bold_italic_s bold_italic_r start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_\u03b1 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , italic_i end_POSTSUBSCRIPT \u22c5 bold_italic_r start_POSTSUBSCRIPT italic_t , italic_i end_POSTSUBSCRIPT ,where \ud835\udc99tsubscript\ud835\udc99\ud835\udc61\\bm{x}_{t}bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the query for adaptive attention (\\ie, the input of the LSTM1subscriptLSTM1\\text{LSTM}_{1}LSTM start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT), \ud835\udc94\ud835\udc93tv\ud835\udc94subscriptsuperscript\ud835\udc93\ud835\udc63\ud835\udc61\\bm{sr}^{v}_{t}bold_italic_s bold_italic_r start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is a sential vector, and \u03b1tvsubscriptsuperscript\ud835\udefc\ud835\udc63\ud835\udc61\\alpha^{v}_{t}italic_\u03b1 start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and \ud835\udf36trsubscriptsuperscript\ud835\udf36\ud835\udc5f\ud835\udc61\\bm{\\alpha}^{r}_{t}bold_italic_\u03b1 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are the attention weights for the sential vector and region features.", "Training Stage. In the training stage, we train the three components (GSRL, SSP and captioning model) separately:", "Training objective of GSRL. For the GSRL model, we use a binary cross-entropy (BCE) loss between the predicted similarity scores a^ijsubscript^\ud835\udc4e\ud835\udc56\ud835\udc57\\hat{a}_{ij}over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT and its ground truth aij*subscriptsuperscript\ud835\udc4e\ud835\udc56\ud835\udc57a^{*}_{ij}italic_a start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT as the training loss:LGSRL=\u2211ijBCE(a^ij,aij*).subscript\ud835\udc3fGSRLsubscript\ud835\udc56\ud835\udc57BCEsubscript^\ud835\udc4e\ud835\udc56\ud835\udc57subscriptsuperscript\ud835\udc4e\ud835\udc56\ud835\udc57\\displaystyle L_{\\text{GSRL}}=\\textstyle{\\sum}_{ij}\\text{BCE}(\\hat{a}_{ij},a^{*}_{ij}).italic_L start_POSTSUBSCRIPT GSRL end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT BCE ( over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) .(11)", "Training objective of SSP. For S-level SSP, we use a cross-entropy (XE) loss between prediction s^tsubscript^\ud835\udc60\ud835\udc61\\hat{s}_{t}over^ start_ARG italic_s end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and its ground truth st*subscriptsuperscript\ud835\udc60\ud835\udc61s^{*}_{t}italic_s start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as the training objective. For R-level SSP, we use a mean square (MSE) loss between prediction \ud835\udc77^tsubscriptbold-^\ud835\udc77\ud835\udc61\\bm{\\hat{P}}_{t}overbold_^ start_ARG bold_italic_P end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and its ground truth \ud835\udc77*tsubscriptsuperscript\ud835\udc77\ud835\udc61\\bm{P^{*}}_{t}bold_italic_P start_POSTSUPERSCRIPT bold_* end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as the training objective:LSSPS=\u2211tXE(s^t,st*),LSSPR=\u2211t\ud835\udfcf(nt>1)MSE(\ud835\udc77^t,\ud835\udc77*t),formulae-sequencesubscriptsuperscript\ud835\udc3f\ud835\udc46SSPsubscript\ud835\udc61XEsubscript^\ud835\udc60\ud835\udc61subscriptsuperscript\ud835\udc60\ud835\udc61subscriptsuperscript\ud835\udc3f\ud835\udc45SSPsubscript\ud835\udc61subscript1subscript\ud835\udc5b\ud835\udc611MSEsubscriptbold-^\ud835\udc77\ud835\udc61subscriptsuperscript\ud835\udc77\ud835\udc61\\displaystyle L^{S}_{\\text{SSP}}=\\textstyle{\\sum}_{t}\\text{XE}(\\hat{s}_{t},s^{*}_{t}),L^{R}_{\\text{SSP}}=\\textstyle{\\sum}_{t}\\mathbf{1}_{(n_{t}>1)}\\text{MSE}(\\bm{\\hat{P}}_{t},\\bm{P^{*}}_{t}),italic_L start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT start_POSTSUBSCRIPT SSP end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT XE ( over^ start_ARG italic_s end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_s start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , italic_L start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT start_POSTSUBSCRIPT SSP end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_1 start_POSTSUBSCRIPT ( italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT > 1 ) end_POSTSUBSCRIPT MSE ( overbold_^ start_ARG bold_italic_P end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_P start_POSTSUPERSCRIPT bold_* end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,(12)where \ud835\udfcf(nt>1)subscript1subscript\ud835\udc5b\ud835\udc611\\mathbf{1}_{(n_{t}>1)}bold_1 start_POSTSUBSCRIPT ( italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT > 1 ) end_POSTSUBSCRIPT is an indicator function, being 1 if nt>1subscript\ud835\udc5b\ud835\udc611n_{t}>1italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT > 1 and 0 otherwise.", "Training objective of captioning model. We follow the conventions of previous captioning works and use a two-stage training scheme: XE and RL stages. In the XE stage, we use an XE loss between predicted words and ground truth words as the training loss. In the RL stage, we use a self-critical baseline\u00a0[51]. At each step, we sample from p(yt|\ud835\udcae,\u211b)\ud835\udc5dconditionalsubscript\ud835\udc66\ud835\udc61\ud835\udcae\u211bp(y_{t}|\\mathcal{S},\\mathcal{R})italic_p ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | caligraphic_S , caligraphic_R ) and p(gt|\ud835\udcae,\u211b)\ud835\udc5dconditionalsubscript\ud835\udc54\ud835\udc61\ud835\udcae\u211bp(g_{t}|\\mathcal{S},\\mathcal{R})italic_p ( italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | caligraphic_S , caligraphic_R ) to obtain the next word yt+1subscript\ud835\udc66\ud835\udc611y_{t+1}italic_y start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT and sub-role st+1bsubscriptsuperscript\ud835\udc60\ud835\udc4f\ud835\udc611s^{b}_{t+1}italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT. Then we calcuate the reward r(\ud835\udc9as)\ud835\udc5fsuperscript\ud835\udc9a\ud835\udc60r(\\bm{y}^{s})italic_r ( bold_italic_y start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ) of the sampled sentence \ud835\udc9assuperscript\ud835\udc9a\ud835\udc60\\bm{y}^{s}bold_italic_y start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT. Baseline b\ud835\udc4fbitalic_b is the reward of the greedily generated sentence. Thus, the gradient expression of the training loss is:\u2207\u03b8L=\u2212(r(\ud835\udc9as)\u2212b)(\u2207\u03b8log\u2061p(\ud835\udc9as)+\u2207\u03b8log\u2061p(\ud835\udc88s)),subscript\u2207\ud835\udf03\ud835\udc3f\ud835\udc5fsuperscript\ud835\udc9a\ud835\udc60\ud835\udc4fsubscript\u2207\ud835\udf03\ud835\udc5dsuperscript\ud835\udc9a\ud835\udc60subscript\u2207\ud835\udf03\ud835\udc5dsuperscript\ud835\udc88\ud835\udc60\\nabla_{\\theta}L=-(r(\\bm{y}^{s})-b)(\\nabla_{\\theta}\\log p(\\bm{y}^{s})+\\nabla_{\\theta}\\log p(\\bm{g}^{s})),\u2207 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT italic_L = - ( italic_r ( bold_italic_y start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ) - italic_b ) ( \u2207 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_y start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ) + \u2207 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_g start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ) ) ,(13)where \ud835\udc88ssuperscript\ud835\udc88\ud835\udc60\\bm{g}^{s}bold_italic_g start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT is the sequence of role-shift gates.", "Inference. In testing stage, given an image and one \ud835\udcb1\ud835\udcae\u211b\ud835\udcb1\ud835\udcae\u211b\\mathcal{VSR}caligraphic_V caligraphic_S caligraphic_R, we sequentially use the GSRL, SSP, and captioning model to generate the final captions. Meanwhile, our framework can be easily extended from one \ud835\udcb1\ud835\udcae\u211b\ud835\udcb1\ud835\udcae\u211b\\mathcal{VSR}caligraphic_V caligraphic_S caligraphic_R to multiple \ud835\udcb1\ud835\udcae\u211bs\ud835\udcb1\ud835\udcae\u211b\ud835\udc60\\mathcal{VSR}scaligraphic_V caligraphic_S caligraphic_R italic_s as the control signal. Taking an example of two \ud835\udcb1\ud835\udcae\u211bs\ud835\udcb1\ud835\udcae\u211b\ud835\udc60\\mathcal{VSR}scaligraphic_V caligraphic_S caligraphic_R italic_s, we first use GSRL and SSP to obtain semantic structures and grounded regions features: (\ud835\udcaea,\u211ba)superscript\ud835\udcae\ud835\udc4esuperscript\u211b\ud835\udc4e(\\mathcal{S}^{a},\\mathcal{R}^{a})( caligraphic_S start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT , caligraphic_R start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT ) and (\ud835\udcaeb,\u211bb)superscript\ud835\udcae\ud835\udc4fsuperscript\u211b\ud835\udc4f(\\mathcal{S}^{b},\\mathcal{R}^{b})( caligraphic_S start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT , caligraphic_R start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT ). Then, as shown in Figure\u00a04, we merge them by two steps4: (a) find the sub-roles in both \ud835\udcaeasuperscript\ud835\udcae\ud835\udc4e\\mathcal{S}^{a}caligraphic_S start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT and \ud835\udcaebsuperscript\ud835\udcae\ud835\udc4f\\mathcal{S}^{b}caligraphic_S start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT which refer to the same visual regions (\\eg, s1asubscriptsuperscript\ud835\udc60\ud835\udc4e1s^{a}_{1}italic_s start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and s1bsubscriptsuperscript\ud835\udc60\ud835\udc4f1s^{b}_{1}italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT refer to the same proposal set); (b) insert all other sub-roles between the nearest two selected sub-roles (\\eg, s2*subscriptsuperscript\ud835\udc602s^{*}_{2}italic_s start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are still between s1*subscriptsuperscript\ud835\udc601s^{*}_{1}italic_s start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and s3*subscriptsuperscript\ud835\udc603s^{*}_{3}italic_s start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT). Concerning the order of sub-roles from different verbs, we follow the rank of two verbs (\\eg, s2asubscriptsuperscript\ud835\udc60\ud835\udc4e2s^{a}_{2}italic_s start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is in front of s2bsubscriptsuperscript\ud835\udc60\ud835\udc4f2s^{b}_{2}italic_s start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT).", "Flickr30K Entities\u00a0[47]. It builds upon the Flickr30K\u00a0[74] dataset, by manually grounding each noun phrase in the descriptions with one or more visual regions. It consists of 31,000 images, and each image is associated with five captions. We use the same splits as\u00a0[26] in our experiments.", "COCO Entities\u00a0[16]. It builds upon the COCO\u00a0[12] dataset which consists of 120,000 images and each image is annotated with five captions. Different from Flickr30K Entities where all grounding entities are annotated by humans, all annotations in COCO Entities are detected automatically. Especially, they align each entity to all the detected proposals with the same object class.", "Although we only assume that there exists at least one verb (\\ie, activity) in each image; unfortunately, there are still a few samples (\\ie, 3.26% in COCO Entities and 0.04% in Flickr30K Entities) having no verbs in their captions. We use the same split as\u00a0[16] and further drop the those samples with no verb in the training and testing stages4. We will try to cover these extreme cases and leave it for future work.", "Proposal Generation and Grouping. We utilize a Faster R-CNN\u00a0[50] with ResNet-101\u00a0[24] to obtain all proposals for each image. Especially, we use the model released by\u00a0[3], which is finetuned on VG dataset\u00a0[29]. For COCO Entities, since the \u201cground truth\u201d annotations for each noun phrase are the proposals with the same class, we group the proposals by their detected class labels. But for Flickr30K Entities, we directly regard each proposal as a proposal set.", "VSR Annotations. Since there are no ground truth semantic role annotations for CIC datasets, we use a pretrained SRL tool\u00a0[53] to annotate verbs and semantic roles for each caption, and regard them as ground truth annotations. For each detected verb, we convert it into its base form and build a verb dictionary for each dataset. The dictionary sizes for COCO and Flickr30K are 2,662 and 2,926, respectively. There are a total of 24 types of semantic roles for all verbs.", "Experimental Settings.For the S-level SSP, the head number of multi-head attention is set to 8, and the hidden size of the transformer is set to 512. The length of the transformer is set to 10.For the R-level SSP, we set the maximum number of entities for each role to 10.For the RL training of the captioning model, we use CIDEr-D\u00a0[61] score as the training reward. Due to the limited space, we leave more detailed parameter settings in the supplementary material.", "Settings. To evaluate the controllability of proposed framework, we followed the conventions of prior CIC works\u00a0[16, 10, 78], and utilized the VSR aligned with ground truth captions as the control signals. Specifically, we compared the proposed framework with several carefully designed baselines666All baselines use the same visual regions as models with VSRs.: 1) C-LSTM: It is a Controllable LSTM model\u00a0[63]. Given the features of all grounded visual regions, it first averages all region features, and then uses an LSTM to generate the captions. 2) C-UpDn: It is a Controllable UpDn model\u00a0[3], which uses an adaptive attention to generate the captions. 3) SCT\u00a0[16]: It regards the set of visual regions as a control signal, and utilizes a chunk-shift captioning model to generate the captions. 4) Ours w/o verb: We ablate our model by removing the verb information in both the SSP and captioning model. 5) Ours (oracle verb): It is an ideal situation, where the captioning model directly outputs the oracle format of the verb when the attending role is the verb.", "Evaluation Metrics. To evaluate the quality of the generated captions, we use five accuracy-based metrics, including BLEU-4 (B4)\u00a0[45], METEOR (M)\u00a0[5], ROUGE (R)\u00a0[34], CIDEr-D (C)\u00a0[61], and SPICE (S)\u00a0[2]. Particularly, we evaluate the generated captions against the single ground truth caption. We also propose a new recall-based metric to evaluate whether the roles of the generated sentence are consistent with the ground truth caption (\\ie, VSR). It measures the recall rate of the verb, semantic roles, and ordered role pairs, which are denoted as RVV{}_{\\text{V}}start_FLOATSUBSCRIPT V end_FLOATSUBSCRIPT, RSR1SR1{}_{\\text{SR1}}start_FLOATSUBSCRIPT SR1 end_FLOATSUBSCRIPT and RSR2SR2{}_{\\text{SR2}}start_FLOATSUBSCRIPT SR2 end_FLOATSUBSCRIPT, respectively.", "Quantitative Results. The quantitative results are reported in Table\u00a01. From Table\u00a01, we can observe that our framework can achieve the best performance over almost all metrics and benchmarks. By comparing the two different proposal settings (\\ie, GSRL and GT), we can find that the accuracy of GSRL is a major bottleneck of the whole framework. Meanwhile, the ablative model (Ours w/o verb) can only achieve slightly better performance than baseline SCT and much worse performance than our full model, which reflects the importance of the verb in semantic structure learning and caption generation.", "Visualizations. In Figure\u00a06, we illustrate some examples of the generated captions. We can observe that our framework always learns a human-like semantic structure based on the VSR and grounded visual regions (\\eg, Arg1thingthing{}_{\\text{thing}}start_FLOATSUBSCRIPT thing end_FLOATSUBSCRIPT \u2013 sit \u2013 Arg2positionposition{}_{\\text{position}}start_FLOATSUBSCRIPT position end_FLOATSUBSCRIPT \u2013 LOC \u2013 MNR). According to the semantic structures, the captioning model can generate near-perfect descriptions. As a by-product, a well-trained SSP can automatically produce several verb-specific semantic structures for a set of user-interested roles, and we show some examples in Figure\u00a06. For each verb and role set, we illustrate the top two structures by using beam search. Particularly, we are surprised to find that we can even learn some structures that never appear in original datasets (the blue tick ones).", "One of the well-known advantages of controllable image captioning is the ability to generate diverse image captions by feeding different control signals. Thus, we also evaluate the diversity of the captions generated by our framework.", "Settings. We evaluated the quality of diverse captions in two settings: 1) Given a VSR and grounded visual regions of each role aligned with the ground truth caption, we first use an SSP to select two semantic structures, and then respectively generate two diverse captions. For fair comparisons, we utilize the same set of visual regions on two strong baselines: a) BS: an UpDn model uses beam search to produce two captions, and b) SCT: an SCT model takes a permutation of all region sets to generate two captions. 2) For each verb, we can randomly sample a subset of all semantic roles to construct new VSRs. Specifically, we sample two more sets of semantic roles, and generate two diverse captions for each role set following the same manner.", "Evaluation Metrics. We used two types of metrics to evaluate the diverse captions: 1) Accuracy-based: we followed the conventions of the previous works\u00a0[16, 20, 65] and reported the best-1 accuracy, \\ie, the generated caption with the maximum score for each metric is chosen. Analogously, we evaluate the generated captions against the single ground truth caption. 2) Diversity-based: we followed\u00a0[10] and used two metrics which only focus on the language similarity: Div-n (D-n)\u00a0[4, 20] and self-CIDEr (s-C)\u00a0[66].", "Quantitative Results. The quantitative results are reported in Table\u00a02. From Table\u00a02, we can observe that the diverse captions generated by our framework in both two settings have much higher accuracy (\\eg, CIDEr 267.3 vs. 222.5 in SCT), and that the diversity is slightly behind SCT (\\eg, self-CIDEr 67.0 vs. 69.1 in SCT). This is because SCT generates captions by randomly shuffling regions. Instead, we tend to learn more reasonable structures. Thus, we can achieve much higher results on accuracy, \\ie, our method can achieve a better trade-off between quality and diversity on diverse image captioning than the two strong baselines.", "Visualizations. We further illustrate the generated captions of two images with different VSRs in Figure\u00a07. The captions are generated effectively according to the given VSR, and the diversity of VSR leads to significant diverse captions.", "In this paper, we argued that all existing objective control signals for CIC have overlooked two indispensable characteristics: event-compatible and sample-suitable. To this end, we proposed a novel control signal called VSR. VSR consists of a verb and several semantic roles, \\ie, all components are guaranteed to be event-compatible. Meanwhile, VSR only restricts the involved semantic roles, which is also sample-suitable for all the images containing the activity. We have validated the effectiveness of VSR through extensive experiments. Moving forward, we will plan to 1) design a more effective captioning model to benefit more from the VSR signals; 2) extend VSR to other controllable text generation tasks, \\eg, video captioning\u00a0[69]; 3) design a more general framework to cover the images without verbs.", "Acknowledgements.This work was supported by the National Natural Science Foundation of China (U19B2043,61976185), Zhejiang Natural Science Foundation (LR19F020002), Zhejiang Innovation Foundation (2019R52002), and Fundamental Research Funds for Central Universities."], "figure_types": {"ada35e2c099fbde9d07a279311f4abe698341cd8/12-Table3-1.png": "table", "ada35e2c099fbde9d07a279311f4abe698341cd8/13-Figure8-1.png": "photograph(s)", "ada35e2c099fbde9d07a279311f4abe698341cd8/13-Figure9-1.png": "photograph(s)", "ada35e2c099fbde9d07a279311f4abe698341cd8/15-Table4-1.png": "table", "ada35e2c099fbde9d07a279311f4abe698341cd8/2-Figure2-1.png": "photograph(s)", "ada35e2c099fbde9d07a279311f4abe698341cd8/4-Figure3-1.png": "schematic", "ada35e2c099fbde9d07a279311f4abe698341cd8/6-Figure4-1.png": "schematic", "ada35e2c099fbde9d07a279311f4abe698341cd8/7-Figure5-1.png": "photograph(s)", "ada35e2c099fbde9d07a279311f4abe698341cd8/7-Figure6-1.png": "table", "ada35e2c099fbde9d07a279311f4abe698341cd8/7-Table1-1.png": "table", "ada35e2c099fbde9d07a279311f4abe698341cd8/8-Figure7-1.png": "photograph(s)", "ada35e2c099fbde9d07a279311f4abe698341cd8/8-Table2-1.png": "table"}}, "1609.08144": {"paper_id": "paper_127", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "arxiv_url": "https://arxiv.org/abs/1609.08144", "s2orc_url": "https://www.semanticscholar.org/paper/c6850869aa5e78a107c378d2e8bfa39633158c0c", "all_figures_tables": {"c6850869aa5e78a107c378d2e8bfa39633158c0c/11-Figure4-1.png": "Figure 4: Log perplexity vs. steps for normal (non-quantized) training and quantization-aware training on WMT\u201914 English to French during maximum likelihood training. Notice the training losses are similar, with the quantization-aware loss being slightly better. Our conjecture for quantization-aware training being slightly better is that the clipping constraints act as additional regularization which improves the model quality.", "c6850869aa5e78a107c378d2e8bfa39633158c0c/13-Table2-1.png": "Table 2: WMT\u201914 En\u2192Fr BLEU score with respect to different values of \u03b1 and \u03b2. The model in this experiment trained using ML without RL refinement. A single WMT En\u2192Fr model achieves a BLEU score of 30.3 on the development set when the beam search scoring function is purely based on the sequence probability (i.e., both \u03b1 and \u03b2 are 0). Slightly larger \u03b1 and \u03b2 values improve BLEU score by up to +1.1 (\u03b1 = 0.2, \u03b2 = 0.2), with a wide range of \u03b1 and \u03b2 values giving results very close to the best BLEU scores.", "c6850869aa5e78a107c378d2e8bfa39633158c0c/13-Table3-1.png": "Table 3: WMT En\u2192Fr BLEU score with respect to different values of \u03b1 and \u03b2. The model used here is trained using ML, then refined with RL. Compared to the results in Table 2, coverage penalty and length normalization appear to be less effective for models after RL-based model refinements. Results are obtained on the development set.", "c6850869aa5e78a107c378d2e8bfa39633158c0c/15-Figure5-1.png": "Figure 5: Log perplexity vs. steps for Adam, SGD and Adam-then-SGD on WMT En\u2192Fr during maximum likelihood training. Adam converges much faster than SGD at the beginning. Towards the end, however, Adam-then-SGD is gradually better. Notice the bump in the red curve (Adam-then-SGD) at around 60k steps where we switch from Adam to SGD. We suspect that this bump occurs due to different optimization trajectories of Adam vs. SGD. When we switch from Adam to SGD, the model first suffers a little, but is able to quickly recover afterwards.", "c6850869aa5e78a107c378d2e8bfa39633158c0c/16-Table4-1.png": "Table 4: Single model results on WMT En\u2192Fr (newstest2014)", "c6850869aa5e78a107c378d2e8bfa39633158c0c/16-Table5-1.png": "Table 5: Single model results on WMT En\u2192De (newstest2014)", "c6850869aa5e78a107c378d2e8bfa39633158c0c/17-Table7-1.png": "Table 7: Model ensemble results on WMT En\u2192Fr (newstest2014)", "c6850869aa5e78a107c378d2e8bfa39633158c0c/17-Table8-1.png": "Table 8: Model ensemble results on WMT En\u2192De (newstest2014). See Table 5 for a comparison against non-ensemble models.", "c6850869aa5e78a107c378d2e8bfa39633158c0c/18-Table10-1.png": "Table 10: Mean of side-by-side scores on production data", "c6850869aa5e78a107c378d2e8bfa39633158c0c/18-Table9-1.png": "Table 9: Human side-by-side evaluation scores of WMT En\u2192Fr models.", "c6850869aa5e78a107c378d2e8bfa39633158c0c/19-Figure6-1.png": "Figure 6: Histogram of side-by-side scores on 500 sampled sentences from Wikipedia and news websites for a typical language pair, here English \u2192 Spanish (PBMT blue, GNMT red, Human orange). It can be seen that there is a wide distribution in scores, even for the human translation when rated by other humans, which shows how ambiguous the task is. It is clear that GNMT is much more accurate than PBMT.", "c6850869aa5e78a107c378d2e8bfa39633158c0c/23-Table11-1.png": "Table 11: Some example translations from PBMT [15], our GNMT system (the \"NMT before RL\", Table 9), and Human. Source and target sentences (human translations) are from the public benchmark WMT En\u2192Fr (newstest2014) data set. The right-hand column shows the human ratings on a scale of 0 (complete nonsense) to 6 (perfect translation). We disagree with some of the human ratings, e.g., the translation \u201cElle a \u00e9t\u00e9 rep\u00e9r\u00e9 trois jours plus tard par un promeneur de chien pi\u00e9g\u00e9 dans la carri\u00e8re\u201d contains grammatical mistakes and changes semantics, and is still scored 6. We present it to illustrate the potential problems of the scoring process.", "c6850869aa5e78a107c378d2e8bfa39633158c0c/4-Figure1-1.png": "Figure 1: The model architecture of GNMT, Google\u2019s Neural Machine Translation system. On the left is the encoder network, on the right is the decoder network, in the middle is the attention module. The bottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green nodes gather information from right to left. The other layers of the encoder are uni-directional. Residual connections start from the layer third from the bottom in the encoder and decoder. The model is partitioned into multiple GPUs to speed up training. In our setup, we have 8 encoder LSTM layers (1 bi-directional layer and 7 uni-directional layers), and 8 decoder layers. With this setting, one model replica is partitioned 8-ways and is placed on 8 different GPUs typically belonging to one host machine. During training, the bottom bi-directional encoder layers compute in parallel first. Once both finish, the uni-directional encoder layers can start computing, each on a separate GPU. To retain as much parallelism as possible during running the decoder layers, we use the bottom decoder layer output only for obtaining recurrent attention context, which is sent directly to all the remaining decoder layers. The softmax layer is also partitioned and placed on multiple GPUs. Depending on the output vocabulary size we either have them run on the same GPUs as the encoder and decoder networks, or have them run on a separate set of dedicated GPUs.", "c6850869aa5e78a107c378d2e8bfa39633158c0c/5-Figure2-1.png": "Figure 2: The difference between normal stacked LSTM and our stacked LSTM with residual connections. On the left: simple stacked LSTM layers [39]. On the right: our implementation of stacked LSTM layers with residual connections. With residual connections, input to the bottom LSTM layer (x0i \u2019s to LSTM1) is element-wise added to the output from the bottom layer (x1i \u2019s). This sum is then fed to the top LSTM layer (LSTM2) as the new input.", "c6850869aa5e78a107c378d2e8bfa39633158c0c/6-Figure3-1.png": "Figure 3: The structure of bi-directional connections in the first layer of the encoder. LSTM layer LSTMf processes information from left to right, while LSTM layer LSTMb processes information from right to left. Output from LSTMf and LSTMb are first concatenated and then fed to the next LSTM layer LSTM1."}, "referred_figures_tables": [["c6850869aa5e78a107c378d2e8bfa39633158c0c/13-Table3-1.png"], ["c6850869aa5e78a107c378d2e8bfa39633158c0c/4-Figure1-1.png"], ["c6850869aa5e78a107c378d2e8bfa39633158c0c/13-Table2-1.png", "c6850869aa5e78a107c378d2e8bfa39633158c0c/13-Table3-1.png", "c6850869aa5e78a107c378d2e8bfa39633158c0c/13-Table2-1.png", "c6850869aa5e78a107c378d2e8bfa39633158c0c/13-Table3-1.png"], ["c6850869aa5e78a107c378d2e8bfa39633158c0c/11-Figure4-1.png"], ["c6850869aa5e78a107c378d2e8bfa39633158c0c/4-Figure1-1.png"], ["c6850869aa5e78a107c378d2e8bfa39633158c0c/11-Figure4-1.png"], ["c6850869aa5e78a107c378d2e8bfa39633158c0c/11-Figure4-1.png"]], "question_id": [16, 2, 17, 7, 9, 14, 15], "question": ["Do \u03b1 control the strength of the length normalization and \u03b2 control the strength of the coverage penalty each other?", "How can the attention mechanism connecting the bottom layer of the decoder to the top layer of the encoder contribute to improving parallelism?", "In terms of the effectivenesses of coverage penalty and length normalization, how does having RL-based model refinement differ from not having RL-based model refinement?", "Is there a disadvantage to using low-precision arithmetic for inference, such as decreased inference accuracy?", "Is it true that they used the output from the bottom decoder layer for y_{i-1}, not the decoder-RNN output from the past decoding time step?", "Is the \\delta a hyper-parameter?", "Why are the constraint value of \u03b4 and \u03b3 separated?"], "question_section": ["7. Decoder", "Abstract", "7. Decoder", "Introduction", "Model Architecture", "6. Quantizable Model and Quantized Inference", "6. Quantizable Model and Quantized Inference"], "question_trigger_sentence": ["Parameters \u03b1 and \u03b2 control the strength of the length normalization\nand the coverage penalty. ", "To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder.", "Compared to the results in Table 2, coverage penalty and length normalization appear to be less effective for models after RL-based model refinements. ", "To improve inference time, we employ low-precision arithmetic for inference, which is further accelerated by special hardware (Google\u2019s Tensor Processing Unit, or TPU). ", "Our attention module is similar to [2]. More specifically, let y_{i-1} be the decoder-RNN output from the past decoding time step (in our implementation, we use the output from the bottom decoder layer).", "For quantized inference, we explicitly constrain the values of these accumulators to be within [-\\delta, \\delta] to guarantee a certain range that can be used for quantization later.", "The only constraints we add to the model during training are the clipping of the RNN accumulator values into [\u2212\u03b4, \u03b4] and softmax logits into [\u2212\u03b3, \u03b3]."], "question_type": ["Shallow question", "Deep/complex question", "Testing question", "Shallow question", "Shallow question", "Shallow question", "Deep/complex question"], "evidential_info": [[{"context": "We use beam search during decoding to find the sequence Ythat maximizes a score function s(Y,X) given a trained model. Weintroduce two important refinements to the pure max-probability based beamsearch algorithm: a coverage penalty\u00a0[42] and lengthnormalization. With length normalization, we aim to account for thefact that we have to compare hypotheses of different length. Withoutsome form of length-normalization regular beam search will favorshorter results over longer ones on average since a negativelog-probability is added at each step, yielding lower (more negative) scores forlonger sentences. We first tried to simply divideby the length to normalize. We then improved on that original heuristic by dividing bylength^{\\alpha}, with 0<\\alpha<1 where \\alpha is optimized ona development set (\\alpha\\in[0.6-0.7] was usually found to bebest). Eventually we designed the empirically-better scoring functionbelow, which also includes a coverage penalty to favor translationsthat fully cover the source sentence according to the attentionmodule.", "rationale": "Authors find that \"\u03b1\" which represents length normalization and \"\u03b2\" which represents coverage penalty are less effective for models with RLrefinment."}, {"context": "\\begin{split}s(Y,X)&=\\log(P(Y|X))/lp(Y)+cp(X;Y)\\\\lp(Y)&=\\frac{(5+|Y|)^{\\alpha}}{(5+1)^{\\alpha}}\\\\cp(X;Y)&=\\beta*\\sum_{i=1}^{|X|}{\\log(\\min(\\sum_{j=1}^{|Y|}{p_{i,j}},1.0))},\\end{split}(14)where p_{i,j} is the attention probability of the j-th target wordy_{j} on the i-th source word x_{i}. By construction(equation 4), \\sum_{i=0}^{|X|}{p_{i,j}} is equalto 1. Parameters \\alpha and \\beta control the strength ofthe length normalization and the coverage penalty. When \\alpha=0 and\\beta=0, our decoder falls back to pure beam search by probability.", "rationale": "Authors improved the original heuristic  by dividing length to the power of \u03b1 with 0 < \u03b1 < 1 where \u03b1 \u2208 [0.6 \u2212 0.7] on development set which usually found to be best."}, {"context": "We find that length normalization (\\alpha) and coverage penalty(\\beta) are less effective for models with RLrefinement. Table 3 summarizes ourresults. This is understandable, as during RL refinement, the modelsalready learn to pay attention to the full source sentence to notunder-translate or over-translate, which would result in a penalty on theBLEU (or GLEU) scores.", "rationale": "The strength of length normalization and coverage penalty are controlled by parameters \u03b1 and \u03b2."}], [{"context": "Model parallelism places certain constraints on the modelarchitectures we can use. For example, we cannot afford to havebi-directional LSTM layers for all the encoder layers, since doing sowould reduce parallelism among subsequent layers, as each layer wouldhave to wait until both forward and backward directions of the previouslayer have finished. This would effectively constrain us to make use ofonly 2 GPUs in parallel (one for the forward direction and one for thebackward direction). For the attention portion of the model, we chose to align thebottom decoder output to the top encoder output to maximizeparallelism when running the decoder network. Had we aligned the top decoderlayer to the top encoder layer, we would have removed all parallelismin the decoder network and would not benefit from using more than oneGPU for decoding.", "rationale": "LSTM layers reduces parallelism as each layer would have to wait until both forward and backward directions of the previous layer to finish."}, {"context": "Figure 1: The model architecture of GNMT, Google\u2019s Neural Machine Translation system. On the left is the encoder network, on the right is the decoder network, in the middle is the attention module. The bottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green nodes gather information from right to left. The other layers of the encoder are uni-directional. Residual connections start from the layer third from the bottom in the encoder and decoder. The model is partitioned into multiple GPUs to speed up training. In our setup, we have 8 encoder LSTM layers (1 bi-directional layer and 7 uni-directional layers), and 8 decoder layers. With this setting, one model replica is partitioned 8-ways and is placed on 8 different GPUs typically belonging to one host machine. During training, the bottom bi-directional encoder layers compute in parallel first. Once both finish, the uni-directional encoder layers can start computing, each on a separate GPU. To retain as much parallelism as possible during running the decoder layers, we use the bottom decoder layer output only for obtaining recurrent attention context, which is sent directly to all the remaining decoder layers. The softmax layer is also partitioned and placed on multiple GPUs. Depending on the output vocabulary size we either have them run on the same GPUs as the encoder and decoder networks, or have them run on a separate set of dedicated GPUs.", "rationale": "As shown in Figure 1, the setup consists of 8 Encoder LSTM layers (1 bi-directional layer and 7 uni-directional layers) and 8 decoders layers. During training the bottom bi directional encoder layers compute in parallel first. Once finished, the uni-directional encoder layers can start computing, and to retain as much possible parallelism during running the decoder layers, the bottom layers of the decoder ouput only for obtaining recurrent attention context which is sent directly to all the remaining decoder layers."}], [{"context": "Table 2 shows the impact of \\alpha and \\beta onthe BLEU score when decoding the WMT\u201914 English-to-French development set.The model used here for experiments is trained using the ML objectiveonly (without RL refinement). As can be seen from the results, havingsome length normalization and coverage penalty improves BLEU scoreconsiderably (from 30.3 to 31.4).", "rationale": "Models with RL refinement are less affected by the strength of length normalization \"\u03b1\" and coverage penalty \"\u03b2\", and this is due to the fact that during RL refinement models already learn to pay attention to the full source sentence to not under-translate or over-translate."}, {"context": "We find that length normalization (\\alpha) and coverage penalty(\\beta) are less effective for models with RLrefinement. Table 3 summarizes ourresults. This is understandable, as during RL refinement, the modelsalready learn to pay attention to the full source sentence to notunder-translate or over-translate, which would result in a penalty on theBLEU (or GLEU) scores.", "rationale": "During the evaluation of RL-refined models, authors found an overlap between the wins from RL refinement and the decoder fine-tuning (i.e., introducing length normalization and coverage penalty). And on a less fine-tuned decoder, the win from RL would have been bigger, (comparing results between the RL-based and none-RL based can be found in Tables 2 and 3)."}, {"context": "The results of RL fine-tuning on the best En\\rightarrowFr andEn\\rightarrowDe models are presented inTable\u00a06, which show that fine-tuning themodels with RL can improve BLEU scores. On WMT En\\rightarrowFr,model refinement improves BLEU score by close to 1 point. On En\\rightarrowDe,RL-refinement slightly hurts the test performance even though we observe about 0.4 BLEU pointsimprovement on the development set. The results presented inTable\u00a06 are the average of 8 independent models.We also note that there is an overlap between the wins from the RL refinement and the decoderfine-tuning (i.e., the introduction of length normalization and coverage penalty).On a less fine-tuned decoder (e.g., if the decoder does beam search bylog-probability only), the win from RL would have been bigger (as is evidentfrom comparing results in Table\u00a02 andTable\u00a03).", "rationale": "Table 2 shows the results of the impact of \"\u03b1\" and \"\u03b2\" on the BLEU score without RL refinement."}], [{"context": "In this section, we present our approach to speed up inference withquantized arithmetic. Our solution is tailored towards the hardwareoptions available at Google. To reduce quantization errors, additionalconstraints are added to our model during training so that it is quantizablewith minimal impact on the output of the model. That is, once amodel is trained with these additional constraints, it can be subsequentlyquantized without loss to translation quality. Our experimental results suggestthat those additional constraints do not hurt model convergence nor the qualityof a model once it has converged.", "rationale": "Table 1 shows using reduced precision arithmetics has no loss on BLEU at all. and a very minimal loss of 0.0072 on log perplexity for TPU. which matches previous work reporting that quantizing convolution neural network models can retain most of the model quality."}, {"context": "It is worth emphasizing that during training of the model we use full-precisionfloating point numbers. The only constraints we add to the modelduring training are the clipping of the RNN accumulator values into[-\\delta,\\delta] and softmax logits into[-\\gamma,\\gamma]. \\gamma is fixed to be at 25.0, while thevalue for \\delta is gradually annealed from a generous bound of\\delta=8.0 at the beginning of training, to a rather stringent boundof \\delta=1.0 towards the end of training. At inference time,\\delta is fixed at 1.0. Those additional constraints do not degrademodel convergence nor the decoding quality of the model when it hasconverged. In Figure 4, we compare the lossvs. steps for an unconstrained model (the blue curve) and a constrainedmodel (the red curve) on WMT\u201914 English-to-French. We can see thatthe loss for the constrained model is slightly better, possibly due toregularization roles those constraints play.", "rationale": "Authors perform quantized arithmetic in order to speed up inference, and to reduce quantization error, they add additional constraints during training so that it's quantizable with minimal impact on the output of the model."}, {"context": "Our solution strikes a good balance between efficiency andaccuracy. Since the computationally expensive operations (the matrixmultiplications) are done using 8-bit integer operations, ourquantized inference is quite efficient. Also, since error-sensitiveaccumulator values are stored using 16-bit integers, our solution isvery accurate and is robust to quantization errors.", "rationale": "author's solution got a good result balancing between efficiency and accuracy. expensive operations such as matrix multiplications are done using 8-bit integer operations, and since error-sensitive accumulator values are stored using 16-bit integers, their solution is very accurate and robust to quantization errors."}, {"context": "Table\u00a01 shows that decoding using reducedprecision arithmetics on the TPU suffers a very minimal loss of 0.0072 onlog perplexity, and no loss on BLEU at all. This result matchesprevious work reporting that quantizing convolutional neuralnetwork models can retain most of the model quality.", "rationale": "Authors' training method to reduce quantization error was that they used full-precision floating point numbers, and the only constraints they add to the model during training were the clipping of the RNN accumulator values into [\u2212\u03b4, \u03b4] and softmax logits into [\u2212\u03b3, \u03b3]. \u03b3 is fixed to be at 25.0, while the value for \u03b4 is gradually annealed from a generous bound of \u03b4 = 8.0 at the beginning of training, to a rather stringent bound of \u03b4 = 1.0 towards the end of training. At inference time, \u03b4 is fixed at 1.0."}], [{"context": "Our attention module is similar to\u00a0[2]. Morespecifically, let \\mathbf{y}_{i-1} be the decoder-RNN output fromthe past decoding time step (in our implementation, we use the output fromthe bottom decoder layer). Attention context \\mathbf{a}_{i}for the current time step is computed according to the following formulas:st=A\u2062t\u2062t\u2062e\u2062n\u2062t\u2062i\u2062o\u2062n\u2062F\u2062u\u2062n\u2062c\u2062t\u2062i\u2062o\u2062n\u2062(\ud835\udc32i\u22121,\ud835\udc31t)\u2200t,1\u2264t\u2264Mpt=exp\u2061(st)/\u2211t=1Mexp\u2061(st)\u2200t,1\u2264t\u2264M\ud835\udc1ai=\u2211t=1Mpt.\ud835\udc31t\\begin{split}s_{t}&=AttentionFunction(\\mathbf{y}_{i-1},\\mathbf{x}_{t})\\quad\\forall t,\\quad 1\\leq t\\leq M\\\\p_{t}&=\\exp(s_{t})/\\sum_{t=1}^{M}\\exp(s_{t})\\quad\\quad\\forall t,\\quad 1\\leq t\\leq M\\\\\\mathbf{a}_{i}&=\\sum_{t=1}^{M}p_{t}.\\mathbf{x}_{t}\\end{split}start_ROW start_CELL italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = italic_A italic_t italic_t italic_e italic_n italic_t italic_i italic_o italic_n italic_F italic_u italic_n italic_c italic_t italic_i italic_o italic_n ( bold_y start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2200 italic_t , 1 \u2264 italic_t \u2264 italic_M end_CELL end_ROW start_ROW start_CELL italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = roman_exp ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) / \u2211 start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT roman_exp ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2200 italic_t , 1 \u2264 italic_t \u2264 italic_M end_CELL end_ROW start_ROW start_CELL bold_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL = \u2211 start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW(4)where AttentionFunction in our implementation is a feed forward network withone hidden layer.", "rationale": "Authors used same model architecture used in referenced [2] but had a slight change, they only used the decoder-RNN output from the past decoding time step in the bottom decoder layer only."}, {"context": "Figure 1: The model architecture of GNMT, Google\u2019s Neural Machine Translation system. On the left is the encoder network, on the right is the decoder network, in the middle is the attention module. The bottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green nodes gather information from right to left. The other layers of the encoder are uni-directional. Residual connections start from the layer third from the bottom in the encoder and decoder. The model is partitioned into multiple GPUs to speed up training. In our setup, we have 8 encoder LSTM layers (1 bi-directional layer and 7 uni-directional layers), and 8 decoder layers. With this setting, one model replica is partitioned 8-ways and is placed on 8 different GPUs typically belonging to one host machine. During training, the bottom bi-directional encoder layers compute in parallel first. Once both finish, the uni-directional encoder layers can start computing, each on a separate GPU. To retain as much parallelism as possible during running the decoder layers, we use the bottom decoder layer output only for obtaining recurrent attention context, which is sent directly to all the remaining decoder layers. The softmax layer is also partitioned and placed on multiple GPUs. Depending on the output vocabulary size we either have them run on the same GPUs as the encoder and decoder networks, or have them run on a separate set of dedicated GPUs.", "rationale": "In order for authors to retain parallelism as possible, they used the bottom decoder layer output only for obtaining recurrent attention context, which is sent directly to all the remaining decoder layers."}], [{"context": "Recall from equation 6 that in an LSTM stackwith residual connections there are two accumulators: \\mathbf{c}_{t}^{i}along the time axis and \\mathbf{x}_{t}^{i} along the depth axis. Intheory, both of the accumulators are unbounded, but in practice, wenoticed their values remain quite small. For quantized inference, weexplicitly constrain the values of these accumulators to be within[-\\delta, \\delta] to guarantee a certain range that can be used forquantization later. The forward computation of an LSTM stack withresidual connections is modified to the following:", "rationale": "Authors' training method to reduce quantization error was that they used full-precision floating point numbers, and the only constraints they add to the model during training were the clipping of the RNN accumulator values into [\u2212\u03b4, \u03b4] and softmax logits into [\u2212\u03b3, \u03b3]. \u03b3 is fixed to be at 25.0, while the value for \u03b4 is gradually annealed from a generous bound of \u03b4 = 8.0 at the beginning of training, to a rather stringent bound of \u03b4 = 1.0 towards the end of training. At inference time, \u03b4 is fixed at 1.0."}, {"context": "\\begin{split}\\mathbf{v_{t}}&=\\mathbf{W_{s}}*\\mathbf{y_{t}}\\\\\\mathbf{v_{t}^{\\prime}}&=\\max(-\\gamma,\\min(\\gamma,\\mathbf{v_{t}}))\\\\\\mathbf{p_{t}}&=softmax(\\mathbf{v_{t}^{\\prime}})\\end{split}(13)In equation 13, \\mathbf{W_{s}} is the weightmatrix for the linear layer, which has the same number of rows as thenumber of symbols in the target vocabulary with each row correspondingto one unique target symbol. \\mathbf{v} represents the raw logits, which arefirst clipped to be between -\\gamma and \\gamma and then normalizedinto a probability vector \\mathbf{p}. Input \\mathbf{y_{t}} isguaranteed to be between -\\delta and \\delta due to thequantization scheme we applied to the decoder RNN. The clipping range\\gamma for the logits \\mathbf{v} is determined empirically, and inour case, it is set to 25. In quantized inference, the weight matrix\\mathbf{W_{s}} is quantized into 8 bits as inequation 12, and the matrix multiplication is done using8 bit arithmetic. The calculations within the softmax function and theattention model are not quantized during inference.", "rationale": "For quntized inference, authors constrain the values of accumulators to be within [\u2212\u03b4, \u03b4] to guarantee a certain range that can be used for quantization later."}, {"context": "It is worth emphasizing that during training of the model we use full-precisionfloating point numbers. The only constraints we add to the modelduring training are the clipping of the RNN accumulator values into[-\\delta,\\delta] and softmax logits into[-\\gamma,\\gamma]. \\gamma is fixed to be at 25.0, while thevalue for \\delta is gradually annealed from a generous bound of\\delta=8.0 at the beginning of training, to a rather stringent boundof \\delta=1.0 towards the end of training. At inference time,\\delta is fixed at 1.0. Those additional constraints do not degrademodel convergence nor the decoding quality of the model when it hasconverged. In Figure 4, we compare the lossvs. steps for an unconstrained model (the blue curve) and a constrainedmodel (the red curve) on WMT\u201914 English-to-French. We can see thatthe loss for the constrained model is slightly better, possibly due toregularization roles those constraints play.", "rationale": "Authors explains their quantized training method, specifying Input yt to be between \u2212\u03b4 and \u03b4."}], [{"context": "\\begin{split}\\mathbf{v_{t}}&=\\mathbf{W_{s}}*\\mathbf{y_{t}}\\\\\\mathbf{v_{t}^{\\prime}}&=\\max(-\\gamma,\\min(\\gamma,\\mathbf{v_{t}}))\\\\\\mathbf{p_{t}}&=softmax(\\mathbf{v_{t}^{\\prime}})\\end{split}(13)In equation 13, \\mathbf{W_{s}} is the weightmatrix for the linear layer, which has the same number of rows as thenumber of symbols in the target vocabulary with each row correspondingto one unique target symbol. \\mathbf{v} represents the raw logits, which arefirst clipped to be between -\\gamma and \\gamma and then normalizedinto a probability vector \\mathbf{p}. Input \\mathbf{y_{t}} isguaranteed to be between -\\delta and \\delta due to thequantization scheme we applied to the decoder RNN. The clipping range\\gamma for the logits \\mathbf{v} is determined empirically, and inour case, it is set to 25. In quantized inference, the weight matrix\\mathbf{W_{s}} is quantized into 8 bits as inequation 12, and the matrix multiplication is done using8 bit arithmetic. The calculations within the softmax function and theattention model are not quantized during inference.", "rationale": "Authors' training method to reduce quantization error was that they used full-precision floating point numbers, and the only constraints they add to the model during training were the clipping of the RNN accumulator values into [\u2212\u03b4, \u03b4] and softmax logits into [\u2212\u03b3, \u03b3]. \u03b3 is fixed to be at 25.0, while the value for \u03b4 is gradually annealed from a generous bound of \u03b4 = 8.0 at the beginning of training, to a rather stringent bound of \u03b4 = 1.0 towards the end of training. At inference time, \u03b4 is fixed at 1.0."}, {"context": "It is worth emphasizing that during training of the model we use full-precisionfloating point numbers. The only constraints we add to the modelduring training are the clipping of the RNN accumulator values into[-\\delta,\\delta] and softmax logits into[-\\gamma,\\gamma]. \\gamma is fixed to be at 25.0, while thevalue for \\delta is gradually annealed from a generous bound of\\delta=8.0 at the beginning of training, to a rather stringent boundof \\delta=1.0 towards the end of training. At inference time,\\delta is fixed at 1.0. Those additional constraints do not degrademodel convergence nor the decoding quality of the model when it hasconverged. In Figure 4, we compare the lossvs. steps for an unconstrained model (the blue curve) and a constrainedmodel (the red curve) on WMT\u201914 English-to-French. We can see thatthe loss for the constrained model is slightly better, possibly due toregularization roles those constraints play.", "rationale": "v represents the raw logits, which is clipped to be between \u2212\u03b3 and \u03b3, then normalized to a probability vector p. And input yt is between \u2212\u03b4 and \u03b4."}]], "composition": ["Yes, Authors found that \"\u03b1\" which represents the strength of length normalization and \"\u03b2\" which represents coverage penalty are less effective for models with RLrefinment, and improved the original heuristic  by dividing length to the power of \u03b1 with 0 < \u03b1 < 1 where \u03b1 \u2208 [0.6 \u2212 0.7] on development set which usually found to be best.", "First we have to establish that LSTM layers reduces parallelism as each layer would have to wait until both forward and backward directions of the previous layer to finish. Then notice in Figure 1, the model architecture consists of 8 LSTM encoder layers (1 bi-directional and 7 uni-directional layers), and 8 decoder layers. During training the bottom bi-directional encoder layers compute in parallelism first, then the uni-directional encoder layers. So to retain retain and much possible parallelism during the decoder layers, the bottom layers of the decoder output only for obtaining the recurrent attention context which is sent directly to all the remaining decoder layers.", "It was found that models with RL refinement are less affected by length normalization \"\u03b1\" and coverage penalty \"\u03b2\", authors explain this to the fact that during RL refinement, models already learn to pay attention to the full source sentence to not under-translate or over-translate. The authors also found an overlap between the wins from RL refinement and decoder fine-tuning, and the win from RL on a less fine-tuned decoder would have been bigger. The impact of length normalization \"\u03b1\" and coverage penalty \"\u03b2\" on RL-based and non-RL-based models can be found in Tables 2 and 3.", "Quantization models can perform slightly have lower results on neural network models, however in this paper authors performed some constraints during training so that's quantizable with minimal impact on the output of the model, the quantized model even performed slightly better than none-quantized training and they suggest it could be due to regularization roles those constraints had during training.", "Authors used only the decoder-RNN output from the past decoding time step in the bottom decoder layer to obtain recurrent attention context which is sent directly to all the remaining decoder layers.", "Yes it's hyper-parameter, as it's within fixed range during training noting that it's fixed within this range during inference.", "Yes they are separated, as [-\u03b4,\u03b4] is a clipping range to input yt while [\u2212\u03b3, \u03b3] is the clipping range for raw logits."], "Is_figure_in_evidence": [false, true, false, true, true, true, true], "Is_table_in_evidence": [true, false, true, false, false, false, false], "question_key": ["282", "283", "284", "290", "291", "296", "297"], "passages": ["Neural Machine Translation(NMT)\u00a0[41, 2] has recently beenintroduced as a promising approach with the potential of addressingmany shortcomings of traditional machine translation systems.The strength of NMT lies in its ability to learn directly, in anend-to-end fashion, the mapping from input text to associated output text.Its architecture typically consists of two recurrent neural networks (RNNs), oneto consume the input text sequence and one to generate translated output text.NMT is often accompanied by an attention mechanism\u00a0[2]which helps it cope effectively with long input sequences.", "An advantage of Neural Machine Translation is that it sidesteps manybrittle design choices in traditional phrase-based machinetranslation\u00a0[26]. In practice, however, NMT systemsused to be worse in accuracy than phrase-based translation systems,especially when training on very large-scale datasets as used for the verybest publicly available translation systems.Three inherent weaknesses of Neural Machine Translation are responsible for thisgap: its slower training and inference speed, ineffectiveness in dealing withrare words, and sometimesfailure to translate all words in the source sentence. Firstly, it generallytakes a considerable amount of time and computational resources totrain an NMT system on a large-scale translation dataset, thus slowing the rateof experimental turnaround time and innovation. For inference they are generallymuch slower than phrase-based systems due to the large number of parametersused.Secondly, NMT lacks robustness in translating rare words. Though thiscan be addressed in principle by training a \u201ccopy model\u201d to mimic atraditional alignment model\u00a0[31], or by using theattention mechanism to copy rare words\u00a0[37], these approaches areboth unreliable at scale, since the quality of the alignments varies acrosslanguages, and the latent alignments produced by the attentionmechanism are unstable when the network is deep. Also, simple copyingmay not always be the best strategy to cope with rare words, for example whena transliteration is more appropriate. Finally,NMT systems sometimes produce output sentencesthat do not translate all parts of the input sentence \u2013 in otherwords, they fail to completely \u201ccover\u201d the input, which can result insurprising translations.", "This work presents the design and implementation of GNMT, a production NMTsystem at Google, that aims toprovide solutions to the above problems. In our implementation, therecurrent networks are Long Short-Term Memory (LSTM)RNNs\u00a0[23, 17]. Our LSTM RNNs have 8layers, with residual connections between layers to encourage gradientflow\u00a0[21]. For parallelism, we connect the attention fromthe bottom layer of the decoder network to the top layer of theencoder network. To improve inference time, we employ low-precisionarithmetic for inference, which is further accelerated by specialhardware (Google\u2019s Tensor Processing Unit, or TPU). To effectivelydeal with rare words, we use sub-word units (also known as\u201cwordpieces\u201d) [35] for inputs and outputs inour system. Using wordpieces gives a good balance between theflexibility of single characters and the efficiency of full words fordecoding, and also sidesteps the need for special treatment of unknownwords. Our beam search technique includes a length normalization procedure todeal efficiently with the problem of comparing hypotheses of differentlengths during decoding, and a coverage penalty to encourage the modelto translate all of the provided input.", "Our implementation is robust, and performs well on a range of datasetsacross many pairs of languages without the need for language-specificadjustments. Using the same implementation, we are able to achieveresults comparable to or better than previous state-of-the-artsystems on standard benchmarks, while delivering great improvements overGoogle\u2019s phrase-based production translation system.Specifically, on WMT\u201914 English-to-French, our single modelscores 38.95 BLEU, an improvement of 7.5 BLEU from a single modelwithout an external alignment model reported in\u00a0[31] and an improvement of 1.2 BLEU from a single modelwithout an external alignment model reported in\u00a0[45].Our single model is also comparable to a single model in\u00a0[45],while not making use of any alignment model as being used in\u00a0[45].Likewise on WMT\u201914 English-to-German,our single model scores 24.17 BLEU, which is 3.4 BLEU betterthan a previous competitive baseline\u00a0[6]. On production data, ourimplementation is even more effective. Human evaluations show that GNMT hasreduced translation errors by 60% compared to our previous phrase-based systemon many pairs of languages: English \u2194\u2194\\leftrightarrow\u2194 French, English\u2194\u2194\\leftrightarrow\u2194 Spanish, and English \u2194\u2194\\leftrightarrow\u2194 Chinese.Additional experiments suggest the quality of the resulting translation systemgets closer to that of average human translators.", "Statistical Machine Translation (SMT) has been the dominant translationparadigm fordecades\u00a0[3, 4, 5].Practical implementations of SMT are generally phrase-based systems (PBMT)which translate sequences of words or phrases where the lengths maydiffer\u00a0[26].", "Even prior to the advent of direct Neural Machine Translation,neural networks have been used as a component within SMT systems with some success.Perhaps one of the most notable attempts involved the use of a joint languagemodel to learn phrase representations\u00a0[13] which yielded animpressive improvement when combined with phrase-based translation.This approach, however, still makes use of phrase-based translation systemsat its core, and therefore inherits their shortcomings.Other proposed approaches for learning phrase representations\u00a0[7]or learning end-to-end translation with neuralnetworks\u00a0[24] offered encouraging hints, butultimately delivered worse overall accuracy compared to standardphrase-based systems.", "The concept of end-to-end learning for machine translation has beenattempted in the past (e.g.,\u00a0[8]) with limitedsuccess. Following seminal papers in thearea\u00a0[41, 2], NMT translation quality hascrept closer to the level of phrase-based translation systems forcommon research benchmarks. Perhaps the first successful attempt at surpassingphrase-based translation was described in\u00a0[31].On WMT\u201914 English-to-French, this system achieved a 0.5 BLEU improvementcompared to a state-of-the-art phrase-based system.", "Since then, many novel techniques have been proposed to furtherimprove NMT: using an attention mechanism to deal with rarewords\u00a0[37], a mechanism to model translationcoverage\u00a0[42], multi-task and semi-supervised training toincorporate more data\u00a0[14, 29], a characterdecoder\u00a0[9], a characterencoder\u00a0[11], subwordunits\u00a0[38] also to deal with rare word outputs,different kinds of attentionmechanisms\u00a0[30], and sentence-levelloss minimization\u00a0[39, 34].While the translation accuracy of these systems has been encouraging, systematiccomparison with large scale, production quality phrase-based translation systemshas been lacking.", "Our model (see Figure\u00a01) follows the commonsequence-to-sequence learning framework\u00a0[41] withattention\u00a0[2]. It has three components:an encoder network, a decoder network, and an attention network. Theencoder transforms a source sentence into a list of vectors, one vector per input symbol. Giventhis list of vectors, the decoder produces one symbol at a time, untilthe special end-of-sentence symbol (EOS) is produced. The encoder and decoderare connected through an attention module which allows the decoder tofocus on different regions of the source sentence during the course ofdecoding.", "For notation, we use bold lower case to denote vectors (e.g., \ud835\udc2f,\ud835\udc28\ud835\udc22\ud835\udc2fsubscript\ud835\udc28\ud835\udc22\\mathbf{v,o_{i}}bold_v , bold_o start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT), bold upper case to represent matrices (e.g., \ud835\udc14,\ud835\udc16\ud835\udc14\ud835\udc16\\mathbf{U,W}bold_U , bold_W), cursiveupper case to represent sets (e.g., \ud835\udcb1,\ud835\udcaf\ud835\udcb1\ud835\udcaf\\mathscr{V,T}script_V , script_T), capital letters to represent sequences (e.g. X\ud835\udc4bXitalic_X, Y\ud835\udc4cYitalic_Y), and lowercase to represent individual symbols in a sequence, (e.g., x1subscript\ud835\udc651x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, x2subscript\ud835\udc652x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT).", "Let (X,Y)\ud835\udc4b\ud835\udc4c(X,Y)( italic_X , italic_Y ) be a source and target sentence pair. LetX=x1,x2,x3,\u2026,xM\ud835\udc4bsubscript\ud835\udc651subscript\ud835\udc652subscript\ud835\udc653\u2026subscript\ud835\udc65\ud835\udc40X=x_{1},x_{2},x_{3},...,x_{M}italic_X = italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT be thesequence of M\ud835\udc40Mitalic_M symbols in the source sentence and letY=y1,y2,y3,\u2026,yN\ud835\udc4csubscript\ud835\udc661subscript\ud835\udc662subscript\ud835\udc663\u2026subscript\ud835\udc66\ud835\udc41Y=y_{1},y_{2},y_{3},...,y_{N}italic_Y = italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , \u2026 , italic_y start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT be the sequence ofN\ud835\udc41Nitalic_N symbols in the target sentence. The encoder is simply a function ofthe following form:", "\ud835\udc31\ud835\udfcf,\ud835\udc31\ud835\udfd0,\u2026,\ud835\udc31\ud835\udc0c=EncoderRNN(x1,x2,x3,\u2026,xM)subscript\ud835\udc311subscript\ud835\udc312\u2026subscript\ud835\udc31\ud835\udc0c\ud835\udc38\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc45\ud835\udc41\ud835\udc41subscript\ud835\udc651subscript\ud835\udc652subscript\ud835\udc653\u2026subscript\ud835\udc65\ud835\udc40\\mathbf{x_{1},x_{2},...,x_{M}}=EncoderRNN(x_{1},x_{2},x_{3},...,x_{M})bold_x start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPT , \u2026 , bold_x start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT = italic_E italic_n italic_c italic_o italic_d italic_e italic_r italic_R italic_N italic_N ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT )(1)In this equation, \ud835\udc31\ud835\udfcf,\ud835\udc31\ud835\udfd0,\u2026,\ud835\udc31\ud835\udc0csubscript\ud835\udc311subscript\ud835\udc312\u2026subscript\ud835\udc31\ud835\udc0c\\mathbf{x_{1},x_{2},...,x_{M}}bold_x start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPT , \u2026 , bold_x start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT is alist of fixed size vectors. The number of members in the list is thesame as the number of symbols in the source sentence (M\ud835\udc40Mitalic_M in thisexample). Using the chain rule the conditional probability of the sequenceP(Y|X)\ud835\udc43conditional\ud835\udc4c\ud835\udc4bP(Y|X)italic_P ( italic_Y | italic_X ) can be decomposed as:", "P(Y|X)=P(Y|\ud835\udc31\ud835\udfcf,\ud835\udc31\ud835\udfd0,\ud835\udc31\ud835\udfd1,\u2026,\ud835\udc31\ud835\udc0c)=\u220fi=1NP(yi|y0,y1,y2,\u2026,yi\u22121;\ud835\udc31\ud835\udfcf,\ud835\udc31\ud835\udfd0,\ud835\udc31\ud835\udfd1,\u2026,\ud835\udc31\ud835\udc0c)\ud835\udc43conditional\ud835\udc4c\ud835\udc4b\ud835\udc43conditional\ud835\udc4csubscript\ud835\udc311subscript\ud835\udc312subscript\ud835\udc313\u2026subscript\ud835\udc31\ud835\udc0csuperscriptsubscriptproduct\ud835\udc561\ud835\udc41\ud835\udc43conditionalsubscript\ud835\udc66\ud835\udc56subscript\ud835\udc660subscript\ud835\udc661subscript\ud835\udc662\u2026subscript\ud835\udc66\ud835\udc561subscript\ud835\udc311subscript\ud835\udc312subscript\ud835\udc313\u2026subscript\ud835\udc31\ud835\udc0c\\begin{split}P(Y|X)&=P(Y|\\mathbf{x_{1},x_{2},x_{3},...,x_{M}})\\\\&=\\prod_{i=1}^{N}P(y_{i}|y_{0},y_{1},y_{2},...,y_{i-1};\\mathbf{x_{1},x_{2},x_{3},...,x_{M}})\\end{split}start_ROW start_CELL italic_P ( italic_Y | italic_X ) end_CELL start_CELL = italic_P ( italic_Y | bold_x start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT bold_3 end_POSTSUBSCRIPT , \u2026 , bold_x start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL = \u220f start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_P ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_y start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ; bold_x start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT bold_3 end_POSTSUBSCRIPT , \u2026 , bold_x start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT ) end_CELL end_ROW(2)where y0subscript\ud835\udc660y_{0}italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is a special \u201cbeginning of sentence\u201d symbol that isprepended to every target sentence.", "During inference we calculate the probability of the next symbol giventhe source sentence encoding and the decoded target sequence so far:P(yi|y0,y1,y2,y3,\u2026,yi\u22121;\ud835\udc31\ud835\udfcf,\ud835\udc31\ud835\udfd0,\ud835\udc31\ud835\udfd1,\u2026,\ud835\udc31\ud835\udc0c)\ud835\udc43conditionalsubscript\ud835\udc66\ud835\udc56subscript\ud835\udc660subscript\ud835\udc661subscript\ud835\udc662subscript\ud835\udc663\u2026subscript\ud835\udc66\ud835\udc561subscript\ud835\udc311subscript\ud835\udc312subscript\ud835\udc313\u2026subscript\ud835\udc31\ud835\udc0cP(y_{i}|y_{0},y_{1},y_{2},y_{3},...,y_{i-1};\\mathbf{x_{1}},\\mathbf{x_{2}},\\mathbf{x_{3}},...,\\mathbf{x_{M}})italic_P ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , \u2026 , italic_y start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ; bold_x start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT bold_3 end_POSTSUBSCRIPT , \u2026 , bold_x start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT )(3)Our decoder is implemented as a combination of an RNN network and asoftmax layer. The decoder RNN network produces a hidden state\ud835\udc32\ud835\udc22subscript\ud835\udc32\ud835\udc22\\mathbf{y_{i}}bold_y start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT for the next symbol to be predicted, which then goesthrough the softmax layer to generate a probability distribution over candidate output symbols.", "In our experiments we found that for NMT systems to achieve good accuracy,both the encoder and decoder RNNs have to be deep enough to capture subtleirregularities in the source and target languages. This observation issimilar to previous observations that deep LSTMs significantly outperformshallow LSTMs\u00a0[41]. In that work, eachadditional layer reduced perplexity by nearly 10%. Similar to[31], we use a deep stacked Long Short TermMemory (LSTM)\u00a0[23] network for both the encoderRNN and the decoder RNN.", "Our attention module is similar to\u00a0[2]. Morespecifically, let \ud835\udc32i\u22121subscript\ud835\udc32\ud835\udc561\\mathbf{y}_{i-1}bold_y start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT be the decoder-RNN output fromthe past decoding time step (in our implementation, we use the output fromthe bottom decoder layer). Attention context \ud835\udc1aisubscript\ud835\udc1a\ud835\udc56\\mathbf{a}_{i}bold_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTfor the current time step is computed according to the following formulas:st=AttentionFunction(\ud835\udc32i\u22121,\ud835\udc31t)\u2200t,1\u2264t\u2264Mpt=exp\u2061(st)/\u2211t=1Mexp\u2061(st)\u2200t,1\u2264t\u2264M\ud835\udc1ai=\u2211t=1Mpt.\ud835\udc31t\\begin{split}s_{t}&=AttentionFunction(\\mathbf{y}_{i-1},\\mathbf{x}_{t})\\quad\\forall t,\\quad 1\\leq t\\leq M\\\\p_{t}&=\\exp(s_{t})/\\sum_{t=1}^{M}\\exp(s_{t})\\quad\\quad\\forall t,\\quad 1\\leq t\\leq M\\\\\\mathbf{a}_{i}&=\\sum_{t=1}^{M}p_{t}.\\mathbf{x}_{t}\\end{split}start_ROW start_CELL italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = italic_A italic_t italic_t italic_e italic_n italic_t italic_i italic_o italic_n italic_F italic_u italic_n italic_c italic_t italic_i italic_o italic_n ( bold_y start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2200 italic_t , 1 \u2264 italic_t \u2264 italic_M end_CELL end_ROW start_ROW start_CELL italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = roman_exp ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) / \u2211 start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT roman_exp ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2200 italic_t , 1 \u2264 italic_t \u2264 italic_M end_CELL end_ROW start_ROW start_CELL bold_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL = \u2211 start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW(4)where AttentionFunction\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc39\ud835\udc62\ud835\udc5b\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5bAttentionFunctionitalic_A italic_t italic_t italic_e italic_n italic_t italic_i italic_o italic_n italic_F italic_u italic_n italic_c italic_t italic_i italic_o italic_n in our implementation is a feed forward network withone hidden layer.", "As mentioned above, deep stacked LSTMs often give better accuracy overshallower models.However, simply stacking more layers of LSTM works only to a certain number oflayers, beyond which the network becomes too slow and difficult totrain, likely due to exploding and vanishing gradient problems[33, 22]. Inour experience with large-scale translation tasks, simple stacked LSTM layerswork well up to 4layers, barely with 6 layers, and very poorly beyond 8 layers.", "Motivated by the idea of modeling differences between an intermediate layer\u2019soutput and the targets, which has shown to work well for many projects in thepast [16, 21, 40],we introduce residual connectionsamong the LSTM layers in a stack (see Figure\u00a02).More concretely, let LSTMisubscriptLSTM\ud835\udc56\\mathrm{LSTM}_{i}roman_LSTM start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and LSTMi+1subscriptLSTM\ud835\udc561\\mathrm{LSTM}_{i+1}roman_LSTM start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT be the i\ud835\udc56iitalic_i-th and(i+1)\ud835\udc561(i+1)( italic_i + 1 )-th LSTM layers in a stack, whose parameters are\ud835\udc16isuperscript\ud835\udc16\ud835\udc56\\mathbf{W}^{i}bold_W start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and \ud835\udc16i+1superscript\ud835\udc16\ud835\udc561\\mathbf{W}^{i+1}bold_W start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT respectively. At thet\ud835\udc61titalic_t-th time step, for the stacked LSTM without residualconnections, we have:", "\ud835\udc1cti,\ud835\udc26ti=LSTMi(\ud835\udc1ct\u22121i,\ud835\udc26t\u22121i,\ud835\udc31ti\u22121;\ud835\udc16i)\ud835\udc31ti=\ud835\udc26ti\ud835\udc1cti+1,\ud835\udc26ti+1=LSTMi+1(\ud835\udc1ct\u22121i+1,\ud835\udc26t\u22121i+1,\ud835\udc31ti;\ud835\udc16i+1)formulae-sequencesuperscriptsubscript\ud835\udc1c\ud835\udc61\ud835\udc56superscriptsubscript\ud835\udc26\ud835\udc61\ud835\udc56subscriptLSTM\ud835\udc56superscriptsubscript\ud835\udc1c\ud835\udc611\ud835\udc56superscriptsubscript\ud835\udc26\ud835\udc611\ud835\udc56superscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc561superscript\ud835\udc16\ud835\udc56superscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc56superscriptsubscript\ud835\udc26\ud835\udc61\ud835\udc56superscriptsubscript\ud835\udc1c\ud835\udc61\ud835\udc561superscriptsubscript\ud835\udc26\ud835\udc61\ud835\udc561subscriptLSTM\ud835\udc561superscriptsubscript\ud835\udc1c\ud835\udc611\ud835\udc561superscriptsubscript\ud835\udc26\ud835\udc611\ud835\udc561superscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc56superscript\ud835\udc16\ud835\udc561\\begin{split}\\mathbf{c}_{t}^{i},\\mathbf{m}_{t}^{i}&=\\mathrm{LSTM}_{i}(\\mathbf{c}_{t-1}^{i},\\mathbf{m}_{t-1}^{i},\\mathbf{x}_{t}^{i-1};\\mathbf{W}^{i})\\\\\\mathbf{x}_{t}^{i}&=\\mathbf{m}_{t}^{i}\\\\\\mathbf{c}_{t}^{i+1},\\mathbf{m}_{t}^{i+1}&=\\mathrm{LSTM}_{i+1}(\\mathbf{c}_{t-1}^{i+1},\\mathbf{m}_{t-1}^{i+1},\\mathbf{x}_{t}^{i};\\mathbf{W}^{i+1})\\end{split}start_ROW start_CELL bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_CELL start_CELL = roman_LSTM start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_c start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_m start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT ; bold_W start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) end_CELL end_ROW start_ROW start_CELL bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_CELL start_CELL = bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT , bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT end_CELL start_CELL = roman_LSTM start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ( bold_c start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT , bold_m start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ; bold_W start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ) end_CELL end_ROW(5)where \ud835\udc31tisuperscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc56\\mathbf{x}_{t}^{i}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT is the input to LSTMisubscriptLSTM\ud835\udc56\\mathrm{LSTM}_{i}roman_LSTM start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT at time step t\ud835\udc61titalic_t,and \ud835\udc26tisuperscriptsubscript\ud835\udc26\ud835\udc61\ud835\udc56\\mathbf{m}_{t}^{i}bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and \ud835\udc1ctisuperscriptsubscript\ud835\udc1c\ud835\udc61\ud835\udc56\\mathbf{c}_{t}^{i}bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT are the hidden states and memorystates of LSTMisubscriptLSTM\ud835\udc56\\mathrm{LSTM}_{i}roman_LSTM start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT at time step t\ud835\udc61titalic_t, respectively.", "With residual connections between LSTMisubscriptLSTM\ud835\udc56\\mathrm{LSTM}_{i}roman_LSTM start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and LSTMi+1subscriptLSTM\ud835\udc561\\mathrm{LSTM}_{i+1}roman_LSTM start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT, theabove equations become:", "\ud835\udc1cti,\ud835\udc26ti=LSTMi(\ud835\udc1ct\u22121i,\ud835\udc26t\u22121i,\ud835\udc31ti\u22121;\ud835\udc16i)\ud835\udc31ti=\ud835\udc26ti+\ud835\udc31ti\u22121\ud835\udc1cti+1,\ud835\udc26ti+1=LSTMi+1(\ud835\udc1ct\u22121i+1,\ud835\udc26t\u22121i+1,\ud835\udc31ti;\ud835\udc16i+1)formulae-sequencesuperscriptsubscript\ud835\udc1c\ud835\udc61\ud835\udc56superscriptsubscript\ud835\udc26\ud835\udc61\ud835\udc56subscriptLSTM\ud835\udc56superscriptsubscript\ud835\udc1c\ud835\udc611\ud835\udc56superscriptsubscript\ud835\udc26\ud835\udc611\ud835\udc56superscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc561superscript\ud835\udc16\ud835\udc56superscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc56superscriptsubscript\ud835\udc26\ud835\udc61\ud835\udc56superscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc561superscriptsubscript\ud835\udc1c\ud835\udc61\ud835\udc561superscriptsubscript\ud835\udc26\ud835\udc61\ud835\udc561subscriptLSTM\ud835\udc561superscriptsubscript\ud835\udc1c\ud835\udc611\ud835\udc561superscriptsubscript\ud835\udc26\ud835\udc611\ud835\udc561superscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc56superscript\ud835\udc16\ud835\udc561\\begin{split}\\mathbf{c}_{t}^{i},\\mathbf{m}_{t}^{i}&=\\mathrm{LSTM}_{i}(\\mathbf{c}_{t-1}^{i},\\mathbf{m}_{t-1}^{i},\\mathbf{x}_{t}^{i-1};\\mathbf{W}^{i})\\\\\\mathbf{x}_{t}^{i}&=\\mathbf{m}_{t}^{i}+\\mathbf{x}_{t}^{i-1}\\\\\\mathbf{c}_{t}^{i+1},\\mathbf{m}_{t}^{i+1}&=\\mathrm{LSTM}_{i+1}(\\mathbf{c}_{t-1}^{i+1},\\mathbf{m}_{t-1}^{i+1},\\mathbf{x}_{t}^{i};\\mathbf{W}^{i+1})\\end{split}start_ROW start_CELL bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_CELL start_CELL = roman_LSTM start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_c start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_m start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT ; bold_W start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) end_CELL end_ROW start_ROW start_CELL bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_CELL start_CELL = bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT + bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT , bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT end_CELL start_CELL = roman_LSTM start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ( bold_c start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT , bold_m start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ; bold_W start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ) end_CELL end_ROW(6)Residual connections greatly improve the gradient flow in the backwardpass, which allows us to train very deep encoder and decodernetworks. In most of our experiments, we use 8 LSTM layers for the encoderand decoder, though residual connections can allow us to trainsubstantially deeper networks (similar to what was observedin\u00a0[45]).", "For translation systems, the information required to translate certain wordson the output side can appear anywhere on the source side.Often the source side information is approximately left-to-right, similar tothe target side, but depending on the language pair the information fora particular output word can be distributed and even be split up in certainregions of the input side.", "To have the best possible context at each point in the encoder networkit makes sense to use a bi-directionalRNN\u00a0[36] for the encoder, whichwas also used in\u00a0[2]. To allow for maximum possibleparallelization during computation (to bediscussed in more detail in section 3.3),bi-directional connections are only used for the bottom encoder layer \u2013 allother encoder layers are uni-directional. Figure3 illustrates our use of bi-directional LSTMs atthe bottom encoder layer. The layer LSTMfsubscriptLSTM\ud835\udc53\\mathrm{LSTM}_{f}roman_LSTM start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT processes the sourcesentence from left to right, while the layer LSTMbsubscriptLSTM\ud835\udc4f\\mathrm{LSTM}_{b}roman_LSTM start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT processes thesource sentence from right to left. Outputs from LSTMfsubscriptLSTM\ud835\udc53\\mathrm{LSTM}_{f}roman_LSTM start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT(\ud835\udc31\ud835\udc2d\ud835\udc1f\u2192\u2192superscriptsubscript\ud835\udc31\ud835\udc2d\ud835\udc1f\\overrightarrow{\\mathbf{x_{t}^{f}}}over\u2192 start_ARG bold_x start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_f end_POSTSUPERSCRIPT end_ARG) and LSTMbsubscriptLSTM\ud835\udc4f\\mathrm{LSTM}_{b}roman_LSTM start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT(\ud835\udc31\ud835\udc2d\ud835\udc1b\u2190\u2190superscriptsubscript\ud835\udc31\ud835\udc2d\ud835\udc1b\\overleftarrow{\\mathbf{x_{t}^{b}}}over\u2190 start_ARG bold_x start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_b end_POSTSUPERSCRIPT end_ARG) are first concatenated and then fedto the next layer LSTM1subscriptLSTM1\\mathrm{LSTM}_{1}roman_LSTM start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT.", "Due to the complexity of our model, we make use of both modelparallelism and data parallelism to speed up training. Dataparallelism is straightforward: we train n\ud835\udc5bnitalic_n model replicasconcurrently using a Downpour SGD algorithm [12]. Then\ud835\udc5bnitalic_n replicas all share one copy of model parameters, with each replicaasynchronously updating the parameters using a combination of Adam[25] and SGD algorithms. In our experiments,n\ud835\udc5bnitalic_n is often around 10. Each replica works on a mini-batch of m\ud835\udc5amitalic_msentence pairs at a time, which is often 128 in our experiments.", "In addition to data parallelism, model parallelism is used to improvethe speed of the gradient computation on each replica. The encoder anddecoder networks are partitioned along the depth dimension and areplaced on multiple GPUs, effectively running each layer on a different GPU.Since all but the first encoder layer are uni-directional, layer i+1\ud835\udc561i+1italic_i + 1can start its computation before layer i\ud835\udc56iitalic_i is fully finished, which improvestraining speed.The softmax layer is also partitioned, witheach partition responsible for a subset of symbols in the outputvocabulary. Figure 1 shows more details of howpartitioning is done.", "Model parallelism places certain constraints on the modelarchitectures we can use. For example, we cannot afford to havebi-directional LSTM layers for all the encoder layers, since doing sowould reduce parallelism among subsequent layers, as each layer wouldhave to wait until both forward and backward directions of the previouslayer have finished. This would effectively constrain us to make use ofonly 2 GPUs in parallel (one for the forward direction and one for thebackward direction). For the attention portion of the model, we chose to align thebottom decoder output to the top encoder output to maximizeparallelism when running the decoder network. Had we aligned the top decoderlayer to the top encoder layer, we would have removed all parallelismin the decoder network and would not benefit from using more than oneGPU for decoding.", "Neural Machine Translation models often operate with fixed wordvocabularies even though translation is fundamentally an open vocabulary problem(names, numbers, dates etc.). There are two broad categories ofapproaches to address the translation of out-of-vocabulary (OOV)words. One approach is to simply copy rare words from source to target(as most rare words are names or numbers where the correct translationis just a copy), either based on the attentionmodel\u00a0[37], using an external alignmentmodel\u00a0[31], or even using a more complicatedspecial purpose pointingnetwork\u00a0[18]. Another broadcategory of approaches is to use sub-word units, e.g.,chararacters\u00a0[10], mixedword/characters\u00a0[28], or more intelligentsub-words\u00a0[38].", "Our most successful approach falls into the second category (sub-word units), and weadopt the wordpiece model (WPM) implementation initially developed tosolve a Japanese/Korean segmentation problem for the Google speechrecognition system\u00a0[35]. This approach is completelydata-driven and guaranteed to generate a deterministic segmentationfor any possible sequence of characters. It is similar tothe method used in [38] to deal with rare words inNeural Machine Translation.", "For processing arbitrary words, we first break words into wordpiecesgiven a trained wordpiece model. Special word boundary symbols areadded before training of the model such that the original wordsequence can be recovered from the wordpiece sequence withoutambiguity. At decoding time, the model first produces a wordpiecesequence, which is then converted into the corresponding wordsequence.", "Here is an example of a word sequence and the corresponding wordpiece sequence:\u2022Word: Jet makers feud over seat width with big orders at stake\u2022wordpieces: _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake", "In the above example, the word \u201cJet\u201d is broken into two wordpieces\u201c_J\u201d and \u201cet\u201d, and the word \u201cfeud\u201d is broken into twowordpieces \u201c_fe\u201d and \u201cud\u201d. The other words remain as singlewordpieces. \u201c_\u201d is a special character added to mark thebeginning of a word.", "The wordpiece model is generated using a data-driven approach tomaximize the language-model likelihood of the training data, given anevolving word definition. Given a training corpus and a number ofdesired tokens D\ud835\udc37Ditalic_D, the optimization problem is to select D\ud835\udc37Ditalic_Dwordpieces such that the resulting corpus is minimal in the number ofwordpieces when segmented according to the chosen wordpiece model. Ourgreedy algorithm to this optimization problem is similarto\u00a0[38] and is described in more detail in[35]. Compared to the original implementation used in[35], we use a special symbol only at thebeginning of the words and not at both ends. We also cut the numberof basic characters to a manageable number depending on the data(roughly 500 for Western languages, more for Asian languages) and mapthe rest to a special unknown character to avoid polluting the givenwordpiece vocabulary with very rare characters. We find that using a total vocabulary of between 8k and 32k wordpieces achievesboth good accuracy (BLEU scores) and fast decodingspeed across all pairs of language pairs we have tried.", "As mentioned above, in translation it often makes sense to copy rareentity names or numbers directly from the source to the target. Tofacilitate this type of direct copying, we always use a sharedwordpiece model for both the source language and targetlanguage. Using this approach, it is guaranteed that the same stringin source and target sentence will be segmented in exactly the sameway, making it easier for the system to learn to copy these tokens.", "Wordpieces achieve a balance between the flexibility of characters andefficiency of words.We also find that our models get better overall BLEU scores when usingwordpieces \u2013 possibly due to the fact that our models now dealefficiently with an essentially infinite vocabulary without resorting tocharacters only. The latter would make the average lengths of the input and outputsequences much longer, and therefore would require more computation.", "A second approach we use is the mixed word/character model.As in a word model, we keep a fixed-size word vocabulary.However, unlike in a conventional word model where OOV words are collapsedinto a single UNK symbol, we convert OOV words into the sequence of itsconstituent characters.Special prefixes are prepended to the characters, to 1) show the location ofthe characters in a word, and 2) to distinguish them from normal in-vocabularycharacters. There are threeprefixes: <B>,<M>, and <E>, indicating beginning of the word, middleof the word and end of the word, respectively. For example, let\u2019s assume theword Miki is not in the vocabulary. It will be preprocessed into asequence of special tokens: <B>M <M>i <M>k <E>i. The process isdone on both the source and the target sentences. During decoding, theoutput may also contain sequences of special tokens. With theprefixes, it is trivial to reverse the tokenization to the original words aspart of a post-processing step.", "Given a dataset of parallel text containing N\ud835\udc41Nitalic_N input-output sequencepairs, denoted \ud835\udc9f\u2261{(X(i),Y*(i))}i=1N\ud835\udc9fsuperscriptsubscriptsuperscript\ud835\udc4b\ud835\udc56superscript\ud835\udc4cabsent\ud835\udc56\ud835\udc561\ud835\udc41\\mathcal{D}\\equiv\\left\\{(X^{(i)},Y^{*(i)})\\right\\}_{i=1}^{N}caligraphic_D \u2261 { ( italic_X start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT * ( italic_i ) end_POSTSUPERSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT,standard maximum-likelihood training aims at maximizing the sum of logprobabilities of the ground-truth outputs given the correspondinginputs,\ud835\udcaaML(\ud835\udf3d)=\u2211i=1Nlog\u2061P\u03b8(Y*(i)\u2223X(i)).subscript\ud835\udcaaML\ud835\udf3dsuperscriptsubscript\ud835\udc561\ud835\udc41subscript\ud835\udc43\ud835\udf03conditionalsuperscript\ud835\udc4cabsent\ud835\udc56superscript\ud835\udc4b\ud835\udc56\\mathcal{O}_{\\mathrm{ML}}(\\bm{\\mathbf{\\theta}})=\\sum_{i=1}^{N}\\log{P}_{\\theta}(Y^{*(i)}\\mid X^{(i)})~{}.caligraphic_O start_POSTSUBSCRIPT roman_ML end_POSTSUBSCRIPT ( bold_italic_\u03b8 ) = \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_log italic_P start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_Y start_POSTSUPERSCRIPT * ( italic_i ) end_POSTSUPERSCRIPT \u2223 italic_X start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) .(7)The main problem with this objective is that it does not reflect thetask reward function as measured by the BLEU score in translation. Further,this objective does not explicitly encourage a ranking among incorrectoutput sequences \u2013 where outputs with higher BLEU scores should still obtainhigher probabilities under the model \u2013 since incorrect outputs are neverobserved during training. In other words, using maximum-likelihoodtraining only, the model will not learn to be robust to errors made duringdecoding since they are never observed, which is quite a mismatch betweenthe training and testing procedure.", "Several recent papers\u00a0[34, 39, 32]have considered different ways of incorporating the task reward intooptimization of neural sequence-to-sequence models.In this work, we also attempt to refine a model pre-trained on themaximum likelihood objective to directly optimize for the task reward.We show that, even on large datasets, refinement of state-of-the-artmaximum-likelihood models using task reward improves the resultsconsiderably.", "We consider model refinement using the expected reward objective (alsoused in\u00a0[34]), which can be expressed as\ud835\udcaaRL(\ud835\udf3d)=\u2211i=1N\u2211Y\u2208\ud835\udcb4P\u03b8(Y\u2223X(i))r(Y,Y*(i)).subscript\ud835\udcaaRL\ud835\udf3dsuperscriptsubscript\ud835\udc561\ud835\udc41subscript\ud835\udc4c\ud835\udcb4subscript\ud835\udc43\ud835\udf03conditional\ud835\udc4csuperscript\ud835\udc4b\ud835\udc56\ud835\udc5f\ud835\udc4csuperscript\ud835\udc4cabsent\ud835\udc56\\mathcal{O}_{\\mathrm{RL}}(\\bm{\\mathbf{\\theta}})=\\sum_{i=1}^{N}\\sum_{Y\\in\\mathcal{Y}}{P}_{\\theta}(Y\\mid X^{(i)})~{}r(Y,Y^{*(i)}).caligraphic_O start_POSTSUBSCRIPT roman_RL end_POSTSUBSCRIPT ( bold_italic_\u03b8 ) = \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_Y \u2208 caligraphic_Y end_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_Y \u2223 italic_X start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) italic_r ( italic_Y , italic_Y start_POSTSUPERSCRIPT * ( italic_i ) end_POSTSUPERSCRIPT ) .(8)Here, r(Y,Y*(i))\ud835\udc5f\ud835\udc4csuperscript\ud835\udc4cabsent\ud835\udc56r(Y,Y^{*(i)})italic_r ( italic_Y , italic_Y start_POSTSUPERSCRIPT * ( italic_i ) end_POSTSUPERSCRIPT ) denotes the per-sentencescore, and we are computing an expectation over all of the outputsentences Y\ud835\udc4cYitalic_Y, up to a certain length.", "The BLEU score has some undesirable properties when used for singlesentences, as it was designed to be a corpus measure. We therefore use a slightlydifferent score for our RL experiments which we call the \u201cGLEU score\u201d.For the GLEU score, we record all sub-sequences of 1, 2, 3 or 4 tokensin output and target sequence (n-grams). We then compute a recall, which is theratio of the number of matching n-grams to the number of total n-grams in thetarget (ground truth) sequence, and a precision, which is the ratio of the number ofmatching n-grams to the number of total n-grams in the generated output sequence.Then GLEU score is simply the minimum of recall and precision. This GLEUscore\u2019s range is always between 0 (no matches) and 1 (all match) andit is symmetrical when switching output and target. According to ourexperiments, GLEU score correlates quite well with the BLEU metric on acorpus level but does not have its drawbacks for our per sentence reward objective.", "As is common practice in reinforcement learning, we subtract the mean rewardfrom r(Y,Y*(i))\ud835\udc5f\ud835\udc4csuperscript\ud835\udc4cabsent\ud835\udc56r(Y,Y^{*(i)})italic_r ( italic_Y , italic_Y start_POSTSUPERSCRIPT * ( italic_i ) end_POSTSUPERSCRIPT ) in equation 8. Themean is estimated to be the sample mean of m\ud835\udc5amitalic_m sequencesdrawn independently from distribution P\u03b8(Y\u2223X(i))subscript\ud835\udc43\ud835\udf03conditional\ud835\udc4csuperscript\ud835\udc4b\ud835\udc56{P}_{\\theta}(Y\\mid X^{(i)})italic_P start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_Y \u2223 italic_X start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ). In our implementation, m\ud835\udc5amitalic_m is set to be 15.To further stabilize training, we optimize a linear combination of ML(equation 7) and RL (equation 8) objectivesas follows:\ud835\udcaaMixed(\ud835\udf3d)=\u03b1*\ud835\udcaaML(\ud835\udf3d)+\ud835\udcaaRL(\ud835\udf3d)subscript\ud835\udcaaMixed\ud835\udf3d\ud835\udefcsubscript\ud835\udcaaML\ud835\udf3dsubscript\ud835\udcaaRL\ud835\udf3d\\mathcal{O}_{\\mathrm{Mixed}}(\\bm{\\mathbf{\\theta}})=\\alpha*\\mathcal{O}_{\\mathrm{ML}}(\\bm{\\mathbf{\\theta}})+\\mathcal{O}_{\\mathrm{RL}}(\\bm{\\mathbf{\\theta}})caligraphic_O start_POSTSUBSCRIPT roman_Mixed end_POSTSUBSCRIPT ( bold_italic_\u03b8 ) = italic_\u03b1 * caligraphic_O start_POSTSUBSCRIPT roman_ML end_POSTSUBSCRIPT ( bold_italic_\u03b8 ) + caligraphic_O start_POSTSUBSCRIPT roman_RL end_POSTSUBSCRIPT ( bold_italic_\u03b8 )(9)\u03b1\ud835\udefc\\alphaitalic_\u03b1 in our implementation is typically set to be 0.0170.0170.0170.017.", "In our setup, we first train a model using the maximum likelihoodobjective (equation 7) until convergence. We thenrefine this model using a mixed maximum likelihood and expected rewardobjective (equation9), until BLEU score on a development set is no longer improving.The second step is optional.", "One of the main challenges in deploying our Neural Machine Translationmodel to our interactive production translation service is that it iscomputationally intensive at inference, making low latency translationdifficult, and high volume deployment computationally expensive.Quantized inference using reduced precision arithmetic isone technique that can significantly reduce the cost of inference for thesemodels, often providing efficiency improvements on the same computational devices.For example, in [43], it is demonstratedthat a convolutional neural network model can be sped up by a factor of 4-6with minimal loss on classification accuracy on the ILSVRC-12benchmark. In [27], it is demonstrated thatneural network model weights can be quantized to only three states,-1, 0, and +1.", "Many of those previous studies [19, 20, 43, 27]however mostly focus on CNN models withrelatively few layers. Deep LSTMs with long sequences posea novel challenge in that quantization errors can be significantlyamplified after many unrolled steps or after going through a deepLSTM stack.", "In this section, we present our approach to speed up inference withquantized arithmetic. Our solution is tailored towards the hardwareoptions available at Google. To reduce quantization errors, additionalconstraints are added to our model during training so that it is quantizablewith minimal impact on the output of the model. That is, once amodel is trained with these additional constraints, it can be subsequentlyquantized without loss to translation quality. Our experimental results suggestthat those additional constraints do not hurt model convergence nor the qualityof a model once it has converged.", "Recall from equation 6 that in an LSTM stackwith residual connections there are two accumulators: \ud835\udc1ctisuperscriptsubscript\ud835\udc1c\ud835\udc61\ud835\udc56\\mathbf{c}_{t}^{i}bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTalong the time axis and \ud835\udc31tisuperscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc56\\mathbf{x}_{t}^{i}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT along the depth axis. Intheory, both of the accumulators are unbounded, but in practice, wenoticed their values remain quite small. For quantized inference, weexplicitly constrain the values of these accumulators to be within[-\u03b4\ud835\udeff\\deltaitalic_\u03b4, \u03b4\ud835\udeff\\deltaitalic_\u03b4] to guarantee a certain range that can be used forquantization later. The forward computation of an LSTM stack withresidual connections is modified to the following:", "\ud835\udc1c\u2032ti,\ud835\udc26ti=LSTMi(\ud835\udc1ct\u22121i,\ud835\udc26t\u22121i,\ud835\udc31ti\u22121;\ud835\udc16i)\ud835\udc1cti=max\u2061(\u2212\u03b4,min\u2061(\u03b4,\ud835\udc1c\u2032ti))\ud835\udc31\u2032ti=\ud835\udc26ti+\ud835\udc31ti\u22121\ud835\udc31ti=max\u2061(\u2212\u03b4,min\u2061(\u03b4,\ud835\udc31\u2032ti))\ud835\udc1c\u2032ti+1,\ud835\udc26ti+1=LSTMi+1(\ud835\udc1ct\u22121i+1,\ud835\udc26t\u22121i+1,\ud835\udc31ti;\ud835\udc16i+1)\ud835\udc1cti+1=max\u2061(\u2212\u03b4,min\u2061(\u03b4,\ud835\udc1c\u2032ti+1))formulae-sequencesuperscriptsubscriptsuperscript\ud835\udc1c\u2032\ud835\udc61\ud835\udc56superscriptsubscript\ud835\udc26\ud835\udc61\ud835\udc56subscriptLSTM\ud835\udc56superscriptsubscript\ud835\udc1c\ud835\udc611\ud835\udc56superscriptsubscript\ud835\udc26\ud835\udc611\ud835\udc56superscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc561superscript\ud835\udc16\ud835\udc56superscriptsubscript\ud835\udc1c\ud835\udc61\ud835\udc56\ud835\udeff\ud835\udeffsuperscriptsubscriptsuperscript\ud835\udc1c\u2032\ud835\udc61\ud835\udc56superscriptsubscriptsuperscript\ud835\udc31\u2032\ud835\udc61\ud835\udc56superscriptsubscript\ud835\udc26\ud835\udc61\ud835\udc56superscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc561superscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc56\ud835\udeff\ud835\udeffsuperscriptsubscriptsuperscript\ud835\udc31\u2032\ud835\udc61\ud835\udc56superscriptsubscriptsuperscript\ud835\udc1c\u2032\ud835\udc61\ud835\udc561superscriptsubscript\ud835\udc26\ud835\udc61\ud835\udc561subscriptLSTM\ud835\udc561superscriptsubscript\ud835\udc1c\ud835\udc611\ud835\udc561superscriptsubscript\ud835\udc26\ud835\udc611\ud835\udc561superscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc56superscript\ud835\udc16\ud835\udc561superscriptsubscript\ud835\udc1c\ud835\udc61\ud835\udc561\ud835\udeff\ud835\udeffsuperscriptsubscriptsuperscript\ud835\udc1c\u2032\ud835\udc61\ud835\udc561\\begin{split}\\mathbf{c^{\\prime}}_{t}^{i},\\mathbf{m}_{t}^{i}&=\\mathrm{LSTM}_{i}(\\mathbf{c}_{t-1}^{i},\\mathbf{m}_{t-1}^{i},\\mathbf{x}_{t}^{i-1};\\mathbf{W}^{i})\\\\\\mathbf{c}_{t}^{i}&=\\max(-\\delta,\\min(\\delta,\\mathbf{c^{\\prime}}_{t}^{i}))\\\\\\mathbf{x^{\\prime}}_{t}^{i}&=\\mathbf{m}_{t}^{i}+\\mathbf{x}_{t}^{i-1}\\\\\\mathbf{x}_{t}^{i}&=\\max(-\\delta,\\min(\\delta,\\mathbf{x^{\\prime}}_{t}^{i}))\\\\\\mathbf{c^{\\prime}}_{t}^{i+1},\\mathbf{m}_{t}^{i+1}&=\\mathrm{LSTM}_{i+1}(\\mathbf{c}_{t-1}^{i+1},\\mathbf{m}_{t-1}^{i+1},\\mathbf{x}_{t}^{i};\\mathbf{W}^{i+1})\\\\\\mathbf{c}_{t}^{i+1}&=\\max(-\\delta,\\min(\\delta,\\mathbf{c^{\\prime}}_{t}^{i+1}))\\end{split}start_ROW start_CELL bold_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_CELL start_CELL = roman_LSTM start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_c start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_m start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT ; bold_W start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) end_CELL end_ROW start_ROW start_CELL bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_CELL start_CELL = roman_max ( - italic_\u03b4 , roman_min ( italic_\u03b4 , bold_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) ) end_CELL end_ROW start_ROW start_CELL bold_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_CELL start_CELL = bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT + bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_CELL start_CELL = roman_max ( - italic_\u03b4 , roman_min ( italic_\u03b4 , bold_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) ) end_CELL end_ROW start_ROW start_CELL bold_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT , bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT end_CELL start_CELL = roman_LSTM start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ( bold_c start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT , bold_m start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ; bold_W start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ) end_CELL end_ROW start_ROW start_CELL bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT end_CELL start_CELL = roman_max ( - italic_\u03b4 , roman_min ( italic_\u03b4 , bold_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ) ) end_CELL end_ROW(10)Let us expand LSTMisubscriptLSTM\ud835\udc56\\mathrm{LSTM}_{i}roman_LSTM start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in equation 10to include the internal gating logic. For brevity, we drop all thesuperscripts i\ud835\udc56iitalic_i.", "\ud835\udc16=[\ud835\udc161,\ud835\udc162,\ud835\udc163,\ud835\udc164,\ud835\udc165,\ud835\udc166,\ud835\udc167,\ud835\udc168]\ud835\udc22t=sigmoid(\ud835\udc161\ud835\udc31t+\ud835\udc162\ud835\udc26t)\ud835\udc22\u2032t=tanh\u2061(\ud835\udc163\ud835\udc31t+\ud835\udc164\ud835\udc26t)\ud835\udc1ft=sigmoid(\ud835\udc165\ud835\udc31t+\ud835\udc166\ud835\udc26t)\ud835\udc28t=sigmoid(\ud835\udc167\ud835\udc31t+\ud835\udc168\ud835\udc26t)\ud835\udc1ct=\ud835\udc1ct\u22121\u2299\ud835\udc1ft+\ud835\udc22\u2032t\u2299\ud835\udc22t\ud835\udc26t=\ud835\udc1ct\u2299\ud835\udc28t\ud835\udc16subscript\ud835\udc161subscript\ud835\udc162subscript\ud835\udc163subscript\ud835\udc164subscript\ud835\udc165subscript\ud835\udc166subscript\ud835\udc167subscript\ud835\udc168subscript\ud835\udc22\ud835\udc61sigmoidsubscript\ud835\udc161subscript\ud835\udc31\ud835\udc61subscript\ud835\udc162subscript\ud835\udc26\ud835\udc61subscriptsuperscript\ud835\udc22\u2032\ud835\udc61subscript\ud835\udc163subscript\ud835\udc31\ud835\udc61subscript\ud835\udc164subscript\ud835\udc26\ud835\udc61subscript\ud835\udc1f\ud835\udc61sigmoidsubscript\ud835\udc165subscript\ud835\udc31\ud835\udc61subscript\ud835\udc166subscript\ud835\udc26\ud835\udc61subscript\ud835\udc28\ud835\udc61sigmoidsubscript\ud835\udc167subscript\ud835\udc31\ud835\udc61subscript\ud835\udc168subscript\ud835\udc26\ud835\udc61subscript\ud835\udc1c\ud835\udc61direct-productsubscript\ud835\udc1c\ud835\udc611subscript\ud835\udc1f\ud835\udc61direct-productsubscriptsuperscript\ud835\udc22\u2032\ud835\udc61subscript\ud835\udc22\ud835\udc61subscript\ud835\udc26\ud835\udc61direct-productsubscript\ud835\udc1c\ud835\udc61subscript\ud835\udc28\ud835\udc61\\begin{split}\\mathbf{W}&=[\\mathbf{W}_{1},\\mathbf{W}_{2},\\mathbf{W}_{3},\\mathbf{W}_{4},\\mathbf{W}_{5},\\mathbf{W}_{6},\\mathbf{W}_{7},\\mathbf{W}_{8}]\\\\\\mathbf{i}_{t}&=\\text{sigmoid}(\\mathbf{W}_{1}\\mathbf{x}_{t}+\\mathbf{W}_{2}\\mathbf{m}_{t})\\\\\\mathbf{i^{\\prime}}_{t}&=\\tanh(\\mathbf{W}_{3}\\mathbf{x}_{t}+\\mathbf{W}_{4}\\mathbf{m}_{t})\\\\\\mathbf{f}_{t}&=\\text{sigmoid}(\\mathbf{W}_{5}\\mathbf{x}_{t}+\\mathbf{W}_{6}\\mathbf{m}_{t})\\\\\\mathbf{o}_{t}&=\\text{sigmoid}(\\mathbf{W}_{7}\\mathbf{x}_{t}+\\mathbf{W}_{8}\\mathbf{m}_{t})\\\\\\mathbf{c}_{t}&=\\mathbf{c}_{t-1}\\odot\\mathbf{f}_{t}+\\mathbf{i^{\\prime}}_{t}\\odot\\mathbf{i}_{t}\\\\\\mathbf{m}_{t}&=\\mathbf{c}_{t}\\odot\\mathbf{o}_{t}\\end{split}start_ROW start_CELL bold_W end_CELL start_CELL = [ bold_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT ] end_CELL end_ROW start_ROW start_CELL bold_i start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = sigmoid ( bold_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + bold_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL bold_i start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = roman_tanh ( bold_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + bold_W start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL bold_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = sigmoid ( bold_W start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + bold_W start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL bold_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = sigmoid ( bold_W start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + bold_W start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = bold_c start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT \u2299 bold_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + bold_i start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2299 bold_i start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2299 bold_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW(11)When doing quantized inference, we replace all the floating pointoperations in equations 10 and11 with fixed-point integer operations with either8-bit or 16-bit resolution. The weight matrix \ud835\udc16\ud835\udc16\\mathbf{W}bold_W above isrepresented using an 8-bit integer matrix \ud835\udc16\ud835\udc10\ud835\udc16\ud835\udc10\\mathbf{WQ}bold_WQ and a floatvector \ud835\udc2c\ud835\udc2c\\mathbf{s}bold_s, as shown below:", "\ud835\udc2ci=max\u2061(abs(\ud835\udc16[i,:]))\ud835\udc16\ud835\udc10[i,j]=round(\ud835\udc16[i,j]/\ud835\udc2ci\u00d7127.0)subscript\ud835\udc2c\ud835\udc56abs\ud835\udc16\ud835\udc56:\ud835\udc16\ud835\udc10\ud835\udc56\ud835\udc57round\ud835\udc16\ud835\udc56\ud835\udc57subscript\ud835\udc2c\ud835\udc56127.0\\begin{split}\\mathbf{s}_{i}&=\\max(\\text{abs}(\\mathbf{W}[i,:]))\\\\\\mathbf{WQ}[i,j]&=\\text{round}(\\mathbf{W}[i,j]/\\mathbf{s}_{i}\\times 127.0)\\end{split}start_ROW start_CELL bold_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL = roman_max ( abs ( bold_W [ italic_i , : ] ) ) end_CELL end_ROW start_ROW start_CELL bold_WQ [ italic_i , italic_j ] end_CELL start_CELL = round ( bold_W [ italic_i , italic_j ] / bold_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u00d7 127.0 ) end_CELL end_ROW(12)All accumulator values (\ud835\udc1ctisuperscriptsubscript\ud835\udc1c\ud835\udc61\ud835\udc56\\mathbf{c}_{t}^{i}bold_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and \ud835\udc31tisuperscriptsubscript\ud835\udc31\ud835\udc61\ud835\udc56\\mathbf{x}_{t}^{i}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT) are represented using16-bit integers representing the range [\u2212\u03b4,\u03b4]\ud835\udeff\ud835\udeff[-\\delta,\\delta][ - italic_\u03b4 , italic_\u03b4 ]. All matrixmultiplications (e.g., \ud835\udc161\ud835\udc31tsubscript\ud835\udc161subscript\ud835\udc31\ud835\udc61\\mathbf{W}_{1}\\mathbf{x}_{t}bold_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT,\ud835\udc162\ud835\udc26tsubscript\ud835\udc162subscript\ud835\udc26\ud835\udc61\\mathbf{W}_{2}\\mathbf{m}_{t}bold_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, etc.) in equation 11are done using 8-bit integer multiplication accumulated into largeraccumulators. All other operations, including all the activations(sigmoid, tanh\\tanhroman_tanh) and elementwise operations (\u2299direct-product\\odot\u2299, +++)are done using 16-bit integer operations.", "We now turn our attention to the log-linear softmax layer. During training,given the decoder RNN network output \ud835\udc32\ud835\udc2dsubscript\ud835\udc32\ud835\udc2d\\mathbf{y_{t}}bold_y start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT, we compute the probabilityvector \ud835\udc29\ud835\udc2dsubscript\ud835\udc29\ud835\udc2d\\mathbf{p_{t}}bold_p start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT over all candidate output symbols as follows:", "\ud835\udc2f\ud835\udc2d=\ud835\udc16\ud835\udc2c*\ud835\udc32\ud835\udc2d\ud835\udc2f\ud835\udc2d\u2032=max\u2061(\u2212\u03b3,min\u2061(\u03b3,\ud835\udc2f\ud835\udc2d))\ud835\udc29\ud835\udc2d=softmax(\ud835\udc2f\ud835\udc2d\u2032)subscript\ud835\udc2f\ud835\udc2dsubscript\ud835\udc16\ud835\udc2csubscript\ud835\udc32\ud835\udc2dsuperscriptsubscript\ud835\udc2f\ud835\udc2d\u2032\ud835\udefe\ud835\udefesubscript\ud835\udc2f\ud835\udc2dsubscript\ud835\udc29\ud835\udc2d\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65superscriptsubscript\ud835\udc2f\ud835\udc2d\u2032\\begin{split}\\mathbf{v_{t}}&=\\mathbf{W_{s}}*\\mathbf{y_{t}}\\\\\\mathbf{v_{t}^{\\prime}}&=\\max(-\\gamma,\\min(\\gamma,\\mathbf{v_{t}}))\\\\\\mathbf{p_{t}}&=softmax(\\mathbf{v_{t}^{\\prime}})\\end{split}start_ROW start_CELL bold_v start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT end_CELL start_CELL = bold_W start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT * bold_y start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_v start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_CELL start_CELL = roman_max ( - italic_\u03b3 , roman_min ( italic_\u03b3 , bold_v start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT ) ) end_CELL end_ROW start_ROW start_CELL bold_p start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT end_CELL start_CELL = italic_s italic_o italic_f italic_t italic_m italic_a italic_x ( bold_v start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) end_CELL end_ROW(13)In equation 13, \ud835\udc16\ud835\udc2csubscript\ud835\udc16\ud835\udc2c\\mathbf{W_{s}}bold_W start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT is the weightmatrix for the linear layer, which has the same number of rows as thenumber of symbols in the target vocabulary with each row correspondingto one unique target symbol. \ud835\udc2f\ud835\udc2f\\mathbf{v}bold_v represents the raw logits, which arefirst clipped to be between \u2212\u03b3\ud835\udefe-\\gamma- italic_\u03b3 and \u03b3\ud835\udefe\\gammaitalic_\u03b3 and then normalizedinto a probability vector \ud835\udc29\ud835\udc29\\mathbf{p}bold_p. Input \ud835\udc32\ud835\udc2dsubscript\ud835\udc32\ud835\udc2d\\mathbf{y_{t}}bold_y start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT isguaranteed to be between \u2212\u03b4\ud835\udeff-\\delta- italic_\u03b4 and \u03b4\ud835\udeff\\deltaitalic_\u03b4 due to thequantization scheme we applied to the decoder RNN. The clipping range\u03b3\ud835\udefe\\gammaitalic_\u03b3 for the logits \ud835\udc2f\ud835\udc2f\\mathbf{v}bold_v is determined empirically, and inour case, it is set to 25252525. In quantized inference, the weight matrix\ud835\udc16\ud835\udc2csubscript\ud835\udc16\ud835\udc2c\\mathbf{W_{s}}bold_W start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT is quantized into 8 bits as inequation 12, and the matrix multiplication is done using8 bit arithmetic. The calculations within the softmax\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65softmaxitalic_s italic_o italic_f italic_t italic_m italic_a italic_x function and theattention model are not quantized during inference.", "It is worth emphasizing that during training of the model we use full-precisionfloating point numbers. The only constraints we add to the modelduring training are the clipping of the RNN accumulator values into[\u2212\u03b4,\u03b4]\ud835\udeff\ud835\udeff[-\\delta,\\delta][ - italic_\u03b4 , italic_\u03b4 ] and softmax logits into[\u2212\u03b3,\u03b3]\ud835\udefe\ud835\udefe[-\\gamma,\\gamma][ - italic_\u03b3 , italic_\u03b3 ]. \u03b3\ud835\udefe\\gammaitalic_\u03b3 is fixed to be at 25.025.025.025.0, while thevalue for \u03b4\ud835\udeff\\deltaitalic_\u03b4 is gradually annealed from a generous bound of\u03b4=8.0\ud835\udeff8.0\\delta=8.0italic_\u03b4 = 8.0 at the beginning of training, to a rather stringent boundof \u03b4=1.0\ud835\udeff1.0\\delta=1.0italic_\u03b4 = 1.0 towards the end of training. At inference time,\u03b4\ud835\udeff\\deltaitalic_\u03b4 is fixed at 1.01.01.01.0. Those additional constraints do not degrademodel convergence nor the decoding quality of the model when it hasconverged. In Figure 4, we compare the lossvs. steps for an unconstrained model (the blue curve) and a constrainedmodel (the red curve) on WMT\u201914 English-to-French. We can see thatthe loss for the constrained model is slightly better, possibly due toregularization roles those constraints play.", "Our solution strikes a good balance between efficiency andaccuracy. Since the computationally expensive operations (the matrixmultiplications) are done using 8-bit integer operations, ourquantized inference is quite efficient. Also, since error-sensitiveaccumulator values are stored using 16-bit integers, our solution isvery accurate and is robust to quantization errors.", "In Table\u00a01 we compare the inference speedand quality when decoding the WMT\u201914 English-to-French development set (aconcatenation of newstest2012 and newstest2013 test sets for a totalof 6003 sentences) on CPU, GPU and Google\u2019s Tensor Processing Unit(TPU) respectively.111https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html The model used here for comparison is trained withquantization constraints on the ML objective only (i.e., withoutreinforcement learning based model refinement). When the model isdecoded on CPU and GPU, it is not quantized and all operationsare done using full-precision floats. When it is decoded on TPU, certainoperations, such as embedding lookup and attention module, remain on the CPU,and all other quantized operations are off-loadedto the TPU. In all cases, decoding is done on a single machine withtwo Intel Haswell CPUs, which consists in total of 88 CPU cores(hyperthreads). The machine isequipped with an NVIDIA GPU (Tesla k80) for the experiment with GPU or a singleGoogle TPU for the experiment with TPU.", "Table\u00a01 shows that decoding using reducedprecision arithmetics on the TPU suffers a very minimal loss of 0.0072 onlog perplexity, and no loss on BLEU at all. This result matchesprevious work reporting that quantizing convolutional neuralnetwork models can retain most of the model quality.", "Table\u00a01 also shows that decoding our model on CPUis actually 2.3 times faster than on GPU. Firstly, ourdual-CPUs host machine offers a theoretical peak FLOP performance which is morethan two thirds that of the GPU. Secondly, the beam searchalgorithm forces the decoder to incur a non-trivial amount of datatransfer between the host and the GPU at every decoding step. Hence,our current decoder implementation is not fully utilizing the computationcapacities that a GPU can theoretically offer during inference.", "Finally, Table\u00a01 shows that decoding on TPUs is3.4 times faster than decoding on CPUs, demonstrating that quantized arithmeticsis much faster on TPUs than both CPUs or GPUs.", "Unless otherwise noted, we always train and evaluate quantizedmodels in our experiments. Because there is little difference from aquality perspective between a model decoded on CPUs and one decodedon TPUs, we use CPUs to decode for model evaluation during training andexperimentation and use TPUs to serve production traffic.", "We use beam search during decoding to find the sequence Y\ud835\udc4cYitalic_Ythat maximizes a score function s(Y,X)\ud835\udc60\ud835\udc4c\ud835\udc4bs(Y,X)italic_s ( italic_Y , italic_X ) given a trained model. Weintroduce two important refinements to the pure max-probability based beamsearch algorithm: a coverage penalty\u00a0[42] and lengthnormalization. With length normalization, we aim to account for thefact that we have to compare hypotheses of different length. Withoutsome form of length-normalization regular beam search will favorshorter results over longer ones on average since a negativelog-probability is added at each step, yielding lower (more negative) scores forlonger sentences. We first tried to simply divideby the length to normalize. We then improved on that original heuristic by dividing bylength\u03b1\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc54\ud835\udc61superscript\u210e\ud835\udefclength^{\\alpha}italic_l italic_e italic_n italic_g italic_t italic_h start_POSTSUPERSCRIPT italic_\u03b1 end_POSTSUPERSCRIPT, with 0<\u03b1<10\ud835\udefc10<\\alpha<10 < italic_\u03b1 < 1 where \u03b1\ud835\udefc\\alphaitalic_\u03b1 is optimized ona development set (\u03b1\u2208[0.6\u22120.7]\ud835\udefcdelimited-[]0.60.7\\alpha\\in[0.6-0.7]italic_\u03b1 \u2208 [ 0.6 - 0.7 ] was usually found to bebest). Eventually we designed the empirically-better scoring functionbelow, which also includes a coverage penalty to favor translationsthat fully cover the source sentence according to the attentionmodule.", "More concretely, the scoring function s(Y,X)\ud835\udc60\ud835\udc4c\ud835\udc4bs(Y,X)italic_s ( italic_Y , italic_X ) that we employ torank candidate translations is defined as follows:", "s(Y,X)=log\u2061(P(Y|X))/lp(Y)+cp(X;Y)lp(Y)=(5+|Y|)\u03b1(5+1)\u03b1cp(X;Y)=\u03b2*\u2211i=1|X|log\u2061(min\u2061(\u2211j=1|Y|pi,j,1.0)),\ud835\udc60\ud835\udc4c\ud835\udc4b\ud835\udc43conditional\ud835\udc4c\ud835\udc4b\ud835\udc59\ud835\udc5d\ud835\udc4c\ud835\udc50\ud835\udc5d\ud835\udc4b\ud835\udc4c\ud835\udc59\ud835\udc5d\ud835\udc4csuperscript5\ud835\udc4c\ud835\udefcsuperscript51\ud835\udefc\ud835\udc50\ud835\udc5d\ud835\udc4b\ud835\udc4c\ud835\udefdsuperscriptsubscript\ud835\udc561\ud835\udc4bsuperscriptsubscript\ud835\udc571\ud835\udc4csubscript\ud835\udc5d\ud835\udc56\ud835\udc571.0\\begin{split}s(Y,X)&=\\log(P(Y|X))/lp(Y)+cp(X;Y)\\\\lp(Y)&=\\frac{(5+|Y|)^{\\alpha}}{(5+1)^{\\alpha}}\\\\cp(X;Y)&=\\beta*\\sum_{i=1}^{|X|}{\\log(\\min(\\sum_{j=1}^{|Y|}{p_{i,j}},1.0))},\\end{split}start_ROW start_CELL italic_s ( italic_Y , italic_X ) end_CELL start_CELL = roman_log ( italic_P ( italic_Y | italic_X ) ) / italic_l italic_p ( italic_Y ) + italic_c italic_p ( italic_X ; italic_Y ) end_CELL end_ROW start_ROW start_CELL italic_l italic_p ( italic_Y ) end_CELL start_CELL = divide start_ARG ( 5 + | italic_Y | ) start_POSTSUPERSCRIPT italic_\u03b1 end_POSTSUPERSCRIPT end_ARG start_ARG ( 5 + 1 ) start_POSTSUPERSCRIPT italic_\u03b1 end_POSTSUPERSCRIPT end_ARG end_CELL end_ROW start_ROW start_CELL italic_c italic_p ( italic_X ; italic_Y ) end_CELL start_CELL = italic_\u03b2 * \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_X | end_POSTSUPERSCRIPT roman_log ( roman_min ( \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_Y | end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT , 1.0 ) ) , end_CELL end_ROW(14)where pi,jsubscript\ud835\udc5d\ud835\udc56\ud835\udc57p_{i,j}italic_p start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT is the attention probability of the j\ud835\udc57jitalic_j-th target wordyjsubscript\ud835\udc66\ud835\udc57y_{j}italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT on the i\ud835\udc56iitalic_i-th source word xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. By construction(equation 4), \u2211i=0|X|pi,jsuperscriptsubscript\ud835\udc560\ud835\udc4bsubscript\ud835\udc5d\ud835\udc56\ud835\udc57\\sum_{i=0}^{|X|}{p_{i,j}}\u2211 start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_X | end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT is equalto 1. Parameters \u03b1\ud835\udefc\\alphaitalic_\u03b1 and \u03b2\ud835\udefd\\betaitalic_\u03b2 control the strength ofthe length normalization and the coverage penalty. When \u03b1=0\ud835\udefc0\\alpha=0italic_\u03b1 = 0 and\u03b2=0\ud835\udefd0\\beta=0italic_\u03b2 = 0, our decoder falls back to pure beam search by probability.", "During beam search, we typically keep 8-12 hypotheses but we find thatusing fewer (4 or 2) has only slight negative effects on BLEU scores. Besides pruningthe number of considered hypotheses, two other forms of pruning areused. Firstly, at each step, we only consider tokens that have localscores that are not more than beamsize\ud835\udc4f\ud835\udc52\ud835\udc4e\ud835\udc5a\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52beamsizeitalic_b italic_e italic_a italic_m italic_s italic_i italic_z italic_e below the best token for thisstep. Secondly, after a normalized best score has been foundaccording to equation 14, we prune all hypothesesthat are more than beamsize\ud835\udc4f\ud835\udc52\ud835\udc4e\ud835\udc5a\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52beamsizeitalic_b italic_e italic_a italic_m italic_s italic_i italic_z italic_e below the best normalized score so far.The latter type of pruning only applies to full hypotheses because itcompares scores in the normalized space, which is only available whena hypothesis ends. This latter form of pruning also has the effect thatvery quickly no more hypotheses will be generated once a sufficientlygood hypothesis has been found, so the search will end quickly. Thepruning speeds up search by 30%\u221240%percent30percent4030\\%-40\\%30 % - 40 % when run on CPUs compared to notpruning (where we simply stop decoding after a predetermined maximum outputlength of twice the source length).Typically we use beamsize=3.0\ud835\udc4f\ud835\udc52\ud835\udc4e\ud835\udc5a\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc523.0beamsize=3.0italic_b italic_e italic_a italic_m italic_s italic_i italic_z italic_e = 3.0, unless otherwise noted.", "To improve throughput during decoding we can put many sentences (typicallyup to 35) of similar length into a batch and decode all of those in parallel tomake use of available hardware optimized for parallel computations. In thiscase the beam search only finishes if all hypotheses for all sentences in thebatch are out of beam, which is slightly less efficient theoretically, but inpractice is of negligible additional computational cost.", "Table 2 shows the impact of \u03b1\ud835\udefc\\alphaitalic_\u03b1 and \u03b2\ud835\udefd\\betaitalic_\u03b2 onthe BLEU score when decoding the WMT\u201914 English-to-French development set.The model used here for experiments is trained using the ML objectiveonly (without RL refinement). As can be seen from the results, havingsome length normalization and coverage penalty improves BLEU scoreconsiderably (from 30.3 to 31.4).", "We find that length normalization (\u03b1\ud835\udefc\\alphaitalic_\u03b1) and coverage penalty(\u03b2\ud835\udefd\\betaitalic_\u03b2) are less effective for models with RLrefinement. Table 3 summarizes ourresults. This is understandable, as during RL refinement, the modelsalready learn to pay attention to the full source sentence to notunder-translate or over-translate, which would result in a penalty on theBLEU (or GLEU) scores.", "We found that the optimal \u03b1\ud835\udefc\\alphaitalic_\u03b1 and \u03b2\ud835\udefd\\betaitalic_\u03b2 vary slightly fordifferent models. Based on tuning results using internal Googledatasets, we use \u03b1=0.2\ud835\udefc0.2\\alpha=0.2italic_\u03b1 = 0.2 and \u03b2=0.2\ud835\udefd0.2\\beta=0.2italic_\u03b2 = 0.2 in our experiments, unlessnoted otherwise.", "In this section, we present our experimental results ontwo publicly available corpora used extensively asbenchmarks for Neural Machine Translation systems:WMT\u201914 English-to-French (WMT En\u2192\u2192\\rightarrow\u2192Fr) andEnglish-to-German (WMT En\u2192\u2192\\rightarrow\u2192De). On these two datasets, webenchmark GNMT models with word-based, character-based, and wordpiece-basedvocabularies. We also present the improved accuracy of our models afterfine-tuning with RL and model ensembling. Our main objectivewith these datasets is to show the contributions of various componentsin our implementation, in particular the wordpiece model, RLmodel refinement, and model ensembling.", "In addition to testing on publicly available corpora, we also test GNMT onGoogle\u2019s translation production corpora, which are two to three decimal orders of magnitudes bigger than the WMT corpora for a given language pair. Wecompare the accuracy of our model against human accuracy and thebest Phrase-Based Machine Translation (PBMT) production system for Google Translate.", "In all experiments, our models consist of 8 encoder layers and 8 decoder layers.(Since the bottom encoder layer is actually bi-directional, in total there are9 logically distinct LSTM passes in the encoder.)The attention network is a simple feedforward network with one hidden layer with 1024 nodes.All of the models use 1024 LSTM nodes per encoder and decoder layers.", "We evaluate our model on the WMT En\u2192\u2192\\rightarrow\u2192Fr dataset, the WMTEn\u2192\u2192\\rightarrow\u2192De dataset, as well as many Google-internalproduction datasets. On WMT En\u2192\u2192\\rightarrow\u2192Fr, the training setcontains 36M sentence pairs. On WMT En\u2192\u2192\\rightarrow\u2192De, the trainingset contains 5M sentence pairs. In both cases, we use newstest2014 as the testsets to compare against previouswork\u00a0[31, 37, 45].The combination of newstest2012 and newstest2013 is used as the development set.", "In addition to WMT, we also evaluateour model on some Google-internal datasets representing a widerspectrum of languages with distinct linguistic properties:English \u2194\u2194\\leftrightarrow\u2194 French, English \u2194\u2194\\leftrightarrow\u2194 Spanish andEnglish \u2194\u2194\\leftrightarrow\u2194 Chinese.", "We evaluate our models using the standard BLEU score metric. To becomparable to previous work\u00a0[41, 31, 45], we reporttokenized BLEU score as computed by the multi-bleu.pl script,downloaded from the public implementation of Moses (on Github), which isalso used in\u00a0[31].", "As is well-known, BLEU score does not fully capture the quality of atranslation. For that reason we also carry out side-by-side (SxS)evaluations where we have human raters evaluate and compare thequality of two translations presented side by side for a given sourcesentence. Side-by-side scores range from 0 to 6, with a score of 0meaning \u201ccompletely nonsense translation\u201d, and a score of 6meaning \u201cperfect translation: the meaning of the translationis completely consistent with the source, and the grammar iscorrect\u201d. A translation is given a score of 4 if \u201cthesentence retains most of the meaning of the source sentence, but mayhave some grammar mistakes\u201d, and a translation is given a score of 2if \u201cthe sentence preserves some of the meaning of the sourcesentence but misses significant parts\u201d. These scores are generatedby human raters who are fluent in both languages and hence oftencapture translation quality better than BLEU scores.", "The models are trained by a system we implemented usingTensorFlow[1].The training setup follows the classicdata parallelism paradigm. There are 12 replicas runningconcurrently on separate machines. Every replica updates the sharedparameters asynchronously.", "We initialize all trainable parameters uniformly between [-0.04, 0.04]. Asis common wisdom in training RNN models, we apply gradient clipping(similar to [41]): all gradients are uniformlyscaled down such that the norm of the modified gradients is no largerthan a fixed constant, which is 5.05.05.05.0 in our case. If the norm of theoriginal gradients is already smaller than or equal to the giventhreshold, then gradients are not changed.", "For the first stage of maximum likelihood training (that is, tooptimize for objective function 7), we use acombination of Adam [25] and simple SGDlearning algorithms provided by the TensorFlow runtime system. We run Adam forthe first 60k steps, after which we switch to simple SGD. Each step intraining is a mini-batch of 128 examples.", "We find that Adam accelerates training at the beginning, but Adamalone converges to a worse point than a combination of Adam first, followed bySGD (Figure\u00a05). For the Adam part, we use a learningrate of 0.00020.00020.00020.0002, and for the SGD part, we use a learning rate of0.50.50.50.5. We find that it is important to also anneal the learning rateafter a certain number of total steps. For the WMT En\u2192\u2192\\rightarrow\u2192Fr dataset,we beginto anneal the learning rate after 1.2M steps, after which we halve thelearning rate every 200k steps for an additional 800k steps. On WMTEn\u2192\u2192\\rightarrow\u2192Fr, it takes around 6 days to train a basic model using 96 NVIDIA K80 GPUs.", "Once a model is fully converged using the ML objective, we switch to RLbased model refinement, i.e., we further optimize the objectivefunction as in equation 9. We refine a model until the BLEUscore does not change much on the development set. For this modelrefinement phase, we simply run the SGD optimization algorithm. The numberof steps needed to refine a model varies from dataset to dataset. ForWMT En\u2192\u2192\\rightarrow\u2192Fr, it takes around 3 days to complete 400k steps.", "To prevent overfitting, we apply dropout during training with a scheme similarto [44]. For the WMT En\u2192\u2192\\rightarrow\u2192Fr and En\u2192\u2192\\rightarrow\u2192De datasets, we set thedropout probability to be 0.20.20.20.2 and 0.30.30.30.3 respectively. Due to various technical reasons, dropout isonly applied during the ML training phase, not during the RL refinement phase.", "The exact hyper-parameters vary from dataset to dataset and from modelto model. For the WMT En\u2192\u2192\\rightarrow\u2192De dataset, since it is significantlysmaller than the WMT En\u2192\u2192\\rightarrow\u2192Fr dataset, we use a higher dropoutprobability, and also train smaller models for fewer steps overall.On the production data sets, we typically do not use dropout, andwe train the models for more steps.", "The models in our experiments are word-based, character-based, mixedword-character-based or several wordpiece models with varyingvocabulary sizes.", "For the word model, we selected the most frequent 212K source words asthe source vocabulary and the most popular 80k target words as thetarget vocabulary. Words not in the source vocabulary or the targetvocabulary (unknown words) are converted into special<first_char>_UNK_<last_char>symbols. Note, in this case, there is more than one UNK (e.g., ourproduction word models have roughly 5000 different UNKs in this case). We thenuse the attention mechanism to copy a corresponding word from thesource to replace these unknown words during decoding\u00a0[37].", "The mixed word-character model is similar to the word model, except theout-of-vocabulary (OOV) words are converted into sequences ofcharacters with special delimiters around them as described in section4.2 in more detail. Inour experiments, the vocabulary size for the mixed word-charactermodel is 32K. For the pure character model, we simply split all wordsinto constituent characters, resulting typically in a few hundred basiccharacters (including special symbols appearing in the data). For thewordpiece models, we train 3 different models with vocabulary sizes of8K, 16K, and 32K.", "Table 4 summarizes our results on the WMTEn\u2192\u2192\\rightarrow\u2192Fr dataset. In this table, we also compare against otherstrong baselines without model ensembling. As can be seen from thetable, \u201cWPM-32K\u201d, a wordpiece model with a shared source and targetvocabulary of 32K wordpieces, performs well on this dataset and achieves thebest quality as well as the fastest inference speed.", "The pure character model (char input, char output) works surprisinglywell on this task, not much worse than the best wordpiece models in BLEUscore. However, these models are rather slow to train and slow to use as thesequences are much longer.", "Our best model, WPM-32K, achieves a BLEU score of 38.95. Note that thisBLEU score represents the averaged score of 8 models we trained. The maximumBLEU score of the 8 models is higher at 39.37. We point outthat our models are completely self-contained, as opposed to previous modelsreported in\u00a0[45], which depend onsome external alignment models to achieve their best results. Also note thatall our test set numbers were achieved by picking an optimal model on thedevelopment set which was then used to decode the test set.", "Note that the timing numbers for this section are obtained on CPUs, not TPUs.We use here the same CPU machine as described above, and run the decoder witha batchsize of 16 sentences in parallel and a maximum of 4 concurrenthypotheses at any time per sentence. The time per sentence is the totaldecoding time divided by the number of respective sentences in the test set.", "Similarly, the results of WMT En\u2192\u2192\\rightarrow\u2192De are presented inTable\u00a05. Again, we find that wordpiece modelsachieves the best BLEU scores.", "WMT En\u2192\u2192\\rightarrow\u2192Deis considered a more difficult task than WMT En\u2192\u2192\\rightarrow\u2192Fr asit has much less training data, and German, as a more morphologicallyrich language, needs a huge vocabulary for word models. Thus it ismore advantageous to use wordpiece or mixed word/character models,which provide a gain of more than 2 BLEU points on top of theword model and about 4 BLEU points on top of previously reported resultsin\u00a0[6, 45].Our best model, WPM-32K, achieves a BLEU score of 24.61, which is averaged over 8 runs.Consistently, on the production corpora, wordpiece models tendto be better than other models both in terms of speed and accuracy.", "The models trained in the previous section are optimized forlog-likelihood of the next step predictionwhich may not correlate well with translation quality, as discussed insection\u00a05. We use RL training to fine-tune sentence BLEU scoresafter normal maximum-likelihood training.", "The results of RL fine-tuning on the best En\u2192\u2192\\rightarrow\u2192Fr andEn\u2192\u2192\\rightarrow\u2192De models are presented inTable\u00a06, which show that fine-tuning themodels with RL can improve BLEU scores. On WMT En\u2192\u2192\\rightarrow\u2192Fr,model refinement improves BLEU score by close to 1 point. On En\u2192\u2192\\rightarrow\u2192De,RL-refinement slightly hurts the test performance even though we observe about 0.4 BLEU pointsimprovement on the development set. The results presented inTable\u00a06 are the average of 8 independent models.We also note that there is an overlap between the wins from the RL refinement and the decoderfine-tuning (i.e., the introduction of length normalization and coverage penalty).On a less fine-tuned decoder (e.g., if the decoder does beam search bylog-probability only), the win from RL would have been bigger (as is evidentfrom comparing results in Table\u00a02 andTable\u00a03).", "We ensemble 8 RL-refined models to obtain a state-of-the-artresult of 41.16 BLEU points on the WMT En\u2192\u2192\\rightarrow\u2192Fr dataset. Our results arereported in Table\u00a07.", "We ensemble 8 RL-refined models to obtain a state-of-the-artresult of 26.30 BLEU points on the WMT En\u2192\u2192\\rightarrow\u2192De dataset. Our results arereported in Table\u00a08.", "Finally, to better understand the quality of our models and the effectof RL refinement, we carried out a four-way side-by-side humanevaluation to compare our NMT translations against the reference translationsand the best phrase-based statistical machine translations.During the side-by-side comparison,humans are asked to rate four translations given a source sentence.The four translations are:1) the best phrase-based translations as downloadedfrom http://matrix.statmt.org/systems/show/2065,2) an ensemble of 8 ML-trained models,3) an ensemble of 8 ML-trained and then RL-refined models, and4) reference human translations as taken directly from newstest2014,Our results are presented in Table\u00a09.", "The results show that even though RL refinement can achieve betterBLEU scores, it barely improves the human impression of the translationquality. This could be due to a combination of factors including: 1)the relatively small sample size for the experiment (only 500examples for side-by-side), 2) the improvement in BLEU score by RLis relatively small after model ensembling (0.81), which may be at ascale that human side-by-side evaluations are insensitive to, and 3) thepossiblemismatch between BLEU as a metric and real translation quality as perceived byhuman raters. Table\u00a011 contains some exampletranslations from PBMT, \"NMT before RL\" and \"Human\", along with theside-by-side scores that human raters assigned to each translation(some of which we disagree with, see the table caption).", "We have carried out extensive experiments on many Google-internal productiondata sets.As the experiments above cast doubt on whether RL improves the real translationquality or simply the BLEU metric, RL-based modelrefinement is not used during these experiments.Given the larger volume of training data available in the Google corpora,dropout is also not needed in these experiments.", "In this section we describe our experiments with human perception of thetranslation quality. We asked human raters to rate translations ina three-way side-by-side comparison. The three sides are from: 1) translationsfrom the production phrase-based statistical translation system used by Google,2) translations from our GNMT system, and 3) translations by humans fluent inboth languages. Reported here in Table 10 are averaged ratedscores for English\u2194\u2194\\leftrightarrow\u2194 French, English \u2194\u2194\\leftrightarrow\u2194 Spanish andEnglish \u2194\u2194\\leftrightarrow\u2194Chinese. All the GNMT models are wordpiece models, without modelensembling, and use a shared source and target vocabulary with 32K wordpieces.On each pair of languages, the evaluation data consist of 500randomly sampled sentences from Wikipedia and news websites, and thecorresponding human translations to the target language. Theresults show that our model reduces translation errors by more than 60%compared to the PBMT model on these major pairs of languages. A typicaldistribution of side-by-side scores is shown in Figure 6.", "As expected, on this metric the GNMT system improves also compared to the PBMTsystem. In some cases human and GNMT translations arenearly indistinguishable on the relatively simplistic and isolated sentencessampled from Wikipedia and news articles for this experiment. Note that we haveobserved that human raters, even though fluent in both languages, do notnecessarily fully understand each randomly sampled sentence sufficiently andhence cannot necessarily generate the best possible translation or rate agiven translation accurately. Also note that, although the scale for thescores goes from0 (complete nonsense) to 6 (perfect translation) the human translationsget an imperfect score of only around 5 in Table 10, which showspossible ambiguities in the translations and also possibly non-calibrated ratersand translators with a varying level of proficiency.", "Testing our GNMT system on particularly difficult translation cases and longerinputs than just single sentences is the subject of future work.", "In this paper, we describe in detail the implementation of Google\u2019s NeuralMachine Translation (GNMT) system, including all the techniques that are criticalto its accuracy, speed, and robustness.On the public WMT\u201914 translation benchmark, our system\u2019s translation qualityapproaches or surpasses all currently published results.More importantly, we also show that our approach carries over to much largerproduction data sets, which have several orders of magnitude more data, todeliver high quality translations.", "Our key findings are:1) that wordpiece modeling effectively handles open vocabulariesand the challenge of morphologically rich languages for translation qualityand inference speed,2) that a combination of model and data parallelism can be used to efficientlytrain state-of-the-art sequence-to-sequence NMT models in roughly a week,3) that model quantization drastically accelerates translation inference,allowing the use of these large models in a deployed production environment, and4) that many additional details like length-normalization, coverage penalties,and similar are essential to making NMT systems work well on real data.", "Using human-rated side-by-side comparison as a metric, we show that ourGNMT system approaches the accuracy achieved by average bilingual humantranslators on some of our test sets.In particular, compared to the previous phrase-based production system,this GNMT system delivers roughly a 60% reduction in translation errorson several popular language pairs."], "figure_types": {"c6850869aa5e78a107c378d2e8bfa39633158c0c/11-Figure4-1.png": "plot", "c6850869aa5e78a107c378d2e8bfa39633158c0c/13-Table2-1.png": "table", "c6850869aa5e78a107c378d2e8bfa39633158c0c/13-Table3-1.png": "table", "c6850869aa5e78a107c378d2e8bfa39633158c0c/15-Figure5-1.png": "plot", "c6850869aa5e78a107c378d2e8bfa39633158c0c/16-Table4-1.png": "table", "c6850869aa5e78a107c378d2e8bfa39633158c0c/16-Table5-1.png": "table", "c6850869aa5e78a107c378d2e8bfa39633158c0c/17-Table7-1.png": "table", "c6850869aa5e78a107c378d2e8bfa39633158c0c/17-Table8-1.png": "table", "c6850869aa5e78a107c378d2e8bfa39633158c0c/18-Table10-1.png": "table", "c6850869aa5e78a107c378d2e8bfa39633158c0c/18-Table9-1.png": "table", "c6850869aa5e78a107c378d2e8bfa39633158c0c/19-Figure6-1.png": "plot", "c6850869aa5e78a107c378d2e8bfa39633158c0c/23-Table11-1.png": "table", "c6850869aa5e78a107c378d2e8bfa39633158c0c/4-Figure1-1.png": "schematic", "c6850869aa5e78a107c378d2e8bfa39633158c0c/5-Figure2-1.png": "schematic", "c6850869aa5e78a107c378d2e8bfa39633158c0c/6-Figure3-1.png": "schematic"}}, "1508.04025": {"paper_id": "paper_128", "title": "Effective Approaches to Attention-based Neural Machine Translation", "arxiv_url": "https://arxiv.org/abs/1508.04025", "s2orc_url": "https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0", "all_figures_tables": {"93499a7c7f699b6630a86fad964536f9423bb6d0/1-Figure1-1.png": "Figure 1: Neural machine translation \u2013 a stacking recurrent architecture for translating a source sequence A B C D into a target sequence X Y Z. Here, &lt;eos&gt;marks the end of a sentence.", "93499a7c7f699b6630a86fad964536f9423bb6d0/3-Figure2-1.png": "Figure 2: Global attentional model \u2013 at each time step t, the model infers a variable-length alignment weight vector at based on the current target state ht and all source states h\u0304s. A global context vector ct is then computed as the weighted average, according to at, over all the source states.", "93499a7c7f699b6630a86fad964536f9423bb6d0/4-Figure3-1.png": "Figure 3: Local attention model \u2013 the model first predicts a single aligned position pt for the current target word. A window centered around the source position pt is then used to compute a context vector ct, a weighted average of the source hidden states in the window. The weights at are inferred from the current target state ht and those source states h\u0304s in the window.", "93499a7c7f699b6630a86fad964536f9423bb6d0/5-Figure4-1.png": "Figure 4: Input-feeding approach \u2013 Attentional vectors h\u0303t are fed as inputs to the next time steps to inform the model about past alignment decisions.", "93499a7c7f699b6630a86fad964536f9423bb6d0/6-Table1-1.png": "Table 1: WMT\u201914 English-German results \u2013 shown are the perplexities (ppl) and the tokenized BLEU scores of various systems on newstest2014. We highlight the best system in bold and give progressive improvements in italic between consecutive systems. local-p referes to the local attention with predictive alignments. We indicate for each attention model the alignment score function used in pararentheses.", "93499a7c7f699b6630a86fad964536f9423bb6d0/6-Table2-1.png": "Table 2: WMT\u201915 English-German results \u2013 NIST BLEU scores of the existing WMT\u201915 SOTA system and our best one on newstest2015.", "93499a7c7f699b6630a86fad964536f9423bb6d0/7-Figure5-1.png": "Figure 5: Learning curves \u2013 test cost (ln perplexity) on newstest2014 for English-German NMTs as training progresses.", "93499a7c7f699b6630a86fad964536f9423bb6d0/7-Figure6-1.png": "Figure 6: Length Analysis \u2013 translation qualities of different systems as sentences become longer.", "93499a7c7f699b6630a86fad964536f9423bb6d0/7-Table3-1.png": "Table 3: WMT\u201915 German-English results \u2013 performances of various systems (similar to Table 1). The base system already includes source reversing on which we add global attention, dropout, input feeding, and unk replacement.", "93499a7c7f699b6630a86fad964536f9423bb6d0/8-Table4-1.png": "Table 4: Attentional Architectures \u2013 performances of different attentional models. We trained two local-m (dot) models; both have ppl &gt; 7.0.", "93499a7c7f699b6630a86fad964536f9423bb6d0/8-Table6-1.png": "Table 6: AER scores \u2013 results of various models on the RWTH English-German alignment data.", "93499a7c7f699b6630a86fad964536f9423bb6d0/9-Table5-1.png": "Table 5: Sample translations \u2013 for each example, we show the source (src), the human translation (ref), the translation from our best model (best), and the translation of a non-attentional model (base). We italicize some correct translation segments and highlight a few wrong ones in bold."}, "referred_figures_tables": [["93499a7c7f699b6630a86fad964536f9423bb6d0/1-Figure1-1.png"], ["93499a7c7f699b6630a86fad964536f9423bb6d0/6-Table1-1.png"], ["93499a7c7f699b6630a86fad964536f9423bb6d0/3-Figure2-1.png", "93499a7c7f699b6630a86fad964536f9423bb6d0/4-Figure3-1.png"], ["93499a7c7f699b6630a86fad964536f9423bb6d0/3-Figure2-1.png"]], "question_id": [1, 3, 9, 10], "question": ["What kinds of domain knowledge do the authors refer to in this context?", "What are the pros and cons of a global approach and a local approach?", "In this sentence, do the current target state and all source states mean hidden states of the encoder?", "What does \"variable-length alignment\" mean?"], "question_section": ["Introduction", "Introduction", "Attention-based Models", "Attention-based Models"], "question_trigger_sentence": ["NMT is appealing since it requires minimal domain knowledge and is conceptually simple.", "In this work, we design, with simplicity and effectiveness in mind, two novel types of attention-based models: a global approach in which all source words are attended and a local one whereby only a subset of source words are considered at a time.", "Global attentional model \u2013 at each time step t, the model infers a variable-length alignment weight vector at based on the current target state h_t and all source states \\bar{h_s}", "In this model type, a variable-length alignment vector a_t, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state h_t with each source hidden state \\bar{h_s}"], "question_type": ["Shallow question", "Shallow question", "Shallow question", "Testing question"], "evidential_info": [[{"context": "Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al., 2015). NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by Luong et al. (2015) reads through all the source words until the end-of-sentence symbol <eos> is reached. It then starts emitting one target word at a time, as illustrated in Figure 1.", "rationale": "Neural Machine Translation achieves state-of-the-art in large-scale translation tasks, and it requires minimal translation domain knowledge, as the model by Luong et al. (2015) reads through all source words until the end of sentence, then starts to emit one target word a time as illustrated in Figure 1."}], [{"context": "The global attention has a drawback that it has to attend to all words on thesource side for each target word, which is expensive and can potentially render it impractical totranslate longer sequences, e.g., paragraphs or documents.To address this deficiency, we propose a local attentional mechanism thatchooses to focus only on a small subset of the source positions per target word.", "rationale": "A drawback of the global attention is it has to attend to all words on the source side for each target word, which is expensive and potentially will render it impractical to translate longer sequences. so authors propose a local attention mechanism to address this problem, which focuses only on a small subset of the source positions per target word."}, {"context": "Our local attention mechanism selectively focuses on a small window ofcontext and is differentiable. This approach has an advantage of avoiding the expensive computation incurred inthe soft attention and at the same time, is easier to train than the hardattention approach.In concrete details, the model first generates an aligned position p_{t} for each target word at time t. Thecontext vector \\mbox{\\boldmath{$c$}}_{t} is then derived as a weighted average over the set of source hidden states within the window [p_{t}-D,p_{t}+D]; D isempirically selected.888If the window crosses the sentence boundaries, wesimply ignore the outside part and consider words in the window. Unlike the global approach, the local alignment vector \\mbox{\\boldmath{$a$}}_{t} is now fixed-dimensional, i.e., \\in\\mathbb{R}^{2D+1}. We consider two variants of the model as below.", "rationale": "Global approach resembles the model of [Bahdanau et al.,  2015] but is simpler architecturally. and the local approach computationally less expensive, also the local approach is differentiable almost everywhere, making it easier to implement and train."}, {"context": "In this work, we design, with simplicity and effectiveness in mind, two noveltypes of attention-based models: a global approach in which all sourcewords are attended and a local one whereby only a subset of source wordsare considered at a time. The former approach resembles the model of[Bahdanau et al., 2015] but is simpler architecturally. The latter can be viewed as aninteresting blend between the hard and soft attention modelsproposed in [Xu et al., 2015]: it is computationally less expensive than theglobal model or the soft attention; at the same time, unlike the hard attention,the local attention isdifferentiable almost everywhere, making it easier to implement andtrain.222There is a recent work by ?), which is verysimilar to our local attention and applied to the image generation task.However, as we detail later, our model is much simpler and can achieve good performance for NMT. Besides, we also examine variousalignment functions for our attention-based models.", "rationale": "Authors local approach selectively focuses on a small window of context and is differentiable, thereby has the advantage of avoiding the expensive computation incurred in the soft attention and easier to train than hard attention."}, {"context": "As shown in Table\u00a01, we achieve progressive improvements when(a) reversing the source sentence, +1.3 BLEU, as proposed in [Sutskever et al., 2014]and (b) using dropout, +1.4 BLEU. On top of that, (c) the globalattention approach gives a significant boost of +2.8 BLEU, makingour model slightly better than the base attentional system of?) (row RNNSearch). When (d) using the input-feedingapproach, we seize another notable gain of +1.3 BLEU and outperform theirsystem. The local attention model with predictive alignments (row local-p) provesto be even better, giving us a further improvement of +0.9 BLEU on top of theglobal attention model.It is interesting to observe the trend previously reported in[Luong et al., 2015] that perplexity strongly correlates with translation quality.In total, we achieve a significant gain of5.0 BLEU points over the non-attentional baseline, which already includesknown techniques such as source reversing and dropout.", "rationale": "Local attention model achieve lower AERs than global one."}, {"context": "We also found that the alignments produced by local attention models achievelower AERs than those of the global one. The AER obtained by the ensemble, whilegood, is not better than the local-m AER, suggesting the well-knownobservation that AER and translation scores are not well correlated [Fraser and Marcu, 2007].We show some alignment visualizations in Appendix\u00a0A.", "rationale": "Global attention gives a significant boost of +2.8 BLEU making it better than the base attention system. And the local attention model gives further improvement of +0.9 BLEU on top of the global attention model."}], [{"context": "The idea of a global attentional model is to consider all the hidden states ofthe encoder when deriving the context vector c_{t}. In this model type, avariable-length alignment vector \\mbox{\\boldmath{$a$}}_{t}, whose size equals the number of timesteps on the source side, is derived by comparing the current target hiddenstate \\mbox{\\boldmath{$h$}}_{t} with each source hidden state \\mbox{\\boldmath{$\\bar{h}$}}_{s}:\\displaystyle\\mbox{\\boldmath{$a$}}_{t}(s)\\displaystyle=\\operatorname{align}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s})(7)\\displaystyle=\\frac{\\exp\\left(\\operatorname{score}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s})\\right)}{\\sum_{s^{\\prime}}\\exp\\left(\\operatorname{score}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s^{\\prime}})\\right)}Here, \\operatorname{score} is referred as a content-based function for which we consider three differentalternatives:\\operatorname{score}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s})\\!=\\!\\begin{cases}\\mbox{\\boldmath{$h$}}_{t}^{\\top}\\mbox{\\boldmath{$\\bar{h}$}}_{s}&\\mbox{{\\it dot}}\\\\\\mbox{\\boldmath{$h$}}_{t}^{\\top}\\mbox{\\boldmath{$W_{a}$}}\\mbox{\\boldmath{$\\bar{h}$}}_{s}&\\mbox{{\\it general}}\\\\\\mbox{\\boldmath{$v$}}_{a}^{\\top}\\tanh\\left(\\mbox{\\boldmath{$W_{a}$}}[\\mbox{\\boldmath{$h$}}_{t};\\mbox{\\boldmath{$\\bar{h}$}}_{s}]\\right)&\\mbox{{\\it concat}}\\end{cases}", "rationale": "The idea behind the global attentional model is to consider all the hidden states of the encoder when deriving the context vector ct."}, {"context": "Figure 2: Global attentional model \u2013 at each time step t, the model infers a variable-length alignment weight vector at based on the current target state ht and all source states h\u00afs. A global context vector ct is then computed as the weighted average, according to at, over all the source states.", "rationale": "In the global attentional model, the model infers a variable-length alignment weight vector at each time step t based on the current target state ht and all source states h\u00afs."}, {"context": "Figure 3: Local attention model \u2013 the model first predicts a single aligned position pt for the current target word. A window centered around the source position pt is then used to compute a context vector ct, a weighted average of the source hidden states in the window. The weights at are inferred from the current target state ht and those source states h\u00afs in the window.", "rationale": "In the local attention model, the model predicts a single aligned position pt for the current target word. then a window centered around the source position pt is used to compute a context vector ct. the weights at inferred from the current target state ht and the source states h\u00afs in the windows."}], [{"context": "The idea of a global attentional model is to consider all the hidden states ofthe encoder when deriving the context vector c_{t}. In this model type, avariable-length alignment vector \\mbox{\\boldmath{$a$}}_{t}, whose size equals the number of timesteps on the source side, is derived by comparing the current target hiddenstate \\mbox{\\boldmath{$h$}}_{t} with each source hidden state \\mbox{\\boldmath{$\\bar{h}$}}_{s}:\\displaystyle\\mbox{\\boldmath{$a$}}_{t}(s)\\displaystyle=\\operatorname{align}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s})(7)\\displaystyle=\\frac{\\exp\\left(\\operatorname{score}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s})\\right)}{\\sum_{s^{\\prime}}\\exp\\left(\\operatorname{score}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s^{\\prime}})\\right)}Here, \\operatorname{score} is referred as a content-based function for which we consider three differentalternatives:\\operatorname{score}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s})\\!=\\!\\begin{cases}\\mbox{\\boldmath{$h$}}_{t}^{\\top}\\mbox{\\boldmath{$\\bar{h}$}}_{s}&\\mbox{{\\it dot}}\\\\\\mbox{\\boldmath{$h$}}_{t}^{\\top}\\mbox{\\boldmath{$W_{a}$}}\\mbox{\\boldmath{$\\bar{h}$}}_{s}&\\mbox{{\\it general}}\\\\\\mbox{\\boldmath{$v$}}_{a}^{\\top}\\tanh\\left(\\mbox{\\boldmath{$W_{a}$}}[\\mbox{\\boldmath{$h$}}_{t};\\mbox{\\boldmath{$\\bar{h}$}}_{s}]\\right)&\\mbox{{\\it concat}}\\end{cases}", "rationale": "In the global attention model type, a variable-length alignment vector at is derived by comparing the current target hidden state ht with each source hidden state  hs as next equation, noting that the size of it equals the number of time steps on the source side."}, {"context": "Figure 2: Global attentional model \u2013 at each time step t, the model infers a variable-length alignment weight vector at based on the current target state ht and all source states h\u00afs. A global context vector ct is then computed as the weighted average, according to at, over all the source states.", "rationale": "In the global attentional model, the model infers a variable-length alignment weight vector at each time step t based on the current target state ht and all source states h\u00afs."}]], "composition": ["Authors refer to translation domain knowledge, as they refer to Luong's et al. (2015) Neural Machine Translation as it reads through all source words until the end of a sentence, then starts translation by emitting one target word at a time as illustrated in Figure 1.", "A drawback of the global attention is it had to attend to all words on the source side for each target word, which is expensive and potentially will render it impractical to translate longer sequences, and despite that global attention gives a significant boost of +2.8 BLEU making it better than the base attention system, but the local approach gave further improvement of +0.9 BLEU on top of the global attention model. Also the local approach achieved lower AERs. Not to mention that the local approach is simpler, easier to implement and train, and computationally less expensive. as it focus only on a small subset of the source positions per target word.", "A possible answer is yes as a global attention model considers all the hidden states of the encoder when deriving the context. However, it's not clear which sentence the questioner refers to and the question needs more elaboration.", "a variable-length alignment is a vector derived by comparing the current target hidden state with each source hidden state, and the size of it equals the number of time steps on the source side as it's explained in Figure 2."], "Is_figure_in_evidence": [true, false, true, true], "Is_table_in_evidence": [false, true, false, false], "question_key": ["298", "300", "305", "306"], "passages": ["Neural Machine Translation (NMT) achieved state-of-the-art performances inlarge-scale translation tasks such as from English to French [Luong et al., 2015] andEnglish to German [Jean et al., 2015]. NMT is appealing since it requires minimaldomain knowledge and is conceptually simple. The model by ?) reads through all the source words until the end-of-sentence symbol <<<eos>>> is reached. It then starts emitting one target word at a time, as illustrated in Figure\u00a01. NMT is often a large neural network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences. This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint. Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT [Koehn et al., 2003].", "In parallel, the concept of \u201cattention\u201d has gained popularity recently intraining neural networks, allowing models to learn alignments between differentmodalities, e.g., between image objects and agent actions in the dynamic controlproblem [Mnih et al., 2014], between speech frames and text in the speech recognitiontask [jan14], or between visual features of a picture and its textdescription in the image caption generation task [Xu et al., 2015]. In the context ofNMT, ?) has successfully applied such attentional mechanism tojointly translate and align words. To the best of our knowledge, there has notbeen any other work exploring the use of attention-based architectures for NMT.", "In this work, we design, with simplicity and effectiveness in mind, two noveltypes of attention-based models: a global approach in which all sourcewords are attended and a local one whereby only a subset of source wordsare considered at a time. The former approach resembles the model of[Bahdanau et al., 2015] but is simpler architecturally. The latter can be viewed as aninteresting blend between the hard and soft attention modelsproposed in [Xu et al., 2015]: it is computationally less expensive than theglobal model or the soft attention; at the same time, unlike the hard attention,the local attention isdifferentiable almost everywhere, making it easier to implement andtrain.222There is a recent work by ?), which is verysimilar to our local attention and applied to the image generation task.However, as we detail later, our model is much simpler and can achieve good performance for NMT. Besides, we also examine variousalignment functions for our attention-based models.", "Experimentally, we demonstrate that both of our approaches areeffective in the WMT translation tasks between English and German in bothdirections. Our attentional models yield a boost of up to 5.0 BLEU overnon-attentional systems which already incorporate known techniques such asdropout. For English to German translation, we achieve new state-of-the-art(SOTA)results for both WMT\u201914 and WMT\u201915, outperforming previous SOTA systems, backed byNMT models and n\ud835\udc5bnitalic_n-gram LM rerankers, by more than 1.0 BLEU. We conductextensive analysis to evaluate our models in terms of learning, the ability tohandle long sentences, choices of attentional architectures, alignment quality, and translationoutputs.", "A neural machine translation system is a neural network that directly models the conditional probability p(y|x)\ud835\udc5dconditional\ud835\udc66\ud835\udc65p(y|x)italic_p ( italic_y | italic_x ) of translatinga source sentence, x1,\u2026,xnsubscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc5bx_{1},\\ldots,x_{n}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, to a target sentence, y1,\u2026,ymsubscript\ud835\udc661\u2026subscript\ud835\udc66\ud835\udc5ay_{1},\\ldots,y_{m}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT.333All sentences are assumed to terminate with a special \u201cend-of-sentence\u201d token <<<eos>>>.A basic form of NMT consists of two components: (a) an encoder which computes a representation \ud835\udc94\ud835\udc94sbold_italic_s for each source sentence and (b) a decoder which generates one target word at a time and hence decomposes the conditional probability as:log\u2061p(y|x)=\u2211j=1mlog\u2061p(yj|y<j,\ud835\udc94)\ud835\udc5dconditional\ud835\udc66\ud835\udc65superscriptsubscript\ud835\udc571\ud835\udc5a\ud835\udc5dconditionalsubscript\ud835\udc66\ud835\udc57subscript\ud835\udc66absent\ud835\udc57\ud835\udc94\\log p(y|x)=\\sum_{j=1}^{m}\\nolimits\\log p\\left(y_{j}|y_{<j},\\mbox{\\boldmath{$s$}}\\right)roman_log italic_p ( italic_y | italic_x ) = \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT roman_log italic_p ( italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT < italic_j end_POSTSUBSCRIPT , bold_italic_s )(1)", "A natural choice to model such a decomposition in the decoder is to use a recurrent neural network (RNN) architecture, which most of the recent NMT work such as [Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Cho et al., 2014, Bahdanau et al., 2015, Luong et al., 2015, Jean et al., 2015] have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation \ud835\udc94\ud835\udc94sbold_italic_s.", "?) used an RNN with the standard hidden unit for the decoder and aconvolutional neural network for encoding the source sentence representation. Onthe other hand, both ?) and ?) stackedmultiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit forboth the encoder and the decoder. ?), ?), and?) all adopted a different version of the RNN with anLSTM-inspired hidden unit, the gated recurrent unit (GRU), for bothcomponents.444They all used a single RNN layer except for the latter twoworks which utilized a bidirectional RNN for the encoder.", "In more detail, one can parameterize the probability of decoding each word yjsubscript\ud835\udc66\ud835\udc57y_{j}italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT as:p(yj|y<j,\ud835\udc94)=softmax\u2061(g(\ud835\udc89j))\ud835\udc5dconditionalsubscript\ud835\udc66\ud835\udc57subscript\ud835\udc66absent\ud835\udc57\ud835\udc94softmax\ud835\udc54subscript\ud835\udc89\ud835\udc57p\\left(y_{j}|y_{<j},\\mbox{\\boldmath{$s$}}\\right)=\\operatorname{softmax}\\left(g\\left(\\mbox{\\boldmath{$h$}}_{j}\\right)\\right)italic_p ( italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT < italic_j end_POSTSUBSCRIPT , bold_italic_s ) = roman_softmax ( italic_g ( bold_italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) )(2)with g\ud835\udc54gitalic_g being the transformation function that outputs a vocabulary-sizedvector.555One can provide g\ud835\udc54gitalic_g with other inputs such as the currentlypredicted word yjsubscript\ud835\udc66\ud835\udc57y_{j}italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT as in [Bahdanau et al., 2015]. Here, \ud835\udc89jsubscript\ud835\udc89\ud835\udc57\\mbox{\\boldmath{$h$}}_{j}bold_italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is the RNN hiddenunit, abstractly computed as:\ud835\udc89j=f(\ud835\udc89j\u22121,\ud835\udc94),subscript\ud835\udc89\ud835\udc57\ud835\udc53subscript\ud835\udc89\ud835\udc571\ud835\udc94\\mbox{\\boldmath{$h$}}_{j}=f(\\mbox{\\boldmath{$h$}}_{j-1},\\mbox{\\boldmath{$s$}}),bold_italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_f ( bold_italic_h start_POSTSUBSCRIPT italic_j - 1 end_POSTSUBSCRIPT , bold_italic_s ) ,(3)where f\ud835\udc53fitalic_f computes the current hidden state given the previous hidden state andcan be either a vanilla RNN unit, a GRU, or an LSTM unit. In [Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Cho et al., 2014, Luong et al., 2015], the source representation \ud835\udc94\ud835\udc94sbold_italic_s is only used once to initialize the decoder hidden state. On the other hand, in [Bahdanau et al., 2015, Jean et al., 2015] and this work, \ud835\udc94\ud835\udc94sbold_italic_s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next.", "In this work, following [Sutskever et al., 2014, Luong et al., 2015], we use the stacking LSTM architecture for our NMT systems, as illustrated in Figure\u00a01.We use the LSTM unit defined in [Zaremba et al., 2015]. Our training objective is formulated as follows:Jt=\u2211(x,y)\u2208\ud835\udd3b\u2212log\u2061p(y|x)subscript\ud835\udc3d\ud835\udc61subscript\ud835\udc65\ud835\udc66\ud835\udd3b\ud835\udc5dconditional\ud835\udc66\ud835\udc65J_{t}=\\sum_{(x,y)\\in\\mathbb{D}}\\nolimits-\\log p(y|x)italic_J start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT ( italic_x , italic_y ) \u2208 blackboard_D end_POSTSUBSCRIPT - roman_log italic_p ( italic_y | italic_x )(4)with \ud835\udd3b\ud835\udd3b\\mathbb{D}blackboard_D being our parallel training corpus.", "Our various attention-based models are classifed into two broad categories, global and local. These classes differ in terms of whether the \u201cattention\u201d is placed on all source positions or on only a few source positions. We illustrate these two model types in Figure\u00a02 and 3 respectively.", "Common to these two types of models is the fact that at each time step t\ud835\udc61titalic_t in the decoding phase, both approaches first take as input the hidden state \ud835\udc89tsubscript\ud835\udc89\ud835\udc61\\mbox{\\boldmath{$h$}}_{t}bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at the top layer of a stacking LSTM. The goal is then to derive a context vector \ud835\udc84tsubscript\ud835\udc84\ud835\udc61\\mbox{\\boldmath{$c$}}_{t}bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT that captures relevant source-side information to help predict the current target word ytsubscript\ud835\udc66\ud835\udc61y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. While these models differ in how the context vector \ud835\udc84tsubscript\ud835\udc84\ud835\udc61\\mbox{\\boldmath{$c$}}_{t}bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is derived, they share the same subsequent steps.", "Specifically, given the target hidden state \ud835\udc89tsubscript\ud835\udc89\ud835\udc61\\mbox{\\boldmath{$h$}}_{t}bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and the source-side context vector \ud835\udc84tsubscript\ud835\udc84\ud835\udc61\\mbox{\\boldmath{$c$}}_{t}bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, we employ a simple concatenation layer to combine the information from both vectors to produce an attentional hidden state as follows:\ud835\udc89~t=tanh\u2061(\ud835\udc7e\ud835\udc84[\ud835\udc84t;\ud835\udc89t])subscriptbold-~\ud835\udc89\ud835\udc61subscript\ud835\udc7e\ud835\udc84subscript\ud835\udc84\ud835\udc61subscript\ud835\udc89\ud835\udc61\\mbox{\\boldmath{$\\tilde{h}$}}_{t}=\\tanh(\\mbox{\\boldmath{$W_{c}$}}[\\mbox{\\boldmath{$c$}}_{t};\\mbox{\\boldmath{$h$}}_{t}])overbold_~ start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = roman_tanh ( bold_italic_W start_POSTSUBSCRIPT bold_italic_c end_POSTSUBSCRIPT [ bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] )(5)", "The attentional vector \ud835\udc89~tsubscriptbold-~\ud835\udc89\ud835\udc61\\mbox{\\boldmath{$\\tilde{h}$}}_{t}overbold_~ start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is then fed through the softmax layer to produce the predictive distribution formulated as:p(yt|y<t,x)=softmax\u2061(\ud835\udc7e\ud835\udc94\ud835\udc89~t)\ud835\udc5dconditionalsubscript\ud835\udc66\ud835\udc61subscript\ud835\udc66absent\ud835\udc61\ud835\udc65softmaxsubscript\ud835\udc7e\ud835\udc94\ud835\udc89~\ud835\udc61p(y_{t}|y_{<t},x)=\\operatorname{softmax}(\\mbox{\\boldmath{$W_{s}$}}\\mbox{\\boldmath{$\\tilde{h}$}}_{t})italic_p ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT , italic_x ) = roman_softmax ( roman_Ws ~h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(6)", "We now detail how each model type computes the source-side context vector \ud835\udc84tsubscript\ud835\udc84\ud835\udc61\\mbox{\\boldmath{$c$}}_{t}bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.", "The idea of a global attentional model is to consider all the hidden states ofthe encoder when deriving the context vector ctsubscript\ud835\udc50\ud835\udc61c_{t}italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. In this model type, avariable-length alignment vector \ud835\udc82tsubscript\ud835\udc82\ud835\udc61\\mbox{\\boldmath{$a$}}_{t}bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, whose size equals the number of timesteps on the source side, is derived by comparing the current target hiddenstate \ud835\udc89tsubscript\ud835\udc89\ud835\udc61\\mbox{\\boldmath{$h$}}_{t}bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with each source hidden state \ud835\udc89\u00afssubscriptbold-\u00af\ud835\udc89\ud835\udc60\\mbox{\\boldmath{$\\bar{h}$}}_{s}overbold_\u00af start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT:\ud835\udc82t(s)subscript\ud835\udc82\ud835\udc61\ud835\udc60\\displaystyle\\mbox{\\boldmath{$a$}}_{t}(s)bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_s )=align\u2061(\ud835\udc89t,\ud835\udc89\u00afs)absentalignsubscript\ud835\udc89\ud835\udc61subscriptbold-\u00af\ud835\udc89\ud835\udc60\\displaystyle=\\operatorname{align}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s})= roman_align ( bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , overbold_\u00af start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )(7)=exp\u2061(score\u2061(\ud835\udc89t,\ud835\udc89\u00afs))\u2211s\u2032exp\u2061(score\u2061(\ud835\udc89t,\ud835\udc89\u00afs\u2032))absentscoresubscript\ud835\udc89\ud835\udc61subscriptbold-\u00af\ud835\udc89\ud835\udc60subscriptsuperscript\ud835\udc60\u2032scoresubscript\ud835\udc89\ud835\udc61subscriptbold-\u00af\ud835\udc89superscript\ud835\udc60\u2032\\displaystyle=\\frac{\\exp\\left(\\operatorname{score}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s})\\right)}{\\sum_{s^{\\prime}}\\exp\\left(\\operatorname{score}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s^{\\prime}})\\right)}= divide start_ARG roman_exp ( roman_score ( bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , overbold_\u00af start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_exp ( roman_score ( bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , overbold_\u00af start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) ) end_ARGHere, scorescore\\operatorname{score}roman_score is referred as a content-based function for which we consider three differentalternatives:score\u2061(\ud835\udc89t,\ud835\udc89\u00afs)={\ud835\udc89t\u22a4\ud835\udc89\u00afs\ud835\udc51\ud835\udc5c\ud835\udc61\ud835\udc89t\u22a4\ud835\udc7e\ud835\udc82\ud835\udc89\u00afs\ud835\udc54\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc59\ud835\udc97a\u22a4tanh\u2061(\ud835\udc7e\ud835\udc82[\ud835\udc89t;\ud835\udc89\u00afs])\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61scoresubscript\ud835\udc89\ud835\udc61subscriptbold-\u00af\ud835\udc89\ud835\udc60casessuperscriptsubscript\ud835\udc89\ud835\udc61topsubscriptbold-\u00af\ud835\udc89\ud835\udc60\ud835\udc51\ud835\udc5c\ud835\udc61superscriptsubscript\ud835\udc89\ud835\udc61topsubscript\ud835\udc7e\ud835\udc82\ud835\udc89\u00af\ud835\udc60\ud835\udc54\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc59superscriptsubscript\ud835\udc97\ud835\udc4etopsubscript\ud835\udc7e\ud835\udc82subscript\ud835\udc89\ud835\udc61subscriptbold-\u00af\ud835\udc89\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61\\operatorname{score}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s})\\!=\\!\\begin{cases}\\mbox{\\boldmath{$h$}}_{t}^{\\top}\\mbox{\\boldmath{$\\bar{h}$}}_{s}&\\mbox{{\\it dot}}\\\\\\mbox{\\boldmath{$h$}}_{t}^{\\top}\\mbox{\\boldmath{$W_{a}$}}\\mbox{\\boldmath{$\\bar{h}$}}_{s}&\\mbox{{\\it general}}\\\\\\mbox{\\boldmath{$v$}}_{a}^{\\top}\\tanh\\left(\\mbox{\\boldmath{$W_{a}$}}[\\mbox{\\boldmath{$h$}}_{t};\\mbox{\\boldmath{$\\bar{h}$}}_{s}]\\right)&\\mbox{{\\it concat}}\\end{cases}roman_score ( bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , overbold_\u00af start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = { start_ROW start_CELL bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT overbold_\u00af start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_CELL start_CELL dot end_CELL end_ROW start_ROW start_CELL bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT roman_Wa \u00afh start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_CELL start_CELL general end_CELL end_ROW start_ROW start_CELL bold_italic_v start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT roman_tanh ( bold_italic_W start_POSTSUBSCRIPT bold_italic_a end_POSTSUBSCRIPT [ bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; overbold_\u00af start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ] ) end_CELL start_CELL concat end_CELL end_ROW", "Besides, in our early attempts to build attention-based models, we usea location-based function in which the alignment scores arecomputed from solely the target hidden state \ud835\udc89tsubscript\ud835\udc89\ud835\udc61\\mbox{\\boldmath{$h$}}_{t}bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT asfollows:\ud835\udc82t=softmax\u2061(\ud835\udc7e\ud835\udc82\ud835\udc89t)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5bsubscript\ud835\udc82\ud835\udc61softmaxsubscript\ud835\udc7e\ud835\udc82\ud835\udc89\ud835\udc61\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\ud835\udc59\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\\mbox{\\boldmath{$a$}}_{t}=\\operatorname{softmax}(\\mbox{\\boldmath{$W_{a}$}}\\mbox{\\boldmath{$h$}}_{t})\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{ }\\mbox{{\\it location}}bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = roman_softmax ( roman_Wa roman_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) italic_location(8)Given the alignment vector as weights, thecontext vector ctsubscript\ud835\udc50\ud835\udc61c_{t}italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is computed as the weighted average over all the source hidden states.666Eq.\u00a0(8) implies thatall alignment vectors \ud835\udc82tsubscript\ud835\udc82\ud835\udc61\\mbox{\\boldmath{$a$}}_{t}bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are of the same length. For short sentences, we onlyuse the top part of \ud835\udc82tsubscript\ud835\udc82\ud835\udc61\\mbox{\\boldmath{$a$}}_{t}bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and for long sentences, we ignore words near the end.", "Comparison to [Bahdanau et al., 2015] \u2013While our global attention approach is similar in spirit to the model proposedby ?), there are several key differences which reflect how we haveboth simplified and generalized from the original model. First, we simply usehidden states at the top LSTM layers in both the encoder and decoder asillustrated in Figure\u00a02. ?), on the other hand,use the concatenation of the forward and backward sourcehidden states in the bi-directional encoder and target hiddenstates in their non-stacking uni-directional decoder. Second, our computationpath is simpler; we go from \ud835\udc89t\u2192\ud835\udc82t\u2192\ud835\udc84t\u2192\ud835\udc89~t\u2192subscript\ud835\udc89\ud835\udc61subscript\ud835\udc82\ud835\udc61\u2192subscript\ud835\udc84\ud835\udc61\u2192subscriptbold-~\ud835\udc89\ud835\udc61\\mbox{\\boldmath{$h$}}_{t}\\rightarrow\\mbox{\\boldmath{$a$}}_{t}\\rightarrow\\mbox{\\boldmath{$c$}}_{t}\\rightarrow\\mbox{\\boldmath{$\\tilde{h}$}}_{t}bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2192 bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2192 bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2192 overbold_~ start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT then make a prediction as detailed in Eq.\u00a0(5), Eq.\u00a0(6), andFigure\u00a02. On the other hand, at any time t\ud835\udc61titalic_t, ?) build from the previous hidden state \ud835\udc89t\u22121\u2192\ud835\udc82t\u2192\ud835\udc84t\u2192\ud835\udc89t\u2192subscript\ud835\udc89\ud835\udc611subscript\ud835\udc82\ud835\udc61\u2192subscript\ud835\udc84\ud835\udc61\u2192subscript\ud835\udc89\ud835\udc61\\mbox{\\boldmath{$h$}}_{t-1}\\rightarrow\\mbox{\\boldmath{$a$}}_{t}\\rightarrow\\mbox{\\boldmath{$c$}}_{t}\\rightarrow\\mbox{\\boldmath{$h$}}_{t}bold_italic_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT \u2192 bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2192 bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2192 bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, which, in turn, goes through a deep-output and a maxout layer before making predictions.777We will refer to this difference again in Section\u00a03.3. Lastly, ?) only experimented with one alignment function, the concat product; whereas we show later that the other alternatives are better.", "The global attention has a drawback that it has to attend to all words on thesource side for each target word, which is expensive and can potentially render it impractical totranslate longer sequences, e.g., paragraphs or documents.To address this deficiency, we propose a local attentional mechanism thatchooses to focus only on a small subset of the source positions per target word.", "This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by ?) to tackle the image captiongeneration task. In their work, soft attention refers to the global attentionapproach in which weights are placed \u201csoftly\u201d over all patches in the sourceimage. The hard attention, on the other hand, selects one patchof the image to attend to at a time. While less expensive at inference time, thehard attention model is non-differentiable and requires more complicatedtechniques such as variance reduction or reinforcement learning to train.", "Our local attention mechanism selectively focuses on a small window ofcontext and is differentiable. This approach has an advantage of avoiding the expensive computation incurred inthe soft attention and at the same time, is easier to train than the hardattention approach.In concrete details, the model first generates an aligned position ptsubscript\ud835\udc5d\ud835\udc61p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT for each target word at time t\ud835\udc61titalic_t. Thecontext vector \ud835\udc84tsubscript\ud835\udc84\ud835\udc61\\mbox{\\boldmath{$c$}}_{t}bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is then derived as a weighted average over the set of source hidden states within the window [pt\u2212D,pt+D]subscript\ud835\udc5d\ud835\udc61\ud835\udc37subscript\ud835\udc5d\ud835\udc61\ud835\udc37[p_{t}-D,p_{t}+D][ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_D , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_D ]; D\ud835\udc37Ditalic_D isempirically selected.888If the window crosses the sentence boundaries, wesimply ignore the outside part and consider words in the window. Unlike the global approach, the local alignment vector \ud835\udc82tsubscript\ud835\udc82\ud835\udc61\\mbox{\\boldmath{$a$}}_{t}bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is now fixed-dimensional, i.e., \u2208\u211d2D+1absentsuperscript\u211d2\ud835\udc371\\in\\mathbb{R}^{2D+1}\u2208 blackboard_R start_POSTSUPERSCRIPT 2 italic_D + 1 end_POSTSUPERSCRIPT. We consider two variants of the model as below.", "Monotonic alignment (local-m) \u2013 we simply set pt=tsubscript\ud835\udc5d\ud835\udc61\ud835\udc61p_{t}\\!=\\!titalic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t assuming that source and target sequences are roughlymonotonically aligned. The alignment vector \ud835\udc82tsubscript\ud835\udc82\ud835\udc61\\mbox{\\boldmath{$a$}}_{t}bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is defined according toEq.\u00a0(7).999local-m is the same asthe global model except that the vector \ud835\udc82tsubscript\ud835\udc82\ud835\udc61\\mbox{\\boldmath{$a$}}_{t}bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT isfixed-length and shorter. ", "Predictive alignment (local-p) \u2013 instead of assuming monotonic alignments, our model predicts an aligned position as follows:pt=S\u22c5sigmoid\u2061(\ud835\udc97p\u22a4tanh\u2061(\ud835\udc7e\ud835\udc91\ud835\udc89t)),subscript\ud835\udc5d\ud835\udc61\u22c5\ud835\udc46sigmoidsuperscriptsubscript\ud835\udc97\ud835\udc5dtopsubscript\ud835\udc7e\ud835\udc91\ud835\udc89\ud835\udc61p_{t}=S\\cdot\\operatorname{sigmoid}(\\mbox{\\boldmath{$v$}}_{p}^{\\top}\\tanh(\\mbox{\\boldmath{$W_{p}$}}\\mbox{\\boldmath{$h$}}_{t})),italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_S \u22c5 roman_sigmoid ( bold_italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT roman_tanh ( roman_Wp roman_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) ,(9)\ud835\udc7e\ud835\udc91subscript\ud835\udc7e\ud835\udc91W_{p}bold_italic_W start_POSTSUBSCRIPT bold_italic_p end_POSTSUBSCRIPT and \ud835\udc97psubscript\ud835\udc97\ud835\udc5d\\mbox{\\boldmath{$v$}}_{p}bold_italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT are the model parameters which will be learnedto predict positions. S\ud835\udc46Sitalic_S is the source sentence length. As a result of sigmoidsigmoid\\operatorname{sigmoid}roman_sigmoid, pt\u2208[0,S]subscript\ud835\udc5d\ud835\udc610\ud835\udc46p_{t}\\in[0,S]italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2208 [ 0 , italic_S ]. To favor alignment points near ptsubscript\ud835\udc5d\ud835\udc61p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, we place a Gaussian distribution centered around ptsubscript\ud835\udc5d\ud835\udc61p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Specifically, our alignment weights are nowdefined as:\ud835\udc82t(s)=align\u2061(\ud835\udc89t,\ud835\udc89\u00afs)exp\u2061(\u2212(s\u2212pt)22\u03c32)subscript\ud835\udc82\ud835\udc61\ud835\udc60alignsubscript\ud835\udc89\ud835\udc61subscriptbold-\u00af\ud835\udc89\ud835\udc60superscript\ud835\udc60subscript\ud835\udc5d\ud835\udc6122superscript\ud835\udf0e2\\mbox{\\boldmath{$a$}}_{t}(s)=\\operatorname{align}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s})\\exp\\left(-\\frac{(s-p_{t})^{2}}{2\\sigma^{2}}\\right)bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_s ) = roman_align ( bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , overbold_\u00af start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) roman_exp ( - divide start_ARG ( italic_s - italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG )(10)We use the same alignalign\\operatorname{align}roman_align function as inEq.\u00a0(7) and the standard deviation is empirically set as\u03c3=D2\ud835\udf0e\ud835\udc372\\sigma\\!=\\!\\frac{D}{2}italic_\u03c3 = divide start_ARG italic_D end_ARG start_ARG 2 end_ARG. Note that ptsubscript\ud835\udc5d\ud835\udc61p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is a real nummber; whereas s\ud835\udc60sitalic_sis an integer within the window centered at ptsubscript\ud835\udc5d\ud835\udc61p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.101010local-p is similar to thelocal-m model except that we dynamicallycompute ptsubscript\ud835\udc5d\ud835\udc61p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and use a truncated Gaussian distribution to modify the original alignmentweights align\u2061(\ud835\udc89t,\ud835\udc89\u00afs)alignsubscript\ud835\udc89\ud835\udc61subscriptbold-\u00af\ud835\udc89\ud835\udc60\\operatorname{align}(\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$\\bar{h}$}}_{s})roman_align ( bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , overbold_\u00af start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) as shown in Eq.\u00a0(10). By utilizing ptsubscript\ud835\udc5d\ud835\udc61p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTto derive \ud835\udc82tsubscript\ud835\udc82\ud835\udc61\\mbox{\\boldmath{$a$}}_{t}bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, we can compute backprop gradients for \ud835\udc7e\ud835\udc91subscript\ud835\udc7e\ud835\udc91W_{p}bold_italic_W start_POSTSUBSCRIPT bold_italic_p end_POSTSUBSCRIPT and \ud835\udc97psubscript\ud835\udc97\ud835\udc5d\\mbox{\\boldmath{$v$}}_{p}bold_italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT.This model is differentiable almost everywhere.", "Comparison to [Gregor et al., 2015] \u2013have proposed a selective attention mechanism, verysimilar to our local attention, for the image generation task. Their approachallows the model to select an image patch of varying location and zoom. We,instead, use the same \u201czoom\u201d for all target positions, which greatlysimplifies the formulation and still achieves goodperformance.", "In our proposed global and local approaches, the attentional decisions are madeindependently, which is suboptimal. Whereas, in standard MT, a coverageset is often maintained during the translation process to keep track of whichsource words have been translated. Likewise, in attentional NMTs, alignmentdecisions should be made jointly taking into account past alignment information.To address that, we propose an input-feeding approach in which attentionalvectors \ud835\udc89~tsubscriptbold-~\ud835\udc89\ud835\udc61\\mbox{\\boldmath{$\\tilde{h}$}}_{t}overbold_~ start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are concatenated with inputs at the next time steps as illustrated inFigure\u00a04.111111If n\ud835\udc5bnitalic_n is the number of LSTM cells, theinput size of the first LSTM layer is 2n2\ud835\udc5b2n2 italic_n; those of subsequentlayers are n\ud835\udc5bnitalic_n. The effects of having such connections are two-fold:(a) we hope to make the model fully aware of previous alignment choices and (b)we create a very deep network spanning both horizontally and vertically.", "Comparison to other work \u2013 ?)use context vectors, similar toour \ud835\udc84tsubscript\ud835\udc84\ud835\udc61\\mbox{\\boldmath{$c$}}_{t}bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, in building subsequent hidden states, which can alsoachieve the \u201ccoverage\u201d effect. However, there has not been any analysis ofwhether such connections are useful as done in this work. Also,our approach is more general; as illustrated in Figure\u00a04, it can beapplied to general stacking recurrent architectures, including non-attentionalmodels.", "?) propose a doubly attentional approach with anadditional constraint added to the training objective to make sure the modelpays equal attention to all parts of the image during the caption generation process. Such a constraint can also be useful to capture the coverage set effect in NMT that we mentioned earlier. However, we chose to use the input-feeding approach since it provides flexibility for the model to decide on any attentional constraints it deems suitable.", "We evaluate the effectiveness of our models on the WMT translation tasks betweenEnglish and German in both directions. newstest2013 (3000 sentences) is used asa development set to select our hyperparameters. Translation performances arereported in case-sensitive BLEU [Papineni et al., 2002] on newstest2014 (2737 sentences) andnewstest2015 (2169 sentences). Following [Luong et al., 2015], we reporttranslation quality using two types of BLEU: (a) tokenized121212All texts are tokenized with tokenizer.perl and BLEUscores are computed with multi-bleu.perl. BLEU to be comparable withexisting NMT work and (b) NIST131313With the mteval-v13ascript as per WMT guideline. BLEU to be comparablewith WMT results.", "All our models are trained on the WMT\u201914 training data consisting of 4.5Msentences pairs (116M English words, 110M German words). Similar to [Jean et al., 2015], we limit our vocabularies to be the top 50K most frequent words for both languages. Words not in these shortlisted vocabularies are converted into a universal token <<<unk>>>.", "When training our NMT systems, following [Bahdanau et al., 2015, Jean et al., 2015], we filter outsentence pairs whose lengths exceed 50 words and shuffle mini-batches as weproceed. Our stacking LSTM models have 4 layers, each with 1000 cells, and1000-dimensional embeddings. We follow [Sutskever et al., 2014, Luong et al., 2015] in trainingNMT with similar settings: (a) our parameters are uniformly initialized in[\u22120.1,0.1]0.10.1[-0.1,0.1][ - 0.1 , 0.1 ], (b) we train for 10 epochs using plain SGD, (c) a simple learningrate schedule is employed \u2013 we start with a learning rate of 1; after 5 epochs,we begin to halve the learning rate every epoch, (d) our mini-batch size is 128,and (e) the normalized gradient is rescaled whenever its norm exceeds 5.Additionally, we also use dropout with probability 0.20.20.20.2 for our LSTMs as suggested by[Zaremba et al., 2015]. For dropout models, we train for 12 epochs and start halvingthe learning rate after 8 epochs. For localattention models, we empirically set the window size D=10\ud835\udc3710D=10italic_D = 10.", "Our code is implemented in MATLAB. When running on a single GPU device Tesla K40, we achieve a speed of 1K target words per second. It takes 7\u201310 days to completely train a model.", "We compare our NMT systems in the English-German task with various othersystems. These include the winning system in WMT\u201914[Buck et al., 2014], a phrase-based system whose language models were trained on ahuge monolingual text, the Common Crawl corpus. For end-to-end NMT systems, to the best of our knowledge, [Jean et al., 2015] is the only work experimenting with this language pair and currently the SOTA system.We only present results for some of our attention models and will lateranalyze the rest in Section\u00a05.", "As shown in Table\u00a01, we achieve progressive improvements when(a) reversing the source sentence, +1.31.31.31.3 BLEU, as proposed in [Sutskever et al., 2014]and (b) using dropout, +1.41.41.41.4 BLEU. On top of that, (c) the globalattention approach gives a significant boost of +2.82.82.82.8 BLEU, makingour model slightly better than the base attentional system of?) (row RNNSearch). When (d) using the input-feedingapproach, we seize another notable gain of +1.31.31.31.3 BLEU and outperform theirsystem. The local attention model with predictive alignments (row local-p) provesto be even better, giving us a further improvement of +0.90.90.90.9 BLEU on top of theglobal attention model.It is interesting to observe the trend previously reported in[Luong et al., 2015] that perplexity strongly correlates with translation quality.In total, we achieve a significant gain of5.0 BLEU points over the non-attentional baseline, which already includesknown techniques such as source reversing and dropout.", "The unknown replacement technique proposed in [Luong et al., 2015, Jean et al., 2015] yields another nicegain of +1.91.91.91.9 BLEU, demonstrating that our attentional modelsdo learn useful alignments for unknown works. Finally, by ensembling 8 differentmodels of various settings, e.g., using different attention approaches, withand without dropout etc., we were able to achieve a new SOTA result of23.023.023.0{}23.0BLEU, outperforming the existing best system [Jean et al., 2015] by +1.41.41.41.4 BLEU.", "Latest results in WMT\u201915 \u2013 despite the fact that our models were trainedon WMT\u201914 with slightly less data, we test them on newstest2015 to demonstratethat they can generalize well to different test sets. As shown in Table\u00a02, our bestsystem establishes a new SOTA performance of 25.925.925.9{}25.9 BLEU,outperforming the existing best system backed by NMT and a 5-gram LM rerankerby +1.01.01.01.0BLEU.", "We carry out a similar set of experiments for the WMT\u201915 translation task from Germanto English.While our systems have not yet matched the performance of theSOTA system, we nevertheless show the effectiveness of ourapproaches with large and progressive gains in terms of BLEU as illustrated inTable\u00a03.The attentional mechanism gives us +2.22.22.22.2 BLEU gain and on top of that, weobtain another boost of up to +1.01.01.01.0 BLEU from the input-feeding approach.Using a better alignment function, the content-based dot product one,together with dropout yields another gain of +2.72.72.72.7 BLEU. Lastly, whenapplying the unknown word replacement technique, we seize an additional +2.12.12.12.1BLEU, demonstrating the usefulness of attention in aligning rare words.", "We conduct extensive analysis to better understand our models in termsof learning, the ability to handle long sentences,choices of attentional architectures, and alignment quality. All resultsreported here are on English-German newstest2014.", "We compare models built on top of one another as listed in Table\u00a01. It ispleasant to observe in Figure\u00a05 a clear separation between non-attentional and attentionalmodels. The input-feeding approach and the local attentionmodel also demonstrate their abilities in driving the test costs lower. Thenon-attentional model withdropout (the blue + curve) learns slower than other non-dropout models, butas time goes by, it becomes more robust in terms of minimizing test errors.", "We follow [Bahdanau et al., 2015] to group sentences of similar lengths together andcompute a BLEU score per group. Figure\u00a06 shows thatour attentional models are more effective than the non-attentional one inhandling long sentences: the quality does not degrade as sentencesbecome longer. Our best model (the blue + curve) outperforms all other systems in all length buckets.", "We examine different attention models (global, local-m, local-p) and differentalignment functions (location, dot, general, concat) as described inSection\u00a03. Due to limitedresources, we cannot run all the possible combinations.However, results in Table\u00a04 do give us some idea aboutdifferent choices.The location-based function does not learn goodalignments: the global (location) model can only obtain a smallgain when performing unknown word replacement compared to using other alignmentfunctions.141414There is a subtle difference in how we retrieve alignmentsfor the different alignment functions. At time step t\ud835\udc61titalic_t in which we receiveyt\u22121subscript\ud835\udc66\ud835\udc611y_{t-1}italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT as input and then compute \ud835\udc89t,\ud835\udc82t,\ud835\udc84tsubscript\ud835\udc89\ud835\udc61subscript\ud835\udc82\ud835\udc61subscript\ud835\udc84\ud835\udc61\\mbox{\\boldmath{$h$}}_{t},\\mbox{\\boldmath{$a$}}_{t},\\mbox{\\boldmath{$c$}}_{t}bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and \ud835\udc89~tsubscriptbold-~\ud835\udc89\ud835\udc61\\mbox{\\boldmath{$\\tilde{h}$}}_{t}overbold_~ start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT beforepredicting ytsubscript\ud835\udc66\ud835\udc61y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, the alignment vector \ud835\udc82tsubscript\ud835\udc82\ud835\udc61\\mbox{\\boldmath{$a$}}_{t}bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is used as alignmentweights for (a) the predicted word ytsubscript\ud835\udc66\ud835\udc61y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT in the location-basedalignment functions and (b) the input word yt\u22121subscript\ud835\udc66\ud835\udc611y_{t-1}italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT in the content-basedfunctions.For content-based functions, our implementation concat does not yield good performancesand more analysis should be done to understand thereason.151515With concat, the perplexities achieved by different models are 6.7 (global), 7.1(local-m), and 7.1 (local-p). Such high perplexities could be due to the factthat we simplify the matrix \ud835\udc7e\ud835\udc82subscript\ud835\udc7e\ud835\udc82W_{a}bold_italic_W start_POSTSUBSCRIPT bold_italic_a end_POSTSUBSCRIPT to set the part that corresponds to \ud835\udc89\u00afssubscriptbold-\u00af\ud835\udc89\ud835\udc60\\mbox{\\boldmath{$\\bar{h}$}}_{s}overbold_\u00af start_ARG bold_italic_h end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTto identity. It is interesting to observe that dot workswell for the global attention and general is better for the localattention.Among the different models, the local attention model with predictive alignments (local-p) is best, both in terms of perplexities and BLEU.", "A by-product of attentional models are word alignments. While [Bahdanau et al., 2015]visualized alignments for some sample sentences andobserved gains in translation quality as an indication of a working attentionmodel, no work has assessed the alignments learned as a whole. In contrast, weset out to evaluate the alignment quality using the alignment error rate (AER)metric.", "Given the gold alignment data provided by RWTH for 508 English-GermanEuroparl sentences, we \u201cforce\u201d decode our attentional models toproduce translations that match the references. We extract only one-to-onealignments by selecting the source word with the highest alignmentweight per target word. Nevertheless, as shown in Table\u00a06, we were able to achieve AER scorescomparable to the one-to-many alignments obtained by the Berkeley aligner[Liang et al., 2006].161616We concatenate the 508 sentence pairs with 1Msentence pairs from WMT and run the Berkeley aligner.", "We also found that the alignments produced by local attention models achievelower AERs than those of the global one. The AER obtained by the ensemble, whilegood, is not better than the local-m AER, suggesting the well-knownobservation that AER and translation scores are not well correlated [Fraser and Marcu, 2007].We show some alignment visualizations in Appendix\u00a0A.", "We show in Table\u00a05 sample translations in both directions. It itappealing to observe the effect of attentional models in correctly translatingnames such as \u201cMiranda Kerr\u201d and \u201cRoger Dow\u201d. Non-attentional models, while producing sensible names from a languagemodel perspective, lack the direct connections from the source side to makecorrect translations. We also observed an interesting case in the secondexample, which requires translating the doubly-negated phrase, \u201cnot incompatible\u201d.The attentional model correctly produces \u201cnicht \u2026\u2026\\dots\u2026 unvereinbar\u201d;whereas the non-attentional model generates \u201cnicht vereinbar\u201d, meaning\u201cnot compatible\u201d.171717The reference uses a more fancy translation of\u201cincompatible\u201d, which is \u201cim Widerspruch zu etwas stehen\u201d. Both models, however, failed to translate \u201cpassengerexperience\u201d. The attentionalmodel also demonstrates its superiority in translating long sentences as inthe last example.", "In this paper, we propose two simple and effective attentional mechanisms forneural machine translation: the global approach which always looks at allsource positions and the local one that only attends to a subset of sourcepositions at a time. We test the effectiveness of our models in the WMTtranslation tasks between English and German in both directions.Our local attention yields large gains of up to5.05.05.0{}5.0 BLEU over non-attentional models which already incorporate knowntechniques such as dropout. For the English to German translation direction, ourensemble model has established new state-of-the-artresults for both WMT\u201914 and WMT\u201915, outperforming existingbest systems, backed by NMT models and n\ud835\udc5bnitalic_n-gram LM rerankers, by more than 1.0 BLEU.", "We have compared various alignment functions and shed light on which functionsare best for which attentional models.Our analysis shows that attention-based NMT models are superior tonon-attentional ones in many cases, for example in translating names andhandling longsentences."], "figure_types": {"93499a7c7f699b6630a86fad964536f9423bb6d0/1-Figure1-1.png": "schematic", "93499a7c7f699b6630a86fad964536f9423bb6d0/3-Figure2-1.png": "schematic", "93499a7c7f699b6630a86fad964536f9423bb6d0/4-Figure3-1.png": "schematic", "93499a7c7f699b6630a86fad964536f9423bb6d0/5-Figure4-1.png": "schematic", "93499a7c7f699b6630a86fad964536f9423bb6d0/6-Table1-1.png": "table", "93499a7c7f699b6630a86fad964536f9423bb6d0/6-Table2-1.png": "table", "93499a7c7f699b6630a86fad964536f9423bb6d0/7-Figure5-1.png": "plot", "93499a7c7f699b6630a86fad964536f9423bb6d0/7-Figure6-1.png": "plot", "93499a7c7f699b6630a86fad964536f9423bb6d0/7-Table3-1.png": "table", "93499a7c7f699b6630a86fad964536f9423bb6d0/8-Table4-1.png": "table", "93499a7c7f699b6630a86fad964536f9423bb6d0/8-Table6-1.png": "table", "93499a7c7f699b6630a86fad964536f9423bb6d0/9-Table5-1.png": "table"}}, "2204.03458": {"paper_id": "paper_132", "title": "Video Diffusion Models", "arxiv_url": "https://arxiv.org/abs/2204.03458", "s2orc_url": "https://www.semanticscholar.org/paper/3b2a675bb617ae1a920e8e29d535cdf27826e999", "all_figures_tables": {"3b2a675bb617ae1a920e8e29d535cdf27826e999/14-Figure5-1.png": "Figure 5: More samples accompanying Fig. 2.", "3b2a675bb617ae1a920e8e29d535cdf27826e999/3-Figure1-1.png": "Figure 1: The 3D U-Net architecture for x\u0302\u03b8 in the diffusion model. Each block represents a 4D tensor with axes labeled as frames\u00d7 height\u00d7width\u00d7 channels, processed in a space-time factorized manner as described in Section 3. The input is a noisy video zt, conditioning c, and the log SNR \u03bbt. The downsampling/upsampling blocks adjust the spatial input resolution height\u00d7 width by a factor of 2 through each of the K blocks. The channel counts are specified using channel multipliers M1, M2, ..., MK , and the upsampling pass has concatenation skip connections to the downsampling pass.", "3b2a675bb617ae1a920e8e29d535cdf27826e999/5-Table1-1.png": "Table 1: Unconditional video modeling results on UCF101.", "3b2a675bb617ae1a920e8e29d535cdf27826e999/6-Table3-1.png": "Table 3: Video prediction on Kinetics-600. Method FVD\u2193 IS\u2191", "3b2a675bb617ae1a920e8e29d535cdf27826e999/7-Figure2-1.png": "Figure 2: Text-conditioned video samples from a cascade of two models. First samples are generated from a 16x64x64 frameskip 4 model. Then those samples are treated as ground truth for simultaneous super-resolution and autoregressive extension to 64x128x128 using a 9x128x128 frameskip 1 model. Both models are conditioned on the text prompt. In this figure, the text prompt, low resolution frames, and high resolution frames are visualized in sequence. See Fig. 5 for more samples.", "3b2a675bb617ae1a920e8e29d535cdf27826e999/7-Table4-1.png": "Table 4: Improved sample quality due to image-video joint training on text-to-video generation. Image frames FVD\u2193 FID-avg\u2193 IS-avg\u2191 FID-first\u2193 IS-first\u2191", "3b2a675bb617ae1a920e8e29d535cdf27826e999/8-Figure3-1.png": "Figure 3: Example frames from a random selection of videos generated by our 16x64x64 textconditioned model. Left: unguided samples, right: guided samples using classifier-free guidance.", "3b2a675bb617ae1a920e8e29d535cdf27826e999/8-Table5-1.png": "Table 5: Effect of classifier-free guidance on text-to-video generation (large models). Sample quality is reported for 16x64x64 models trained on frameskip 1 and 4 data. The model was jointly trained on 8 independent image frames per 16-frame video.", "3b2a675bb617ae1a920e8e29d535cdf27826e999/8-Table6-1.png": "Table 6: Generating 64x64x64 videos using autoregressive extension of 16x64x64 models. Guidance weight Conditioning method FVD\u2193 FID-avg\u2193 IS-avg\u2191 FID-first\u2193 IS-first\u2191", "3b2a675bb617ae1a920e8e29d535cdf27826e999/9-Figure4-1.png": "Figure 4: Comparing the replacement method (left) vs the reconstruction guidance method (right) for conditioning for block-autoregressive generation of 64 frames from a 16 frame model. Video frames are displayed over time from left to right; each row is an independent sample. The replacement method suffers from a lack of temporal coherence, unlike the reconstruction guidance method."}, "referred_figures_tables": [["3b2a675bb617ae1a920e8e29d535cdf27826e999/7-Figure2-1.png"]], "question_id": [1], "question": ["Does making higher resolution have to be incorporated into the network? Can't we do this as a separate process?"], "question_section": ["Abstract"], "question_trigger_sentence": ["To generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods."], "question_type": ["Testing question"], "evidential_info": [[{"context": "We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model\u00a0(Sohl-Dickstein et\u00a0al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.", "rationale": "They modify little other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators."}, {"context": "The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate of at least 24 frames per second. To manage the computational requirements of training our models, we only train on a small subset of say 16 frames at a time. However, at test time we can generate longer videos by extending our samples. For example, we could first generate a video \\mathbf{x}^{\\text{a}}\\sim p_{\\theta}(\\mathbf{x}) consisting of 16 frames, and then extend it with a second sample \\mathbf{x}^{\\text{b}}\\sim p_{\\theta}(\\mathbf{x}^{\\text{b}}|\\mathbf{x}^{\\text{a}}). If \\mathbf{x}^{\\text{b}} consists of frames following \\mathbf{x}^{\\text{a}}, this allows us to autoregressively extend our sampled videos to arbitrary lengths, which we demonstrate in Section\u00a04.3.3. Alternatively, we could choose \\mathbf{x}^{\\text{a}} to represent a video of lower frame rate, and then define \\mathbf{x}^{\\text{b}} to be those frames in between the frames of \\mathbf{x}^{\\text{a}}.This allows one to then to upsample a video temporally, similar to how Menick and Kalchbrenner (2019) generate high resolution images through spatial upsampling.", "rationale": "Thier approach uses the standard diffusion modelformalism."}, {"context": "Reconstruction guidance also extends to the case of spatial interpolation (or super-resolution), in which the mean squared error loss is imposed on a downsampled version of the model prediction, and backpropagation is performed through this downsampling.In this setting, we have low resolution ground truth videos \\mathbf{x}^{a} (e.g. at the 64x64 spatial resolution), which may be generated from a low resolution model, and we wish to upsample them into high resolution videos (e.g. at the 128x128 spatial resolution) using an unconditional high resolution diffusion model \\hat{\\mathbf{x}}_{\\theta}. To accomplish this, we adjust the high resolution model as follows:\ud835\udc31~\u03b8(\ud835\udc33t)=\ud835\udc31^\u03b8(\ud835\udc33t)\u2212wr\u2062\u03b1t2\u2207\ud835\udc33t\u2225\ud835\udc31a\u2212\ud835\udc31^\u03b8a(\ud835\udc33t)\u222522\\displaystyle\\tilde{\\mathbf{x}}_{\\theta}(\\mathbf{z}_{t})=\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{z}_{t})-\\frac{w_{r}\\alpha_{t}}{2}\\nabla_{\\mathbf{z}_{t}}\\lVert\\mathbf{x}^{a}-\\hat{\\mathbf{x}}^{a}_{\\theta}(\\mathbf{z}_{t})\\rVert_{2}^{2}over~ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - divide start_ARG italic_w start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG \u2207 start_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2225 bold_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT - over^ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT(8)where \\hat{\\mathbf{x}}^{a}_{\\theta}(\\mathbf{z}_{t}) is our model\u2019s reconstruction of the low-resolution video from \\mathbf{z}_{t}, which is obtained by downsampling the high resolution output of the model using a differentiable downsampling algorithm such as bilinear interpolation.Note that it is also possible to simultaneously condition on low resolution videos while autoregressively extending samples at the high resolution using the same reconstruction guidance method. In Fig.\u00a02, we show samples of this approach for extending 16x64x64 low resolution samples at frameskip 4 to 64x128x128 samples at frameskip 1 using a 9x128x128 diffusion model.", "rationale": "Reconstruction guidance can extended to the case of super-resolution. They have low resolution ground truth videos, and then upsample them into high resolution videos using an unconditional high resolution diffusion model. The reconstruction of the low-resolution video is utilized to adjust the high-resolution model."}, {"context": "Our approach to video generation using diffusion models is to use the standard diffusion model formalism described in Section\u00a02 with a neural network architecture suitable for video data. Each of our models is trained to jointly model a fixed number of frames at a fixed spatial resolution. To extend sampling to longer sequences of frames or higher spatial resolutions, we will repurpose our models with a conditioning technique described later in Section\u00a03.1.", "rationale": "This paper refers Menick and Kalchbrenner (2019), which utilize spatial upsampling to generate high resolution images."}]], "composition": ["Video diffusion models modify little of the archicture to accommodate video data within the memory constraints of deep learning accelerators. They approach with the standard diffusion modelformalism. In their method, one of skill to make high resolution video is the spatial upsampling introduced by Menick and Kalchbrenner (2019).\nAlso, reconstruction guidance is extended to constuct the high-resolution model. When they have low resolution ground truth videos, it upsamples them into high resolution videos using an unconditional high resolution diffusion model."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["340"], "passages": ["Diffusion models have recently been producing high quality results in image generation and audio generation\u00a0(e.g. Kingma et\u00a0al., 2021; Saharia et\u00a0al., 2021a, b; Dhariwal and Nichol, 2021; Ho et\u00a0al., 2021; Nichol et\u00a0al., 2021; Song et\u00a0al., 2021; Whang et\u00a0al., 2021; Salimans and Ho, 2021; Chen et\u00a0al., 2021; Kong et\u00a0al., 2021), and there is significant interest in validating diffusion models in new data modalities. In this work, we present first results on video generation using diffusion models, for both unconditional and conditional settings.", "We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model\u00a0(Sohl-Dickstein et\u00a0al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.", "A diffusion model\u00a0(Sohl-Dickstein et\u00a0al., 2015; Song and Ermon, 2019; Ho et\u00a0al., 2020) specified in continuous time\u00a0(Tzen and Raginsky, 2019; Song et\u00a0al., 2021; Chen et\u00a0al., 2021; Kingma et\u00a0al., 2021) is a generative model with latents \ud835\udc33={\ud835\udc33t|t\u2208[0,1]}\ud835\udc33conditional-setsubscript\ud835\udc33\ud835\udc61\ud835\udc6101\\mathbf{z}=\\{\\mathbf{z}_{t}\\,|\\,t\\in[0,1]\\}bold_z = { bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_t \u2208 [ 0 , 1 ] } obeying a forward process q(\ud835\udc33|\ud835\udc31)\ud835\udc5econditional\ud835\udc33\ud835\udc31q(\\mathbf{z}|\\mathbf{x})italic_q ( bold_z | bold_x ) starting at data \ud835\udc31\u223cp(\ud835\udc31)similar-to\ud835\udc31\ud835\udc5d\ud835\udc31\\mathbf{x}\\sim p(\\mathbf{x})bold_x \u223c italic_p ( bold_x ). The forward process is a Gaussian process that satisfies the Markovian structure:q(\ud835\udc33t|\ud835\udc31)=\ud835\udca9(\ud835\udc33t;\u03b1t\ud835\udc31,\u03c3t2\ud835\udc08),q(\ud835\udc33t|\ud835\udc33s)=\ud835\udca9(\ud835\udc33t;(\u03b1t/\u03b1s)\ud835\udc33s,\u03c3t|s2\ud835\udc08)formulae-sequence\ud835\udc5econditionalsubscript\ud835\udc33\ud835\udc61\ud835\udc31\ud835\udca9subscript\ud835\udc33\ud835\udc61subscript\ud835\udefc\ud835\udc61\ud835\udc31superscriptsubscript\ud835\udf0e\ud835\udc612\ud835\udc08\ud835\udc5econditionalsubscript\ud835\udc33\ud835\udc61subscript\ud835\udc33\ud835\udc60\ud835\udca9subscript\ud835\udc33\ud835\udc61subscript\ud835\udefc\ud835\udc61subscript\ud835\udefc\ud835\udc60subscript\ud835\udc33\ud835\udc60superscriptsubscript\ud835\udf0econditional\ud835\udc61\ud835\udc602\ud835\udc08\\displaystyle q(\\mathbf{z}_{t}|\\mathbf{x})=\\mathcal{N}(\\mathbf{z}_{t};\\alpha_{t}\\mathbf{x},\\sigma_{t}^{2}\\mathbf{I}),\\quad q(\\mathbf{z}_{t}|\\mathbf{z}_{s})=\\mathcal{N}(\\mathbf{z}_{t};(\\alpha_{t}/\\alpha_{s})\\mathbf{z}_{s},\\sigma_{t|s}^{2}\\mathbf{I})italic_q ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_x ) = caligraphic_N ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; italic_\u03b1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_x , italic_\u03c3 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_I ) , italic_q ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = caligraphic_N ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; ( italic_\u03b1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT / italic_\u03b1 start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_\u03c3 start_POSTSUBSCRIPT italic_t | italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_I )(1)where 0\u2264s<t\u226410\ud835\udc60\ud835\udc6110\\leq s<t\\leq 10 \u2264 italic_s < italic_t \u2264 1, \u03c3t|s2=(1\u2212e\u03bbt\u2212\u03bbs)\u03c3t2subscriptsuperscript\ud835\udf0e2conditional\ud835\udc61\ud835\udc601superscript\ud835\udc52subscript\ud835\udf06\ud835\udc61subscript\ud835\udf06\ud835\udc60superscriptsubscript\ud835\udf0e\ud835\udc612\\sigma^{2}_{t|s}=(1-e^{\\lambda_{t}-\\lambda_{s}})\\sigma_{t}^{2}italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t | italic_s end_POSTSUBSCRIPT = ( 1 - italic_e start_POSTSUPERSCRIPT italic_\u03bb start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bb start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) italic_\u03c3 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, and \u03b1t,\u03c3tsubscript\ud835\udefc\ud835\udc61subscript\ud835\udf0e\ud835\udc61\\alpha_{t},\\sigma_{t}italic_\u03b1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_\u03c3 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT specify a differentiable noise schedule whose log signal-to-noise-ratio \u03bbt=log\u2061[\u03b1t2/\u03c3t2]subscript\ud835\udf06\ud835\udc61superscriptsubscript\ud835\udefc\ud835\udc612superscriptsubscript\ud835\udf0e\ud835\udc612\\lambda_{t}=\\log[\\alpha_{t}^{2}/\\sigma_{t}^{2}]italic_\u03bb start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = roman_log [ italic_\u03b1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_\u03c3 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] decreases with t\ud835\udc61titalic_t until q(\ud835\udc331)\u2248\ud835\udca9(\ud835\udfce,\ud835\udc08)\ud835\udc5esubscript\ud835\udc331\ud835\udca90\ud835\udc08q(\\mathbf{z}_{1})\\approx\\mathcal{N}(\\mathbf{0},\\mathbf{I})italic_q ( bold_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) \u2248 caligraphic_N ( bold_0 , bold_I ).", "Learning to reverse the forward process for generation can be reduced to learning to denoise \ud835\udc33t\u223cq(\ud835\udc33t|\ud835\udc31)similar-tosubscript\ud835\udc33\ud835\udc61\ud835\udc5econditionalsubscript\ud835\udc33\ud835\udc61\ud835\udc31\\mathbf{z}_{t}\\sim q(\\mathbf{z}_{t}|\\mathbf{x})bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u223c italic_q ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_x ) into an estimate \ud835\udc31^\u03b8(\ud835\udc33t,\u03bbt)\u2248\ud835\udc31subscript^\ud835\udc31\ud835\udf03subscript\ud835\udc33\ud835\udc61subscript\ud835\udf06\ud835\udc61\ud835\udc31\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{z}_{t},\\lambda_{t})\\approx\\mathbf{x}over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_\u03bb start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2248 bold_x for all t\ud835\udc61titalic_t (we will drop the dependence on \u03bbtsubscript\ud835\udf06\ud835\udc61\\lambda_{t}italic_\u03bb start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to simplify notation).We train this denoising model \ud835\udc31^\u03b8subscript^\ud835\udc31\ud835\udf03\\hat{\\mathbf{x}}_{\\theta}over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT using a weighted mean squared error loss\ud835\udd3c\u03f5,t[w(\u03bbt)\u2016\ud835\udc31^\u03b8(\ud835\udc33t)\u2212\ud835\udc31\u201622]subscript\ud835\udd3cbold-italic-\u03f5\ud835\udc61delimited-[]\ud835\udc64subscript\ud835\udf06\ud835\udc61subscriptsuperscriptnormsubscript^\ud835\udc31\ud835\udf03subscript\ud835\udc33\ud835\udc61\ud835\udc3122\\displaystyle\\mathbb{E}_{{\\boldsymbol{\\epsilon}},t}\\!\\left[w(\\lambda_{t})\\|\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{z}_{t})-\\mathbf{x}\\|^{2}_{2}\\right]blackboard_E start_POSTSUBSCRIPT bold_italic_\u03f5 , italic_t end_POSTSUBSCRIPT [ italic_w ( italic_\u03bb start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2225 over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - bold_x \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ](2)over uniformly sampled times t\u2208[0,1]\ud835\udc6101t\\in[0,1]italic_t \u2208 [ 0 , 1 ]. This reduction of generation to denoising can be justified as optimizing a weighted variational lower bound on the data log likelihood under the diffusion model, or as a form of denoising score matching (Vincent, 2011; Song and Ermon, 2019; Ho et\u00a0al., 2020; Kingma et\u00a0al., 2021). In practice, we use the \u03f5bold-italic-\u03f5{\\boldsymbol{\\epsilon}}bold_italic_\u03f5-prediction parameterization, defined as \ud835\udc31^\u03b8(\ud835\udc33t)=(\ud835\udc33t\u2212\u03c3t\u03f5\u03b8(\ud835\udc33t))/\u03b1tsubscript^\ud835\udc31\ud835\udf03subscript\ud835\udc33\ud835\udc61subscript\ud835\udc33\ud835\udc61subscript\ud835\udf0e\ud835\udc61subscriptbold-italic-\u03f5\ud835\udf03subscript\ud835\udc33\ud835\udc61subscript\ud835\udefc\ud835\udc61\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{z}_{t})=(\\mathbf{z}_{t}-\\sigma_{t}{\\boldsymbol{\\epsilon}}_{\\theta}(\\mathbf{z}_{t}))/\\alpha_{t}over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03c3 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) / italic_\u03b1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and train \u03f5\u03b8subscriptbold-italic-\u03f5\ud835\udf03{\\boldsymbol{\\epsilon}}_{\\theta}bold_italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT using a mean squared error in \u03f5bold-italic-\u03f5{\\boldsymbol{\\epsilon}}bold_italic_\u03f5 space with t\ud835\udc61titalic_t sampled according to a cosine schedule\u00a0(Nichol and Dhariwal, 2021). This corresponds to a particular weighting w(\u03bbt)\ud835\udc64subscript\ud835\udf06\ud835\udc61w(\\lambda_{t})italic_w ( italic_\u03bb start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) for learning a scaled score estimate \u03f5\u03b8(\ud835\udc33t)\u2248\u2212\u03c3t\u2207\ud835\udc33tlog\u2061p(\ud835\udc33t)subscriptbold-italic-\u03f5\ud835\udf03subscript\ud835\udc33\ud835\udc61subscript\ud835\udf0e\ud835\udc61subscript\u2207subscript\ud835\udc33\ud835\udc61\ud835\udc5dsubscript\ud835\udc33\ud835\udc61{\\boldsymbol{\\epsilon}}_{\\theta}(\\mathbf{z}_{t})\\approx-\\sigma_{t}\\nabla_{\\mathbf{z}_{t}}\\log p(\\mathbf{z}_{t})bold_italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2248 - italic_\u03c3 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2207 start_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), where p(\ud835\udc33t)\ud835\udc5dsubscript\ud835\udc33\ud835\udc61p(\\mathbf{z}_{t})italic_p ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is the true density of \ud835\udc33tsubscript\ud835\udc33\ud835\udc61\\mathbf{z}_{t}bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT under \ud835\udc31\u223cp(\ud835\udc31)similar-to\ud835\udc31\ud835\udc5d\ud835\udc31\\mathbf{x}\\sim p(\\mathbf{x})bold_x \u223c italic_p ( bold_x )\u00a0(Ho et\u00a0al., 2020; Kingma et\u00a0al., 2021; Song et\u00a0al., 2021). We also train using the \ud835\udc2f\ud835\udc2f\\mathbf{v}bold_v-prediction parameterization for certain models\u00a0(Salimans and Ho, 2021).", "We use a variety of diffusion model samplers in this work. One is the discrete time ancestral sampler (Ho et\u00a0al., 2020) with sampling variances derived from lower and upper bounds on reverse process entropy\u00a0(Sohl-Dickstein et\u00a0al., 2015; Ho et\u00a0al., 2020; Nichol and Dhariwal, 2021). To define this sampler, first note that the forward process can be described in reverse as q(\ud835\udc33s|\ud835\udc33t,\ud835\udc31)=\ud835\udca9(\ud835\udc33s;\ud835\udf41~s|t(\ud835\udc33t,\ud835\udc31),\u03c3~s|t2\ud835\udc08)\ud835\udc5econditionalsubscript\ud835\udc33\ud835\udc60subscript\ud835\udc33\ud835\udc61\ud835\udc31\ud835\udca9subscript\ud835\udc33\ud835\udc60subscript~\ud835\udf41conditional\ud835\udc60\ud835\udc61subscript\ud835\udc33\ud835\udc61\ud835\udc31subscriptsuperscript~\ud835\udf0e2conditional\ud835\udc60\ud835\udc61\ud835\udc08q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})=\\mathcal{N}(\\mathbf{z}_{s};\\tilde{\\boldsymbol{\\mu}}_{s|t}(\\mathbf{z}_{t},\\mathbf{x}),\\tilde{\\sigma}^{2}_{s|t}\\mathbf{I})italic_q ( bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x ) = caligraphic_N ( bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ; over~ start_ARG bold_italic_\u03bc end_ARG start_POSTSUBSCRIPT italic_s | italic_t end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x ) , over~ start_ARG italic_\u03c3 end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s | italic_t end_POSTSUBSCRIPT bold_I ) (noting s<t\ud835\udc60\ud835\udc61s<titalic_s < italic_t), where\ud835\udf41~s|t(\ud835\udc33t,\ud835\udc31)=e\u03bbt\u2212\u03bbs(\u03b1s/\u03b1t)\ud835\udc33t+(1\u2212e\u03bbt\u2212\u03bbs)\u03b1s\ud835\udc31and\u03c3~s|t2=(1\u2212e\u03bbt\u2212\u03bbs)\u03c3s2.formulae-sequencesubscript~\ud835\udf41conditional\ud835\udc60\ud835\udc61subscript\ud835\udc33\ud835\udc61\ud835\udc31superscript\ud835\udc52subscript\ud835\udf06\ud835\udc61subscript\ud835\udf06\ud835\udc60subscript\ud835\udefc\ud835\udc60subscript\ud835\udefc\ud835\udc61subscript\ud835\udc33\ud835\udc611superscript\ud835\udc52subscript\ud835\udf06\ud835\udc61subscript\ud835\udf06\ud835\udc60subscript\ud835\udefc\ud835\udc60\ud835\udc31andsubscriptsuperscript~\ud835\udf0e2conditional\ud835\udc60\ud835\udc611superscript\ud835\udc52subscript\ud835\udf06\ud835\udc61subscript\ud835\udf06\ud835\udc60superscriptsubscript\ud835\udf0e\ud835\udc602\\displaystyle\\tilde{\\boldsymbol{\\mu}}_{s|t}(\\mathbf{z}_{t},\\mathbf{x})=e^{\\lambda_{t}-\\lambda_{s}}(\\alpha_{s}/\\alpha_{t})\\mathbf{z}_{t}+(1-e^{\\lambda_{t}-\\lambda_{s}})\\alpha_{s}\\mathbf{x}\\quad\\text{and}\\quad\\tilde{\\sigma}^{2}_{s|t}=(1-e^{\\lambda_{t}-\\lambda_{s}})\\sigma_{s}^{2}.over~ start_ARG bold_italic_\u03bc end_ARG start_POSTSUBSCRIPT italic_s | italic_t end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x ) = italic_e start_POSTSUPERSCRIPT italic_\u03bb start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bb start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( italic_\u03b1 start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT / italic_\u03b1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + ( 1 - italic_e start_POSTSUPERSCRIPT italic_\u03bb start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bb start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) italic_\u03b1 start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT bold_x and over~ start_ARG italic_\u03c3 end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s | italic_t end_POSTSUBSCRIPT = ( 1 - italic_e start_POSTSUPERSCRIPT italic_\u03bb start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bb start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) italic_\u03c3 start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .(3)Starting at \ud835\udc331\u223c\ud835\udca9(\ud835\udfce,\ud835\udc08)similar-tosubscript\ud835\udc331\ud835\udca90\ud835\udc08\\mathbf{z}_{1}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})bold_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u223c caligraphic_N ( bold_0 , bold_I ), the ancestral sampler follows the rule\ud835\udc33ssubscript\ud835\udc33\ud835\udc60\\displaystyle\\mathbf{z}_{s}bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT=\ud835\udf41~s|t(\ud835\udc33t,\ud835\udc31^\u03b8(\ud835\udc33t))+(\u03c3~s|t2)1\u2212\u03b3(\u03c3t|s2)\u03b3\u03f5absentsubscript~\ud835\udf41conditional\ud835\udc60\ud835\udc61subscript\ud835\udc33\ud835\udc61subscript^\ud835\udc31\ud835\udf03subscript\ud835\udc33\ud835\udc61superscriptsubscriptsuperscript~\ud835\udf0e2conditional\ud835\udc60\ud835\udc611\ud835\udefesuperscriptsubscriptsuperscript\ud835\udf0e2conditional\ud835\udc61\ud835\udc60\ud835\udefebold-italic-\u03f5\\displaystyle=\\tilde{\\boldsymbol{\\mu}}_{s|t}(\\mathbf{z}_{t},\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{z}_{t}))+\\sqrt{(\\tilde{\\sigma}^{2}_{s|t})^{1-\\gamma}(\\sigma^{2}_{t|s})^{\\gamma}}{\\boldsymbol{\\epsilon}}= over~ start_ARG bold_italic_\u03bc end_ARG start_POSTSUBSCRIPT italic_s | italic_t end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) + square-root start_ARG ( over~ start_ARG italic_\u03c3 end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s | italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 1 - italic_\u03b3 end_POSTSUPERSCRIPT ( italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t | italic_s end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_\u03b3 end_POSTSUPERSCRIPT end_ARG bold_italic_\u03f5(4)where \u03f5bold-italic-\u03f5{\\boldsymbol{\\epsilon}}bold_italic_\u03f5 is standard Gaussian noise, \u03b3\ud835\udefe\\gammaitalic_\u03b3 is a hyperparameter that controls the stochasticity of the sampler\u00a0(Nichol and Dhariwal, 2021), and s,t\ud835\udc60\ud835\udc61s,titalic_s , italic_t follow a uniformly spaced sequence from 1 to 0.", "Another sampler, which we found especially effective with our new method for conditional generation\u00a0(Section\u00a03.1), is the predictor-corrector sampler\u00a0(Song et\u00a0al., 2021). Our version of this sampler alternates between the ancestral sampler step 4 and a Langevin correction step of the form\ud835\udc33s\u2190\ud835\udc33s\u221212\u03b4\u03c3s\u03f5\u03b8(\ud835\udc33s)+\u03b4\u03c3s\u03f5\u2032\u2190subscript\ud835\udc33\ud835\udc60subscript\ud835\udc33\ud835\udc6012\ud835\udeffsubscript\ud835\udf0e\ud835\udc60subscriptbold-italic-\u03f5\ud835\udf03subscript\ud835\udc33\ud835\udc60\ud835\udeffsubscript\ud835\udf0e\ud835\udc60superscriptbold-italic-\u03f5\u2032\\displaystyle\\mathbf{z}_{s}\\leftarrow\\mathbf{z}_{s}-\\frac{1}{2}\\delta\\sigma_{s}{\\boldsymbol{\\epsilon}}_{\\theta}(\\mathbf{z}_{s})+\\sqrt{\\delta}\\sigma_{s}{\\boldsymbol{\\epsilon}}^{\\prime}bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT \u2190 bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_\u03b4 italic_\u03c3 start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT bold_italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) + square-root start_ARG italic_\u03b4 end_ARG italic_\u03c3 start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT bold_italic_\u03f5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT(5)where \u03b4\ud835\udeff\\deltaitalic_\u03b4 is a step size which we fix to 0.10.10.10.1 here, and \u03f5\u2032superscriptbold-italic-\u03f5\u2032{\\boldsymbol{\\epsilon}}^{\\prime}bold_italic_\u03f5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is another independent sample of standard Gaussian noise. The purpose of the Langevin step is to help the marginal distribution of each \ud835\udc33ssubscript\ud835\udc33\ud835\udc60\\mathbf{z}_{s}bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT generated by the sampler to match the true marginal under the forward process starting at \ud835\udc31\u223cp(\ud835\udc31)similar-to\ud835\udc31\ud835\udc5d\ud835\udc31\\mathbf{x}\\sim p(\\mathbf{x})bold_x \u223c italic_p ( bold_x ).", "In the conditional generation setting, the data \ud835\udc31\ud835\udc31\\mathbf{x}bold_x is equipped with a conditioning signal \ud835\udc1c\ud835\udc1c\\mathbf{c}bold_c, which may represent a class label, text caption, or other type of conditioning. To train a diffusion model to fit p(\ud835\udc31|\ud835\udc1c)\ud835\udc5dconditional\ud835\udc31\ud835\udc1cp(\\mathbf{x}|\\mathbf{c})italic_p ( bold_x | bold_c ), the only modification that needs to be made is to provide \ud835\udc1c\ud835\udc1c\\mathbf{c}bold_c to the model as \ud835\udc31^\u03b8(\ud835\udc33t,\ud835\udc1c)subscript^\ud835\udc31\ud835\udf03subscript\ud835\udc33\ud835\udc61\ud835\udc1c\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{z}_{t},\\mathbf{c})over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_c ). Improvements to sample quality can be obtained in this setting by using classifier-free guidance (Ho and Salimans, 2021). This method samples using adjusted model predictions \u03f5~\u03b8subscript~bold-italic-\u03f5\ud835\udf03\\tilde{{\\boldsymbol{\\epsilon}}}_{\\theta}over~ start_ARG bold_italic_\u03f5 end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT, constructed via\u03f5~\u03b8(\ud835\udc33t,\ud835\udc1c)=(1+w)\u03f5\u03b8(\ud835\udc33t,\ud835\udc1c)\u2212w\u03f5\u03b8(\ud835\udc33t),subscript~bold-italic-\u03f5\ud835\udf03subscript\ud835\udc33\ud835\udc61\ud835\udc1c1\ud835\udc64subscriptbold-italic-\u03f5\ud835\udf03subscript\ud835\udc33\ud835\udc61\ud835\udc1c\ud835\udc64subscriptbold-italic-\u03f5\ud835\udf03subscript\ud835\udc33\ud835\udc61\\displaystyle\\tilde{{\\boldsymbol{\\epsilon}}}_{\\theta}(\\mathbf{z}_{t},\\mathbf{c})=(1+w){\\boldsymbol{\\epsilon}}_{\\theta}(\\mathbf{z}_{t},\\mathbf{c})-w{\\boldsymbol{\\epsilon}}_{\\theta}(\\mathbf{z}_{t}),over~ start_ARG bold_italic_\u03f5 end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_c ) = ( 1 + italic_w ) bold_italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_c ) - italic_w bold_italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,(6)where w\ud835\udc64witalic_w is the guidance strength, \u03f5\u03b8(\ud835\udc33t,\ud835\udc1c)=1\u03c3t(\ud835\udc33t\u2212\ud835\udc31^\u03b8(\ud835\udc33t,\ud835\udc1c))subscriptbold-italic-\u03f5\ud835\udf03subscript\ud835\udc33\ud835\udc61\ud835\udc1c1subscript\ud835\udf0e\ud835\udc61subscript\ud835\udc33\ud835\udc61subscript^\ud835\udc31\ud835\udf03subscript\ud835\udc33\ud835\udc61\ud835\udc1c{\\boldsymbol{\\epsilon}}_{\\theta}(\\mathbf{z}_{t},\\mathbf{c})=\\frac{1}{\\sigma_{t}}(\\mathbf{z}_{t}-\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{z}_{t},\\mathbf{c}))bold_italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_c ) = divide start_ARG 1 end_ARG start_ARG italic_\u03c3 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_c ) ) is the regular conditional model prediction, and \u03f5\u03b8(\ud835\udc33t)subscriptbold-italic-\u03f5\ud835\udf03subscript\ud835\udc33\ud835\udc61{\\boldsymbol{\\epsilon}}_{\\theta}(\\mathbf{z}_{t})bold_italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is a prediction from an unconditional model jointly trained with the conditional model (if \ud835\udc1c\ud835\udc1c\\mathbf{c}bold_c consists of embedding vectors, unconditional modeling can be represented as \ud835\udc1c=\ud835\udfce\ud835\udc1c0\\mathbf{c}=\\mathbf{0}bold_c = bold_0). For w>0\ud835\udc640w>0italic_w > 0 this adjustment has the effect of over-emphasizing the effect of conditioning on the signal \ud835\udc1c\ud835\udc1c\\mathbf{c}bold_c, which tends to produce samples of lower diversity but higher quality compared to sampling from the regular conditional model (Ho and Salimans, 2021). The method can be interpreted as a way to guide the samples towards areas where an implicit classifier p(\ud835\udc1c|\ud835\udc33t)\ud835\udc5dconditional\ud835\udc1csubscript\ud835\udc33\ud835\udc61p(\\mathbf{c}|\\mathbf{z}_{t})italic_p ( bold_c | bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) has high likelihood, and is an adaptation of the explicit classifier guidance method proposed by Dhariwal and Nichol (2021).", "Our approach to video generation using diffusion models is to use the standard diffusion model formalism described in Section\u00a02 with a neural network architecture suitable for video data. Each of our models is trained to jointly model a fixed number of frames at a fixed spatial resolution. To extend sampling to longer sequences of frames or higher spatial resolutions, we will repurpose our models with a conditioning technique described later in Section\u00a03.1.", "In prior work on image modeling, the standard architecture for \ud835\udc31^\u03b8subscript^\ud835\udc31\ud835\udf03\\hat{\\mathbf{x}}_{\\theta}over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT in an image diffusion model is a U-Net\u00a0(Ronneberger et\u00a0al., 2015; Salimans et\u00a0al., 2017), which is a neural network architecture constructed as a spatial downsampling pass followed by a spatial upsampling pass with skip connections to the downsampling pass activations. The network is built from layers of 2D convolutional residual blocks, for example in the style of the Wide ResNet\u00a0(Zagoruyko and Komodakis, 2016), and each such convolutional block is followed by a spatial attention block\u00a0(Vaswani et\u00a0al., 2017; Wang et\u00a0al., 2018; Chen et\u00a0al., 2018). Conditioning information, such as \ud835\udc1c\ud835\udc1c\\mathbf{c}bold_c and \u03bbtsubscript\ud835\udf06\ud835\udc61\\lambda_{t}italic_\u03bb start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, is provided to the network in the form of an embedding vector added into each residual block (we find it helpful for our models to process these embedding vectors using several MLP layers before adding).", "We propose to extend this image diffusion model architecture to video data, given by a block of a fixed number of frames, using a particular type of 3D U-Net\u00a0(\u00c7i\u00e7ek et\u00a0al., 2016) that is factorized over space and time. First, we modify the image model architecture by changing each 2D convolution into a space-only 3D convolution, for instance, we change each 3x3 convolution into a 1x3x3 convolution (the first axis indexes video frames, the second and third index the spatial height and width). The attention in each spatial attention block remains as attention over space; i.e., the first axis is treated as a batch axis. Second, after each spatial attention block, we insert a temporalattention block that performs attention over the first axis and treats the spatial axes as batch axes.We use relative position embeddings\u00a0(Shaw et\u00a0al., 2018) in each temporal attention block so that the network can distinguish ordering of frames in a way that does not require an absolute notion of video time. We visualize the model architecture in Fig.\u00a01.", "The use of factorized space-time attention is known to be a good choice in video transformers for its computational efficiency\u00a0(Arnab et\u00a0al., 2021; Bertasius et\u00a0al., 2021; Ho et\u00a0al., 2019).An advantage of our factorized space-time architecture, which is unique to our video generation setting, is that it is particularly straightforward to mask the model to run on independent images rather than a video, simply by removing the attention operation inside each time attention block and fixing the attention matrix to exactly match each key and query vector at each video timestep. The utility of doing so is that it allows us to jointly train the model on both video and image generation.We find in our experiments that this joint training is important for sample quality\u00a0(Section\u00a04).", "The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate of at least 24 frames per second. To manage the computational requirements of training our models, we only train on a small subset of say 16 frames at a time. However, at test time we can generate longer videos by extending our samples. For example, we could first generate a video \ud835\udc31a\u223cp\u03b8(\ud835\udc31)similar-tosuperscript\ud835\udc31asubscript\ud835\udc5d\ud835\udf03\ud835\udc31\\mathbf{x}^{\\text{a}}\\sim p_{\\theta}(\\mathbf{x})bold_x start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT \u223c italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x ) consisting of 16 frames, and then extend it with a second sample \ud835\udc31b\u223cp\u03b8(\ud835\udc31b|\ud835\udc31a)similar-tosuperscript\ud835\udc31bsubscript\ud835\udc5d\ud835\udf03conditionalsuperscript\ud835\udc31bsuperscript\ud835\udc31a\\mathbf{x}^{\\text{b}}\\sim p_{\\theta}(\\mathbf{x}^{\\text{b}}|\\mathbf{x}^{\\text{a}})bold_x start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT \u223c italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT | bold_x start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT ). If \ud835\udc31bsuperscript\ud835\udc31b\\mathbf{x}^{\\text{b}}bold_x start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT consists of frames following \ud835\udc31asuperscript\ud835\udc31a\\mathbf{x}^{\\text{a}}bold_x start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT, this allows us to autoregressively extend our sampled videos to arbitrary lengths, which we demonstrate in Section\u00a04.3.3. Alternatively, we could choose \ud835\udc31asuperscript\ud835\udc31a\\mathbf{x}^{\\text{a}}bold_x start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT to represent a video of lower frame rate, and then define \ud835\udc31bsuperscript\ud835\udc31b\\mathbf{x}^{\\text{b}}bold_x start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT to be those frames in between the frames of \ud835\udc31asuperscript\ud835\udc31a\\mathbf{x}^{\\text{a}}bold_x start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT.This allows one to then to upsample a video temporally, similar to how Menick and Kalchbrenner (2019) generate high resolution images through spatial upsampling.", "Both approaches require one to sample from a conditional model, p\u03b8(\ud835\udc31b|\ud835\udc31a)subscript\ud835\udc5d\ud835\udf03conditionalsuperscript\ud835\udc31bsuperscript\ud835\udc31ap_{\\theta}(\\mathbf{x}^{\\text{b}}|\\mathbf{x}^{\\text{a}})italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT | bold_x start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT ). This conditional model could be trained explicitly, but it can also be derived approximately from our unconditional model p\u03b8(\ud835\udc31)subscript\ud835\udc5d\ud835\udf03\ud835\udc31p_{\\theta}(\\mathbf{x})italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x ) by imputation, which has the advantage of not requiring a separately trained model. For example, Song et\u00a0al. (2021) present a general method for conditional sampling from a jointly trained diffusion model p\u03b8(\ud835\udc31=[\ud835\udc31a,\ud835\udc31b])subscript\ud835\udc5d\ud835\udf03\ud835\udc31superscript\ud835\udc31asuperscript\ud835\udc31bp_{\\theta}(\\mathbf{x}=[\\mathbf{x}^{\\text{a}},\\mathbf{x}^{\\text{b}}])italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x = [ bold_x start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT , bold_x start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT ] ): In their approach to sampling from p\u03b8(\ud835\udc31b|\ud835\udc31a)subscript\ud835\udc5d\ud835\udf03conditionalsuperscript\ud835\udc31bsuperscript\ud835\udc31ap_{\\theta}(\\mathbf{x}^{\\text{b}}|\\mathbf{x}^{\\text{a}})italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT | bold_x start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT ), the sampling procedure for updating \ud835\udc33sbsubscriptsuperscript\ud835\udc33b\ud835\udc60\\mathbf{z}^{\\text{b}}_{s}bold_z start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is unchanged from the standard method for sampling from p\u03b8(\ud835\udc33s|\ud835\udc33t)subscript\ud835\udc5d\ud835\udf03conditionalsubscript\ud835\udc33\ud835\udc60subscript\ud835\udc33\ud835\udc61p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), with \ud835\udc33s=[\ud835\udc33sa,\ud835\udc33sb]subscript\ud835\udc33\ud835\udc60subscriptsuperscript\ud835\udc33a\ud835\udc60subscriptsuperscript\ud835\udc33b\ud835\udc60\\mathbf{z}_{s}=[\\mathbf{z}^{\\text{a}}_{s},\\mathbf{z}^{\\text{b}}_{s}]bold_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = [ bold_z start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , bold_z start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ], but the samples for \ud835\udc33sasubscriptsuperscript\ud835\udc33a\ud835\udc60\\mathbf{z}^{\\text{a}}_{s}bold_z start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT are replaced by exact samples from the forward process, q(\ud835\udc33sa|\ud835\udc31a)\ud835\udc5econditionalsubscriptsuperscript\ud835\udc33a\ud835\udc60superscript\ud835\udc31aq(\\mathbf{z}^{\\text{a}}_{s}|\\mathbf{x}^{\\text{a}})italic_q ( bold_z start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | bold_x start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT ), at each iteration. The samples \ud835\udc33sasubscriptsuperscript\ud835\udc33a\ud835\udc60\\mathbf{z}^{\\text{a}}_{s}bold_z start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT then have the correct marginal distribution by construction, and the samples \ud835\udc33sbsubscriptsuperscript\ud835\udc33b\ud835\udc60\\mathbf{z}^{\\text{b}}_{s}bold_z start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT will conform with \ud835\udc33sasubscriptsuperscript\ud835\udc33a\ud835\udc60\\mathbf{z}^{\\text{a}}_{s}bold_z start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT through their effect on the denoising model \ud835\udc31^\u03b8([\ud835\udc33ta,\ud835\udc33tb])subscript^\ud835\udc31\ud835\udf03subscriptsuperscript\ud835\udc33a\ud835\udc61subscriptsuperscript\ud835\udc33b\ud835\udc61\\hat{\\mathbf{x}}_{\\theta}([\\mathbf{z}^{\\text{a}}_{t},\\mathbf{z}^{\\text{b}}_{t}])over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( [ bold_z start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_z start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] ). Similarly, we could sample \ud835\udc33sasubscriptsuperscript\ud835\udc33a\ud835\udc60\\mathbf{z}^{\\text{a}}_{s}bold_z start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT from q(\ud835\udc33sa|\ud835\udc31a,\ud835\udc33ta)\ud835\udc5econditionalsubscriptsuperscript\ud835\udc33a\ud835\udc60superscript\ud835\udc31asubscriptsuperscript\ud835\udc33a\ud835\udc61q(\\mathbf{z}^{\\text{a}}_{s}|\\mathbf{x}^{\\text{a}},\\mathbf{z}^{\\text{a}}_{t})italic_q ( bold_z start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | bold_x start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT , bold_z start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), which follows the correct conditional distribution in addition to the correct marginal. We will refer to both of these approaches as the replacement method for conditional sampling from diffusion models.", "When we tried the replacement method to conditional sampling, we found it to not work well for our video models: Although samples \ud835\udc31bsuperscript\ud835\udc31b\\mathbf{x}^{\\text{b}}bold_x start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT looked good in isolation, they were often not coherent with \ud835\udc31asuperscript\ud835\udc31a\\mathbf{x}^{\\text{a}}bold_x start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT. This is caused by a fundamental problem with this replacement sampling method. That is, the latents \ud835\udc33sbsubscriptsuperscript\ud835\udc33b\ud835\udc60\\mathbf{z}^{\\text{b}}_{s}bold_z start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT are updated in the direction provided by \ud835\udc31^\u03b8b(\ud835\udc33t)\u2248\ud835\udd3cq[\ud835\udc31b|\ud835\udc33t]subscriptsuperscript^\ud835\udc31b\ud835\udf03subscript\ud835\udc33\ud835\udc61subscript\ud835\udd3c\ud835\udc5edelimited-[]conditionalsuperscript\ud835\udc31\ud835\udc4fsubscript\ud835\udc33\ud835\udc61\\hat{\\mathbf{x}}^{\\text{b}}_{\\theta}(\\mathbf{z}_{t})\\approx\\mathbb{E}_{q}[\\mathbf{x}^{b}|\\mathbf{z}_{t}]over^ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2248 blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ bold_x start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT | bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ], while what is needed instead is \ud835\udd3cq[\ud835\udc31b|\ud835\udc33t,\ud835\udc31a]subscript\ud835\udd3c\ud835\udc5edelimited-[]conditionalsuperscript\ud835\udc31\ud835\udc4fsubscript\ud835\udc33\ud835\udc61superscript\ud835\udc31\ud835\udc4e\\mathbb{E}_{q}[\\mathbf{x}^{b}|\\mathbf{z}_{t},\\mathbf{x}^{a}]blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ bold_x start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT | bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT ]. Writing this in terms of the score of the data distribution, we get \ud835\udd3cq[\ud835\udc31b|\ud835\udc33t,\ud835\udc31a]=\ud835\udd3cq[\ud835\udc31b|\ud835\udc33t]+(\u03c3t2/\u03b1t)\u2207\ud835\udc33tblog\u2061q(\ud835\udc31a|\ud835\udc33t)subscript\ud835\udd3c\ud835\udc5edelimited-[]conditionalsuperscript\ud835\udc31\ud835\udc4fsubscript\ud835\udc33\ud835\udc61superscript\ud835\udc31\ud835\udc4esubscript\ud835\udd3c\ud835\udc5edelimited-[]conditionalsuperscript\ud835\udc31\ud835\udc4fsubscript\ud835\udc33\ud835\udc61subscriptsuperscript\ud835\udf0e2\ud835\udc61subscript\ud835\udefc\ud835\udc61subscript\u2207subscriptsuperscript\ud835\udc33\ud835\udc4f\ud835\udc61\ud835\udc5econditionalsuperscript\ud835\udc31\ud835\udc4esubscript\ud835\udc33\ud835\udc61\\mathbb{E}_{q}[\\mathbf{x}^{b}|\\mathbf{z}_{t},\\mathbf{x}^{a}]=\\mathbb{E}_{q}[\\mathbf{x}^{b}|\\mathbf{z}_{t}]+(\\sigma^{2}_{t}/\\alpha_{t})\\nabla_{\\mathbf{z}^{b}_{t}}\\log q(\\mathbf{x}^{a}|\\mathbf{z}_{t})blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ bold_x start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT | bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT ] = blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ bold_x start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT | bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] + ( italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT / italic_\u03b1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2207 start_POSTSUBSCRIPT bold_z start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_q ( bold_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT | bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), where the second term is missing in the replacement method. Assuming a perfect denoising model, plugging in this missing term would make conditional sampling exact. Since q(\ud835\udc31a|\ud835\udc33t)\ud835\udc5econditionalsuperscript\ud835\udc31\ud835\udc4esubscript\ud835\udc33\ud835\udc61q(\\mathbf{x}^{a}|\\mathbf{z}_{t})italic_q ( bold_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT | bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is not available in closed form, however, we instead propose to approximate it using a Gaussian of the form q(\ud835\udc31a|\ud835\udc33t)\u2248\ud835\udca9[\ud835\udc31^\u03b8a(\ud835\udc33t),(\u03c3t2/\u03b1t2)I]\ud835\udc5econditionalsuperscript\ud835\udc31\ud835\udc4esubscript\ud835\udc33\ud835\udc61\ud835\udca9subscriptsuperscript^\ud835\udc31a\ud835\udf03subscript\ud835\udc33\ud835\udc61subscriptsuperscript\ud835\udf0e2\ud835\udc61subscriptsuperscript\ud835\udefc2\ud835\udc61Iq(\\mathbf{x}^{a}|\\mathbf{z}_{t})\\approx\\mathcal{N}[\\hat{\\mathbf{x}}^{\\text{a}}_{\\theta}(\\mathbf{z}_{t}),(\\sigma^{2}_{t}/\\alpha^{2}_{t})\\text{I}]italic_q ( bold_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT | bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2248 caligraphic_N [ over^ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , ( italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT / italic_\u03b1 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) I ], where \ud835\udc31^\u03b8a(\ud835\udc33t)subscriptsuperscript^\ud835\udc31a\ud835\udf03subscript\ud835\udc33\ud835\udc61\\hat{\\mathbf{x}}^{\\text{a}}_{\\theta}(\\mathbf{z}_{t})over^ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is a reconstruction of the conditioning data \ud835\udc31asuperscript\ud835\udc31a\\mathbf{x}^{\\text{a}}bold_x start_POSTSUPERSCRIPT a end_POSTSUPERSCRIPT provided by our denoising model. Assuming a perfect model, this approximation becomes exact as t\u21920\u2192\ud835\udc610t\\rightarrow 0italic_t \u2192 0, and empirically we find it to be good for larger t\ud835\udc61titalic_t also. Plugging in the approximation, and adding a weighting factor wrsubscript\ud835\udc64\ud835\udc5fw_{r}italic_w start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, our proposed method to conditional sampling is a variant of the replacement method with an adjusted denoising model, \ud835\udc31~\u03b8bsubscriptsuperscript~\ud835\udc31\ud835\udc4f\ud835\udf03\\tilde{\\mathbf{x}}^{b}_{\\theta}over~ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT, defined by\ud835\udc31~\u03b8b(\ud835\udc33t)=\ud835\udc31^\u03b8b(\ud835\udc33t)\u2212wr\u03b1t2\u2207\ud835\udc33tb\u2225\ud835\udc31a\u2212\ud835\udc31^\u03b8a(\ud835\udc33t)\u222522.\\displaystyle\\tilde{\\mathbf{x}}^{b}_{\\theta}(\\mathbf{z}_{t})=\\hat{\\mathbf{x}}^{b}_{\\theta}(\\mathbf{z}_{t})-\\frac{w_{r}\\alpha_{t}}{2}\\nabla_{\\mathbf{z}^{b}_{t}}\\lVert\\mathbf{x}^{a}-\\hat{\\mathbf{x}}^{a}_{\\theta}(\\mathbf{z}_{t})\\rVert_{2}^{2}~{}.over~ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = over^ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - divide start_ARG italic_w start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG \u2207 start_POSTSUBSCRIPT bold_z start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2225 bold_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT - over^ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .(7)The additional gradient term in this expression can be interpreted as a form of guidance (Dhariwal and Nichol, 2021; Ho and Salimans, 2021) based on the model\u2019s reconstruction of the conditioning data, and we therefore refer to this method as reconstruction-guided sampling, or simply reconstruction guidance. Like with other forms of guidance, we find that choosing a larger weighting factor, wr>1subscript\ud835\udc64\ud835\udc5f1w_{r}>1italic_w start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT > 1, tends to improve sample quality. We empirically investigate reconstruction guidance in Section\u00a04.3.3, where we find it to work surprisingly well, especially when combined with predictor-corrector samplers using Langevin diffusion\u00a0(Song et\u00a0al., 2021).", "Reconstruction guidance also extends to the case of spatial interpolation (or super-resolution), in which the mean squared error loss is imposed on a downsampled version of the model prediction, and backpropagation is performed through this downsampling.In this setting, we have low resolution ground truth videos \ud835\udc31asuperscript\ud835\udc31\ud835\udc4e\\mathbf{x}^{a}bold_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT (e.g. at the 64x64 spatial resolution), which may be generated from a low resolution model, and we wish to upsample them into high resolution videos (e.g. at the 128x128 spatial resolution) using an unconditional high resolution diffusion model \ud835\udc31^\u03b8subscript^\ud835\udc31\ud835\udf03\\hat{\\mathbf{x}}_{\\theta}over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT. To accomplish this, we adjust the high resolution model as follows:\ud835\udc31~\u03b8(\ud835\udc33t)=\ud835\udc31^\u03b8(\ud835\udc33t)\u2212wr\u03b1t2\u2207\ud835\udc33t\u2225\ud835\udc31a\u2212\ud835\udc31^\u03b8a(\ud835\udc33t)\u222522\\displaystyle\\tilde{\\mathbf{x}}_{\\theta}(\\mathbf{z}_{t})=\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{z}_{t})-\\frac{w_{r}\\alpha_{t}}{2}\\nabla_{\\mathbf{z}_{t}}\\lVert\\mathbf{x}^{a}-\\hat{\\mathbf{x}}^{a}_{\\theta}(\\mathbf{z}_{t})\\rVert_{2}^{2}over~ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - divide start_ARG italic_w start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG \u2207 start_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2225 bold_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT - over^ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT(8)where \ud835\udc31^\u03b8a(\ud835\udc33t)subscriptsuperscript^\ud835\udc31\ud835\udc4e\ud835\udf03subscript\ud835\udc33\ud835\udc61\\hat{\\mathbf{x}}^{a}_{\\theta}(\\mathbf{z}_{t})over^ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is our model\u2019s reconstruction of the low-resolution video from \ud835\udc33tsubscript\ud835\udc33\ud835\udc61\\mathbf{z}_{t}bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, which is obtained by downsampling the high resolution output of the model using a differentiable downsampling algorithm such as bilinear interpolation.Note that it is also possible to simultaneously condition on low resolution videos while autoregressively extending samples at the high resolution using the same reconstruction guidance method. In Fig.\u00a02, we show samples of this approach for extending 16x64x64 low resolution samples at frameskip 4 to 64x128x128 samples at frameskip 1 using a 9x128x128 diffusion model.", "We report our results on video diffusion models for unconditional video generation\u00a0(Section\u00a04.1), conditional video generation (video prediction)\u00a0(Section\u00a04.2), and text-conditioned video generation\u00a0(Section\u00a04.3). We evaluate our models using standard metrics such as FVD\u00a0(Unterthiner et\u00a0al., 2018), FID\u00a0(Heusel et\u00a0al., 2017), and IS\u00a0(Salimans et\u00a0al., 2016); details on evaluation are provided below alongside each benchmark.Samples and additional results are provided at https://video-diffusion.github.io/.Architecture hyperparameters, training details, and compute resources are listed in\u00a0Appendix\u00a0A.", "To demonstrate our approach on unconditional generation, we use a popular benchmark of Soomro et\u00a0al. (2012) for unconditional modeling of video. The benchmark consists of short clips of people performing one of 101 activities, and was originally collected for the purpose of training action recognition models. We model short segments of 16 frames from this dataset, downsampled to a spatial resolution of 64x64. In Table\u00a01 we present perceptual quality scores for videos generated by our model, and we compare against methods from the literature, finding that our method strongly improves upon the previous state-of-the-art. ", "We use the data loader provided by TensorFlow Datasets (TFD, 2022) without further processing, and we train on all 13,320 videos. Similar to previous methods, we use the C3D network (Tran et\u00a0al., 2015)111We use the C3D model as implemented at github.com/pfnet-research/tgan2 (Saito et\u00a0al., 2020). for calculating FID and IS, using 10,000 samples generated from our model. C3D internally resizes input data to the 112x112 spatial resolution, so perceptual scores are approximately comparable even when the data is sampled at a different resolution originally. As discussed by Yushchenko et\u00a0al. (2019), methods in the literature are unfortunately not always consistent in the data preprocessing that is used, which may lead to small differences in reported scores between papers. The Inception Score we calculate for real data (\u224860absent60\\approx 60\u2248 60) is consistent with that reported by Kahembwe and Ramamoorthy (2020), who also report a higher real data Inception score of \u224890absent90\\approx 90\u2248 90 for data sampled at the 128x128 resolution, which indicates that our 64x64 model might be at a disadvantage compared to works that generate at a higher resolution. Nevertheless, our model obtains the best perceptual quality metrics that we could find in the literature.", "A common benchmark task for evaluating generative models of video is video prediction, where the model is given the first frame(s) of a video and is asked to generate the remainder. Models that do well on this conditional generation task are usually trained explicitly for this conditional setting, for example by being autoregressive across frames. Although our models are instead only trained unconditionally, we can adapt them to the video prediction setting by using the guidance method proposed in section\u00a03.1. Here we evaluate this method on two popular video prediction benchmarks, obtaining state-of-the-art results.", "We evaluate video prediction performance on BAIR Robot Pushing\u00a0(Ebert et\u00a0al., 2017), a standard benchmark in the video literature consisting of approximately 44000 videos of robot pushing motions at the 64x64 spatial resolution. Methods for this benchmark are conditioned on 1 frame and generate the next 15. Results are listed in Table\u00a03. Following the evaluation protocol of Babaeizadeh et\u00a0al. (2021) and others, we calculate FVD\u00a0(Unterthiner et\u00a0al., 2018) using the I3D network\u00a0(Carreira and Zisserman, 2017) by comparing 100\u00d7256100256100\\times 256100 \u00d7 256 model samples against the 256256256256 examples in the evaluation set.", "We additionally evaluate video prediction performance on the Kinetics-600 benchmark\u00a0(Kay et\u00a0al., 2017; Carreira et\u00a0al., 2018). Kinetics-600 contains approximately 400 thousand training videos depicting 600 different activities. We train unconditional models on this dataset at the 64\u00d764646464\\times 6464 \u00d7 64 resolution and evaluate on 50 thousand randomly sampled videos from the test set, where we condition on a randomly sampled subsequence of 5 frames and generate the next 11 frames. Like previous works, we calculate FVD and Inception Score using the I3D network\u00a0(Carreira and Zisserman, 2017). See Table\u00a03 for results. In our reported results we sample test videos without replacement, and we use the same randomly selected subsequences for generating model samples and for defining the ground truth, since this results in the lowest bias and variance in the reported FVD metric. However, from personal communication we learned that (Luc et\u00a0al., 2020; Clark et\u00a0al., 2019) instead sampled with replacement, and used a different random seed when sampling the ground truth data. We find that this way of evaluating raises the FVD obtained by our model slightly, from 16.216.216.216.2 to 16.916.916.916.9. Inception Score is unaffected.", "The remaining experiments reported are on text-conditioned video generation. In this text-conditioned video generation setting, we employ a dataset of 10 million captioned videos, and we condition the diffusion model on captions in the form of BERT-large embeddings\u00a0(Devlin et\u00a0al., 2019) processed using attention pooling. We consider two model sizes: a small model for the joint training ablation, and a large model for generating the remaining results (both architectures are described in detail in Appendix\u00a0A), and we explore the effects of joint video-image training, classifier-free guidance, and our newly proposed reconstruction guidance method for autoregressive extension and simultaneous spatial and temporal super-resolution. We report the following metrics in this section on 4096 samples: the video metric FVD, and the Inception-based image metrics FID and IS measured by averaging activations across frames (FID/IS-avg) and by measuring the first frame only (FID/IS-first). For FID and FVD, we report two numbers which are measured against the training and validation sets, respectively. For IS, we report two numbers which are averaged scores across 1 split and 10 splits of samples, respectively.", "As described in Section\u00a03, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.", "Table\u00a04 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.", "Table\u00a05 reports results that verify the effectiveness of classifier-free guidance\u00a0(Ho and Salimans, 2021) on text-to-video generation. As expected, there is clearimprovement in the Inception Score-like metrics with higher guidance weight, while the FID-like metrics improve and then degrade with increasing guidance weight. Similar findings have been reported on text-to-image generation\u00a0(Nichol et\u00a0al., 2021).", "Figure\u00a03 shows the effect of classifier-free guidance\u00a0(Ho and Salimans, 2021) on a text-conditioned video model. Similar to what was observed in other work that used classifier-free guidance on text-conditioned image generation\u00a0(Nichol et\u00a0al., 2021) and class-conditioned image generation\u00a0(Ho and Salimans, 2021; Dhariwal and Nichol, 2021), adding guidance increases the sample fidelity of each individual image and emphases the effect of the conditioning signal.", "In Section\u00a03.1 we proposed the reconstruction guidance method for conditional sampling from diffusion models, an improvement over the replacement method ofSong et\u00a0al. (2021).In Table\u00a06 we present results on generating longer videos using both techniques, and find that our proposed method indeed improves over the replacement method in terms of perceptual quality scores.", "Figure\u00a04 shows the samples of our reconstruction guidance method for conditional sampling compared to the replacement method (Section\u00a03.1) for the purposes of generating long samples in a block-autoregressive manner (Section\u00a04.3.3). The samples from the replacement method clearly show a lack of temporal coherence, since frames from different blocks throughout the generated videos appear to be uncorrelated samples (conditioned on \ud835\udc1c\ud835\udc1c\\mathbf{c}bold_c). The samples from the reconstruction guidance method, by contrast, are clearly temporally coherent over the course of the entire autoregressive generation process. Figure\u00a02 additionally shows samples of using the reconstruction guidance method to simultaneously condition on low frequency, low resolution videos while autoregressively extending temporally at a high resolution.", "Prior work on video generation has usually employed other types of generative models,notably, autoregressive models, VAEs, GANs, and normalizing flows (e.g. Babaeizadeh et\u00a0al., 2017, 2021; Lee et\u00a0al., 2018; Kumar et\u00a0al., 2019; Clark et\u00a0al., 2019; Weissenborn et\u00a0al., 2019; Yan et\u00a0al., 2021; Walker et\u00a0al., 2021). Related work on model classes similar to diffusion models includes\u00a0(Kadkhodaie and Simoncelli, 2020, 2021). Concurrent work (Yang et\u00a0al., 2022) proposes a diffusion-based approach to video generation that uses an image diffusion model to predict each individual frame within a RNN temporal autoregressive model. Our video diffusion model, by contrast, jointly models entire videos (blocks of frames) using a 3D video architecture with interleaved spatial and temporal attention, and we extend to long sequence lengths by filling in frames or autoregressive temporal extension.", "We have introduced diffusion models for video modeling, thus bringing recent advances in generative modeling using diffusion models to the video domain. We have shown that with straightforwardextensions of conventional U-Net architectures for 2D image modeling to 3D space-time, with factorized space-time attention blocks, one can learn effective generative models for video data using the standard formulation of the diffusion model.This includes unconditional models, text-conditioned models, and video prediction models.", "We have additionally demonstrated the benefits of joint image-video training and classifier-free guidance for video diffusion models on both video and image sample quality metrics, and we also introduced a new reconstruction-guided conditional sampling method that outperforms existing replacement or imputation methods for conditional sampling from unconditionally trained models. Our reconstruction guidance method can generate long sequences using either frame interpolation (or temporal super-resolution) or extrapolation in an auto-regressive fashion, and also can perform spatial super-resolution. We look forward to investigating this method in a wider variety of conditioning settings.", "Our goal with this work is to advance research on methods in generative modeling, and our methods have the potential to positively impact creative downstream applications. As with prior work in generative modeling, however, our methods have the potential for causing harmful impact and could enhance malicious or unethical uses of generative models, such as fake content generation, harassment, and misinformation spread, and thus we have decided not to release our models. Like all generative models, our models reflect the biases of their training datasets and thus may require curation to ensure fair results from sampling. In particular, our text-to-video models inherit the challenges faced by prior work on text-to-image models, and our future work will involve auditing for forms of social bias, similar to Buolamwini and Gebru (2018); Burns et\u00a0al. (2018); Steed and Caliskan (2021); Cho et\u00a0al. (2022) for image-to-text and image labeling models. We see our work as only a starting point for further investigation on video diffusion models and investigation into their societal implications, and we will aim to explore benchmark evaluations for social and cultural bias in the video generation setting and make the necessary research advances to address them."], "figure_types": {"3b2a675bb617ae1a920e8e29d535cdf27826e999/14-Figure5-1.png": "photograph(s)", "3b2a675bb617ae1a920e8e29d535cdf27826e999/3-Figure1-1.png": "schematic", "3b2a675bb617ae1a920e8e29d535cdf27826e999/5-Table1-1.png": "table", "3b2a675bb617ae1a920e8e29d535cdf27826e999/6-Table3-1.png": "table", "3b2a675bb617ae1a920e8e29d535cdf27826e999/7-Figure2-1.png": "photograph(s)", "3b2a675bb617ae1a920e8e29d535cdf27826e999/7-Table4-1.png": "table", "3b2a675bb617ae1a920e8e29d535cdf27826e999/8-Figure3-1.png": "photograph(s)", "3b2a675bb617ae1a920e8e29d535cdf27826e999/8-Table5-1.png": "table", "3b2a675bb617ae1a920e8e29d535cdf27826e999/8-Table6-1.png": "table", "3b2a675bb617ae1a920e8e29d535cdf27826e999/9-Figure4-1.png": "photograph(s)"}}, "2204.08110": {"paper_id": "paper_134", "title": "Language Contamination Helps Explain the Cross-lingual Capabilities of English Pretrained Models", "arxiv_url": "https://arxiv.org/abs/2204.08110", "s2orc_url": "https://www.semanticscholar.org/paper/48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775", "all_figures_tables": {"48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/1-Figure1-1.png": "Figure 1: Estimated non-English data in English pretraining corpora (token count and total percentage); even small percentages lead to many tokens. C4.En (\u2020) is estimated from the first 50M examples in the corpus.", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/10-Table5-1.png": "Table 5: Full results for the zero-shot BPC experiments in Section 3. Results noted with * correspond to cases of high UNK rates in the tokenization of the data (Section B).", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/11-Table6-1.png": "Table 6: Full results for the frozen POS tagging experiments in Section 3.", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/12-Table7-1.png": "Table 7: Full results for the finetuned POS tagging experiments in Section 3.", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/2-Table1-1.png": "Table 1: Results of the qualitative analysis of the nonEnglish lines in various pretraining corpora. Type abbreviations are defined in Section 2.2.", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/3-Figure2-1.png": "Figure 2: Average performance by each model across all languages for the task. Lower is better for BPC.", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/4-Table2-1.png": "Table 2: Spearman correlations between task performance and (a) in-language data amounts in pretraining corpora (lang. data) and (b) language similarity with English (en sim.). \u2217p &lt; 0.05 and \u2217\u2217p &lt; 0.001.", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/8-Table3-1.png": "Table 3: Full results for the automatic language composition analysis of pretraining corpora presented in Section 2. The last two columns include the total data that BERT and RoBERTa were trained on, respectively; C4 contains the data T5 was trained on, and contains the estimates for the first 50M examples in the full C4 dataset; \u2020 represents the projected estimate for the full dataset.", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/9-Table4-1.png": "Table 4: The average number of subword tokens per white-spaced word (and the percentage of UNKed out tokens) in the Wiki40b validation set for each language. Cases where more than 10% of tokens are unked out are in bold."}, "referred_figures_tables": [["48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/1-Figure1-1.png", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/1-Figure1-1.png"], ["48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/1-Figure1-1.png"], ["48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/4-Table2-1.png"], ["48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/4-Table2-1.png"], ["48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/4-Table2-1.png"], ["48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/1-Figure1-1.png", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/2-Table1-1.png"], ["48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/1-Figure1-1.png"], ["48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/2-Table1-1.png"], ["48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/2-Table1-1.png"], ["48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/1-Figure1-1.png"]], "question_id": [8, 15, 16, 17, 19, 22, 1, 3, 4, 7], "question": ["How well RoBERTa language modeling on Wiki-40B?", "What is training method used for decreasing the gap between monolingual model and multilingual model?", "What are the two factors to show potential reason for cross-lingual generalization", "How did the authors find potential causes of cross-lingual transfer?", "Which factor is more related to model performance between pretraining data size and language similarity?", "What is used for measure the quantities of non-English data?", "What is the range of the number of non-English tokens found in English corpus? ", "How many categories used in non-English text classifier?", "Is the line contains both English and non-English text is the most common in classifier?", "Is RoBERTa better for cross-lingual transfer rather than BERT?"], "question_section": ["3.1. Non-English MLM Evaluation", "3.2. POS Performance Across Languages", "3.3. Potential Reasons for Cross-lingual Generalization", "3.3. Potential Reasons for Cross-lingual Generalization", "3.3. Potential Reasons for Cross-lingual Generalization", "5. Limitations", "2.1Automatic Evaluation of Language Composition", "Qualitative Analysis of Non-English Texts", "2.2 Qualitative Analysis of Non-English Texts", "3.1. Non-English MLM Evaluation"], "question_trigger_sentence": ["We use Wiki-40B, a multilingual language modeling dataset that covers 41 languages Guo et al. (2020). Following the Wiki-40B paper, we report bits per character (BPC) to allow comparison between models with different tokenizations of the text. This finding is consistent with Tran (2020), who also found RoBERTa transfers well cross-lingually.", "To test if the effects of foreign language data carry through after finetuning, we also finetune a subset of the models (BERT, RoBERTa, mBERT, XLMR) for non-English POS tagging (Figure 1(c)).", "We then investigate the correlation between potential transfer causes and model performance (Table 2).", "We then investigate the correlation between potential transfer causes and model performance.", "We then investigate the correlation between potential transfer causes and model performance (Table 2). Specifically, we consider the quantity of target language data found in the model\u2019s pretraining corpus and the language similarity to English as potential causes of cross-lingual transfer.\nWe find that across tasks, RoBERTa task performance is most strongly correlated with the amount of target language data seen during pretraining. \nWe also consider the effect of language similarity on task performance, which is often hypothesized to facilitate cross-lingual transfer. We use the syntactic distance of languages calculated by Malaviya et al. (2017); more similar languages score lower. ", "We manually audit the types of mistakes made by the language classifier in Section 2.", "An obvious factor that affects the amount of non-English data in each corpus is the overall size of the dataset; however, even when controlling for size by looking at the percentage of non-English data, we still see that the smaller corpora (Wikipedia, BookCorpus, and Stories) have relatively less non-English data.\nWe also see that non-English text makes up small percentages of the overall data, though this still leads to millions of tokens in large datasets.", "We also perform a closer analysis on a random subset (200 per corpus) of non-English lines predicted by the language classifier. The majority of lines across datasets consist only of non-English text. The next most common type of non-English data is BiL; this contains many subtypes of data, such as codeswitching and foreign language dialogue within English text.", "We also perform a closer analysis on a random subset (200 per corpus) of non-English lines predicted by the language classifier. The next most common type of non-English data is BiL; this contains many subtypes of data, such as codeswitching and foreign language dialogue within English text.", "This finding is consistent with Tran (2020), who also found RoBERTa transfers well cross-lingually."], "question_type": ["Testing question", "Shallow question", "Testing question", "Deep/complex question", "Shallow question", "Shallow question", "Testing question", "Testing question", "Shallow question", "Shallow question"], "evidential_info": [[{"context": "We first measure the perplexity of English pretrained MLMs in other languages. We use Wiki-40B, a multilingual language modeling dataset that covers 41 languages Guo et\u00a0al. (2020). Following the Wiki-40B paper, we report bits per character (BPC) to allow comparison between models with different tokenizations of the text.", "rationale": "We first measure the perplexity of English pretrained MLMs in other languages. We use Wiki-40B, a multilingual language modeling dataset that covers 41 languages Guo et al. (2020). Following the Wiki-40B paper, we report bits per character (BPC) to allow comparison between models with different tokenizations of the text."}, {"context": "We find that both BERT models perform notably worse on modeling other languages; however, RoBERTa, reduces the gap with the multilingual models from 2.51 BPC to 0.87 BPC (Figure 1(a)). This finding is consistent with Tran (2020), who also found RoBERTa transfers well cross-lingually.", "rationale": "We first consider the performance of the encoders when probed for POS knowledge (Figure 1(b)).444For T5, this means that we evaluate the output of the encoder and discard the decoder. Unsurprisingly, on average all of the English models underperform the multilingual models. Similar to MLM, we find that RoBERTa performs better than BERT when probed for POS features on other languages; surprisingly, it also strongly outperforms T5, despite C4 containing more absolute non-English data than the RoBERTa corpus."}, {"context": "We first consider the performance of the encoders when probed for POS knowledge (Figure 1(b)).444For T5, this means that we evaluate the output of the encoder and discard the decoder. Unsurprisingly, on average all of the English models underperform the multilingual models. Similar to MLM, we find that RoBERTa performs better than BERT when probed for POS features on other languages; surprisingly, it also strongly outperforms T5, despite C4 containing more absolute non-English data than the RoBERTa corpus.", "rationale": "RoBERTa performs better than BERT."}], [{"context": "To test if the effects of foreign language data carry through after finetuning, we also finetune a subset of the models (BERT{}_{base}, RoBERTa{}_{base}, mBERT, XLMR{}_{base}) for non-English POS tagging (Figure 1(c)). After finetuning, the gap between the mono- and multilingual models is much smaller: RoBERTa only averages 2.65 points worse than XLM-R, compared to 12.5 points when probing.", "rationale": "To test if the effects of foreign language data carry through after finetuning, we also finetune a subset of the models (BERT{}_{base}, RoBERTa{}_{base}, mBERT, XLMR{}_{base}) for non-English POS tagging (Figure 1(c)). After finetuning, the gap between the mono- and multilingual models is much smaller: RoBERTa only averages 2.65 points worse than XLM-R, compared to 12.5 points when probing."}], [{"context": "We then investigate the correlation between potential transfer causes and model performance (Table 2). Specifically, we consider the quantity of target language data found in the model\u2019s pretraining corpus and the language similarity to English as potential causes of cross-lingual transfer.", "rationale": "We then investigate the correlation between potential transfer causes and model performance (Table 2). Specifically, we consider the quantity of target language data found in the model\u2019s pretraining corpus and the language similarity to English as potential causes of cross-lingual transfer."}, {"context": "We also consider the effect of language similarity on task performance, which is often hypothesized to facilitate cross-lingual transfer. We use the syntactic distance of languages calculated by Malaviya et\u00a0al. (2017); more similar languages score lower. However, we generally find that this is less correlated with performance than the quantity of target text, particularly for RoBERTa.", "rationale": "We also consider the effect of language similarity on task performance, which is often hypothesized to facilitate cross-lingual transfer. We use the syntactic distance of languages calculated by Malaviya et al. (2017); more similar languages score lower. However, we generally find that this is less correlated with performance than the quantity of target text, particularly for RoBERTa."}], [{"context": "We then investigate the correlation between potential transfer causes and model performance (Table 2). Specifically, we consider the quantity of target language data found in the model\u2019s pretraining corpus and the language similarity to English as potential causes of cross-lingual transfer.", "rationale": "We then investigate the correlation between potential transfer causes and model performance (Table 2). Specifically, we consider the quantity of target language data found in the model\u2019s pretraining corpus and the language similarity to English as potential causes of cross-lingual transfer."}, {"context": "We also consider the effect of language similarity on task performance, which is often hypothesized to facilitate cross-lingual transfer. We use the syntactic distance of languages calculated by Malaviya et\u00a0al. (2017); more similar languages score lower. However, we generally find that this is less correlated with performance than the quantity of target text, particularly for RoBERTa.", "rationale": "We also consider the effect of language similarity on task performance, which is often hypothesized to facilitate cross-lingual transfer. We use the syntactic distance of languages calculated by Malaviya et al. (2017); more similar languages score lower. However, we generally find that this is less correlated with performance than the quantity of target text, particularly for RoBERTa."}], [{"context": "We then investigate the correlation between potential transfer causes and model performance (Table 2). Specifically, we consider the quantity of target language data found in the model\u2019s pretraining corpus and the language similarity to English as potential causes of cross-lingual transfer.", "rationale": "We then investigate the correlation between potential transfer causes and model performance (Table 2). Specifically, we consider the quantity of target language data found in the model\u2019s pretraining corpus and the language similarity to English as potential causes of cross-lingual transfer."}, {"context": "We find that across tasks, RoBERTa task performance is most strongly correlated with the amount of target language data seen during pretraining. BERT and T5 task performance are less correlated with observed pretrained data, likely due to tokenization artifacts (Appendix B). Indeed, when we control for languages not written with Latin script on T5, the correlation between performance and the amount of target pretraining data increases to \\rho= 0.313.", "rationale": "We find that across tasks, RoBERTa task performance is most strongly correlated with the amount of target language data seen during pretraining. BERT and T5 task performance are less correlated with observed pretrained data, likely due to tokenization artifacts (Appendix B). Indeed, when we control for languages not written with Latin script on T5, the correlation between performance and the amount of target pretraining data increases to \\rho= 0.313."}, {"context": "We also consider the effect of language similarity on task performance, which is often hypothesized to facilitate cross-lingual transfer. We use the syntactic distance of languages calculated by Malaviya et\u00a0al. (2017); more similar languages score lower. However, we generally find that this is less correlated with performance than the quantity of target text, particularly for RoBERTa.", "rationale": "We also consider the effect of language similarity on task performance, which is often hypothesized to facilitate cross-lingual transfer. We use the syntactic distance of languages calculated by Malaviya et al. (2017); more similar languages score lower. However, we generally find that this is less correlated with performance than the quantity of target text, particularly for RoBERTa."}], [{"context": "We also see that non-English text makes up small percentages of the overall data, though this still leads to millions of tokens in large datasets.The largest individual languages after English only make up 0.01%, 0.15%, and 0.05% of the BERT, RoBERTa, and T5 training data, respectively.Multilingual pretraining work has shown that models generalize to new languages from varying amounts of data Delvin (2019); Lample and Conneau (2019); Conneau et\u00a0al. (2020); however, these approaches intentionally select data across languages, and most upsample low-resource languages during training.Without these considerations, it is an open question how well the models trained on these relatively small amounts of non-English data generalize.", "rationale": "We first measure how much non-English text exists in commonly used English pretraining corpora with two analyses: an automatic language identification to estimate the amount of foreign language data in these corpora, and a manual qualitative analysis of the text classified as non-English."}, {"context": "We also perform a closer analysis on a random subset (200 per corpus) of non-English lines predicted by the language classifier (Table 1). Each example is manually coded into one of six categories. The first set covers various kinds of foreign language data: NE, where the line contains only non-English language text; BiL, or bilingual, where the line contains both English and non-English text; Trans., in which the English and non-English data that are translations of each other; and Ent., where the line is primarily English but contains non-English entities. The last two codes pertain to errors made by the language classifier: En., where the line only contains English text, and XX, which refers to lines that contain no natural language.", "rationale": "We also perform a closer analysis on a random subset (200 per corpus) of non-English lines predicted by the language classifier (Table 1). Each example is manually coded into one of six categories. The first set covers various kinds of foreign language data: NE, where the line contains only non-English language text; BiL, or bilingual, where the line contains both English and non-English text; Trans., in which the English and non-English data that are translations of each other; and Ent., where the line is primarily English but contains non-English entities. The last two codes pertain to errors made by the language classifier: En., where the line only contains English text, and XX, which refers to lines that contain no natural language."}, {"context": "Our analysis finds that these corpora include very small percentages that amount to overall significant amounts of non-English text (Figure 1), particularly those derived from web-crawled data.Furthermore, the models trained on this data perform surprisingly well on other languages; this transfer is strongly correlated with the amount of target language data seen during pretraining. Notably, we find that the English T5 outperforms mBERT on POS tagging in multiple languages with no finetuning.", "rationale": "We also see that non-English text makes up small percentages of the overall data, though this still leads to millions of tokens in large datasets.The largest individual languages after English only make up 0.01%, 0.15%, and 0.05% of the BERT, RoBERTa, and T5 training data, respectively.Multilingual pretraining work has shown that models generalize to new languages from varying amounts of data Delvin (2019); Lample and Conneau (2019); Conneau et al. (2020); however, these approaches intentionally select data across languages, and most upsample low-resource languages during training.Without these considerations, it is an open question how well the models trained on these relatively small amounts of non-English data generalize."}, {"context": "We first measure how much non-English text exists in commonly used English pretraining corpora with two analyses: an automatic language identification to estimate the amount of foreign language data in these corpora, and a manual qualitative analysis of the text classified as non-English.", "rationale": "Our analysis finds that these corpora include very small percentages that amount to overall significant amounts of non-English text (Figure 1), particularly those derived from web-crawled data.Furthermore, the models trained on this data perform surprisingly well on other languages; this transfer is strongly correlated with the amount of target language data seen during pretraining. Notably, we find that the English T5 outperforms mBERT on POS tagging in multiple languages with no finetuning."}], [{"context": "A summary of the language identification experiments is presented in Figure 1.111Full results of this evaluation are detailed in Appendix C. We see that every corpus contains notable quantities of non-English data, with our estimates ranging between 300k to 406M tokens. An obvious factor that affects the amount of non-English data in each corpus is the overall size of the dataset; however, even when controlling for size by looking at the percentage of non-English data, we still see that the smaller corpora (Wikipedia, BookCorpus, and Stories) have relatively less non-English data.", "rationale": "300k to 406M tokens"}], [{"context": "We also perform a closer analysis on a random subset (200 per corpus) of non-English lines predicted by the language classifier (Table 1). Each example is manually coded into one of six categories. The first set covers various kinds of foreign language data: NE, where the line contains only non-English language text; BiL, or bilingual, where the line contains both English and non-English text; Trans., in which the English and non-English data that are translations of each other; and Ent., where the line is primarily English but contains non-English entities. The last two codes pertain to errors made by the language classifier: En., where the line only contains English text, and XX, which refers to lines that contain no natural language.", "rationale": "Each is example is manually categorized into six classes."}], [{"context": "We also perform a closer analysis on a random subset (200 per corpus) of non-English lines predicted by the language classifier (Table 1). Each example is manually coded into one of six categories. The first set covers various kinds of foreign language data: NE, where the line contains only non-English language text; BiL, or bilingual, where the line contains both English and non-English text; Trans., in which the English and non-English data that are translations of each other; and Ent., where the line is primarily English but contains non-English entities. The last two codes pertain to errors made by the language classifier: En., where the line only contains English text, and XX, which refers to lines that contain no natural language.", "rationale": "Lines containing both English and non-English text are referred to as BiL."}, {"context": "The majority of lines across datasets consist only of non-English text. The next most common type of non-English data is BiL; this contains many subtypes of data, such as codeswitching and foreign language dialogue within English text. These datasets also include parallel data at both the sentence- and word-level.222e.g., \u201d\u5927\u5b66 \u3010\u3060\u3044\u30fb\u304c\u304f\u3011\u2013 college\u201d, OpenWebTextWe note that all observed translations are between English and another language.Finally, some of the examples classified as non-English are actually English texts with non-English phrases.", "rationale": "BiL data is the second most common type of non-English data."}], [{"context": "We find that both BERT models perform notably worse on modeling other languages; however, RoBERTa, reduces the gap with the multilingual models from 2.51 BPC to 0.87 BPC (Figure 1(a)). This finding is consistent with Tran (2020), who also found RoBERTa transfers well cross-lingually.", "rationale": "RoBERTa performance is reported here."}]], "composition": ["RoBERTa performs at about 2.6 BPC on the MLM task with the Wiki-40B dataset. RoBERTa performs better than BERT.", "It is fine tuning.", "They are quantity of target language data in the pre-training corpora and language similarity.", "Authors do not discuss how they pointed to these potential causes.", "Pretraining data size is more related to model performance.", "Automatic language identification and manual qualitative analysis measure non-English data. They are denominated in lines, tokens, and percentages across the paper.", "Non-English tokens make up 300k to 406M in the datasets investigated.", "Non-English text classifier uses six categories.", "No, it is not.", "Yes, it is."], "Is_figure_in_evidence": [true, true, false, false, false, true, true, false, false, true], "Is_table_in_evidence": [false, false, true, true, true, true, false, true, true, false], "question_key": ["359", "365", "366", "367", "369", "371", "373", "375", "376", "378"], "passages": ["Pretrained language models have become an integral part of NLP systems. They come in two flavors: monolingual, where the model is trained on text from a single language, and multilingual, where the model is jointly trained on data from many different languages. Monolingual pretrained models are generally applied to tasks in the same language, whereas multilingual ones are used for cross-lingual tasks or transfer.", "Recent work has claimed that monolingual pretrained models are also surprisingly good at transferring between languages, despite ostensibly having never seen the target language before (Gogoulou et\u00a0al., 2021; Li et\u00a0al., 2021, inter alia).However, because of the large scale of pretraining data and because many pretraining corpora are not publicly available, it is currently unknown how much foreign language data exists in monolingual pretraining corpora.In this paper, we show that (1) these data are almost certainly contaminated with very small percentages of text from other languages and that (2) cross-lingual transfer is possible from such data leakage in the pretraining corpus.", "More specifically, we quantify how multilingual English pretrained models are in two steps. First, we analyze common English pretraining corpora with a large-scale automatic evaluation to estimate their language composition, as well as a smaller-scale manual analysis. Second, we perform experiments across fifty languages on masked language modeling and part-of-speech (POS) tagging to measure how well the models trained on these pretraining corpora perform outside of English.", "Our analysis finds that these corpora include very small percentages that amount to overall significant amounts of non-English text (Figure 1), particularly those derived from web-crawled data.Furthermore, the models trained on this data perform surprisingly well on other languages; this transfer is strongly correlated with the amount of target language data seen during pretraining. Notably, we find that the English T5 outperforms mBERT on POS tagging in multiple languages with no finetuning.", "Overall, these results indicate that the considered models are actually multilingual and that their ability to transfer across languages is not zero-shot, despite what has been recently claimed.Given the effort required to fully remove all non-English data, we question whether it is practically possible to train truly monolingual models at scale.", "We first measure how much non-English text exists in commonly used English pretraining corpora with two analyses: an automatic language identification to estimate the amount of foreign language data in these corpora, and a manual qualitative analysis of the text classified as non-English.", "We consider the following pretraining datasets: English Wikipedia(11.8GB); BookCorpus (Zhu et\u00a0al. 2015, 4.2GB); Stories (Trinh and Le 2018, 31GB); OpenWebText (Gokaslan and Cohen 2019, 38GB), which is an open-source version of WebText Radford et\u00a0al. (2019); CC-NEWS (Liu et\u00a0al. 2019, 76 GB); and C4.En (Raffel et\u00a0al. 2020, 305GB), as provided by Dodge et\u00a0al. (2021). We use the versions of Wikipedia, BookCorpus, and CC-NEWS used to pretrain RoBERTa.", "We use the FastText language identification model\u00a0Joulin et\u00a0al. (2017) to label every line in each corpus and keep lines as non-English if they score above a set confidence threshold (0.6). Due to the large size of C4, we subsample the first 50M examples (or 14%); we classify the entirety of all other datasets. Since language detection is imperfect, particularly for low-resource languages Caswell et\u00a0al. (2021), we present the results of this analysis as an estimate of the non-English data in each dataset and perform a qualitative analysis of potential errors in the following section.", "A summary of the language identification experiments is presented in Figure 1.111Full results of this evaluation are detailed in Appendix C. We see that every corpus contains notable quantities of non-English data, with our estimates ranging between 300k to 406M tokens. An obvious factor that affects the amount of non-English data in each corpus is the overall size of the dataset; however, even when controlling for size by looking at the percentage of non-English data, we still see that the smaller corpora (Wikipedia, BookCorpus, and Stories) have relatively less non-English data.", "Indeed, a major factor of language leakage is the method in which the data was collected: the datasets derived from web crawls contain higher percentages of non-English text (OpenWebText andCCNews). This is true even for C4, where the dataset was filtered with a classifier to exclude non-English text Raffel et\u00a0al. (2020). Since automatic methods for language identification are imperfect, the datasets with more manual filtering (such as Wikipedia, which has human editors curating its content) are less prone to non-English data than those relying on classifiers.Due to these challenges, it is likely impossible to fully remove non-English text from a web-crawled dataset at scale.", "We also see that non-English text makes up small percentages of the overall data, though this still leads to millions of tokens in large datasets.The largest individual languages after English only make up 0.01%, 0.15%, and 0.05% of the BERT, RoBERTa, and T5 training data, respectively.Multilingual pretraining work has shown that models generalize to new languages from varying amounts of data Delvin (2019); Lample and Conneau (2019); Conneau et\u00a0al. (2020); however, these approaches intentionally select data across languages, and most upsample low-resource languages during training.Without these considerations, it is an open question how well the models trained on these relatively small amounts of non-English data generalize.", "We also perform a closer analysis on a random subset (200 per corpus) of non-English lines predicted by the language classifier (Table 1). Each example is manually coded into one of six categories. The first set covers various kinds of foreign language data: NE, where the line contains only non-English language text; BiL, or bilingual, where the line contains both English and non-English text; Trans., in which the English and non-English data that are translations of each other; and Ent., where the line is primarily English but contains non-English entities. The last two codes pertain to errors made by the language classifier: En., where the line only contains English text, and XX, which refers to lines that contain no natural language.", "The majority of lines across datasets consist only of non-English text. The next most common type of non-English data is BiL; this contains many subtypes of data, such as codeswitching and foreign language dialogue within English text. These datasets also include parallel data at both the sentence- and word-level.222e.g., \u201d\u5927\u5b66 \u3010\u3060\u3044\u30fb\u304c\u304f\u3011\u2013 college\u201d, OpenWebTextWe note that all observed translations are between English and another language.Finally, some of the examples classified as non-English are actually English texts with non-English phrases.", "Our analysis also shows that the language classifier performs worse on the non-web crawled data. For example, it misclassified a quarter of the sampled lines from Stories as non-English when they in fact only contain English text; many of these lines stem from snippets of dialogue in the dataset. We generally observe that lines coded as En tend to be shorter than the correctly labeled lines and often contain non-standard English. The language classifier also struggles to handle noisy lines, for which it has no appropriate language label.", "We now ask: how well do models pretrained on these putatively English corpora perform on non-English tasks? While the English data is more multilingual than previously thought, there are many differences between monolingual and multilingual pretraining; non-English data are often tokenized into more subword units333For example, the Basque UD treebank requires on average 1.78, 2.59, and 2.66 tokens per word to be encoded by XLMR, RoBERTa, and BERT, respectively.and are much less frequently observed during monolingual training.", "We evaluate popular English pretrained models on tasks in more than 50 languages: (masked) language modeling, POS probing, and finetuned POS tagging.We compare the performance of monolingual BERT Devlin et\u00a0al. (2019), RoBERTa Liu et\u00a0al. (2019), and T5 Raffel et\u00a0al. (2020) against multilingual mBERT Delvin (2019) and XLM-R Conneau et\u00a0al. (2020). We report average performance across five runs with different random seeds for the POS evaluations. The full results and all languages can be found in Appendix D.", "We first measure the perplexity of English pretrained MLMs in other languages. We use Wiki-40B, a multilingual language modeling dataset that covers 41 languages Guo et\u00a0al. (2020). Following the Wiki-40B paper, we report bits per character (BPC) to allow comparison between models with different tokenizations of the text.", "We find that both BERT models perform notably worse on modeling other languages; however, RoBERTa, reduces the gap with the multilingual models from 2.51 BPC to 0.87 BPC (Figure 1(a)). This finding is consistent with Tran (2020), who also found RoBERTa transfers well cross-lingually.", "Next, we evaluate how well monolingual English models perform on non-English downstream tasks, using part-of-speech (POS) tagging as a case study.", "We first consider the performance of the encoders when probed for POS knowledge (Figure 1(b)).444For T5, this means that we evaluate the output of the encoder and discard the decoder. Unsurprisingly, on average all of the English models underperform the multilingual models. Similar to MLM, we find that RoBERTa performs better than BERT when probed for POS features on other languages; surprisingly, it also strongly outperforms T5, despite C4 containing more absolute non-English data than the RoBERTa corpus.", "This difference is likely due to two factors. First, in terms of relative percentages, RoBERTa is exposed to more non-English text than T5 (0.78% compared to only 0.22%). Secondly, RoBERTa\u2019s subword vocabulary is robust to unexpected inputs and does not substitute an UNK token any input tokens; in contrast, T5 and BERT have high rates of UNK tokens for some non-Latin languages (Appendix B).555UNK tokens refer to placeholder tokens used when the model receives an input not covered by its vocabulary.However, for many high-resource languages the English models perform competitively, with T5 outperforming mBERT on German and Portuguese, among others.", "To test if the effects of foreign language data carry through after finetuning, we also finetune a subset of the models (BERTbase\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52{}_{base}start_FLOATSUBSCRIPT italic_b italic_a italic_s italic_e end_FLOATSUBSCRIPT, RoBERTabase\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52{}_{base}start_FLOATSUBSCRIPT italic_b italic_a italic_s italic_e end_FLOATSUBSCRIPT, mBERT, XLMRbase\ud835\udc4f\ud835\udc4e\ud835\udc60\ud835\udc52{}_{base}start_FLOATSUBSCRIPT italic_b italic_a italic_s italic_e end_FLOATSUBSCRIPT) for non-English POS tagging (Figure 1(c)). After finetuning, the gap between the mono- and multilingual models is much smaller: RoBERTa only averages 2.65 points worse than XLM-R, compared to 12.5 points when probing.", "We then investigate the correlation between potential transfer causes and model performance (Table 2). Specifically, we consider the quantity of target language data found in the model\u2019s pretraining corpus and the language similarity to English as potential causes of cross-lingual transfer.", "We find that across tasks, RoBERTa task performance is most strongly correlated with the amount of target language data seen during pretraining. BERT and T5 task performance are less correlated with observed pretrained data, likely due to tokenization artifacts (Appendix B). Indeed, when we control for languages not written with Latin script on T5, the correlation between performance and the amount of target pretraining data increases to \u03c1=\ud835\udf0cabsent\\rho=italic_\u03c1 = 0.313.", "We also consider the effect of language similarity on task performance, which is often hypothesized to facilitate cross-lingual transfer. We use the syntactic distance of languages calculated by Malaviya et\u00a0al. (2017); more similar languages score lower. However, we generally find that this is less correlated with performance than the quantity of target text, particularly for RoBERTa.", "In this paper, we demonstrate that English pretrained models are exposed to a considerable amount of non-English data during pretraining, particularly in the case of more recent models that are trained on larger corpora derived from web crawls. We also find that this non-English text acts as a significant source of signal for cross-lingual transfer.", "Other recent work has focused on documenting the composition of pretraining corpora Dodge et\u00a0al. (2021); Gururangan et\u00a0al. (2022). Caswell et\u00a0al. (2021) manually audit a variety of multilingual datasets, finding data quality issues that are worse for low-resource languages and, similarly to our work, that texts for many languages are misclassified.In contrast, our focus is on the presence of foreign language data in primarily English corpora.", "Prior work has also shown the ability of monolingual models to transfer to other languages across a wide range of tasks Gogoulou et\u00a0al. (2021); Li et\u00a0al. (2021); Tran (2020); Artetxe et\u00a0al. (2020); Chi et\u00a0al. (2020), but these works do not consider the effect of foreign language data leakage as a source of signal. Notably, de\u00a0Souza et\u00a0al. (2021) mention the presence of foreign language data in their corpora but assume the small amounts observed will not affect model performance. However, our findings demonstrate that the amount of foreign language data directly correlates with cross-lingual transfer.", "An obvious follow-up to our findings would be to retrain the models with text that is verified to only contain English data; this would confirm the effect the leaked non-English data has on the models. We reiterate that the standard method for filtering these datasets, automatic language classifiers, is imperfect. This, and the infeasibility of manual filtering due to the scale of the data, means that controlling for the language the model is pretrained on is nearly impossible.", "However, the presence of foreign language data in pretraining corpora is not inherently problematic. Models trained on these datasets perform exceedingly well on their target languages and generalize to other languages much better than expected. Rather, it is important to remember that these models are not performing zero-shot transfer when used in other languages, given the scale and data with which they were pretrained.", "Our work has a number of limitations. First, we measure the quantities of non-English data using a language classifier. The amounts of foreign language data we report are estimates for each dataset, as the classifier likely misclassified some examples. We manually audit the types of mistakes made by the language classifier in Section 2. Additionally, we evaluate downstream performance via POS tagging, and it is possible that the models would exhibit different behavior on other NLP tasks.", "We also only consider the effect of foreign language contamination for English pretrained models. It is unclear to what extent this phenomenon affects monolingual models for other languages; however, since many of the resources evaluated in this work are also used to pretrain non-English monolingual models (e.g., Wikipedia), similar effects would likely be observed."], "figure_types": {"48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/1-Figure1-1.png": "plot", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/10-Table5-1.png": "table", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/11-Table6-1.png": "table", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/12-Table7-1.png": "table", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/2-Table1-1.png": "table", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/3-Figure2-1.png": "plot", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/4-Table2-1.png": "table", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/8-Table3-1.png": "table", "48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775/9-Table4-1.png": "table"}}, "1511.04599": {"paper_id": "paper_14", "title": "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks", "arxiv_url": "https://arxiv.org/abs/1511.04599", "s2orc_url": "https://www.semanticscholar.org/paper/52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35", "all_figures_tables": {"52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/1-Figure1-1.png": "Figure 1: An example of adversarial perturbations. First row: the original image x that is classified as k\u0302(x)=\u201cwhale\u201d. Second row: the image x + r classified", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/3-Figure2-1.png": "Figure 2: Adversarial examples for a linear binary classifier.", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/3-Figure3-1.png": "Figure 3: Illustration of Algorithm 1 for n = 2. Assume x0 \u2208 R n. The green plane is the graph of x 7\u2192 f(x0)+\u2207f(x0) T (x\u2212x0), which is tangent to the classifier function (wire-framed graph) x 7\u2192 f(x). The orange line indicates where f(x0) +\u2207f(x0) T (x\u2212x0) = 0. x1 is obtained from x0 by projecting x0 on the orange hyperplane of Rn.", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/4-Figure4-1.png": "Figure 4: For x0 belonging to class 4, let Fk = {x : fk(x) \u2212 f4(x) = 0}. These hyperplanes are depicted in solid lines and the boundary of P is shown in green dotted line.", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/4-Figure5-1.png": "Figure 5: For x0 belonging to class 4, let Fk = {x : fk(x) \u2212 f4(x) = 0}. The linearized zero level sets are shown in dashed lines and the boundary of the polyhedron P\u03030 in green.", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/6-Table1-1.png": "Table 1: The adversarial robustness of different classifiers on different datasets. The time required to compute one sample for each method is given in the time columns. The times are computed on a Mid-2015 MacBook Pro without CUDA support. The asterisk marks determines the values computed using a GTX 750 Ti GPU.", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/6-Table2-1.png": "Table 2: Values of \u03c1\u0302\u221eadv for four different networks based on DeepFool (smallest l\u221e perturbation) and fast gradient sign method with 90% of misclassification.", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/7-Figure6-1.png": "Figure 6", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/7-Figure7-1.png": "Figure 7: Fine-tuning based on magnified DeepFool\u2019s adversarial perturbations.", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/8-Figure8-1.png": "Figure 8: From \u201c1\u201d to \u201c7\u201d : original image classified as \u201c1\u201d and the DeepFool perturbed images classified as \u201c7\u201d using different values of \u03b1.", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/8-Figure9-1.png": "Figure 9: How the adversarial robustness is judged by different methods. The values are normalized by the corresponding \u03c1\u0302advs of the original network.", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/8-Table3-1.png": "Table 3: The test error of networks after the fine-tuning on adversarial examples (after five epochs). Each columns correspond to a different type of augmented perturbation."}, "referred_figures_tables": [["52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/1-Figure1-1.png", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/4-Figure5-1.png", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/7-Figure7-1.png", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/8-Figure8-1.png", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/8-Figure9-1.png", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/6-Table1-1.png", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/6-Table1-1.png", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/8-Table3-1.png"], ["52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/1-Figure1-1.png"], ["52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/8-Figure9-1.png", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/6-Table1-1.png", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/6-Table2-1.png"], ["52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/4-Figure4-1.png", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/4-Figure4-1.png"], ["52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/6-Table2-1.png"], ["52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/6-Table2-1.png"]], "question_id": [6, 0, 1, 2, 4, 8], "question": ["How is the authors' work different from the \u201cfast gradient sign\u201d method?", "What does an \"adversarial perturbation\" mean?", "What are the metrics used to compare the efficiency of different methods which compute the adversarial perturbations?", "What does an \"affine classifier\" mean?", "The paper's algorithm yields very small perturbations which are believed to be good approximations of the minimal perturbation. Quantitatively, how far is the paper's approximation from the minimal perturbation?", "Why did the authors measure the perturbations using the L`2 norm?"], "question_section": ["Introduction", "Abstract", "Introduction", "DeepFool for binary classifiers", "DeepFool for multiclass classifiers", "DeepFool for multiclass classifiers"], "question_trigger_sentence": [" Closer to our work, the authors of [4] introduced the \u201cfast gradient sign\u201d method, which computes the adversarial perturbations for a given classifier very efficiently.", "Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.", "We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods", "We begin by analyzing the case where f is an affine classifier f(x) = wT x + b, and then derive the general algorithm, which can be applied to any differentiable binary classifier f.", "However, we have observed in practice that our algorithm yields very small perturbations which are believed to be good approximations of the minimal perturbation.", "In this paper, we have measured the perturbations using the L`2 norm."], "question_type": ["Testing question", "Testing question", "Shallow question", "Testing question", "Testing question", "Deep/complex question"], "evidential_info": [[{"context": "We compare the proposed DeepFool approach to state-of-the-art techniques to compute adversarial perturbations in [18] and [4]. The method in [18] solves a series of penalized optimization problems to find the minimal perturbation, whereas [4] estimates the minimal perturbation by taking the sign of the gradient\\displaystyle\\hat{\\bm{r}}(\\bm{x})=\\epsilon\\,\\text{sign}\\left(\\nabla_{\\bm{x}}J(\\bm{\\theta},\\bm{x},y)\\right),with J the cost used to train the neural network, \\bm{\\theta} is the model parameters, and y is the label of \\bm{x}. The method is called fast gradient sign method. In practice, in the absence of general rules to choose the parameter \\epsilon, we chose the smallest \\epsilon such that 90\\% of the data are misclassified after perturbation.555Using this method, we observed empirically that one cannot reach 100\\% misclassification rate on some datasets. In fact, even by increasing \\epsilon to be very large, this method can fail in misclassifying all samples.", "rationale": "The DeepFool creates perturbations that are hardly noticeable compared to the fast gradient sign method."}, {"context": "It can be seen that DeepFool estimates smaller perturbations (hence closer to minimal perturbation defined in (1)) than the ones computed using the competitive approaches. For example, the average perturbation obtained using DeepFool is 5 times lower than the one estimated with [4]. On the ILSVRC2012 challenge dataset, the average perturbation is one order of magnitude smaller compared to the fast gradient method. It should be noted moreover that the proposed approach also yields slightly smaller perturbation vectors than the method in [18]. The proposed approach is hence more accurate in detecting directions that can potentially fool neural networks. As a result, DeepFool can be used as a valuable tool to accurately assess the robustness of classifiers.On the complexity aspect, the proposed approach is substantially faster than the standard method proposed in [18].In fact, while the approach [18] involves a costly minimization of a series of objective functions, we observed empirically that DeepFool converges in a few iterations (i.e., less than 3) to a perturbation vector that fools the classifier. Hence, the proposed approach reaches a more accurate perturbation vector compared to state-of-the-art methods, while being computationally efficient. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large-scale datasets. In that context, we provide the first quantitative evaluation of the robustness of state-of-the-art classifiers on the large-scale ImageNet dataset. It can be seen that despite their very good test accuracy, these methods are extremely unstable to adversarial perturbations: a perturbation that is 1000 smaller in magnitude than the original image is sufficient to fool state-of-the-art deep neural networks.", "rationale": "Fast gradient sign method just takes the sign of the gradient of the cost used to train the model and multiplies it by some constant."}, {"context": "We illustrate in Figure 1 perturbed images generated by the fast gradient sign and DeepFool. It can be observed that the proposed method generates adversarial perturbations which are hardly perceptible, while the fast gradient sign method outputs a perturbation image with higher norm.", "rationale": "The estimation of minimal adversarial perturbation is important for the accurate estimation of the robustness of the model. For example, fine-tuning the NIN model with the fast gradient sign adversarial samples and estimating its robustness with the same method show exaggerated results, while using the DeepFool will show that the robustness might actually drop in the first few epochs of training."}, {"context": "In this section, we fine-tune the networks of Table 1 on adversarial examples to build more robust classifiers for the MNIST and CIFAR-10 tasks. Specifically, for each network, we performed two experiments: (i) Fine-tuning the network on DeepFool\u2019s adversarial examples, (ii) Fine-tuning the network on the fast gradient sign adversarial examples.We fine-tune the networks by performing 5 additional epochs, with a 50\\% decreased learning rate only on the perturbed training set. For each experiment, the same training data was used through all 5 extra epochs. For the sake of completeness, we also performed 5 extra epochs on the original data. The evolution of \\hat{\\rho}_{\\text{adv}} for the different fine-tuning strategies is shown in Figures 5(a) to 5(d), where the robustness \\hat{\\rho}_{\\text{adv}} is estimated using DeepFool, since this is the most accurate method, as shown in Table 1. Observe that fine-tuning with DeepFool adversarial examples significantly increases the robustness of the networks to adversarial perturbations even after one extra epoch. For example, the robustness of the networks on MNIST is improved by 50% and NIN\u2019s robustness is increased by about 40%. On the other hand, quite surprisingly, the method in [4] can lead to a decreased robustness to adversarial perturbations of the network. We hypothesize that this behavior is due to the fact that perturbations estimated using the fast gradient sign method are much larger than minimal adversarial perturbations. Fine-tuning the network with overly perturbed images decreases the robustness of the networks to adversarial perturbations.To verify this hypothesis, we compare in Figure 7 the adversarial robustness of a network that is fine-tuned with the adversarial examples obtained using DeepFool, where norms of perturbations have been deliberately multiplied by \\alpha=1,2,3. Interestingly, we see that by magnifying the norms of the adversarial perturbations, the robustness of the fine-tuned network is decreased. This might explain why overly perturbed images decrease the robustness of MNIST networks: these perturbations can really change the class of the digits, hence fine-tuning based on these examples can lead to a drop of the robustness (for an illustration, see Figure 8). This lends credence to our hypothesis, and further shows the importance of designing accurate methods to compute minimal perturbations.", "rationale": "Fine-tuning with the fast gradient sign method's adversarial samples may lead to a drop in the actual performance of the model potentially due to the regularizer effect that may happen in geometric data augmentation schemes. Also, the original work of the fast gradient sign method used modified training compared to straightforward fine-tuning in the paper. On the other hand, fine-tuning with DeepFool can improve the accuracy of the models on different datasets."}, {"context": "Table 3 lists the accuracies of the fine-tuned networks. It can be seen that fine-tuning with DeepFool can improve the accuracy of the networks. Conversely, fine-tuning with the approach in [4] has led to a decrease of the test accuracy in all our experiments. This confirms the explanation that the fast gradient sign method outputs overly perturbed images that lead to images that are unlikely to occur in the test data. Hence, it decreases the performance of the method as it acts as a regularizer that does not represent the distribution of the original data. This effect is analogous to geometric data augmentation schemes, where large transformations of the original samples have a counter-productive effect on generalization.666While the authors of [4] reported an increased generalization performance on the MNIST task (from 0.94\\% to 0.84\\%) using adversarial regularization, it should be noted that the their experimental setup is significantly different as [4] trained the network based on a modified cost function, while we performed straightforward fine-tuning.", "rationale": "In terms of an increase in the robustness of the models, fine-tuning with the adversarial samples from the DeepFool method showed consistent improvement, compared to the fast gradient sign method which sometimes leads to a decrease in the robustness."}, {"context": "To emphasize the importance of a correct estimation of the minimal perturbation, we now show that using approximate methods can lead to wrong conclusions regarding the adversarial robustness of networks. We fine-tune the NIN classifier on the fast gradient sign adversarial examples. We follow the procedure described earlier but this time, we decreased the learning rate by 90%. We have evaluated the adversarial robustness of this network at different extra epochs using DeepFool and the fast gradient sign method. As one can see in Figure 9, the red plot exaggerates the effect of training on the adversarial examples. Moreover, it is not sensitive enough to demonstrate the loss of robustness at the first extra epoch. These observations confirm that using an accurate tool to measure the robustness of classifiers is crucial to derive conclusions about the robustness of networks. ", "rationale": "The average perturbation for the models with the DeepFool method gives much smaller perturbations, while the fast gradient sign method may give perturbations that are 5 times more. However, the fast gradient sign method has better complexity compared to DeepFool."}, {"context": "Our main contributions are the following:\u2022We propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.\u2022We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods 2) augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbations.\u2022We show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness. Hence, our method provides a better understanding of this intriguing phenomenon and of its influence factors.We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in [18]. The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in [18] is time-consuming and therefore does not scale to large datasets. In [14], the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on Pascal3D+ annotations. Recently, Tsai et al. [19] provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. [13] generated synthetic unrecognizable images, which are classified with high confidence. The authors of [3] also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of [4] introduced the \u201cfast gradient sign\u201d method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, [5] introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in [18] was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in [2] that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon.", "rationale": "Although the fast gradient sign method computes the adversarial perturbations efficiently, the gradients that are generated usually lead to sub-optimal solutions."}], [{"context": "Deep neural networks are powerful learning models that achieve state-of-the-art pattern recognition performance in many research areas such as bioinformatics [1, 16], speech [12, 6], and computer vision [10, 8]. Though deep networks have exhibited very good performance in classification tasks, they have recently been shown to be particularly unstable to adversarial perturbations of the data [18]. In fact, very small and often imperceptible perturbations of the data samples are sufficient to fool state-of-the-art classifiers and result in incorrect classification. (e.g., Figure 1). Formally, for a given classifier, we define an adversarial perturbation as the minimal perturbation \\bm{r} that is sufficient to change the estimated label \\hat{k}(\\bm{x}):\\displaystyle\\Delta(\\bm{x};\\hat{k}):=\\min_{\\bm{r}}\\|\\bm{r}\\|_{2}\\text{ subject to }\\hat{k}(\\bm{x}+\\bm{r})\\neq\\hat{k}(\\bm{x}),(1)where \\bm{x} is an image and \\hat{k}(\\bm{x}) is the estimated label. We call \\Delta(\\bm{x};\\hat{k}) the robustness of \\hat{k} at point \\bm{x}. The robustness of classifier \\hat{k} is then defined as", "rationale": "Adversarial perturbation is a small and imperceptible perturbation to the given data that leads to misclassification by the model. The paper formally defines it in the context of image classification as the minimum perturbation r applied to image x so that the predicted class is not the same as before the perturbation."}, {"context": "\\rho_{\\text{adv}}(\\hat{k})=\\mathbb{E}_{\\bm{x}}\\frac{\\Delta(\\bm{x};\\hat{k})}{\\|\\bm{x}\\|_{2}},(2)where \\mathbb{E}_{\\bm{x}} is the expectation over the distribution of data.The study of adversarial perturbations helps us understand what features are used by a classifier.The existence of such examples is seemingly in contradiction with the generalization ability of the learning algorithms. While deep networks achieve state-of-the-art performance in image classification tasks, they are not robust at all to small adversarial perturbations and tend to misclassify minimally perturbed data that looks visually similar to clean samples.Though adversarial attacks are specific to the classifier, it seems that the adversarial perturbations are generalizable across different models [18]. This can actually become a real concern from a security point of view.", "rationale": "Adversarial perturbations are important for understanding the limits of modern architectures and evaluating the robustness of the models."}, {"context": "An accurate method for finding the adversarial perturbations is thus necessary to study and compare the robustness of different classifiers to adversarial perturbations. It might be the key to a better understanding of the limits of current architectures and to design methods to increase robustness. Despite the importance of the vulnerability of state-of-the-art classifiers to adversarial instability, no well-founded method has been proposed to compute adversarial perturbations and we fill this gap in this paper.", "rationale": "Adversarial perturbations are generalizable over different types of models, not only classifiers. It allows the calculation of the robustness of the models."}], [{"context": "In order to evaluate the robustness to adversarial perturbations of a classifier f, we compute the average robustness \\hat{\\rho}_{\\text{adv}}(f), defined by\\hat{\\rho}_{\\text{adv}}(f)=\\frac{1}{|\\mathscr{D}|}\\sum_{\\bm{x}\\in\\mathscr{D}}\\frac{\\|\\hat{\\bm{r}}(\\bm{x})\\|_{2}}{\\|\\bm{x}\\|_{2}},(15)where \\hat{\\bm{r}}(\\bm{x}) is the estimated minimal perturbation obtained using DeepFool, and \\mathscr{D} denotes the test set444For ILSVRC2012, we used the validation data..", "rationale": "The average robustness of the models over different datasets and the average running time required to compute the adversarial sample are used to compare the methods for finding adversarial perturbations."}, {"context": "We report in Table 1 the accuracy and average robustness \\hat{\\rho}_{\\text{adv}} of each classifier computed using different methods. We also show the running time required for each method to compute one adversarial sample.", "rationale": "The robustness of the model over a test set is calculated as the average of the ratio of the calculated perturbation and the data sample. The 2-norm is used to find the norms of samples and perturbations."}, {"context": "It should be noted that, when perturbations are measured using the \\ell_{\\infty} norm, the above conclusions remain unchanged: DeepFool yields adversarial perturbations that are smaller (hence closer to the optimum) compared to other methods for computing adversarial examples. Table 2 reports the \\ell_{\\infty} robustness to adversarial perturbations measured by \\hat{\\rho}_{\\text{adv}}^{\\infty}(f)=\\frac{1}{|\\mathscr{D}|}\\sum_{\\bm{x}\\in\\mathscr{D}}\\frac{\\|\\hat{\\bm{r}}(\\bm{x})\\|_{\\infty}}{\\|\\bm{x}\\|_{\\infty}}, where \\hat{\\bm{r}}(\\bm{x}) is computed respectively using DeepFool (with p=\\infty, see Section 3.3), and the Fast gradient sign method for MNIST and CIFAR-10 tasks.", "rationale": "Even with the infinity-norm, the DeepFool method finds perturbations that are closer to minimal compared to other methods."}, {"context": "To emphasize the importance of a correct estimation of the minimal perturbation, we now show that using approximate methods can lead to wrong conclusions regarding the adversarial robustness of networks. We fine-tune the NIN classifier on the fast gradient sign adversarial examples. We follow the procedure described earlier but this time, we decreased the learning rate by 90%. We have evaluated the adversarial robustness of this network at different extra epochs using DeepFool and the fast gradient sign method. As one can see in Figure 9, the red plot exaggerates the effect of training on the adversarial examples. Moreover, it is not sensitive enough to demonstrate the loss of robustness at the first extra epoch. These observations confirm that using an accurate tool to measure the robustness of classifiers is crucial to derive conclusions about the robustness of networks. ", "rationale": "The estimation of minimal perturbation is important to draw correct conclusions about the robustness of the model and its improvement."}], [{"context": "Let f(\\bm{x}) be an affine classifier, i.e., f(\\bm{x})=\\mathbf{W}^{\\top}\\bm{x}+\\bm{b} for a given \\mathbf{W} and \\bm{b}. Since the mapping \\hat{k} is the outcome of a one-vs-all classification scheme, the minimal perturbation to fool the classifier can be rewritten as follows\\begin{split}&\\operatorname*{arg\\,min}_{\\bm{r}}\\|\\bm{r}\\|_{2}\\\\&\\text{s.t. }\\exists k:\\bm{w}_{k}^{\\top}(\\bm{x}_{0}+\\bm{r})+b_{k}\\geq\\bm{w}_{\\hat{k}(\\bm{x}_{0})}^{\\top}(\\bm{x}_{0}+\\bm{r})+b_{\\hat{k}(\\bm{x}_{0})},\\end{split}(6)where \\bm{w}_{k} is the k^{\\text{th}} column of \\mathbf{W}. Geometrically, the above problem corresponds to the computation of the distance between \\bm{x}_{0} and the complement of the convex polyhedron P,\\displaystyle P=\\bigcap_{k=1}^{c}\\{\\bm{x}:f_{\\hat{k}(\\bm{x}_{0})}(\\bm{x})\\geq f_{k}(\\bm{x})\\},(7)where \\bm{x}_{0} is located inside P.We denote this distance by \\text{{dist}}(\\bm{x}_{0},P^{c}).The polyhedron P defines the region of the space where f outputs the label \\hat{k}(\\bm{x}_{0}). This setting is depicted in Figure 4. The solution to the problem in Eq. (6) can be computed in closed form as follows. Define \\hat{l}(\\bm{x}_{0}) to be the closest hyperplane of the boundary of P (e.g. \\hat{l}(\\bm{x}_{0})=3 in Figure 4). Formally, \\hat{l}(\\bm{x}_{0}) can be computed as follows\\hat{l}(\\bm{x}_{0})=\\operatorname*{arg\\,min}_{k\\neq{\\hat{k}(\\bm{x}_{0})}}\\frac{\\left|f_{k}(\\bm{x}_{0})-f_{\\hat{k}(\\bm{x}_{0})}(\\bm{x}_{0})\\right|}{\\|\\bm{w}_{k}-\\bm{w}_{\\hat{k}(\\bm{x}_{0})}\\|_{2}}.(8)The minimum perturbation \\bm{r}_{*}(\\bm{x}_{0}) is the vector that projects \\bm{x}_{0} on the hyperplane indexed by \\hat{l}(\\bm{x}_{0}), i.e.,\\bm{r}_{*}(\\bm{x}_{0})=\\frac{\\left|f_{\\hat{l}(\\bm{x}_{0})}(\\bm{x}_{0})-f_{\\hat{k}(\\bm{x}_{0})}(\\bm{x}_{0})\\right|}{\\|\\bm{w}_{\\hat{l}(\\bm{x}_{0})}-\\bm{w}_{\\hat{k}(\\bm{x}_{0})}\\|_{2}^{2}}(\\bm{w}_{\\hat{l}(\\bm{x}_{0})}-\\bm{w}_{\\hat{k}(\\bm{x}_{0})}).(9)In other words, we find the closest projection of \\bm{x}_{0} on faces of P.", "rationale": "The affine classifier can be a scalar-valued function f: R^n -> R, where f(x) = w^T * x + b (i.e dot product followed by the addition of a constant)."}, {"context": "As a multiclass classifier can be viewed as aggregation of binary classifiers, we first propose the algorithm for binary classifiers.That is, we assume here \\hat{k}(\\bm{x})=\\text{sign}(f(\\bm{x})), where f is an arbitrary scalar-valued image classification function f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}. We also denote by \\mathscr{F}\\triangleq\\{\\bm{x}:f(\\bm{x})=0\\} the level set at zero of f.We begin by analyzing the case where f is an affine classifier f(\\bm{x})=\\bm{w}^{T}\\bm{x}+b, and then derive the general algorithm, which can be applied to any differentiable binary classifier f.", "rationale": "More general form of an affine classifier can have vector function f: R^n -> R^m, where f(x) = W^T * x + B, for a given matrix and vector W and B."}], [{"context": "It can be seen that DeepFool estimates smaller perturbations (hence closer to minimal perturbation defined in (1)) than the ones computed using the competitive approaches. For example, the average perturbation obtained using DeepFool is 5 times lower than the one estimated with [4]. On the ILSVRC2012 challenge dataset, the average perturbation is one order of magnitude smaller compared to the fast gradient method. It should be noted moreover that the proposed approach also yields slightly smaller perturbation vectors than the method in [18]. The proposed approach is hence more accurate in detecting directions that can potentially fool neural networks. As a result, DeepFool can be used as a valuable tool to accurately assess the robustness of classifiers.On the complexity aspect, the proposed approach is substantially faster than the standard method proposed in [18].In fact, while the approach [18] involves a costly minimization of a series of objective functions, we observed empirically that DeepFool converges in a few iterations (i.e., less than 3) to a perturbation vector that fools the classifier. Hence, the proposed approach reaches a more accurate perturbation vector compared to state-of-the-art methods, while being computationally efficient. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large-scale datasets. In that context, we provide the first quantitative evaluation of the robustness of state-of-the-art classifiers on the large-scale ImageNet dataset. It can be seen that despite their very good test accuracy, these methods are extremely unstable to adversarial perturbations: a perturbation that is 1000 smaller in magnitude than the original image is sufficient to fool state-of-the-art deep neural networks.", "rationale": "DeepFool estimates minimal adversarial perturbations better (i.e generates smaller perturbations) compared to other state-of-the-art methods."}, {"context": "It should be noted that, when perturbations are measured using the \\ell_{\\infty} norm, the above conclusions remain unchanged: DeepFool yields adversarial perturbations that are smaller (hence closer to the optimum) compared to other methods for computing adversarial examples. Table 2 reports the \\ell_{\\infty} robustness to adversarial perturbations measured by \\hat{\\rho}_{\\text{adv}}^{\\infty}(f)=\\frac{1}{|\\mathscr{D}|}\\sum_{\\bm{x}\\in\\mathscr{D}}\\frac{\\|\\hat{\\bm{r}}(\\bm{x})\\|_{\\infty}}{\\|\\bm{x}\\|_{\\infty}}, where \\hat{\\bm{r}}(\\bm{x}) is computed respectively using DeepFool (with p=\\infty, see Section 3.3), and the Fast gradient sign method for MNIST and CIFAR-10 tasks.", "rationale": "In the case of simple binary classifiers DeepFool converges to the edge of classes, thus very small constant n (n = 0.02 in the experiments) is used to multiply the calculated perturbation vector r."}, {"context": "Our main contributions are the following:\u2022We propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.\u2022We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods 2) augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbations.\u2022We show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness. Hence, our method provides a better understanding of this intriguing phenomenon and of its influence factors.We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in [18]. The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in [18] is time-consuming and therefore does not scale to large datasets. In [14], the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on Pascal3D+ annotations. Recently, Tsai et al. [19] provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. [13] generated synthetic unrecognizable images, which are classified with high confidence. The authors of [3] also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of [4] introduced the \u201cfast gradient sign\u201d method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, [5] introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in [18] was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in [2] that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon.", "rationale": "The authors claim that DeepFool generates better adversarial perturbations as they are smaller compared to other methods when using the infinity-norm or l-2 norm."}, {"context": "In practice, the above algorithm can often converge to a point on the zero level set \\mathscr{F}. In order to reach the other side of the classification boundary, the final perturbation vector \\hat{\\bm{r}} is multiplied by a constant 1+\\eta, with \\eta\\ll 1. In our experiments, we have used \\eta=0.02. ", "rationale": "DeepFool is proposed as a solid baseline for adversarial perturbation computation and robustness estimation of the models."}, {"context": "It should be noted that the optimization strategy of DeepFool is strongly tied to existing optimization techniques. In the binary case, it can be seen as Newton\u2019s iterative algorithm for finding roots of a nonlinear system of equations in the underdetermined case [15]. This algorithm is known as the normal flow method. The convergence analysis of this optimization technique can be found for example in [21]. Our algorithm in the binary case can alternatively be seen as a gradient descent algorithm with an adaptive step size that is automatically chosen at each iteration. The linearization in Algorithm 2 is also similar to a sequential convex programming where the constraints are linearized at each step.", "rationale": "The quality of the estimation of the minimal adversarial perturbation in DeepFool heavily depends on the existing optimization methods. In the case of simple classifiers, the actual convergence analysis of DeepFool's optimization can be found in the work [15, 21]."}], [{"context": "In this paper, we have measured the perturbations using the \\ell_{2} norm. Our framework is however not limited to this choice, and the proposed algorithm can simply be adapted to find minimal adversarial perturbations for any \\ell_{p} norm (p\\in[1,\\infty)). To do so, the update steps in line 10 and 11 in Algorithm 2 must be respectively substituted by the following updates\\displaystyle\\hat{l}\\displaystyle\\leftarrow\\operatorname*{arg\\,min}_{k\\neq{\\hat{k}(\\bm{x}_{0})}}\\frac{\\left|f^{\\prime}_{k}\\right|}{\\|\\bm{w}^{\\prime}_{k}\\|_{q}},(11)\\displaystyle\\bm{r}_{i}\\displaystyle\\leftarrow\\frac{|f^{\\prime}_{\\hat{l}}|}{\\|\\bm{w}^{\\prime}_{\\hat{l}}\\|_{q}^{q}}|\\bm{w}^{\\prime}_{\\hat{l}}|^{q-1}\\odot\\text{sign}(\\bm{w}^{\\prime}_{\\hat{l}}),(12)where \\odot is the pointwise product and q=\\frac{p}{p-1}.333To see this, one can apply Holder\u2019s inequality to obtain a lower bound on the \\ell_{p} norm of the perturbation. In particular, when p=\\infty (i.e., the supremum norm \\ell_{\\infty}), these update steps become\\displaystyle\\hat{l}\\displaystyle\\leftarrow\\operatorname*{arg\\,min}_{k\\neq{\\hat{k}(\\bm{x}_{0})}}\\frac{\\left|f^{\\prime}_{k}\\right|}{\\|\\bm{w}^{\\prime}_{k}\\|_{1}},(13)\\displaystyle\\bm{r}_{i}\\displaystyle\\leftarrow\\frac{|f^{\\prime}_{\\hat{l}}|}{\\|\\bm{w}^{\\prime}_{\\hat{l}}\\|_{1}}\\text{sign}(\\bm{w}^{\\prime}_{\\hat{l}}).(14)", "rationale": "The paper measures the perturbations in the l-2 norm, however, the DeepFool algorithm can be easily adapted to any l-p norm."}, {"context": "An accurate method for finding the adversarial perturbations is thus necessary to study and compare the robustness of different classifiers to adversarial perturbations. It might be the key to a better understanding of the limits of current architectures and to design methods to increase robustness. Despite the importance of the vulnerability of state-of-the-art classifiers to adversarial instability, no well-founded method has been proposed to compute adversarial perturbations and we fill this gap in this paper.", "rationale": "When using the l-infinity norm, the claims of the paper regarding the effectiveness of the DeepFool method remain the same."}, {"context": "It should be noted that, when perturbations are measured using the \\ell_{\\infty} norm, the above conclusions remain unchanged: DeepFool yields adversarial perturbations that are smaller (hence closer to the optimum) compared to other methods for computing adversarial examples. Table 2 reports the \\ell_{\\infty} robustness to adversarial perturbations measured by \\hat{\\rho}_{\\text{adv}}^{\\infty}(f)=\\frac{1}{|\\mathscr{D}|}\\sum_{\\bm{x}\\in\\mathscr{D}}\\frac{\\|\\hat{\\bm{r}}(\\bm{x})\\|_{\\infty}}{\\|\\bm{x}\\|_{\\infty}}, where \\hat{\\bm{r}}(\\bm{x}) is computed respectively using DeepFool (with p=\\infty, see Section 3.3), and the Fast gradient sign method for MNIST and CIFAR-10 tasks.", "rationale": "The DeepFool method can be seen as a solid baseline for efficiently and accurately finding adversarial perturbations."}, {"context": "Our main contributions are the following:\u2022We propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.\u2022We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods 2) augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbations.\u2022We show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness. Hence, our method provides a better understanding of this intriguing phenomenon and of its influence factors.We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in [18]. The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in [18] is time-consuming and therefore does not scale to large datasets. In [14], the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on Pascal3D+ annotations. Recently, Tsai et al. [19] provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. [13] generated synthetic unrecognizable images, which are classified with high confidence. The authors of [3] also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of [4] introduced the \u201cfast gradient sign\u201d method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, [5] introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in [18] was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in [2] that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon.", "rationale": "There were no well-founded methods before DeepFool for finding the adversarial perturbations for state-of-the-art models."}]], "composition": ["The fast gradient sign method is very quick but may lead to sub-optimal perturbations thus damaging the overall robustness estimation, and fine-tuning with such adversarial samples may sometimes result in a drop in the overall performance of the model. On the other hand, DeepFool creates adversarial perturbations that are closer to the absolute minimum compared to others thus giving us a more reliable tool in terms of robustness estimation and fine-tuning.", "Adversarial perturbation is a small and unnoticeable change to the data that fool the given model (i.e give a different class after applying the perturbation). It allows an understanding limits of existing architectures and calculation of the robustness of the models.", "The metrics that are used to compare different methods of finding adversarial perturbations are: the average robustness of the model estimated in some type of norm (2-norm or infinity-norm in the paper); and the average running time needed to find the estimated minimal perturbation.", "The affine classifier is the classifier in the form of an affine function. The general form that is used in the paper is the function f: R^n -> R^m, where f(x) = W^T * x + B, for the given matrix and vector W and B.", "The authors only claim that the DeepFool can be used as a baseline for adversarial perturbation calculation and that it heavily depends on existing optimization methods. In the paper, its effectiveness is proven relative to other state-of-the-art methods. Although the analysis of how far the estimated perturbation from the actual minimal perturbation can be found in referenced papers, the more sophisticated analysis is not mentioned in the paper. Thus, it is difficult to answer the question entirely.", "The authors claim that the DeepFool algorithm is a well-founded baseline for finding adversarial perturbations for state-of-the-art models. Although the use of the l-2 norm is not explicitly justified within the paper, it is a reasonable choice taking into account the scarcity of baseline methods. Also, the method can be easily adapted to any l-p norm and the claims of the paper seem to hold for the l-infinity norm."], "Is_figure_in_evidence": [true, true, true, true, false, false], "Is_table_in_evidence": [true, false, true, false, true, true], "question_key": ["399", "400", "401", "402", "404", "407"], "passages": ["Deep neural networks are powerful learning models that achieve state-of-the-art pattern recognition performance in many research areas such as bioinformatics [1, 16], speech [12, 6], and computer vision [10, 8]. Though deep networks have exhibited very good performance in classification tasks, they have recently been shown to be particularly unstable to adversarial perturbations of the data [18]. In fact, very small and often imperceptible perturbations of the data samples are sufficient to fool state-of-the-art classifiers and result in incorrect classification. (e.g., Figure 1). Formally, for a given classifier, we define an adversarial perturbation as the minimal perturbation \ud835\udc93\ud835\udc93\\bm{r}bold_italic_r that is sufficient to change the estimated label k^(\ud835\udc99)^\ud835\udc58\ud835\udc99\\hat{k}(\\bm{x})over^ start_ARG italic_k end_ARG ( bold_italic_x ):\u0394(\ud835\udc99;k^):=min\ud835\udc93\u2061\u2016\ud835\udc93\u20162\u00a0subject to\u00a0k^(\ud835\udc99+\ud835\udc93)\u2260k^(\ud835\udc99),assign\u0394\ud835\udc99^\ud835\udc58subscript\ud835\udc93subscriptnorm\ud835\udc932\u00a0subject to\u00a0^\ud835\udc58\ud835\udc99\ud835\udc93^\ud835\udc58\ud835\udc99\\displaystyle\\Delta(\\bm{x};\\hat{k}):=\\min_{\\bm{r}}\\|\\bm{r}\\|_{2}\\text{ subject to }\\hat{k}(\\bm{x}+\\bm{r})\\neq\\hat{k}(\\bm{x}),roman_\u0394 ( bold_italic_x ; over^ start_ARG italic_k end_ARG ) := roman_min start_POSTSUBSCRIPT bold_italic_r end_POSTSUBSCRIPT \u2225 bold_italic_r \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT subject to over^ start_ARG italic_k end_ARG ( bold_italic_x + bold_italic_r ) \u2260 over^ start_ARG italic_k end_ARG ( bold_italic_x ) ,(1)where \ud835\udc99\ud835\udc99\\bm{x}bold_italic_x is an image and k^(\ud835\udc99)^\ud835\udc58\ud835\udc99\\hat{k}(\\bm{x})over^ start_ARG italic_k end_ARG ( bold_italic_x ) is the estimated label. We call \u0394(\ud835\udc99;k^)\u0394\ud835\udc99^\ud835\udc58\\Delta(\\bm{x};\\hat{k})roman_\u0394 ( bold_italic_x ; over^ start_ARG italic_k end_ARG ) the robustness of k^^\ud835\udc58\\hat{k}over^ start_ARG italic_k end_ARG at point \ud835\udc99\ud835\udc99\\bm{x}bold_italic_x. The robustness of classifier k^^\ud835\udc58\\hat{k}over^ start_ARG italic_k end_ARG is then defined as", "\u03c1adv(k^)=\ud835\udd3c\ud835\udc99\u0394(\ud835\udc99;k^)\u2016\ud835\udc99\u20162,subscript\ud835\udf0cadv^\ud835\udc58subscript\ud835\udd3c\ud835\udc99\u0394\ud835\udc99^\ud835\udc58subscriptnorm\ud835\udc992\\rho_{\\text{adv}}(\\hat{k})=\\mathbb{E}_{\\bm{x}}\\frac{\\Delta(\\bm{x};\\hat{k})}{\\|\\bm{x}\\|_{2}},italic_\u03c1 start_POSTSUBSCRIPT adv end_POSTSUBSCRIPT ( over^ start_ARG italic_k end_ARG ) = blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT divide start_ARG roman_\u0394 ( bold_italic_x ; over^ start_ARG italic_k end_ARG ) end_ARG start_ARG \u2225 bold_italic_x \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG ,(2)where \ud835\udd3c\ud835\udc99subscript\ud835\udd3c\ud835\udc99\\mathbb{E}_{\\bm{x}}blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT is the expectation over the distribution of data.The study of adversarial perturbations helps us understand what features are used by a classifier.The existence of such examples is seemingly in contradiction with the generalization ability of the learning algorithms. While deep networks achieve state-of-the-art performance in image classification tasks, they are not robust at all to small adversarial perturbations and tend to misclassify minimally perturbed data that looks visually similar to clean samples.Though adversarial attacks are specific to the classifier, it seems that the adversarial perturbations are generalizable across different models [18]. This can actually become a real concern from a security point of view.", "An accurate method for finding the adversarial perturbations is thus necessary to study and compare the robustness of different classifiers to adversarial perturbations. It might be the key to a better understanding of the limits of current architectures and to design methods to increase robustness. Despite the importance of the vulnerability of state-of-the-art classifiers to adversarial instability, no well-founded method has been proposed to compute adversarial perturbations and we fill this gap in this paper.", "Our main contributions are the following:\u2022We propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.\u2022We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods 2) augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbations.\u2022We show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness. Hence, our method provides a better understanding of this intriguing phenomenon and of its influence factors.We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in [18]. The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in [18] is time-consuming and therefore does not scale to large datasets. In [14], the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on Pascal3D+ annotations. Recently, Tsai et al. [19] provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. [13] generated synthetic unrecognizable images, which are classified with high confidence. The authors of [3] also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of [4] introduced the \u201cfast gradient sign\u201d method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, [5] introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in [18] was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in [2] that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon.", "The rest of paper is organized as follows. In Section 2, we introduce an efficient algorithm to find adversarial perturbations in a binary classifier. The extension to the multiclass problem is provided in Section 3. In Section 4, we propose extensive experiments that confirm the accuracy of our method and outline its benefits in building more robust classifiers.", "As a multiclass classifier can be viewed as aggregation of binary classifiers, we first propose the algorithm for binary classifiers.That is, we assume here k^(\ud835\udc99)=sign(f(\ud835\udc99))^\ud835\udc58\ud835\udc99sign\ud835\udc53\ud835\udc99\\hat{k}(\\bm{x})=\\text{sign}(f(\\bm{x}))over^ start_ARG italic_k end_ARG ( bold_italic_x ) = sign ( italic_f ( bold_italic_x ) ), where f\ud835\udc53fitalic_f is an arbitrary scalar-valued image classification function f:\u211dn\u2192\u211d:\ud835\udc53\u2192superscript\u211d\ud835\udc5b\u211df:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}italic_f : blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2192 blackboard_R. We also denote by \u2131\u225c{\ud835\udc99:f(\ud835\udc99)=0}\u225c\u2131conditional-set\ud835\udc99\ud835\udc53\ud835\udc990\\mathscr{F}\\triangleq\\{\\bm{x}:f(\\bm{x})=0\\}script_F \u225c { bold_italic_x : italic_f ( bold_italic_x ) = 0 } the level set at zero of f\ud835\udc53fitalic_f.We begin by analyzing the case where f\ud835\udc53fitalic_f is an affine classifier f(\ud835\udc99)=\ud835\udc98T\ud835\udc99+b\ud835\udc53\ud835\udc99superscript\ud835\udc98\ud835\udc47\ud835\udc99\ud835\udc4ff(\\bm{x})=\\bm{w}^{T}\\bm{x}+bitalic_f ( bold_italic_x ) = bold_italic_w start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x + italic_b, and then derive the general algorithm, which can be applied to any differentiable binary classifier f\ud835\udc53fitalic_f.", "In the case where the classifier f\ud835\udc53fitalic_f is affine, it can easily be seen that the robustness of f\ud835\udc53fitalic_f at point \ud835\udc990subscript\ud835\udc990\\bm{x}_{0}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, \u0394(\ud835\udc990;f)\u0394subscript\ud835\udc990\ud835\udc53\\Delta(\\bm{x}_{0};f)roman_\u0394 ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ; italic_f )222From now on, we refer to a classifier either by f\ud835\udc53fitalic_f or its corresponding discrete mapping k^^\ud835\udc58\\hat{k}over^ start_ARG italic_k end_ARG. Therefore, \u03c1adv(k^)=\u03c1adv(f)subscript\ud835\udf0cadv^\ud835\udc58subscript\ud835\udf0cadv\ud835\udc53\\rho_{\\text{adv}}(\\hat{k})=\\rho_{\\text{adv}}(f)italic_\u03c1 start_POSTSUBSCRIPT adv end_POSTSUBSCRIPT ( over^ start_ARG italic_k end_ARG ) = italic_\u03c1 start_POSTSUBSCRIPT adv end_POSTSUBSCRIPT ( italic_f ) and \u0394(\ud835\udc99;k^)=\u0394(\ud835\udc99;f)\u0394\ud835\udc99^\ud835\udc58\u0394\ud835\udc99\ud835\udc53\\Delta(\\bm{x};\\hat{k})=\\Delta(\\bm{x};f)roman_\u0394 ( bold_italic_x ; over^ start_ARG italic_k end_ARG ) = roman_\u0394 ( bold_italic_x ; italic_f )., is equal to the distance from \ud835\udc990subscript\ud835\udc990\\bm{x}_{0}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT to the separating affine hyperplane \u2131={\ud835\udc99:\ud835\udc98T\ud835\udc99+b=0}\u2131conditional-set\ud835\udc99superscript\ud835\udc98\ud835\udc47\ud835\udc99\ud835\udc4f0\\mathscr{F}=\\{\\bm{x}:\\bm{w}^{T}\\bm{x}+b=0\\}script_F = { bold_italic_x : bold_italic_w start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x + italic_b = 0 } (Figure 2). The minimal perturbation to change the classifier\u2019s decision corresponds to the orthogonal projection of \ud835\udc990subscript\ud835\udc990\\bm{x}_{0}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT onto \u2131\u2131\\mathscr{F}script_F. It is given by the closed-form formula:\ud835\udc93*(\ud835\udc990)subscript\ud835\udc93subscript\ud835\udc990\\displaystyle\\bm{r}_{*}(\\bm{x}_{0})bold_italic_r start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ):=argmin\u2061\u2016\ud835\udc93\u20162assignabsentargminsubscriptnorm\ud835\udc932\\displaystyle:=\\operatorname*{arg\\,min}\\|\\bm{r}\\|_{2}:= start_OPERATOR roman_arg roman_min end_OPERATOR \u2225 bold_italic_r \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT(3)subject to\u00a0\u00a0sign\u00a0(f(\ud835\udc990+\ud835\udc93))\u2260\u00a0sign(f(\ud835\udc990))subject to\u00a0\u00a0sign\u00a0\ud835\udc53subscript\ud835\udc990\ud835\udc93\u00a0sign\ud835\udc53subscript\ud835\udc990\\displaystyle\\text{ subject to }\\text{ sign }(f(\\bm{x}_{0}+\\bm{r}))\\neq\\text{ sign}(f(\\bm{x}_{0}))subject to sign ( italic_f ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + bold_italic_r ) ) \u2260 sign ( italic_f ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) )=\u2212f(\ud835\udc990)\u2016\ud835\udc98\u201622\ud835\udc98.absent\ud835\udc53subscript\ud835\udc990superscriptsubscriptnorm\ud835\udc9822\ud835\udc98\\displaystyle=-\\frac{f(\\bm{x}_{0})}{\\|\\bm{w}\\|_{2}^{2}}\\bm{w}.= - divide start_ARG italic_f ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG start_ARG \u2225 bold_italic_w \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_w .", "Assuming now that f\ud835\udc53fitalic_f is a general binary differentiable classifier, we adopt an iterative procedure to estimate the robustness \u0394(\ud835\udc990;f)\u0394subscript\ud835\udc990\ud835\udc53\\Delta(\\bm{x}_{0};f)roman_\u0394 ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ; italic_f ). Specifically, at each iteration, f\ud835\udc53fitalic_f is linearized around the current point \ud835\udc99isubscript\ud835\udc99\ud835\udc56\\bm{x}_{i}bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and the minimal perturbation of the linearized classifier is computed asargmin\ud835\udc93i\u2061\u2016\ud835\udc93i\u20162\u00a0subject to\u00a0f(\ud835\udc99i)+\u2207f(\ud835\udc99i)T\ud835\udc93i=0.subscriptargminsubscript\ud835\udc93\ud835\udc56subscriptnormsubscript\ud835\udc93\ud835\udc562\u00a0subject to\u00a0\ud835\udc53subscript\ud835\udc99\ud835\udc56\u2207\ud835\udc53superscriptsubscript\ud835\udc99\ud835\udc56\ud835\udc47subscript\ud835\udc93\ud835\udc560\\displaystyle\\operatorname*{arg\\,min}_{\\bm{r}_{i}}\\|\\bm{r}_{i}\\|_{2}\\text{ subject to }f(\\bm{x}_{i})+\\nabla f(\\bm{x}_{i})^{T}\\bm{r}_{i}=0.start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2225 bold_italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT subject to italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + \u2207 italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0 .(4)The perturbation \ud835\udc93isubscript\ud835\udc93\ud835\udc56\\bm{r}_{i}bold_italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT at iteration i\ud835\udc56iitalic_i of the algorithm is computed using the closed form solution in Eq. (3), and the next iterate \ud835\udc99i+1subscript\ud835\udc99\ud835\udc561\\bm{x}_{i+1}bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT is updated. The algorithm stops when \ud835\udc99i+1subscript\ud835\udc99\ud835\udc561\\bm{x}_{i+1}bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT changes sign of the classifier. The DeepFool algorithm for binary classifiers is summarized in Algorithm 1 and a geometric illustration of the method is shown in Figure 3.", "In practice, the above algorithm can often converge to a point on the zero level set \u2131\u2131\\mathscr{F}script_F. In order to reach the other side of the classification boundary, the final perturbation vector \ud835\udc93^^\ud835\udc93\\hat{\\bm{r}}over^ start_ARG bold_italic_r end_ARG is multiplied by a constant 1+\u03b71\ud835\udf021+\\eta1 + italic_\u03b7, with \u03b7\u226a1much-less-than\ud835\udf021\\eta\\ll 1italic_\u03b7 \u226a 1. In our experiments, we have used \u03b7=0.02\ud835\udf020.02\\eta=0.02italic_\u03b7 = 0.02. ", "We now extend the DeepFool method to the multiclass case. The most common used scheme for multiclass classifiers is one-vs-all. Hence, we also propose our method based on this classification scheme. In this scheme, the classifier has c\ud835\udc50citalic_c outputs where c\ud835\udc50citalic_c is the number of classes. Therefore, a classifier can be defined as f:\u211dn\u2192\u211dc:\ud835\udc53\u2192superscript\u211d\ud835\udc5bsuperscript\u211d\ud835\udc50f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{c}italic_f : blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2192 blackboard_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT and the classification is done by the following mapping:k^(\ud835\udc99)=argmaxk\u2061fk(\ud835\udc99),^\ud835\udc58\ud835\udc99subscriptargmax\ud835\udc58subscript\ud835\udc53\ud835\udc58\ud835\udc99\\hat{k}(\\bm{x})=\\operatorname*{arg\\,max}_{k}f_{k}(\\bm{x}),over^ start_ARG italic_k end_ARG ( bold_italic_x ) = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_x ) ,(5)where fk(\ud835\udc99)subscript\ud835\udc53\ud835\udc58\ud835\udc99f_{k}(\\bm{x})italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_x ) is the output of f(\ud835\udc99)\ud835\udc53\ud835\udc99f(\\bm{x})italic_f ( bold_italic_x ) that corresponds to the kthsuperscript\ud835\udc58thk^{\\text{th}}italic_k start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT class.Similarly to the binary case, we first present the proposed approach for the linear case and then we generalize it to other classifiers.", "Let f(\ud835\udc99)\ud835\udc53\ud835\udc99f(\\bm{x})italic_f ( bold_italic_x ) be an affine classifier, i.e., f(\ud835\udc99)=\ud835\udc16\u22a4\ud835\udc99+\ud835\udc83\ud835\udc53\ud835\udc99superscript\ud835\udc16top\ud835\udc99\ud835\udc83f(\\bm{x})=\\mathbf{W}^{\\top}\\bm{x}+\\bm{b}italic_f ( bold_italic_x ) = bold_W start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT bold_italic_x + bold_italic_b for a given \ud835\udc16\ud835\udc16\\mathbf{W}bold_W and \ud835\udc83\ud835\udc83\\bm{b}bold_italic_b. Since the mapping k^^\ud835\udc58\\hat{k}over^ start_ARG italic_k end_ARG is the outcome of a one-vs-all classification scheme, the minimal perturbation to fool the classifier can be rewritten as followsargmin\ud835\udc93\u2061\u2016\ud835\udc93\u20162s.t.\u00a0\u2203k:\ud835\udc98k\u22a4(\ud835\udc990+\ud835\udc93)+bk\u2265\ud835\udc98k^(\ud835\udc990)\u22a4(\ud835\udc990+\ud835\udc93)+bk^(\ud835\udc990),:subscriptargmin\ud835\udc93subscriptdelimited-\u2225\u2225\ud835\udc932s.t.\u00a0\ud835\udc58superscriptsubscript\ud835\udc98\ud835\udc58topsubscript\ud835\udc990\ud835\udc93subscript\ud835\udc4f\ud835\udc58superscriptsubscript\ud835\udc98^\ud835\udc58subscript\ud835\udc990topsubscript\ud835\udc990\ud835\udc93subscript\ud835\udc4f^\ud835\udc58subscript\ud835\udc990\\begin{split}&\\operatorname*{arg\\,min}_{\\bm{r}}\\|\\bm{r}\\|_{2}\\\\&\\text{s.t. }\\exists k:\\bm{w}_{k}^{\\top}(\\bm{x}_{0}+\\bm{r})+b_{k}\\geq\\bm{w}_{\\hat{k}(\\bm{x}_{0})}^{\\top}(\\bm{x}_{0}+\\bm{r})+b_{\\hat{k}(\\bm{x}_{0})},\\end{split}start_ROW start_CELL end_CELL start_CELL start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_r end_POSTSUBSCRIPT \u2225 bold_italic_r \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL s.t. \u2203 italic_k : bold_italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + bold_italic_r ) + italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2265 bold_italic_w start_POSTSUBSCRIPT over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + bold_italic_r ) + italic_b start_POSTSUBSCRIPT over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT , end_CELL end_ROW(6)where \ud835\udc98ksubscript\ud835\udc98\ud835\udc58\\bm{w}_{k}bold_italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is the kthsuperscript\ud835\udc58thk^{\\text{th}}italic_k start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT column of \ud835\udc16\ud835\udc16\\mathbf{W}bold_W. Geometrically, the above problem corresponds to the computation of the distance between \ud835\udc990subscript\ud835\udc990\\bm{x}_{0}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and the complement of the convex polyhedron P\ud835\udc43Pitalic_P,P=\u22c2k=1c{\ud835\udc99:fk^(\ud835\udc990)(\ud835\udc99)\u2265fk(\ud835\udc99)},\ud835\udc43superscriptsubscript\ud835\udc581\ud835\udc50conditional-set\ud835\udc99subscript\ud835\udc53^\ud835\udc58subscript\ud835\udc990\ud835\udc99subscript\ud835\udc53\ud835\udc58\ud835\udc99\\displaystyle P=\\bigcap_{k=1}^{c}\\{\\bm{x}:f_{\\hat{k}(\\bm{x}_{0})}(\\bm{x})\\geq f_{k}(\\bm{x})\\},italic_P = \u22c2 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT { bold_italic_x : italic_f start_POSTSUBSCRIPT over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ( bold_italic_x ) \u2265 italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_x ) } ,(7)where \ud835\udc990subscript\ud835\udc990\\bm{x}_{0}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is located inside P\ud835\udc43Pitalic_P.We denote this distance by \ud835\udc1d\ud835\udc22\ud835\udc2c\ud835\udc2d(\ud835\udc990,Pc)\ud835\udc1d\ud835\udc22\ud835\udc2c\ud835\udc2dsubscript\ud835\udc990superscript\ud835\udc43\ud835\udc50\\text{{dist}}(\\bm{x}_{0},P^{c})dist ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_P start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ).The polyhedron P\ud835\udc43Pitalic_P defines the region of the space where f\ud835\udc53fitalic_f outputs the label k^(\ud835\udc990)^\ud835\udc58subscript\ud835\udc990\\hat{k}(\\bm{x}_{0})over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ). This setting is depicted in Figure 4. The solution to the problem in Eq. (6) can be computed in closed form as follows. Define l^(\ud835\udc990)^\ud835\udc59subscript\ud835\udc990\\hat{l}(\\bm{x}_{0})over^ start_ARG italic_l end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) to be the closest hyperplane of the boundary of P\ud835\udc43Pitalic_P (e.g. l^(\ud835\udc990)=3^\ud835\udc59subscript\ud835\udc9903\\hat{l}(\\bm{x}_{0})=3over^ start_ARG italic_l end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = 3 in Figure 4). Formally, l^(\ud835\udc990)^\ud835\udc59subscript\ud835\udc990\\hat{l}(\\bm{x}_{0})over^ start_ARG italic_l end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) can be computed as followsl^(\ud835\udc990)=argmink\u2260k^(\ud835\udc990)\u2061|fk(\ud835\udc990)\u2212fk^(\ud835\udc990)(\ud835\udc990)|\u2016\ud835\udc98k\u2212\ud835\udc98k^(\ud835\udc990)\u20162.^\ud835\udc59subscript\ud835\udc990subscriptargmin\ud835\udc58^\ud835\udc58subscript\ud835\udc990subscript\ud835\udc53\ud835\udc58subscript\ud835\udc990subscript\ud835\udc53^\ud835\udc58subscript\ud835\udc990subscript\ud835\udc990subscriptnormsubscript\ud835\udc98\ud835\udc58subscript\ud835\udc98^\ud835\udc58subscript\ud835\udc9902\\hat{l}(\\bm{x}_{0})=\\operatorname*{arg\\,min}_{k\\neq{\\hat{k}(\\bm{x}_{0})}}\\frac{\\left|f_{k}(\\bm{x}_{0})-f_{\\hat{k}(\\bm{x}_{0})}(\\bm{x}_{0})\\right|}{\\|\\bm{w}_{k}-\\bm{w}_{\\hat{k}(\\bm{x}_{0})}\\|_{2}}.over^ start_ARG italic_l end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_k \u2260 over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT divide start_ARG | italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) - italic_f start_POSTSUBSCRIPT over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) | end_ARG start_ARG \u2225 bold_italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - bold_italic_w start_POSTSUBSCRIPT over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG .(8)The minimum perturbation \ud835\udc93*(\ud835\udc990)subscript\ud835\udc93subscript\ud835\udc990\\bm{r}_{*}(\\bm{x}_{0})bold_italic_r start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) is the vector that projects \ud835\udc990subscript\ud835\udc990\\bm{x}_{0}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT on the hyperplane indexed by l^(\ud835\udc990)^\ud835\udc59subscript\ud835\udc990\\hat{l}(\\bm{x}_{0})over^ start_ARG italic_l end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ), i.e.,\ud835\udc93*(\ud835\udc990)=|fl^(\ud835\udc990)(\ud835\udc990)\u2212fk^(\ud835\udc990)(\ud835\udc990)|\u2016\ud835\udc98l^(\ud835\udc990)\u2212\ud835\udc98k^(\ud835\udc990)\u201622(\ud835\udc98l^(\ud835\udc990)\u2212\ud835\udc98k^(\ud835\udc990)).subscript\ud835\udc93subscript\ud835\udc990subscript\ud835\udc53^\ud835\udc59subscript\ud835\udc990subscript\ud835\udc990subscript\ud835\udc53^\ud835\udc58subscript\ud835\udc990subscript\ud835\udc990superscriptsubscriptnormsubscript\ud835\udc98^\ud835\udc59subscript\ud835\udc990subscript\ud835\udc98^\ud835\udc58subscript\ud835\udc99022subscript\ud835\udc98^\ud835\udc59subscript\ud835\udc990subscript\ud835\udc98^\ud835\udc58subscript\ud835\udc990\\bm{r}_{*}(\\bm{x}_{0})=\\frac{\\left|f_{\\hat{l}(\\bm{x}_{0})}(\\bm{x}_{0})-f_{\\hat{k}(\\bm{x}_{0})}(\\bm{x}_{0})\\right|}{\\|\\bm{w}_{\\hat{l}(\\bm{x}_{0})}-\\bm{w}_{\\hat{k}(\\bm{x}_{0})}\\|_{2}^{2}}(\\bm{w}_{\\hat{l}(\\bm{x}_{0})}-\\bm{w}_{\\hat{k}(\\bm{x}_{0})}).bold_italic_r start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = divide start_ARG | italic_f start_POSTSUBSCRIPT over^ start_ARG italic_l end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) - italic_f start_POSTSUBSCRIPT over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) | end_ARG start_ARG \u2225 bold_italic_w start_POSTSUBSCRIPT over^ start_ARG italic_l end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT - bold_italic_w start_POSTSUBSCRIPT over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( bold_italic_w start_POSTSUBSCRIPT over^ start_ARG italic_l end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT - bold_italic_w start_POSTSUBSCRIPT over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ) .(9)In other words, we find the closest projection of \ud835\udc990subscript\ud835\udc990\\bm{x}_{0}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT on faces of P\ud835\udc43Pitalic_P.", "We now extend the DeepFool algorithm to the general case of multiclass differentiable classifiers. For general non-linear classifiers, the set P\ud835\udc43Pitalic_P in Eq. (7) that describes the region of the space where the classifier outputs label k^(\ud835\udc990)^\ud835\udc58subscript\ud835\udc990\\hat{k}(\\bm{x}_{0})over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) is no longer a polyhedron. Following the explained iterative linearization procedure in the binary case, we approximate the set P\ud835\udc43Pitalic_P at iteration i\ud835\udc56iitalic_i by a polyhedron P~isubscript~\ud835\udc43\ud835\udc56\\tilde{P}_{i}over~ start_ARG italic_P end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTP~i=\u22c2k=1c{\\displaystyle\\tilde{P}_{i}=\\bigcap_{k=1}^{c}\\Big{\\{}over~ start_ARG italic_P end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = \u22c2 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT {\ud835\udc99:fk(\ud835\udc99i)\u2212fk^(\ud835\udc990)(\ud835\udc99i):\ud835\udc99subscript\ud835\udc53\ud835\udc58subscript\ud835\udc99\ud835\udc56subscript\ud835\udc53^\ud835\udc58subscript\ud835\udc990subscript\ud835\udc99\ud835\udc56\\displaystyle\\bm{x}:f_{k}(\\bm{x}_{i})-f_{\\hat{k}(\\bm{x}_{0})}(\\bm{x}_{i})bold_italic_x : italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_f start_POSTSUBSCRIPT over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )(10)+\u2207fk(\ud835\udc99i)\u22a4\ud835\udc99\u2212\u2207fk^(\ud835\udc990)(\ud835\udc99i)\u22a4\ud835\udc99\u22640}.\\displaystyle+\\nabla f_{k}(\\bm{x}_{i})^{\\top}\\bm{x}-\\nabla f_{\\hat{k}(\\bm{x}_{0})}(\\bm{x}_{i})^{\\top}\\bm{x}\\leq 0\\Big{\\}}.+ \u2207 italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT bold_italic_x - \u2207 italic_f start_POSTSUBSCRIPT over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT bold_italic_x \u2264 0 } .We then approximate, at iteration i\ud835\udc56iitalic_i, the distance between \ud835\udc99isubscript\ud835\udc99\ud835\udc56\\bm{x}_{i}bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and the complement of P\ud835\udc43Pitalic_P, \ud835\udc1d\ud835\udc22\ud835\udc2c\ud835\udc2d(\ud835\udc99i,Pc)\ud835\udc1d\ud835\udc22\ud835\udc2c\ud835\udc2dsubscript\ud835\udc99\ud835\udc56superscript\ud835\udc43\ud835\udc50\\text{{dist}}(\\bm{x}_{i},P^{c})dist ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_P start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ), by \ud835\udc1d\ud835\udc22\ud835\udc2c\ud835\udc2d(\ud835\udc99i,P~ic)\ud835\udc1d\ud835\udc22\ud835\udc2c\ud835\udc2dsubscript\ud835\udc99\ud835\udc56superscriptsubscript~\ud835\udc43\ud835\udc56\ud835\udc50\\text{{dist}}(\\bm{x}_{i},\\tilde{P}_{i}^{c})dist ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over~ start_ARG italic_P end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ). Specifically, at each iteration of the algorithm, the perturbation vector that reaches the boundary of the polyhedron P~isubscript~\ud835\udc43\ud835\udc56\\tilde{P}_{i}over~ start_ARG italic_P end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is computed, and the current estimate updated.The method is given in Algorithm 2. It should be noted that the proposed algorithm operates in a greedy way and is not guaranteed to converge to the optimal perturbation in (1). However, we have observed in practice that our algorithm yields very small perturbations which are believed to be good approximations of the minimal perturbation. ", "It should be noted that the optimization strategy of DeepFool is strongly tied to existing optimization techniques. In the binary case, it can be seen as Newton\u2019s iterative algorithm for finding roots of a nonlinear system of equations in the underdetermined case [15]. This algorithm is known as the normal flow method. The convergence analysis of this optimization technique can be found for example in [21]. Our algorithm in the binary case can alternatively be seen as a gradient descent algorithm with an adaptive step size that is automatically chosen at each iteration. The linearization in Algorithm 2 is also similar to a sequential convex programming where the constraints are linearized at each step.", "In this paper, we have measured the perturbations using the \u21132subscript\u21132\\ell_{2}roman_\u2113 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT norm. Our framework is however not limited to this choice, and the proposed algorithm can simply be adapted to find minimal adversarial perturbations for any \u2113psubscript\u2113\ud835\udc5d\\ell_{p}roman_\u2113 start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT norm (p\u2208[1,\u221e)\ud835\udc5d1p\\in[1,\\infty)italic_p \u2208 [ 1 , \u221e )). To do so, the update steps in line 10 and 11 in Algorithm 2 must be respectively substituted by the following updatesl^^\ud835\udc59\\displaystyle\\hat{l}over^ start_ARG italic_l end_ARG\u2190argmink\u2260k^(\ud835\udc990)\u2061|fk\u2032|\u2016\ud835\udc98k\u2032\u2016q,\u2190absentsubscriptargmin\ud835\udc58^\ud835\udc58subscript\ud835\udc990subscriptsuperscript\ud835\udc53\u2032\ud835\udc58subscriptnormsubscriptsuperscript\ud835\udc98\u2032\ud835\udc58\ud835\udc5e\\displaystyle\\leftarrow\\operatorname*{arg\\,min}_{k\\neq{\\hat{k}(\\bm{x}_{0})}}\\frac{\\left|f^{\\prime}_{k}\\right|}{\\|\\bm{w}^{\\prime}_{k}\\|_{q}},\u2190 start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_k \u2260 over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT divide start_ARG | italic_f start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | end_ARG start_ARG \u2225 bold_italic_w start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG ,(11)\ud835\udc93isubscript\ud835\udc93\ud835\udc56\\displaystyle\\bm{r}_{i}bold_italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\u2190|fl^\u2032|\u2016\ud835\udc98l^\u2032\u2016qq|\ud835\udc98l^\u2032|q\u22121\u2299sign(\ud835\udc98l^\u2032),\u2190absentdirect-productsubscriptsuperscript\ud835\udc53\u2032^\ud835\udc59superscriptsubscriptnormsubscriptsuperscript\ud835\udc98\u2032^\ud835\udc59\ud835\udc5e\ud835\udc5esuperscriptsubscriptsuperscript\ud835\udc98\u2032^\ud835\udc59\ud835\udc5e1signsubscriptsuperscript\ud835\udc98\u2032^\ud835\udc59\\displaystyle\\leftarrow\\frac{|f^{\\prime}_{\\hat{l}}|}{\\|\\bm{w}^{\\prime}_{\\hat{l}}\\|_{q}^{q}}|\\bm{w}^{\\prime}_{\\hat{l}}|^{q-1}\\odot\\text{sign}(\\bm{w}^{\\prime}_{\\hat{l}}),\u2190 divide start_ARG | italic_f start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT over^ start_ARG italic_l end_ARG end_POSTSUBSCRIPT | end_ARG start_ARG \u2225 bold_italic_w start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT over^ start_ARG italic_l end_ARG end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT end_ARG | bold_italic_w start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT over^ start_ARG italic_l end_ARG end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT italic_q - 1 end_POSTSUPERSCRIPT \u2299 sign ( bold_italic_w start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT over^ start_ARG italic_l end_ARG end_POSTSUBSCRIPT ) ,(12)where \u2299direct-product\\odot\u2299 is the pointwise product and q=pp\u22121\ud835\udc5e\ud835\udc5d\ud835\udc5d1q=\\frac{p}{p-1}italic_q = divide start_ARG italic_p end_ARG start_ARG italic_p - 1 end_ARG.333To see this, one can apply Holder\u2019s inequality to obtain a lower bound on the \u2113psubscript\u2113\ud835\udc5d\\ell_{p}roman_\u2113 start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT norm of the perturbation. In particular, when p=\u221e\ud835\udc5dp=\\inftyitalic_p = \u221e (i.e., the supremum norm \u2113\u221esubscript\u2113\\ell_{\\infty}roman_\u2113 start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT), these update steps becomel^^\ud835\udc59\\displaystyle\\hat{l}over^ start_ARG italic_l end_ARG\u2190argmink\u2260k^(\ud835\udc990)\u2061|fk\u2032|\u2016\ud835\udc98k\u2032\u20161,\u2190absentsubscriptargmin\ud835\udc58^\ud835\udc58subscript\ud835\udc990subscriptsuperscript\ud835\udc53\u2032\ud835\udc58subscriptnormsubscriptsuperscript\ud835\udc98\u2032\ud835\udc581\\displaystyle\\leftarrow\\operatorname*{arg\\,min}_{k\\neq{\\hat{k}(\\bm{x}_{0})}}\\frac{\\left|f^{\\prime}_{k}\\right|}{\\|\\bm{w}^{\\prime}_{k}\\|_{1}},\u2190 start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_k \u2260 over^ start_ARG italic_k end_ARG ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT divide start_ARG | italic_f start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | end_ARG start_ARG \u2225 bold_italic_w start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ,(13)\ud835\udc93isubscript\ud835\udc93\ud835\udc56\\displaystyle\\bm{r}_{i}bold_italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\u2190|fl^\u2032|\u2016\ud835\udc98l^\u2032\u20161sign(\ud835\udc98l^\u2032).\u2190absentsubscriptsuperscript\ud835\udc53\u2032^\ud835\udc59subscriptnormsubscriptsuperscript\ud835\udc98\u2032^\ud835\udc591signsubscriptsuperscript\ud835\udc98\u2032^\ud835\udc59\\displaystyle\\leftarrow\\frac{|f^{\\prime}_{\\hat{l}}|}{\\|\\bm{w}^{\\prime}_{\\hat{l}}\\|_{1}}\\text{sign}(\\bm{w}^{\\prime}_{\\hat{l}}).\u2190 divide start_ARG | italic_f start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT over^ start_ARG italic_l end_ARG end_POSTSUBSCRIPT | end_ARG start_ARG \u2225 bold_italic_w start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT over^ start_ARG italic_l end_ARG end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG sign ( bold_italic_w start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT over^ start_ARG italic_l end_ARG end_POSTSUBSCRIPT ) .(14)", "We now test our DeepFool algorithm on deep convolutional neural networks architectures applied to MNIST, CIFAR-10, and ImageNet image classification datasets. We consider the following deep neural network architectures:\u2022MNIST: A two-layer fully connected network, and a two-layer LeNet convoluational neural network architecture [9]. Both networks are trained with SGD with momentum using the MatConvNet [20] package.\u2022CIFAR-10: We trained a three-layer LeNet architecture, as well as a Network In Network (NIN) architecture [11].\u2022ILSVRC 2012: We used CaffeNet [7] and GoogLeNet [17] pre-trained models.", "In order to evaluate the robustness to adversarial perturbations of a classifier f\ud835\udc53fitalic_f, we compute the average robustness \u03c1^adv(f)subscript^\ud835\udf0cadv\ud835\udc53\\hat{\\rho}_{\\text{adv}}(f)over^ start_ARG italic_\u03c1 end_ARG start_POSTSUBSCRIPT adv end_POSTSUBSCRIPT ( italic_f ), defined by\u03c1^adv(f)=1|\ud835\udc9f|\u2211\ud835\udc99\u2208\ud835\udc9f\u2016\ud835\udc93^(\ud835\udc99)\u20162\u2016\ud835\udc99\u20162,subscript^\ud835\udf0cadv\ud835\udc531\ud835\udc9fsubscript\ud835\udc99\ud835\udc9fsubscriptnorm^\ud835\udc93\ud835\udc992subscriptnorm\ud835\udc992\\hat{\\rho}_{\\text{adv}}(f)=\\frac{1}{|\\mathscr{D}|}\\sum_{\\bm{x}\\in\\mathscr{D}}\\frac{\\|\\hat{\\bm{r}}(\\bm{x})\\|_{2}}{\\|\\bm{x}\\|_{2}},over^ start_ARG italic_\u03c1 end_ARG start_POSTSUBSCRIPT adv end_POSTSUBSCRIPT ( italic_f ) = divide start_ARG 1 end_ARG start_ARG | script_D | end_ARG \u2211 start_POSTSUBSCRIPT bold_italic_x \u2208 script_D end_POSTSUBSCRIPT divide start_ARG \u2225 over^ start_ARG bold_italic_r end_ARG ( bold_italic_x ) \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG \u2225 bold_italic_x \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG ,(15)where \ud835\udc93^(\ud835\udc99)^\ud835\udc93\ud835\udc99\\hat{\\bm{r}}(\\bm{x})over^ start_ARG bold_italic_r end_ARG ( bold_italic_x ) is the estimated minimal perturbation obtained using DeepFool, and \ud835\udc9f\ud835\udc9f\\mathscr{D}script_D denotes the test set444For ILSVRC2012, we used the validation data..", "We compare the proposed DeepFool approach to state-of-the-art techniques to compute adversarial perturbations in [18] and [4]. The method in [18] solves a series of penalized optimization problems to find the minimal perturbation, whereas [4] estimates the minimal perturbation by taking the sign of the gradient\ud835\udc93^(\ud835\udc99)=\u03f5sign(\u2207\ud835\udc99J(\ud835\udf3d,\ud835\udc99,y)),^\ud835\udc93\ud835\udc99italic-\u03f5signsubscript\u2207\ud835\udc99\ud835\udc3d\ud835\udf3d\ud835\udc99\ud835\udc66\\displaystyle\\hat{\\bm{r}}(\\bm{x})=\\epsilon\\,\\text{sign}\\left(\\nabla_{\\bm{x}}J(\\bm{\\theta},\\bm{x},y)\\right),over^ start_ARG bold_italic_r end_ARG ( bold_italic_x ) = italic_\u03f5 sign ( \u2207 start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT italic_J ( bold_italic_\u03b8 , bold_italic_x , italic_y ) ) ,with J\ud835\udc3dJitalic_J the cost used to train the neural network, \ud835\udf3d\ud835\udf3d\\bm{\\theta}bold_italic_\u03b8 is the model parameters, and y\ud835\udc66yitalic_y is the label of \ud835\udc99\ud835\udc99\\bm{x}bold_italic_x. The method is called fast gradient sign method. In practice, in the absence of general rules to choose the parameter \u03f5italic-\u03f5\\epsilonitalic_\u03f5, we chose the smallest \u03f5italic-\u03f5\\epsilonitalic_\u03f5 such that 90%percent9090\\%90 % of the data are misclassified after perturbation.555Using this method, we observed empirically that one cannot reach 100%percent100100\\%100 % misclassification rate on some datasets. In fact, even by increasing \u03f5italic-\u03f5\\epsilonitalic_\u03f5 to be very large, this method can fail in misclassifying all samples.", "We report in Table 1 the accuracy and average robustness \u03c1^advsubscript^\ud835\udf0cadv\\hat{\\rho}_{\\text{adv}}over^ start_ARG italic_\u03c1 end_ARG start_POSTSUBSCRIPT adv end_POSTSUBSCRIPT of each classifier computed using different methods. We also show the running time required for each method to compute one adversarial sample.", "It can be seen that DeepFool estimates smaller perturbations (hence closer to minimal perturbation defined in (1)) than the ones computed using the competitive approaches. For example, the average perturbation obtained using DeepFool is 5555 times lower than the one estimated with [4]. On the ILSVRC2012 challenge dataset, the average perturbation is one order of magnitude smaller compared to the fast gradient method. It should be noted moreover that the proposed approach also yields slightly smaller perturbation vectors than the method in [18]. The proposed approach is hence more accurate in detecting directions that can potentially fool neural networks. As a result, DeepFool can be used as a valuable tool to accurately assess the robustness of classifiers.On the complexity aspect, the proposed approach is substantially faster than the standard method proposed in [18].In fact, while the approach [18] involves a costly minimization of a series of objective functions, we observed empirically that DeepFool converges in a few iterations (i.e., less than 3333) to a perturbation vector that fools the classifier. Hence, the proposed approach reaches a more accurate perturbation vector compared to state-of-the-art methods, while being computationally efficient. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large-scale datasets. In that context, we provide the first quantitative evaluation of the robustness of state-of-the-art classifiers on the large-scale ImageNet dataset. It can be seen that despite their very good test accuracy, these methods are extremely unstable to adversarial perturbations: a perturbation that is 1000100010001000 smaller in magnitude than the original image is sufficient to fool state-of-the-art deep neural networks.", "We illustrate in Figure 1 perturbed images generated by the fast gradient sign and DeepFool. It can be observed that the proposed method generates adversarial perturbations which are hardly perceptible, while the fast gradient sign method outputs a perturbation image with higher norm.", "It should be noted that, when perturbations are measured using the \u2113\u221esubscript\u2113\\ell_{\\infty}roman_\u2113 start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT norm, the above conclusions remain unchanged: DeepFool yields adversarial perturbations that are smaller (hence closer to the optimum) compared to other methods for computing adversarial examples. Table 2 reports the \u2113\u221esubscript\u2113\\ell_{\\infty}roman_\u2113 start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT robustness to adversarial perturbations measured by \u03c1^adv\u221e(f)=1|\ud835\udc9f|\u2211\ud835\udc99\u2208\ud835\udc9f\u2016\ud835\udc93^(\ud835\udc99)\u2016\u221e\u2016\ud835\udc99\u2016\u221esuperscriptsubscript^\ud835\udf0cadv\ud835\udc531\ud835\udc9fsubscript\ud835\udc99\ud835\udc9fsubscriptnorm^\ud835\udc93\ud835\udc99subscriptnorm\ud835\udc99\\hat{\\rho}_{\\text{adv}}^{\\infty}(f)=\\frac{1}{|\\mathscr{D}|}\\sum_{\\bm{x}\\in\\mathscr{D}}\\frac{\\|\\hat{\\bm{r}}(\\bm{x})\\|_{\\infty}}{\\|\\bm{x}\\|_{\\infty}}over^ start_ARG italic_\u03c1 end_ARG start_POSTSUBSCRIPT adv end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT ( italic_f ) = divide start_ARG 1 end_ARG start_ARG | script_D | end_ARG \u2211 start_POSTSUBSCRIPT bold_italic_x \u2208 script_D end_POSTSUBSCRIPT divide start_ARG \u2225 over^ start_ARG bold_italic_r end_ARG ( bold_italic_x ) \u2225 start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT end_ARG start_ARG \u2225 bold_italic_x \u2225 start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT end_ARG, where \ud835\udc93^(\ud835\udc99)^\ud835\udc93\ud835\udc99\\hat{\\bm{r}}(\\bm{x})over^ start_ARG bold_italic_r end_ARG ( bold_italic_x ) is computed respectively using DeepFool (with p=\u221e\ud835\udc5dp=\\inftyitalic_p = \u221e, see Section 3.3), and the Fast gradient sign method for MNIST and CIFAR-10 tasks.", "Fine-tuning using adversarial examples", "In this section, we fine-tune the networks of Table 1 on adversarial examples to build more robust classifiers for the MNIST and CIFAR-10 tasks. Specifically, for each network, we performed two experiments: (i) Fine-tuning the network on DeepFool\u2019s adversarial examples, (ii) Fine-tuning the network on the fast gradient sign adversarial examples.We fine-tune the networks by performing 5 additional epochs, with a 50%percent5050\\%50 % decreased learning rate only on the perturbed training set. For each experiment, the same training data was used through all 5555 extra epochs. For the sake of completeness, we also performed 5555 extra epochs on the original data. The evolution of \u03c1^advsubscript^\ud835\udf0cadv\\hat{\\rho}_{\\text{adv}}over^ start_ARG italic_\u03c1 end_ARG start_POSTSUBSCRIPT adv end_POSTSUBSCRIPT for the different fine-tuning strategies is shown in Figures 5(a) to 5(d), where the robustness \u03c1^\ud835\udc4e\ud835\udc51\ud835\udc63subscriptnormal-^\ud835\udf0c\ud835\udc4e\ud835\udc51\ud835\udc63\\hat{\\rho}_{\\text{adv}}over^ start_ARG italic_\u03c1 end_ARG start_POSTSUBSCRIPT adv end_POSTSUBSCRIPT is estimated using DeepFool, since this is the most accurate method, as shown in Table 1. Observe that fine-tuning with DeepFool adversarial examples significantly increases the robustness of the networks to adversarial perturbations even after one extra epoch. For example, the robustness of the networks on MNIST is improved by 50% and NIN\u2019s robustness is increased by about 40%. On the other hand, quite surprisingly, the method in [4] can lead to a decreased robustness to adversarial perturbations of the network. We hypothesize that this behavior is due to the fact that perturbations estimated using the fast gradient sign method are much larger than minimal adversarial perturbations. Fine-tuning the network with overly perturbed images decreases the robustness of the networks to adversarial perturbations.To verify this hypothesis, we compare in Figure 7 the adversarial robustness of a network that is fine-tuned with the adversarial examples obtained using DeepFool, where norms of perturbations have been deliberately multiplied by \u03b1=1,2,3\ud835\udefc123\\alpha=1,2,3italic_\u03b1 = 1 , 2 , 3. Interestingly, we see that by magnifying the norms of the adversarial perturbations, the robustness of the fine-tuned network is decreased. This might explain why overly perturbed images decrease the robustness of MNIST networks: these perturbations can really change the class of the digits, hence fine-tuning based on these examples can lead to a drop of the robustness (for an illustration, see Figure 8). This lends credence to our hypothesis, and further shows the importance of designing accurate methods to compute minimal perturbations.", "Table 3 lists the accuracies of the fine-tuned networks. It can be seen that fine-tuning with DeepFool can improve the accuracy of the networks. Conversely, fine-tuning with the approach in [4] has led to a decrease of the test accuracy in all our experiments. This confirms the explanation that the fast gradient sign method outputs overly perturbed images that lead to images that are unlikely to occur in the test data. Hence, it decreases the performance of the method as it acts as a regularizer that does not represent the distribution of the original data. This effect is analogous to geometric data augmentation schemes, where large transformations of the original samples have a counter-productive effect on generalization.666While the authors of [4] reported an increased generalization performance on the MNIST task (from 0.94%percent0.940.94\\%0.94 % to 0.84%percent0.840.84\\%0.84 %) using adversarial regularization, it should be noted that the their experimental setup is significantly different as [4] trained the network based on a modified cost function, while we performed straightforward fine-tuning.", "To emphasize the importance of a correct estimation of the minimal perturbation, we now show that using approximate methods can lead to wrong conclusions regarding the adversarial robustness of networks. We fine-tune the NIN classifier on the fast gradient sign adversarial examples. We follow the procedure described earlier but this time, we decreased the learning rate by 90%. We have evaluated the adversarial robustness of this network at different extra epochs using DeepFool and the fast gradient sign method. As one can see in Figure 9, the red plot exaggerates the effect of training on the adversarial examples. Moreover, it is not sensitive enough to demonstrate the loss of robustness at the first extra epoch. These observations confirm that using an accurate tool to measure the robustness of classifiers is crucial to derive conclusions about the robustness of networks. ", "In this work, we proposed an algorithm, DeepFool, to compute adversarial examples that fool state-of-the-art classifiers. It is based on an iterative linearization of the classifier to generate minimal perturbations that are sufficient to change classification labels. We provided extensive experimental evidence on three datasets and eight classifiers, showing the superiority of the proposed method over state-of-the-art methods to compute adversarial perturbations, as well as the efficiency of the proposed approach. Due to its accurate estimation of the adversarial perturbations, the proposed DeepFool algorithm provides an efficient and accurate way to evaluate the robustness of classifiers and to enhance their performance by proper fine-tuning.The proposed approach can therefore be used as a reliable tool to accurately estimate the minimal perturbation vectors, and build more robust classifiers.", "This work has been partly supported by the Hasler Foundation, Switzerland, in the framework of the CORA project."], "figure_types": {"52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/1-Figure1-1.png": "photograph(s)", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/3-Figure2-1.png": "schematic", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/3-Figure3-1.png": "schematic", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/4-Figure4-1.png": "schematic", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/4-Figure5-1.png": "schematic", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/6-Table1-1.png": "table", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/6-Table2-1.png": "table", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/7-Figure6-1.png": "plot", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/7-Figure7-1.png": "plot", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/8-Figure8-1.png": "photograph(s)", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/8-Figure9-1.png": "plot", "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35/8-Table3-1.png": "table"}}, "2301.00122": {"paper_id": "paper_144", "title": "Hair and Scalp Disease Detection using Machine Learning and Image Processing", "arxiv_url": "https://arxiv.org/abs/2301.00122", "s2orc_url": "https://www.semanticscholar.org/paper/9317eb761d51f6eb428a157a02e2114a90b409e4", "all_figures_tables": {"9317eb761d51f6eb428a157a02e2114a90b409e4/3-TableI-1.png": "TABLE I: IMAGES PER DISEASE", "9317eb761d51f6eb428a157a02e2114a90b409e4/4-Figure2-1.png": "Fig. 2. System workflow of hair disease detection model.", "9317eb761d51f6eb428a157a02e2114a90b409e4/4-Figure3-1.png": "Fig. 3. Left original image & right non-local means denoised image.", "9317eb761d51f6eb428a157a02e2114a90b409e4/4-Figure4-1.png": "Fig. 4. Image equalization using CLAHE.", "9317eb761d51f6eb428a157a02e2114a90b409e4/5-Figure5-1.png": "Fig. 5. Neural Network Model.", "9317eb761d51f6eb428a157a02e2114a90b409e4/5-Figure6-1.png": "Fig. 6. Training and Validation loss for CNN.", "9317eb761d51f6eb428a157a02e2114a90b409e4/5-TableII-1.png": "TABLE II: HYPERPARAMETERS OF CNN MODEL", "9317eb761d51f6eb428a157a02e2114a90b409e4/6-Figure8-1.png": "Fig. 8. Confusion Matrix of Our Model."}, "referred_figures_tables": [["9317eb761d51f6eb428a157a02e2114a90b409e4/4-Figure2-1.png"], ["9317eb761d51f6eb428a157a02e2114a90b409e4/4-Figure3-1.png"], ["9317eb761d51f6eb428a157a02e2114a90b409e4/4-Figure4-1.png"], ["9317eb761d51f6eb428a157a02e2114a90b409e4/5-Figure5-1.png"], ["9317eb761d51f6eb428a157a02e2114a90b409e4/5-Figure5-1.png", "9317eb761d51f6eb428a157a02e2114a90b409e4/5-TableII-1.png"], ["9317eb761d51f6eb428a157a02e2114a90b409e4/5-TableII-1.png"], ["9317eb761d51f6eb428a157a02e2114a90b409e4/5-TableII-1.png"]], "question_id": [7, 11, 12, 13, 14, 15, 16], "question": ["What were the various pre processing techniques used before feeding the data to Neural network?", "How does the author conclude that non-local means filter is the best filter for denoising the images ? Are there any other filters that can be used for the same task?", "How CLAHE is better than HE for image equalization?", "What is 'autokeras' ? How it works?", "Author took batch_size to be 16 with 50 epochs while training the model . What was the intution behind taking these particular numbers?", "What were the various hyperparameter used in 'grid search'?", "Can using more epochs while training may increase the validation accuracy ? if no why ?"], "question_section": ["Related Works", "Proposed Model", "Proposed model", "Proposed Model", "Results", "Results", "Conclusion"], "question_trigger_sentence": ["every image needs different types of preprocessing before feeding to neural networks. Different scalp skin tones, hair colors, and types around the detection zones make the imaging process more complicated.", "This non-local means filter preserved all the edges and reduced the\nnoise better than the other filters for our application which is\nshown in Fig. 3.", "To overcome the problem, we applied CLAHE (Contrast Limited Adaptive Histogram Equalization) by dividing an image into equal size non-overlapping areas and computing a\nhistogram for each region.", "We used autokeras to find the best model for this problem", "For training the model, we used batch_size = 16 with\n50 epochs for each batch", "We trained our CNN model using the optimalhyperparameters selected from the grid search. ", " At the same time, Training accuracy and validation accuracy increased to 96.2% and\n91.1%, respectively, from epoch 1 to epoch 50, shown in Fig. 7."], "question_type": ["Shallow question", "Deep/complex question", "Testing question", "Shallow question", "Deep/complex question", "Shallow question", "Deep/complex question"], "evidential_info": [[{"context": "In this section, we introduce the system workflow of our model and explain the functions of each module in details. As shown in Fig. 2, first, the captured image is sent to preprocessing steps which are divided into three parts: image equalization, image enhancement, and data balancing.", "rationale": "There are three preprocessing steps used in this paper: image equalization, image enhancement, and data balancing. Among these three, the first two parts are mainly for increasing image quality, and the last part is for model versatility."}, {"context": "Among these three, the first two parts are mainly for increasing image quality, and the last part is for model versatility. After the preprocessing steps, the image is passed to the Neural Network model for the classification task. We used a convolutional neural network that classifies an image successfully into three different classes: alopecia, folliculitis, and psoriasis.", "rationale": "As shown in Fig. 2, first, the captured image is sent to preprocessing steps which are divided into three parts: image equalization, image enhancement, and data balancing."}], [{"context": "Noise is the degradation of image signals caused by external sources [23]. Noise introduces random variations of brightness or color information in the captured images. Most of the time, images on the internet have some noise associated with them. As we have collected most of the data samples from different dermatology websites, the noise in our dataset is not homogeneously distributed, which made it more complex. Therefore, we applied additional filters for", "rationale": "\"Though we achieved better accuracy using the bilateral filter, we got the best results while applying the non-local means filter with patch_size = 3 and patch_distance = 5. This non-local means filter preserved all the edges and reduced the noise better than the other filters for our application which is shown in Fig. 3.\" Comparing with other filter non-local"}, {"context": "denoising the collected images. We started with the gaussian filter for a better image classification process. However, after using the gaussian filter, the images become completely blurred, which leads to the loss of important information and damage to the edges. We then applied the median filter, which worked better than the gaussian filter with kernel_size = 3. Though we achieved better accuracy using the bilateral filter, we got the best results while applying the non-local means filter with patch_size = 3 and patch_distance = 5. This non-local means filter preserved all the edges and reduced the noise better than the other filters for our application which is shown in Fig. 3.", "rationale": "The authors applied additional filters to remove noise."}], [{"context": "extracted from the images: texture, shape, and color. The researchers divided the dataset into 70%-30% train-test-split and applied a support vector machine (SNM) and k-nearest neighbor (KNN) for the classification task. Overall, they achieved 91.4% and 88.9% accuracy using SVM and KNN, respectively, with a 10-fold cross-validation approach. However, using other machine learning algorithms might increase in the accuracy rate, which should have been discussed. Besides, the application of Histogram Equalization (HE) for image enhancement complicated the process of getting accurate texture features from distorted images, as HE itself adds noise to the output image, distorting the signals. Moreover, this study only shed light on alopecia areata disease, ignoring the inter-class differences between other similar type diseases, which increased the likelihood of inaccurate prediction of other diseases as alopecia areata, thereby making this framework less reliable.", "rationale": "\"First, we applied histogram equalization (HE). However, the HE increases the contrast of the background when used in images with low color depth, and information is lost as the histogram is not confined to the local region. To overcome the problem, we applied CLAHE (Contrast Limited Adaptive Histogram Equalization) by dividing an image into equal size non-overlapping areas and computing a histogram for each region. After clipping the histogram, we distributed the clipped value over the histogram equalization, which gives us control of the over-amplification of the contrast\" - HE loose information in image. Compare to that CLAHE gets more control for denoising image."}, {"context": "Often the captured image doesn\u2019t reflect the natural view and needs contrast enhancement to meet the level of realistic view [24]. Especially images with high color depth and after denoising effects need normalization for a better realistic view [25]. First, we applied histogram equalization (HE). However, the HE increases the contrast of the background when used in images with low color depth, and information is lost as the histogram is not confined to the local region. To overcome the problem, we applied CLAHE (Contrast Limited Adaptive Histogram Equalization) by dividing an image into equal size non-overlapping areas and computing a histogram for each region. After clipping the histogram, we distributed the clipped value over the histogram equalization, which gives us control of the over-amplification of the contrast and generates the resultant image shown in Fig. 4.", "rationale": "\"The application of Histogram Equalization (HE) for image enhancement complicated the process of getting accurate texture features from distorted images, as HE itself adds noise to the output image, distorting the signals.\" - Input image gets distorted when pass through HE."}], [{"context": "In this study, CNN is utilized for classification because it takes image\u2019s raw pixel data, trains a model, and extracts the features automatically for better detection. We used autokeras to find the best model for this problem. After trying 25 different combinations, we selected 3 hidden layers with 1 input and 1 output layer as our final model which is shown in Fig. 5. For training the model, we used batch_size = 16 with 50 epochs for each batch. The preprocessed data is divided into 70-30 train-test-split for training and validation purpose. Our model consists of 256 inputs, 3 x 3 square kernel, 3 output units and a softmax output. We used ReLU as our activation function to prevent the exponential growth of required computation and to explore the non-linear relationship between input and output variables. After each convolutional layer, input goes through the pooling layer having 2 x 2 kernel size to reduce the dimensions of the features map. Pooling layer summarizes the presented features in a region and helps to prevent the over-fitting problem by down sampling. We also used dropout layer after each pooling layer to prevent neurons in a layer from synchronously optimizing their weights and converging to the same goal. Our model\u2019s dropout rate is 0.3, which means 30% of the neurons of this layer will be randomly dropped in each epoch.", "rationale": "We used autokeras to find the best model for this problem. After trying 25 different combinations, we selected 3 hidden layers with 1 input and 1 output layer as our final model which is shown in Fig. 5. Autokeras is best way to find model parameter."}], [{"context": "In this study, CNN is utilized for classification because it takes image\u2019s raw pixel data, trains a model, and extracts the features automatically for better detection. We used autokeras to find the best model for this problem. After trying 25 different combinations, we selected 3 hidden layers with 1 input and 1 output layer as our final model which is shown in Fig. 5. For training the model, we used batch_size = 16 with 50 epochs for each batch. The preprocessed data is divided into 70-30 train-test-split for training and validation purpose. Our model consists of 256 inputs, 3 x 3 square kernel, 3 output units and a softmax output. We used ReLU as our activation function to prevent the exponential growth of required computation and to explore the non-linear relationship between input and output variables. After each convolutional layer, input goes through the pooling layer having 2 x 2 kernel size to reduce the dimensions of the features map. Pooling layer summarizes the presented features in a region and helps to prevent the over-fitting problem by down sampling. We also used dropout layer after each pooling layer to prevent neurons in a layer from synchronously optimizing their weights and converging to the same goal. Our model\u2019s dropout rate is 0.3, which means 30% of the neurons of this layer will be randomly dropped in each epoch.", "rationale": "Grid search were used to find optimal hyperparameter."}, {"context": "We trained our CNN model using the optimal hyperparameters selected from the grid search. These hyperparameters are listed in Table II. We divided the dataset into 70%-30% train-test-split where 105 randomly selected images are used for training and 45 random images for testing. After applying the preprocessing steps, we used the training dataset to train the CNN model and evaluated the test dataset using the model.", "rationale": "For training the model, we used batch_size = 16 with 50 epochs for each batch."}], [{"context": "We trained our CNN model using the optimal hyperparameters selected from the grid search. These hyperparameters are listed in Table II. We divided the dataset into 70%-30% train-test-split where 105 randomly selected images are used for training and 45 random images for testing. After applying the preprocessing steps, we used the training dataset to train the CNN model and evaluated the test dataset using the model.", "rationale": "Hyperparameters are selected using grid search."}, {"context": "", "rationale": "Hyperparameters are listed in the table."}], [{"context": "We trained our CNN model using the optimal hyperparameters selected from the grid search. These hyperparameters are listed in Table II. We divided the dataset into 70%-30% train-test-split where 105 randomly selected images are used for training and 45 random images for testing. After applying the preprocessing steps, we used the training dataset to train the CNN model and evaluated the test dataset using the model.", "rationale": "We trained our CNN model using the optimal hyperparameters selected from the grid search."}]], "composition": ["The three preprocessing steps used in this paper are image equalization, image enhancement, and data balancing. First two parts are mainly for increasing image quality, and the last part is for model versatility. The paper answer the question directly.", "The author tested images on multiple filters including gaussian filter, median filter with kernel_size = 3, bilateral filter, and non-local means filter with patch_size = 3 and patch_distance = 5. Comparing with other filter non-local means filter best result by preserving all the edges and reducing noise.", "Input image gets high contrast when pass through HE and hence loose information by adding noise. Compare to that CLAHE is a adaptive histogram equalization method in which the contrast amplification is limited, so as to reduce this problem of noise amplification.", "Autokeras is best way to find model parameter. It automatically tries different combination (in this case is 25) and find size of the model network. In this case the best size is 3 hidden layer with 1 input and 1 output.", "Using grid search the batch_size and epochs is determined. Since these are the optimal value hence used in the training.", "Hyperparameters are Batch Size 16, Epoch 50, Kernel Size 3 x 3, Optimizer Adam, Dropout Rate 0.3, Pooling Size 2 x 2, Activation Function ReLU.", "Optimal hyperparameter is used. Hence for epoch it is the optimal value. More epochs will not give us better accuracy."], "Is_figure_in_evidence": [true, true, true, true, true, false, false], "Is_table_in_evidence": [false, false, false, false, true, true, true], "question_key": ["415", "417", "418", "419", "420", "421", "422"], "passages": [], "figure_types": {"9317eb761d51f6eb428a157a02e2114a90b409e4/3-TableI-1.png": "photograph(s)", "9317eb761d51f6eb428a157a02e2114a90b409e4/4-Figure2-1.png": "schematic", "9317eb761d51f6eb428a157a02e2114a90b409e4/4-Figure3-1.png": "photograph(s)", "9317eb761d51f6eb428a157a02e2114a90b409e4/4-Figure4-1.png": "photograph(s)", "9317eb761d51f6eb428a157a02e2114a90b409e4/5-Figure5-1.png": "schematic", "9317eb761d51f6eb428a157a02e2114a90b409e4/5-Figure6-1.png": "plot", "9317eb761d51f6eb428a157a02e2114a90b409e4/5-TableII-1.png": "table", "9317eb761d51f6eb428a157a02e2114a90b409e4/6-Figure8-1.png": "plot"}}, "1508.06576": {"paper_id": "paper_15", "title": "A Neural Algorithm of Artistic Style", "arxiv_url": "https://arxiv.org/abs/1508.06576", "s2orc_url": "https://www.semanticscholar.org/paper/f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6", "all_figures_tables": {"f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/3-Figure1-1.png": "Figure 1: Convolutional Neural Network (CNN). A given input image is represented as a set of filtered images at each processing stage in the CNN. While the number of different filters increases along the processing hierarchy, the size of the filtered images is reduced by some downsampling mechanism (e.g. max-pooling) leading to a decrease in the total number of units per layer of the network. Content Reconstructions. We can visualise the information at different processing stages in the CNN by reconstructing the input image from only knowing the network\u2019s responses in a particular layer. We reconstruct the input image from from layers \u2018conv1 1\u2019 (a), \u2018conv2 1\u2019 (b), \u2018conv3 1\u2019 (c), \u2018conv4 1\u2019 (d) and \u2018conv5 1\u2019 (e) of the original VGG-Network. We find that reconstruction from lower layers is almost perfect (a,b,c). In higher layers of the network, detailed pixel information is lost while the high-level content of the image is preserved (d,e). Style Reconstructions. On top of the original CNN representations we built a new feature space that captures the style of an input image. The style representation computes correlations between the different features in different layers of the CNN. We reconstruct the style of the input image from style representations built on different subsets of CNN layers ( \u2018conv1 1\u2019 (a), \u2018conv1 1\u2019 and \u2018conv2 1\u2019 (b), \u2018conv1 1\u2019, \u2018conv2 1\u2019 and \u2018conv3 1\u2019 (c), \u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019 and \u2018conv4 1\u2019 (d), \u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019, \u2018conv4 1\u2019 and \u2018conv5 1\u2019 (e)). This creates images that match the style of a given image on an increasing scale while discarding information of the global arrangement of the scene.", "f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/5-Figure2-1.png": "Figure 2: Images that combine the content of a photograph with the style of several well-known artworks. The images were created by finding an image that simultaneously matches the content representation of the photograph and the style representation of the artwork (see Methods). The original photograph depicting the Neckarfront in Tu\u0308bingen, Germany, is shown in A (Photo: Andreas Praefcke). The painting that provided the style for the respective generated image is shown in the bottom left corner of each panel. B The Shipwreck of the Minotaur by J.M.W. Turner, 1805. C The Starry Night by Vincent van Gogh, 1889. D Der Schrei by Edvard Munch, 1893. E Femme nue assise by Pablo Picasso, 1910. F Composition VII by Wassily Kandinsky, 1913.", "f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/7-Figure3-1.png": "Figure 3: Detailed results for the style of the painting Composition VII by Wassily Kandinsky. The rows show the result of matching the style representation of increasing subsets of the CNN layers (see Methods). We find that the local image structures captured by the style representation increase in size and complexity when including style features from higher layers of the network. This can be explained by the increasing receptive field sizes and feature complexity along the network\u2019s processing hierarchy. The columns show different relative weightings between the content and style reconstruction. The number above each column indicates the ratio \u21b5/ between the emphasis on matching the content of the photograph and the style of the artwork (see Methods)."}, "referred_figures_tables": [["f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/3-Figure1-1.png", "f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/5-Figure2-1.png", "f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/5-Figure2-1.png", "f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/7-Figure3-1.png", "f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/7-Figure3-1.png", "f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/7-Figure3-1.png", "f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/7-Figure3-1.png", "f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/7-Figure3-1.png"]], "question_id": [1], "question": ["Can image content and style be \"fully\" or \"completely\" separated?"], "question_section": ["No section heading mentioned explicitly. (Page 3)"], "question_trigger_sentence": ["The key finding of this paper is that the representations of content and style in the Convolutional Neural Network are separable."], "question_type": ["Shallow question"], "evidential_info": [[{"context": "Again, we can visualise the information captured by these style feature spaces built on different layers of the network by constructing an image that matches the style representation of a given input image (Fig 1, style reconstructions). 10,11 Indeed reconstructions from the style features produce texturised versions of the input image that capture its general appearance in terms of colour and localised structures. Moreover, the size and complexity of local image structures from the input image increases along the hierarchy, a result that can be explained by the increasing receptive \ufb01eld sizes and feature complexity. We refer to this multi-scale representation as style representation . The key \ufb01nding of this paper is that the representations of content and style in the Convo- lutional Neural Network are separable. That is, we can manipulate both representations inde- pendently to produce new, perceptually meaningful images. To demonstrate this \ufb01nding, we generate images that mix the content and style representation from two different source images. In particular, we match the content representation of a photograph depicting the \u201cNeckarfront\u201d in T \u00a8 ubingen, Germany and the style representations of several well-known artworks taken from different periods of art (Fig 2). The images are synthesised by \ufb01nding an image that simultaneously matches the content representation of the photograph and the style representation of the respective piece of art (see Methods for details). While the global arrangement of the original photograph is preserved, the colours and local structures that compose the global scenery are provided by the artwork. Effectively, this renders the photograph in the style of the artwork, such that the appearance of the synthesised image resembles the work of art, even though it shows the same content as the photograph. As outlined above, the style representation is a multi-scale representation that includes mul- tiple layers of the neural network. In the images we have shown in Fig 2, the style representation", "rationale": "It is impossible to completely separate the content and style of the image. However, it is possible to extract their features and combine them with other images. It is also difficult to synthesize a combination that matches both content and stylistic constraints, but the loss function that is minimized in the paper allows giving different weights to content and style representations effectively producing visually appealing results."}, {"context": "including only a smaller number of lower layers, leading to different visual experiences (Fig 3, along the rows). When matching the style representations up to higher layers in the network, local images structures are matched on an increasingly large scale, leading to a smoother and more continuous visual experience. Thus, the visually most appealing images are usually cre- ated by matching the style representation up to the highest layers in the network (Fig 3, last row). Of course, image content and style cannot be completely disentangled. When synthesising an image that combines the content of one image with the style of another, there usually does not exist an image that perfectly matches both constraints at the same time. However, the loss function we minimise during image synthesis contains two terms for content and style respectively, that are well separated (see Methods). We can therefore smoothly regulate the emphasis on either reconstructing the content or the style (Fig 3, along the columns). A strong emphasis on style will result in images that match the appearance of the artwork, effectively giving a texturised version of it, but hardly show any of the photograph\u2019s content (Fig 3, \ufb01rst column). When placing strong emphasis on content, one can clearly identify the photograph, but the style of the painting is not as well-matched (Fig 3, last column). For a speci\ufb01c pair of source images one can adjust the trade-off between content and style to create visually appealing images. Here we present an arti\ufb01cial neural system that achieves a separation of image content from style, thus allowing to recast the content of one image in the style of any other image. We demonstrate this by creating new, artistic images that combine the style of several well-known paintings with the content of an arbitrarily chosen photograph. In particular, we derive the neural representations for the content and style of an image from the feature responses of high- performing Deep Neural Networks trained on object recognition. To our knowledge this is the \ufb01rst demonstration of image features separating content from style in whole natural images.", "rationale": "The previous approaches to artistic style transfer usually use texture transfer that directly manipulates the pixels of the image with non-parametric techniques."}, {"context": "Previous work on separating content from style was evaluated on sensory inputs of much lesser complexity, such as characters in different handwriting or images of faces or small \ufb01gures in different poses. 12,13 In our demonstration, we render a given photograph in the style of a range of well-known artworks. This problem is usually approached in a branch of computer vision called non- photorealistic rendering (for recent review see 14 ). Conceptually most closely related are meth- ods using texture transfer to achieve artistic style transfer. 15\u201319 However, these previous ap- proaches mainly rely on non-parametric techniques to directly manipulate the pixel representa- tion of an image. In contrast, by using Deep Neural Networks trained on object recognition, we carry out manipulations in feature spaces that explicitly represent the high level content of an image. Features from Deep Neural Networks trained on object recognition have been previously used for style recognition in order to classify artworks according to the period in which they were created. 20 There, classi\ufb01ers are trained on top of the raw network activations, which we call content representations. We conjecture that a transformation into a stationary feature space such as our style representation might achieve even better performance in style classi\ufb01cation. In general, our method of synthesising images that mix content and style from different sources, provides a new, fascinating tool to study the perception and neural representation of art, style and content-independent image appearance in general. We can design novel stimuli that introduce two independent, perceptually meaningful sources of variation: the appearance and the content of an image. We envision that this will be useful for a wide range of experimen- tal studies concerning visual perception ranging from psychophysics over functional imaging to even electrophysiological neural recordings. In fact, our work offers an algorithmic under- standing of how neural representations can independently capture the content of an image and the style in which it is presented. Importantly, the mathematical form of our style representa- 8", "rationale": "Only the representations of the contents and artistic style are separable in CNNs. And it is possible to manipulate them to generate combinations of contents and styles of different images."}, {"context": "tions generates a clear, testable hypothesis about the representation of image appearance down to the single neuron level. The style representations simply compute the correlations between different types of neurons in the network. Extracting correlations between neurons is a bio- logically plausible computation that is, for example, implemented by so-called complex cells in the primary visual system (V1). 21 Our results suggest that performing a complex-cell like computation at different processing stages along the ventral stream would be a possible way to obtain a content-independent representation of the appearance of a visual input. All in all it is truly fascinating that a neural system, which is trained to perform one of the core computational tasks of biological vision, automatically learns image representations that allow the separation of image content from style. The explanation could be that when learning object recognition, the network has to become invariant to all image variation that preserves object identity. Representations that factorise the variation in the content of an image and the variation in its appearance would be extremely practical for this task. Thus, our ability to abstract content from style and therefore our ability to create and enjoy art might be primarily a preeminent signature of the powerful inference capabilities of our visual system.", "rationale": "It is important to mention that the artistic style representation is just a correlation of filter responses between layers in CNN. The paper suggests that this is a plausible way to obtain the content-independent visual appearance of the image. When the object recognition model is learning, it has to be able to extract features that are invariant to different variations of images. Thus, it allows the separation of content and style representations."}]], "composition": ["The paper suggests that it is impossible to completely separate the content and the style of the image. But it is possible to extract their representations to then combine them with a loss function that allows the generation of visually appealing images that somewhat satisfy (not fully) the content and stylistic constraints. It is important to mention that the artistic style representation is just a correlation of filter responses between layers in CNN. The paper suggests that this is a plausible way to obtain the content-independent visual appearance of the image. When the object recognition model is learning, it has to be able to extract features that are invariant to different variations of images. Thus, it allows the separation of content and style representations. Previous methods use non-parametric techniques that directly manipulate the pixels of the image without such separation of representations."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["442"], "passages": [], "figure_types": {"f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/3-Figure1-1.png": "schematic", "f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/5-Figure2-1.png": "photograph(s)", "f37e90c0bd5c4a9619ccfb763c45cb2d84abd3e6/7-Figure3-1.png": "photograph(s)"}}, "2210.15541": {"paper_id": "paper_151", "title": "Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost", "arxiv_url": "https://arxiv.org/abs/2210.15541", "s2orc_url": "https://www.semanticscholar.org/paper/50790468a774f3ecd663e79932e6da4e813048aa", "all_figures_tables": {"50790468a774f3ecd663e79932e6da4e813048aa/10-Table4-1.png": "Table 4: GLUE benchmark results. Bold results indicate best accuracy for each task.", "50790468a774f3ecd663e79932e6da4e813048aa/15-Figure6-1.png": "Figure 6: Three sparsity patterns with n = 16 and k = 4. Grey-colored blocks on the diagonal indicate manually added self-loops. Any other color indicates a cluster.", "50790468a774f3ecd663e79932e6da4e813048aa/16-Table5-1.png": "Table 5: Asymptotic costs of individual steps within the attention module of SBM-Transformer. The sequence length, number of edges, number of clusters, and head dimension are denoted as n, m, k, and d, respectively.", "50790468a774f3ecd663e79932e6da4e813048aa/16-Table6-1.png": "Table 6: Asymptotic computational costs of different attention mechanisms. The k term denotes different parameters for each model: number of clusters for SBM-Transformer, number of hashing rounds for Reformer [22], number of random features for Performer [10], the projection rank for Linformer [45], and the number of landmarks for Nystr\u00f6mformer [48]. The additional c term in Reformer [22] indicates the number of hashing chunks, set to c = O( 1n ) as default.", "50790468a774f3ecd663e79932e6da4e813048aa/17-Table7-1.png": "Table 7: Hyperparameter settings used synthetic, LRA, and GLUE experiments. For methods other than full attention [40], we use 128 clusters for SBM-Transformer, 2 hashing rounds for Reformer [22], 256 landmarks for Nystr\u00f6mformer [48], and 256 dimensions for Linformer [45] and Performer [10].", "50790468a774f3ecd663e79932e6da4e813048aa/17-Table8-1.png": "Table 8: LRA benchmark results. Bold and underlined results indicate best and 2nd best test accuracy for each task, respectively. Numbers enclosed in parentheses for SBM-Transformer indicate the density of graphs sampled during test time averaged across all attention heads. For the SBMTransformer models, (+I) indicates that self-loops are manually fixed while (+0) indicates model without the modification.", "50790468a774f3ecd663e79932e6da4e813048aa/17-Table9-1.png": "Table 9: LRA benchmark results of SBM-Transformer with increasing density regularization weight \u03bb. Applying a density regularizer helps in encouraging sparser attention patterns which induce less computational cost, while retaining competitive performance.", "50790468a774f3ecd663e79932e6da4e813048aa/18-Figure7-1.png": "Figure 7: Attention density plots within individual attention heads given inputs from the LRA PATHFINDER test set. All examples shown are from a subset of the test set that the model has predicted correctly. For each set of 5 images, the leftmost image shows the original input image of which the title shows the ground-truth label. To its right are attention density plots from two heads of the first layer followed by those from two heads of the second layer. Above each plot is the actual numeric attention density between 0 and 1. The color in each pixel indicates how many other pixels attend to that particular pixel (a color closer to bright yellow indicates more attention).", "50790468a774f3ecd663e79932e6da4e813048aa/19-Figure8-1.png": "Figure 8: Similar visualization as Figure 7 for the LRA IMAGE test set.", "50790468a774f3ecd663e79932e6da4e813048aa/2-Figure1-1.png": "Figure 1: The attention module in SBM-Transformer. In multi-head attention, each attention head samples a bipartite graph connecting queries to keys from an underlying SBM. The adjacency of the sampled graph is used as an attention mask to compute the dot products only for the sampled edges.", "50790468a774f3ecd663e79932e6da4e813048aa/4-Figure2-1.png": "Figure 2: An illustration of the attention mechanism in SBM-Transformer. Each head first maps queries and keys to the node representation space through a shared MLP. The graph sampling module samples an attention mask from a Stochastic Block Model (SBM) parameterized by the node and cluster embeddings. The discrete sampling step is differentiable via a Straight-Through Estimator (STE). Given the mask, the output is computed via masked attention.", "50790468a774f3ecd663e79932e6da4e813048aa/6-Figure3-1.png": "Figure 3: Representative examples from the SBM and resulting mask expectations (darker grid indicates edge probability closer to 1). (a) The expected mask is dense if all nodes and clusters are collapsed within a small region. (b) Clear-cut groups in the embedding space induce a block-diagonal mask. (c) Clusters located far apart from nodes lead to sparse masks.", "50790468a774f3ecd663e79932e6da4e813048aa/7-Figure4-1.png": "Figure 4: Loss (left) and mask density (right) of SBM-Transformer during training on the synthetic task. SBM-Transformer successfully converges to zero loss by tuning itself towards full attention.", "50790468a774f3ecd663e79932e6da4e813048aa/8-Table1-1.png": "Table 1: LRA benchmark results. The sequence lengths are shown next to each task. For SBMTransformer, we report the average attention sparsity across all layers and heads during test time in parentheses. Bold and underlined results indicate best and 2nd best test accuracy for each task.", "50790468a774f3ecd663e79932e6da4e813048aa/8-Table2-1.png": "Table 2: LRA results of SBM-Transformer with increasing sparsity regularization weight \u03bb. Bold results indicate best accuracy for each task and percentage in parentheses indicate average attention density. Sparsity regularization helps in reducing computational cost with small drop in performance.", "50790468a774f3ecd663e79932e6da4e813048aa/9-Figure5-1.png": "Figure 5: Average and standard deviation of density of masks sampled across the test set for each LRA task. The x-axis indicates the lower (L1) and upper (L2) layers and each bar represents the density averaged between the two attention heads in each layer.", "50790468a774f3ecd663e79932e6da4e813048aa/9-Table3-1.png": "Table 3: Per-example relative FLOP count and peak memory usage during LRA inference."}, "referred_figures_tables": [["50790468a774f3ecd663e79932e6da4e813048aa/2-Figure1-1.png", "50790468a774f3ecd663e79932e6da4e813048aa/17-Table8-1.png"]], "question_id": [2], "question": ["In what way can SBM-Transformer be considered better than Reformer?"], "question_section": ["2 Related Work"], "question_trigger_sentence": ["2 Related Work"], "question_type": ["Testing question"], "evidential_info": [[{"context": "Table\u00a08 shows the test accuracies of each method. Our SBM-Transformer achieves the best overall performance, ranking first in two tasks, and second in one other. SBM-Transformer also outperforms full attention in all five tasks while computing 30% or less attention scores on average, which supports our claim that masked attention with partial attention score computations can be preferred over full attention depending on the task. With respect to the attention mask structure, we find that flexibility of SBM is indeed beneficial, as Reformer struggles in ListOps, most likely due to the inability of block-diagonal masks to model hierarchical contexts.", "rationale": "SBM-Transformer allows more flexible attention mask structures, while Reformer can only use block-diagonal masks that cannot model hierarchical contexts."}, {"context": "To contribute to the efficient Transformers lineage, we propose SBM-Transformer, capable of adjusting its attention sparsity data-adaptively based without fully computing the attention score matrix (Figure 1). Leveraging a mixed-membership Stochastic Block Model (SBM) [2], each", "rationale": "SBM-Transformer is the first Transformer architecture that can data-adaptively choose between linear to full attention with respective computational costs."}]], "composition": ["SBM-Transformer allows more flexible attention mask structures between linear to full attention with respective computational costs, while Reformer can only use block-diagonal masks that cannot model hierarchical contexts."], "Is_figure_in_evidence": [true], "Is_table_in_evidence": [false], "question_key": ["455"], "passages": ["The Transformer [40] architecture has been the go-to method for encoding sequential data, due to its superior performance in various tasks such as machine translation [30], image classification [15], and protein language modeling [34]. Its key strength stems from the multi-head attention module, where a so-called attention score matrix computes how contextually important one token is to another for all possible token pairs. Each Transformer layer simultaneously pools the token representations based on the attention scores, eventually returning contextualized features without sequentially traversing through the input sequence as its recurrent neural network-based predecessors [18].", "A well-known drawback of the original Transformer is its high computational cost in time and memory that increases quadratically with sequence length. This is due to the full pairwise computation of attention scores, which prohibits applying it in tasks involving long-range dependencies such as document summarization [19] or high-resolution image processing [53]. Many works have thus focused on developing more efficient alternatives by exploiting fixed or learnable attention sparsity patterns [9, 51, 22, 13], low-rank approximations [45, 48], or kernelized attention modules\u00a0[21, 10].", "Even though the efficient alternatives hold theoretical expressibility guarantees\u00a0[50], they are far from sufficient, still failing to convince practitioners to replace the original Transformer. We believe this is mostly due to their lack of adaptability. They apply the same modifications to unanimously sparsify all the attention modules across layers, without considering the tasks at hand. Such strategy imposes inductive bias too strongly and often leads to sub-optimal cost vs. performance trade-offs in downstream tasks\u00a0[29]. In this work, we argue that to retain the utmost potential of Transformers, each attention module should have the ability to flexibly choose between sparse and full attention. This is especially evident when considering many state-of-the-art systems suggest the need for a mixture of dense and sparse attention layers. For example, a qualitative analysis on pretrained BERT showed that lower layers exhibit broad dense attention while upper layers perform focused sparse attention\u00a0[11]. In the case of GPT-3\u00a0[7], the Transformer blocks are manually arranged to alternate between dense and sparse attention.", "To contribute to the efficient Transformers lineage, we propose SBM-Transformer, capable of adjusting its attention sparsity data-adaptively based without fully computing the attention score matrix (Figure\u00a01). Leveraging a mixed-membership Stochastic Block Model (SBM)\u00a0[2], each attention head samples a bipartite graph connecting queries to keys. Then, the adjacency of the sampled graph is used as an attention mask so that only attention scores corresponding to sampled edges are computed.The overall computational cost is linear in the number of edges, which can range from linear to quadratic in sequence length depending on the data and task under concern. Each attention head is equipped with its own underlying SBM, enabling the model to diversify the attention sparsity across heads and layers. By incorporating a straight-through estimator\u00a0[4] in the discrete graph-sampling step, SBM-Transformer enjoys end-to-end differentiability and can find the proper attention sparsity based solely upon minimizing the predictive loss. The model can also easily be further regularized by penalizing the number of sampled edges, which results in a lighter model using less computational resources during inference. To the best of our knowledge, our method is the first Transformer architecture that can data-adaptively choose between linear to full attention with respective computational costs. To summarize, our main contributions are as follows:", "\u2022We present SBM-Transformer, a novel Transformer of which each attention head can adaptively adjust its attention sparsity as well as computational cost based on the input data.\u2022To demonstrate the benefit of this flexibility, we theoretically prove that SBM-Transformer retains universal approximability, and also stress-test the model under a synthetic task where full attention is required to achieve 100% accuracy.\u2022Evaluations on LRA and GLUE benchmarks show that SBM-Transformer outperforms previous efficient Transformer models as well as the vanilla Transformer with dense attention.", "In this section we discuss previous efficient Transformer variants and several works similar to ours with respect to adaptively learning sparse attention patterns. We also review several works on SBMs.", "Many efficient Transformers tackle to reduce the quadratic cost of multi-head attention with different approaches. While we discuss only a handful of representative approaches, a much more comprehensive survey can be found in\u00a0[39]. The Linear Transformer\u00a0[21] achieves linear complexity by replacing the softmax with a low-rank kernelized function. Linformer\u00a0[45] and Nystr\u00f6mformer\u00a0[48] use a similar approach by low-rank approximating the attention score matrix. Performer\u00a0[10] uses positive orthogonal random features to approximate the softmax kernel. Reformer\u00a0[22] gathers similar tokens together through locality-sensitive hashing (LSH) and performs attention amongst tokens within the same bucket. Of all methods above, our method is most similar to Reformer, in the sense that we adaptively assign queries and keys into clusters and form a low-rank sparse attention pattern. However, our method performs soft-clustering with much less structural constraints, allowing each attention head to represent a wider variety of dependency structure and to adjust its sparsity towards full attention if needed.", "With respect to flexible training between sparse and dense attention, there exist some works that parameterize how sparse the attention pattern should be based on the input. The Adaptive Sparse Transformer\u00a0[12] proposed replacing the usual softmax activation with \u03b1\ud835\udefc\\alphaitalic_\u03b1-entmax, in which the \u03b1\ud835\udefc\\alphaitalic_\u03b1 parameter can be differentiably trained to adjust the activation between softmax and sparsemax activation\u00a0[27]. SparseBERT\u00a0[36] uses a differentiable masking technique where each attention mask is sampled from a Gumbel-sigmoid distribution using data-independent mask probability parameters.While these methodspossess the flexibility to adjust between sparse and full attention based on data, they still require full computation of the attention score matrix before sparsification, and hence are unable to leverage the learned sparsity towards better model efficiency. To the best of our knowledge, ours is the first work to be able to adaptively tune its attention sparsity between sparse to full attention without requiring the explicit computation of the attention score matrix, thereby avoiding quadratic cost when possible.", "The Stochastic Block Model (SBM) is a generative model that encodes the latent structure of graphs by grouping nodes into clusters. By modeling the cluster-membership of each node as well as inter-cluster relationships, SBMs can represent a wide variety of graph structures, which is a feature especially useful for generating new graphs or predicting missing edges in noisy data\u00a0[1]. The standard SBM assigns each node to a single cluster, and the probability of an edge between two nodes strictly depends on the corresponding clusters. Several structural extensions include overlapping SBM\u00a0[24] and mixed-membership SBM\u00a0[2], which allow each node to be assigned to multiple clusters. The underlying SBM used by our framework mostly resembles these two variants, while the edge probability is modeled by a nonlinear function of two node embeddings rather than a bilinear one. There exist many other extensions including degree-corrected SBM\u00a0[20] for multi-graphs and hierarchical SBM\u00a0[31] for multiplex-graphs. Further details can be found in a recent survey\u00a0[16].", "We first introduce the full attention mechanism used in the original Transformer\u00a0[40] as well as masked attention which will serve as a backbone of our approach.", "In vanilla Transformer [40], each attention head takes a sequence of token features as input \ud835\udc7f\u2208\u211dn\u00d7d\ud835\udc7fsuperscript\u211d\ud835\udc5b\ud835\udc51\\bm{X}\\in\\mathbb{R}^{n\\times d}bold_italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_d end_POSTSUPERSCRIPT where n\ud835\udc5bnitalic_n is the sequence length and d\ud835\udc51ditalic_d the embedding dimension. Weight parameters \ud835\udc7eQ,\ud835\udc7eK\u2208\u211dd\u00d7dhsuperscript\ud835\udc7e\ud835\udc44superscript\ud835\udc7e\ud835\udc3esuperscript\u211d\ud835\udc51subscript\ud835\udc51\u210e\\bm{W}^{Q},\\bm{W}^{K}\\in\\mathbb{R}^{d\\times d_{h}}bold_italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and \ud835\udc7eV\u2208\u211dd\u00d7dhsuperscript\ud835\udc7e\ud835\udc49superscript\u211d\ud835\udc51subscript\ud835\udc51\u210e\\bm{W}^{V}\\in\\mathbb{R}^{d\\times d_{h}}bold_italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUPERSCRIPT with head-dimension dhsubscript\ud835\udc51\u210ed_{h}italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT first maps the input features \ud835\udc7f\ud835\udc7f\\bm{X}bold_italic_X into query \ud835\udc78\ud835\udc78\\bm{Q}bold_italic_Q, key \ud835\udc72\ud835\udc72\\bm{K}bold_italic_K, and value \ud835\udc7d\ud835\udc7d\\bm{V}bold_italic_V, respectively. Then, the attention score matrix is computed with scaled dot-product of queries and keys followed by row-wise softmax activation \u03c3(\u22c5)\ud835\udf0e\u22c5\\sigma(\\cdot)italic_\u03c3 ( \u22c5 ). Note that explicit computation of this matrix is the main bottleneck of full attention, incurring \ud835\udcaa(n2)\ud835\udcaasuperscript\ud835\udc5b2\\mathcal{O}(n^{2})caligraphic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) asymptotic cost in both time and memory. The value features \ud835\udc7d\ud835\udc7d\\bm{V}bold_italic_V are then pooled based on the attention scores, returning the output token representations. Altogether, the operation performed by each attention head can be written as\ud835\udc78=\ud835\udc7f\ud835\udc7eQ,\ud835\udc72=\ud835\udc7f\ud835\udc7eK,\ud835\udc7d=\ud835\udc7f\ud835\udc7eVformulae-sequence\ud835\udc78\ud835\udc7fsuperscript\ud835\udc7e\ud835\udc44formulae-sequence\ud835\udc72\ud835\udc7fsuperscript\ud835\udc7e\ud835\udc3e\ud835\udc7d\ud835\udc7fsuperscript\ud835\udc7e\ud835\udc49\\displaystyle\\bm{Q}=\\bm{X}\\bm{W}^{Q},\\;\\;\\bm{K}=\\bm{X}\\bm{W}^{K},\\;\\;\\bm{V}=\\bm{X}\\bm{W}^{V}bold_italic_Q = bold_italic_X bold_italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT , bold_italic_K = bold_italic_X bold_italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , bold_italic_V = bold_italic_X bold_italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT(1)\ud835\ude70\ud835\ude9d\ud835\ude9d\ud835\ude97(\ud835\udc7f)=\u03c3(\ud835\udc78\ud835\udc72Tdh)\ud835\udc7d.\ud835\ude70\ud835\ude9d\ud835\ude9d\ud835\ude97\ud835\udc7f\ud835\udf0e\ud835\udc78superscript\ud835\udc72\ud835\udc47subscript\ud835\udc51\u210e\ud835\udc7d\\displaystyle\\texttt{Attn}(\\bm{X})=\\sigma\\left(\\dfrac{\\bm{Q}\\bm{K}^{T}}{\\sqrt{d_{h}}}\\right)\\bm{V}.Attn ( bold_italic_X ) = italic_\u03c3 ( divide start_ARG bold_italic_Q bold_italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_ARG end_ARG ) bold_italic_V .(2)", "One way to remove the quadratic bottleneck from the attention score matrix is to apply a binary mask \ud835\udc74\u2208{0,1}n\u00d7n\ud835\udc74superscript01\ud835\udc5b\ud835\udc5b\\bm{M}\\in\\{0,1\\}^{n\\times n}bold_italic_M \u2208 { 0 , 1 } start_POSTSUPERSCRIPT italic_n \u00d7 italic_n end_POSTSUPERSCRIPT and compute the scaled dot-products \ud835\udc78i\ud835\udc72jT/dhsubscript\ud835\udc78\ud835\udc56superscriptsubscript\ud835\udc72\ud835\udc57\ud835\udc47subscript\ud835\udc51\u210e\\bm{Q}_{i}\\bm{K}_{j}^{T}/\\sqrt{d_{h}}bold_italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT / square-root start_ARG italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_ARG only if \ud835\udc74ij=1subscript\ud835\udc74\ud835\udc56\ud835\udc571\\bm{M}_{ij}=1bold_italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1. In presence of an attention mask, the operation is modified to\ud835\ude70\ud835\ude9d\ud835\ude9d\ud835\ude97mask(\ud835\udc7f,\ud835\udc74)=\u03c3\ud835\udc74(\ud835\udc74\u2299\ud835\udc78\ud835\udc72Tdh)\ud835\udc7dsubscript\ud835\ude70\ud835\ude9d\ud835\ude9d\ud835\ude97mask\ud835\udc7f\ud835\udc74subscript\ud835\udf0e\ud835\udc74direct-product\ud835\udc74\ud835\udc78superscript\ud835\udc72\ud835\udc47subscript\ud835\udc51\u210e\ud835\udc7d\\displaystyle\\texttt{Attn}_{\\text{mask}}(\\bm{X},\\bm{M})=\\sigma_{\\bm{M}}\\left(\\bm{M}\\odot\\dfrac{\\bm{Q}\\bm{K}^{T}}{\\sqrt{d_{h}}}\\right)\\bm{V}Attn start_POSTSUBSCRIPT mask end_POSTSUBSCRIPT ( bold_italic_X , bold_italic_M ) = italic_\u03c3 start_POSTSUBSCRIPT bold_italic_M end_POSTSUBSCRIPT ( bold_italic_M \u2299 divide start_ARG bold_italic_Q bold_italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_ARG end_ARG ) bold_italic_V(3)\u03c3\ud835\udc74(\ud835\udc68)ij\u2254{exp\u2061(\ud835\udc68ij)\u2211k\u2208{k\u2032|\ud835\udc74ik\u2032=1}exp\u2061(\ud835\udc68ik)if\ud835\udc74ij=10otherwise\u2254subscript\ud835\udf0e\ud835\udc74subscript\ud835\udc68\ud835\udc56\ud835\udc57casessubscript\ud835\udc68\ud835\udc56\ud835\udc57subscript\ud835\udc58conditional-setsuperscript\ud835\udc58\u2032subscript\ud835\udc74\ud835\udc56superscript\ud835\udc58\u20321subscript\ud835\udc68\ud835\udc56\ud835\udc58ifsubscript\ud835\udc74\ud835\udc56\ud835\udc5710otherwise\\displaystyle\\sigma_{\\bm{M}}(\\bm{A})_{ij}\\coloneqq\\begin{cases}\\dfrac{\\exp(\\bm{A}_{ij})}{\\sum_{k\\in\\{k^{\\prime}|\\bm{M}_{ik^{\\prime}}=1\\}}\\exp(\\bm{A}_{ik})}&\\text{if}\\;\\;\\bm{M}_{ij}=1\\\\\\hfil 0&\\text{otherwise}\\end{cases}italic_\u03c3 start_POSTSUBSCRIPT bold_italic_M end_POSTSUBSCRIPT ( bold_italic_A ) start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT \u2254 { start_ROW start_CELL divide start_ARG roman_exp ( bold_italic_A start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k \u2208 { italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT | bold_italic_M start_POSTSUBSCRIPT italic_i italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = 1 } end_POSTSUBSCRIPT roman_exp ( bold_italic_A start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT ) end_ARG end_CELL start_CELL if bold_italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL otherwise end_CELL end_ROW(4)where \u2299direct-product\\odot\u2299 indicates entry-wise multiplication. Note that the masked-softmax \u03c3\ud835\udc74(\u22c5)subscript\ud835\udf0e\ud835\udc74\u22c5\\sigma_{\\bm{M}}(\\cdot)italic_\u03c3 start_POSTSUBSCRIPT bold_italic_M end_POSTSUBSCRIPT ( \u22c5 ) operator only computes unmasked terms, ensuring that each (i,j)\ud835\udc56\ud835\udc57(i,j)( italic_i , italic_j )-th attention score survives as nonzero if and only if \ud835\udc74ij=1subscript\ud835\udc74\ud835\udc56\ud835\udc571\\bm{M}_{ij}=1bold_italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1. This is thus equivalent to filling in the (i,j)\ud835\udc56\ud835\udc57(i,j)( italic_i , italic_j )-th attention score with \u2212\u221e-\\infty- \u221e if \ud835\udc74ij=0subscript\ud835\udc74\ud835\udc56\ud835\udc570\\bm{M}_{ij}=0bold_italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 0, then applying the standard softmax operator. Most sparsity-based efficient Transformers fall under this formulation, while using different methods to either manually fix or learn the mask \ud835\udc74\ud835\udc74\\bm{M}bold_italic_M. For instance, local attention\u00a0[9, 3, 51] with a sliding window sets \ud835\udc74ij=1subscript\ud835\udc74\ud835\udc56\ud835\udc571\\bm{M}_{ij}=1bold_italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1 if |i\u2212j|<c\ud835\udc56\ud835\udc57\ud835\udc50|i-j|<c| italic_i - italic_j | < italic_c for some context window size c\ud835\udc50citalic_c while Reformer\u00a0[22] sets \ud835\udc74ij=1subscript\ud835\udc74\ud835\udc56\ud835\udc571\\bm{M}_{ij}=1bold_italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1 if \ud835\udc78isubscript\ud835\udc78\ud835\udc56\\bm{Q}_{i}bold_italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and \ud835\udc72jsubscript\ud835\udc72\ud835\udc57\\bm{K}_{j}bold_italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT are hashed into the same bucket.", "Here we discuss the details of SBM-Transformer (Figure\u00a02). We first illustrate the forward step of our attention module and how the underlying SBM\u00a0[2] of each head, from which we sample our attention masks, is parameterized by the input tensors. We then discuss how the model enables end-to-end differentiability despite the discrete graph sampling step.", "In our framework, we view the attention mask \ud835\udc74\ud835\udc74\\bm{M}bold_italic_M as an adjacency matrix of a bipartite graph that connects queries to keys, and let each attention head sample an adjacency matrix that best represents the contextual dependencies amongst input tokens.In order to efficiently sample adjacency matrices while avoiding the quadratic cost, the distribution of graphs must first be parameterized with a sub-quadratic number of latent variables. Stochastic Block Models fit perfectly for our purpose as it models graphs that are low-rank structured with k\ud835\udc58kitalic_k latent clusters, allowing full parameterization using \ud835\udcaa(nk)\ud835\udcaa\ud835\udc5b\ud835\udc58\\mathcal{O}(nk)caligraphic_O ( italic_n italic_k ) memory. More concretely, the SBM distribution is defined by two nonnegative node-to-cluster memberships \ud835\udc80,\ud835\udc81\u2208\u211d+n\u00d7k\ud835\udc80\ud835\udc81superscriptsubscript\u211d\ud835\udc5b\ud835\udc58\\bm{Y},\\bm{Z}\\in\\mathbb{R}_{+}^{n\\times k}bold_italic_Y , bold_italic_Z \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n \u00d7 italic_k end_POSTSUPERSCRIPT and a so-called block matrix \ud835\udc69\u2208\u211d+k\u00d7k\ud835\udc69superscriptsubscript\u211d\ud835\udc58\ud835\udc58\\bm{B}\\in\\mathbb{R}_{+}^{k\\times k}bold_italic_B \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k \u00d7 italic_k end_POSTSUPERSCRIPT that stores the inter-cluster connection probabilities. The probability of node i\ud835\udc56iitalic_i being connected to node j\ud835\udc57jitalic_j is computed as p(i,j)=\ud835\udc80i\ud835\udc69\ud835\udc81jT\ud835\udc5d\ud835\udc56\ud835\udc57subscript\ud835\udc80\ud835\udc56\ud835\udc69superscriptsubscript\ud835\udc81\ud835\udc57\ud835\udc47p(i,j)=\\bm{Y}_{i}\\bm{B}\\bm{Z}_{j}^{T}italic_p ( italic_i , italic_j ) = bold_italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_B bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT. Equivalently, the expectation of the adjacency matrix sampled from \ud835\udc68\u223cSBM(\ud835\udc80,\ud835\udc69,\ud835\udc81)similar-to\ud835\udc68\ud835\udc46\ud835\udc35\ud835\udc40\ud835\udc80\ud835\udc69\ud835\udc81\\bm{A}\\sim SBM(\\bm{Y},\\bm{B},\\bm{Z})bold_italic_A \u223c italic_S italic_B italic_M ( bold_italic_Y , bold_italic_B , bold_italic_Z ) can be written as \ud835\udd3c[\ud835\udc68]=\ud835\udc80\ud835\udc69\ud835\udc81T\ud835\udd3cdelimited-[]\ud835\udc68\ud835\udc80\ud835\udc69superscript\ud835\udc81\ud835\udc47\\mathbb{E}[\\bm{A}]=\\bm{Y}\\bm{B}\\bm{Z}^{T}blackboard_E [ bold_italic_A ] = bold_italic_Y bold_italic_B bold_italic_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT.", "For proper parameterization of the SBM, we must infer the nonnegative node-memberships and block matrix from the queries and keys. To do so, we equip each attention head a 2-layer MLPdh\u2192dhsubscriptMLP\u2192subscript\ud835\udc51\u210esubscript\ud835\udc51\u210e\\text{MLP}_{d_{h}\\to d_{h}}MLP start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT \u2192 italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT with ReLU activation, and a set of k\ud835\udc58kitalic_k trainable cluster-embeddings \ud835\udc6a\u2208\u211dk\u00d7dh\ud835\udc6asuperscript\u211d\ud835\udc58subscript\ud835\udc51\u210e\\bm{C}\\in\\mathbb{R}^{k\\times d_{h}}bold_italic_C \u2208 blackboard_R start_POSTSUPERSCRIPT italic_k \u00d7 italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUPERSCRIPT. First, our model computes the block matrix \ud835\udc7a^\u2208\u211d+k\u00d7k^\ud835\udc7asuperscriptsubscript\u211d\ud835\udc58\ud835\udc58\\smash{\\hat{\\bm{S}}}\\in\\mathbb{R}_{+}^{k\\times k}over^ start_ARG bold_italic_S end_ARG \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k \u00d7 italic_k end_POSTSUPERSCRIPT by taking dot products amongst cluster-embeddings \ud835\udc6a\ud835\udc6a\\bm{C}bold_italic_C followed by a 2-dimensional softmax activation. The node embeddings are obtained by processing each query and key through the MLPdh\u2192dhsubscriptMLP\u2192subscript\ud835\udc51\u210esubscript\ud835\udc51\u210e\\text{MLP}_{d_{h}\\to d_{h}}MLP start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT \u2192 italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT, mapping token representations into the node representation space.The memberships of query and key nodes, which we denote by \ud835\udc78^^\ud835\udc78\\smash{\\hat{\\bm{Q}}}over^ start_ARG bold_italic_Q end_ARG and \ud835\udc72^^\ud835\udc72\\smash{\\hat{\\bm{K}}}over^ start_ARG bold_italic_K end_ARG, are then inferred by taking dot products of node and cluster embeddings, followed by a sigmoid function. The block matrix \ud835\udc7a^^\ud835\udc7a\\smash{\\hat{\\bm{S}}}over^ start_ARG bold_italic_S end_ARG, query node-memberships \ud835\udc78^^\ud835\udc78\\smash{\\hat{\\bm{Q}}}over^ start_ARG bold_italic_Q end_ARG, and key node-memberships \ud835\udc72^^\ud835\udc72\\smash{\\hat{\\bm{K}}}over^ start_ARG bold_italic_K end_ARG altogether provide a well-defined parameterization for the SBM. Thus, a bipartite graph adjacency \ud835\udc74\u2208{0,1}n\u00d7m\ud835\udc74superscript01\ud835\udc5b\ud835\udc5a\\bm{M}\\in\\{0,1\\}^{n\\times m}bold_italic_M \u2208 { 0 , 1 } start_POSTSUPERSCRIPT italic_n \u00d7 italic_m end_POSTSUPERSCRIPT can be sampled from \ud835\udc74\u223cSBM(\ud835\udc78^,\ud835\udc7a^,\ud835\udc72^)similar-to\ud835\udc74\ud835\udc46\ud835\udc35\ud835\udc40^\ud835\udc78^\ud835\udc7a^\ud835\udc72\\bm{M}\\sim SBM(\\smash{\\hat{\\bm{Q}}},\\smash{\\hat{\\bm{S}}},\\smash{\\hat{\\bm{K}}})bold_italic_M \u223c italic_S italic_B italic_M ( over^ start_ARG bold_italic_Q end_ARG , over^ start_ARG bold_italic_S end_ARG , over^ start_ARG bold_italic_K end_ARG ) with expectation \ud835\udd3c[\ud835\udc74]=\ud835\udc78^\ud835\udc7a^\ud835\udc72^T\ud835\udd3cdelimited-[]\ud835\udc74^\ud835\udc78^\ud835\udc7asuperscript^\ud835\udc72\ud835\udc47\\mathbb{E}[\\bm{M}]=\\smash{\\hat{\\bm{Q}}}\\smash{\\hat{\\bm{S}}}\\smash{\\hat{\\bm{K}}}^{T}blackboard_E [ bold_italic_M ] = over^ start_ARG bold_italic_Q end_ARG over^ start_ARG bold_italic_S end_ARG over^ start_ARG bold_italic_K end_ARG start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT: the probability of connecting query \ud835\udc78isubscript\ud835\udc78\ud835\udc56\\bm{Q}_{i}bold_italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to key \ud835\udc72jsubscript\ud835\udc72\ud835\udc57\\bm{K}_{j}bold_italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT equals p(i,j)=\ud835\udc78^i\ud835\udc7a^\ud835\udc72^jT\ud835\udc5d\ud835\udc56\ud835\udc57subscript^\ud835\udc78\ud835\udc56^\ud835\udc7asuperscriptsubscript^\ud835\udc72\ud835\udc57\ud835\udc47p(i,j)=\\smash{\\hat{\\bm{Q}}}_{i}\\smash{\\hat{\\bm{S}}}\\smash{\\hat{\\bm{K}}}_{j}^{T}italic_p ( italic_i , italic_j ) = over^ start_ARG bold_italic_Q end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG bold_italic_S end_ARG over^ start_ARG bold_italic_K end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT. Formally, the sampling procedure can be written as", "\ud835\udc7a^^\ud835\udc7a\\displaystyle\\smash{\\hat{\\bm{S}}}over^ start_ARG bold_italic_S end_ARG=\ud835\ude9c\ud835\ude98\ud835\ude8f\ud835\ude9d\ud835\ude96\ud835\ude8a\ud835\udea1(\ud835\udc6a\ud835\udc6aT)absent\ud835\ude9c\ud835\ude98\ud835\ude8f\ud835\ude9d\ud835\ude96\ud835\ude8a\ud835\udea1\ud835\udc6asuperscript\ud835\udc6a\ud835\udc47\\displaystyle=\\texttt{softmax}(\\bm{C}\\bm{C}^{T})= softmax ( bold_italic_C bold_italic_C start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT )(5)\ud835\udc78^^\ud835\udc78\\displaystyle\\smash{\\hat{\\bm{Q}}}over^ start_ARG bold_italic_Q end_ARG=\ud835\ude9c\ud835\ude92\ud835\ude90\ud835\ude96\ud835\ude98\ud835\ude92\ud835\ude8d(MLPdh\u2192dh(\ud835\udc78)\ud835\udc6aT)absent\ud835\ude9c\ud835\ude92\ud835\ude90\ud835\ude96\ud835\ude98\ud835\ude92\ud835\ude8dsubscriptMLP\u2192subscript\ud835\udc51\u210esubscript\ud835\udc51\u210e\ud835\udc78superscript\ud835\udc6a\ud835\udc47\\displaystyle=\\texttt{sigmoid}(\\text{MLP}_{d_{h}\\to d_{h}}(\\bm{Q})\\bm{C}^{T})= sigmoid ( MLP start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT \u2192 italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_Q ) bold_italic_C start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT )(6)\ud835\udc72^^\ud835\udc72\\displaystyle\\smash{\\hat{\\bm{K}}}over^ start_ARG bold_italic_K end_ARG=\ud835\ude9c\ud835\ude92\ud835\ude90\ud835\ude96\ud835\ude98\ud835\ude92\ud835\ude8d(MLPdh\u2192dh(\ud835\udc72)\ud835\udc6aT)absent\ud835\ude9c\ud835\ude92\ud835\ude90\ud835\ude96\ud835\ude98\ud835\ude92\ud835\ude8dsubscriptMLP\u2192subscript\ud835\udc51\u210esubscript\ud835\udc51\u210e\ud835\udc72superscript\ud835\udc6a\ud835\udc47\\displaystyle=\\texttt{sigmoid}(\\text{MLP}_{d_{h}\\to d_{h}}(\\bm{K})\\bm{C}^{T})= sigmoid ( MLP start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT \u2192 italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_K ) bold_italic_C start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT )(7)\ud835\udc74\ud835\udc74\\displaystyle\\bm{M}bold_italic_M\u223cSBM(\ud835\udc78^,\ud835\udc7a^,\ud835\udc72^)similar-toabsent\ud835\udc46\ud835\udc35\ud835\udc40^\ud835\udc78^\ud835\udc7a^\ud835\udc72\\displaystyle\\sim SBM(\\smash{\\hat{\\bm{Q}}},\\smash{\\hat{\\bm{S}}},\\smash{\\hat{\\bm{K}}})\u223c italic_S italic_B italic_M ( over^ start_ARG bold_italic_Q end_ARG , over^ start_ARG bold_italic_S end_ARG , over^ start_ARG bold_italic_K end_ARG )(8)", "For the last sampling step, we incorporate a fast random graph sampling algorithm fastRG (Alg.\u00a01, [35]) that can sample graphs from a SBM in time and memory asymptotically linear in the number of edges.One advantage of fastRG is that each edge can be sampled in parallel, allowing high efficiency with the help of multiprocessing. A more significant feature of the method is that the number of edges, which determines the overall cost, is sampled from a Poisson distribution with input-dependent mean (Line 4). Thus, the model can dynamically adjust its computational cost between linear and quadratic in sequence length based on the data.", "Figure\u00a03 shows example placements of nodes and clusters on the dhsubscript\ud835\udc51\u210ed_{h}italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT-dimensional space to show how the sparse structure is determined. If all nodes and clusters are gathered closely, then all entries in \ud835\udc78^^\ud835\udc78\\smash{\\hat{\\bm{Q}}}over^ start_ARG bold_italic_Q end_ARG and \ud835\udc72^^\ud835\udc72\\smash{\\hat{\\bm{K}}}over^ start_ARG bold_italic_K end_ARG become close to 1, resulting in p(i,j)\u22481\ud835\udc5d\ud835\udc56\ud835\udc571p(i,j)\\approx 1italic_p ( italic_i , italic_j ) \u2248 1 for all i,j\ud835\udc56\ud835\udc57i,jitalic_i , italic_j and hence a dense \ud835\udc74\ud835\udc74\\bm{M}bold_italic_M. If clusters are well-separated but each surrounded by some set of nodes, \ud835\udc7a^^\ud835\udc7a\\smash{\\hat{\\bm{S}}}over^ start_ARG bold_italic_S end_ARG becomes close to diagonal while each row in \ud835\udc78^^\ud835\udc78\\smash{\\hat{\\bm{Q}}}over^ start_ARG bold_italic_Q end_ARG and \ud835\udc72^^\ud835\udc72\\smash{\\hat{\\bm{K}}}over^ start_ARG bold_italic_K end_ARG is close to a one-hot vector indicating the cluster nearby. Such setting leads to a block diagonal mask similar to LSH bucketing of Reformer\u00a0[22]. Lastly, if all clusters are far apart from the nodes, both \ud835\udc78^^\ud835\udc78\\smash{\\hat{\\bm{Q}}}over^ start_ARG bold_italic_Q end_ARG and \ud835\udc72^^\ud835\udc72\\smash{\\hat{\\bm{K}}}over^ start_ARG bold_italic_K end_ARG approximately equal zero, zeroing out all the edge probabilities.", "The graph sampling procedure is naturally a discrete operation. Thus, naive backpropagation cannot learn the proper parameterization for the SBM that minimizes the predictive loss. To cope with this non-differentiability, we incorporate a Straight-Through Estimator (STE)\u00a0[4] to pass the gradient beyond the discrete sampling step. The STE enables providing the gradient \u2202\u2112/\u2202\ud835\udc74ij\u2112subscript\ud835\udc74\ud835\udc56\ud835\udc57\\partial\\mathcal{L}/\\partial\\bm{M}_{ij}\u2202 caligraphic_L / \u2202 bold_italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT to the probability for each sampled edge (i,j)\ud835\udc56\ud835\udc57(i,j)( italic_i , italic_j ) (Eqn.\u00a09). It works as if we had used a continuous mask \ud835\udc74\u2299\ud835\udd3c[\ud835\udc74]direct-product\ud835\udc74\ud835\udd3cdelimited-[]\ud835\udc74\\bm{M}\\odot\\mathbb{E}[\\bm{M}]bold_italic_M \u2299 blackboard_E [ bold_italic_M ] that stores the probability of each sampled edge instead of the binary mask \ud835\udc74\ud835\udc74\\bm{M}bold_italic_M during forward propagation. This way, the probabilities of sampled edges can be learned end-to-end: the gradients provide information on whether each sampled edge was useful or not for prediction.", "\u2202\u2112\u2202pij\u2254\u2202\u2112\u2202\ud835\udc74ij={\u2202\u2112\u2202\ud835\udc68ij\u22c5\ud835\udc78i\ud835\udc72jTdhif\u00a0\ud835\udc74ij=10otherwise\u00a0where\u00a0\ud835\udc68\u2254\ud835\udc74\u2299\ud835\udc78\ud835\udc72Tdh\u2254\u2112subscript\ud835\udc5d\ud835\udc56\ud835\udc57\u2112subscript\ud835\udc74\ud835\udc56\ud835\udc57cases\u22c5\u2112subscript\ud835\udc68\ud835\udc56\ud835\udc57subscript\ud835\udc78\ud835\udc56superscriptsubscript\ud835\udc72\ud835\udc57\ud835\udc47subscript\ud835\udc51\u210eif\u00a0subscript\ud835\udc74\ud835\udc56\ud835\udc5710otherwise\u00a0where\u00a0\ud835\udc68\u2254direct-product\ud835\udc74\ud835\udc78superscript\ud835\udc72\ud835\udc47subscript\ud835\udc51\u210e\\displaystyle\\dfrac{\\partial\\mathcal{L}}{\\partial p_{ij}}\\coloneqq\\dfrac{\\partial\\mathcal{L}}{\\partial\\bm{M}_{ij}}=\\begin{cases}\\dfrac{\\partial\\mathcal{L}}{\\partial\\bm{A}_{ij}}\\cdot\\dfrac{\\bm{Q}_{i}\\bm{K}_{j}^{T}}{\\sqrt{d_{h}}}&\\text{if }\\bm{M}_{ij}=1\\\\\\hfil 0&\\text{otherwise}\\end{cases}\\text{ where }\\bm{A}\\coloneqq\\bm{M}\\odot\\dfrac{\\bm{Q}\\bm{K}^{T}}{\\sqrt{d_{h}}}divide start_ARG \u2202 caligraphic_L end_ARG start_ARG \u2202 italic_p start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT end_ARG \u2254 divide start_ARG \u2202 caligraphic_L end_ARG start_ARG \u2202 bold_italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT end_ARG = { start_ROW start_CELL divide start_ARG \u2202 caligraphic_L end_ARG start_ARG \u2202 bold_italic_A start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT end_ARG \u22c5 divide start_ARG bold_italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_ARG end_ARG end_CELL start_CELL if bold_italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL otherwise end_CELL end_ROW where bold_italic_A \u2254 bold_italic_M \u2299 divide start_ARG bold_italic_Q bold_italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_ARG end_ARG(9)", "While this approach enables backpropagation in the same \ud835\udcaa(m)\ud835\udcaa\ud835\udc5a\\mathcal{O}(m)caligraphic_O ( italic_m ) cost as in the forward step, this comes at the expense of not being able to propagate information through edges that were not sampled. This can be problematic when an edge probability accidentally collapses to zero, after which the edge becomes unlikely to ever be sampled even when it may be useful for the prediction task at hand. Therefore, we add a small perturbation \u03b4>0\ud835\udeff0\\delta>0italic_\u03b4 > 0 to each edge probability pijsubscript\ud835\udc5d\ud835\udc56\ud835\udc57p_{ij}italic_p start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT, allowing the model to explore new edges and resuscitate their sampling probabilities if necessary. We find that a \u03b4\ud835\udeff\\deltaitalic_\u03b4 as small as 0.010.010.010.01 significantly helps in practice, and thus use this edge exploration scheme during training for our experiments.", "Note that the gradient \u2202\u2112/\u2202pij\u2112subscript\ud835\udc5d\ud835\udc56\ud835\udc57\\partial\\mathcal{L}/\\partial p_{ij}\u2202 caligraphic_L / \u2202 italic_p start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT can be positive, which suppresses the probability of edge (i,j)\ud835\udc56\ud835\udc57(i,j)( italic_i , italic_j ). At first, it may seem counter-intuitive why the model would ever limit itself to using fewer edges during training without any sparsity-based regularizations. One explanation is that masked attention provides an easy way to reduce attention scores under finite head dimensions.Under full attention, it is known that the representational space of attention score matrices is limited by the head dimension and softmax activation\u00a0[5].This limitation inevitably introduces unwanted noise in the attention scores especially when working with long sequences.In SBM-Transformer, however, the structural sparsity in masked attention introduces another dimension that induces a larger space of row-stochastic matrices (full attention is a special case of masked attention where \ud835\udc74ij=1subscript\ud835\udc74\ud835\udc56\ud835\udc571\\bm{M}_{ij}=1bold_italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1 for all i,j\ud835\udc56\ud835\udc57i,jitalic_i , italic_j). Therefore, it is reasonable that the model may encourage sparsity to leverage the additional expressiveness assuming the loss landscape has local optima within the sparse attention regime. Our experiments on the LRA benchmark show that this is indeed the case, as our SBM-Transformer converges to an average attention sparsity of 20% to 30% while outperforming Transformer with full attention.We also show in the experiment that we can easily incorporate additional regularization that further encourages sparse attention masks.", "Leveraging previous work on the theoretical expressiveness of sparse attention\u00a0[50, 51], we show that SBM-Transformer with a small modification111Here we consider a variant of SBM-Transformer where self-loops are added manually (i.e. \ud835\udc74ii=1subscript\ud835\udc74\ud835\udc56\ud835\udc561\\bm{M}_{ii}=1bold_italic_M start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT = 1 for all i\ud835\udc56iitalic_i). While this is useful in theoretical analysis, we find that not having self-loops slightly helps in empirical performance and hence omit self-loops for the main experiments. retains the same level of expressibility as full attention. Specifically, we show that the low-rank structure of the underlying SBMs does not degrade the expressive power of Transformer, and that SBM-Transformer can universally approximate arbitrary functions with \ud835\udcaa(n)\ud835\udcaa\ud835\udc5b\\mathcal{O}(n)caligraphic_O ( italic_n ) connections. For brevity, we provide a rough overview of the proof and defer further details to Appendix A.", "According to the main theorem of Yun\u00a0et\u00a0al.\u00a0(2020)\u00a0[49], SBM-Transformer achieves universal approximability if 1) each node attends to itself, 2) the aggregation of all attention patterns contains a Hamiltonian path, and 3) there exists a path between all node pairs. While the first condition is trivially true due to our modification, the other two conditions require careful choice of three SBMs. Here we first parameterize one SBM to hard-assign tokens into k\ud835\udc58kitalic_k equally-sized clusters, inducing a block-diagonal attention pattern. The other two SBMs are parameterized such that the two graphs together form a star graph with k\ud835\udc58kitalic_k global relay tokens. Combining the three attention patterns lead to a parameterization of SBM-Transformer that satisfies all three conditions, hence proving the theorem.", "For empirical evaluations, we first use a synthetic task to show that our model is flexible enough to learn towards full attention when needed in contrast to previous works. We then experiment on Long Range Arena (LRA)\u00a0[38], a benchmark widely used to assess the capacity of efficient Transformers in learning long-range contexts across different modalities. Lastly, we show results on the GLUE benchmark\u00a0[43] to assess the performance of SBM-Transformer in a downstream NLP setting. All experiments were run on a remote GCP server equipped with 16 NVIDIA A100 Tensor Core GPUs.", "We formulate a token-level binary classification task as follows: each input sequence consists of N\ud835\udc41Nitalic_N integers, each of which is uniformly sampled from {1,2,\u2026,N}12\u2026\ud835\udc41\\{1,2,\\dots,N\\}{ 1 , 2 , \u2026 , italic_N }. We use N=256\ud835\udc41256N=256italic_N = 256 in our setup. The prediction target is a sequence of equal length, where each token is labeled 1 if there exists a duplicate somewhere within the sequence, and 0 otherwise. Below is a simple example with N=8\ud835\udc418N=8italic_N = 8 that illustrates the task. We measure the performance of models via binary cross-entropy loss.Input: 1 4 3 7 3 2 3 1 \u21d2\u21d2\\Rightarrow\u21d2 Target: 1 0 1 0 1 0 1 1", "For this task, we compare SBM-Transformer with k=128\ud835\udc58128k=128italic_k = 128 clusters against various efficient Transformers: Linear Transformer [21], Linformer [45], Reformer [22], Performer [10], and Nystr\u00f6mformer [48]. Across all methods, we use a single-layer and single-head architecture with 32 hidden dimensions. Note that due to this constrained setting, the sole head must perform full attention to compare each token to all the others in order to attain 100% accuracy. All models are trained for 2000 epochs where a new batch of sequences is sampled on-the-fly at each epoch. We use a batch size of 256 and learning rate of 1e-3.", "Figure\u00a04 shows the training loss curves of each baseline method as well as SBM-Transformer. Full attention quickly converges to 100% accuracy, which is expected as it computes all possible pairwise interactions by default. Other models that apply low-rank or kernelized attention fail to achieve the same level of accuracy, due to limited expressibility under the constrained setting. Though SBM-Transformer converges more slowly compared to full-attention, it demonstrates the ability to drive itself towards full-attention, eventually attaining zero loss.", "To demonstrate that the flexible inductive bias of SBM-Transformer is effective for modeling long-range dependencies, we test SBM-Transformer against previous work on the LRA benchmark. We also test how the performance is affected with respect to applying a sparsity-based regularizer.", "LRA\u00a0[38] consists of five different testbeds with varying modalities: ListOps\u00a0[28] is a 10-way classification task to map a sequence of single-digit numbers and 4 different set operations, to its corresponding solution. Text\u00a0[26] is a binary classification task where byte-level IMDB movie reviews must be classified into one of positive or negative sentiments. Retrieval\u00a0[32] is also a char-level binary classification task, where two sequences from ACL Anthology papers are given as input, and the model must predict whether there exists a citation link between them. Image\u00a0[23] is a 10-way classification task mapping flattened pixel-sequences from CIFAR-10 to its class. Pathfinder\u00a0[25] provides flattened pixel-sequences from an image and the model must decide whether two circles in the image are connected by a dashed line. For this benchmark, we use the PyTorch implementation of LRA provided by the authors of Nystr\u00f6mformer\u00a0[48] and adhere to the same train-test splits. Performance in all five tasks is measured using classification accuracy.", "We compare SBM-Transformer against the same baselines as with the synthetic task above. For fair comparison, we set all Transformer models to use the default setting used in\u00a0[48], which fixes 2 layers, 2 attention heads, and 64 embedding dimensions. For SBM-Transformer, we use k=128\ud835\udc58128k=128italic_k = 128 clusters. The output token representations are mean-pooled to obtain the sequence representation for all tasks. More details on the architecture setups can be found in Appendix C.", "Table\u00a08 shows the test accuracies of each method. Our SBM-Transformer achieves the best overall performance, ranking first in two tasks, and second in one other. SBM-Transformer also outperforms full attention in all five tasks while computing 30% or less attention scores on average, which supports our claim that masked attention with partial attention score computations can be preferred over full attention depending on the task. With respect to the attention mask structure, we find that flexibility of SBM is indeed beneficial, as Reformer struggles in ListOps, most likely due to the inability of block-diagonal masks to model hierarchical contexts.", "To test if the model can effectively learn under a constraint on the computational cost, we also test the model under a sparsity-based regularizer that discourages excessive use of query-key edges. We penalize each sampled edge by adding to the predictive loss a weighted regularization term \u03bb\u2112s\ud835\udf06subscript\u2112\ud835\udc60\\lambda\\mathcal{L}_{s}italic_\u03bb caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, where \u2112ssubscript\u2112\ud835\udc60\\mathcal{L}_{s}caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT denotes the average mask density across all attention heads. Table\u00a09 shows the performance of SBM-Transformer across varying regularization weights. Under strong regularization, the model surprisingly retains competitive performance while significantly reducing the average mask density.This indicates that similar local optima are shared across regimes with varying attention density in the loss landscape, and the regularization term is able to drive the model towards finding optimal attention scores with smaller density.", "Furthermore, we compare computational costs during inference by measuring FLOP count and peak memory usage. For SBM-Transformer, we test the model trained under \u03bb=10\u22121\ud835\udf06superscript101\\lambda=10^{-1}italic_\u03bb = 10 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT. Due to lack of support for sparse tensor operations in existing FLOP-counters, we measure FLOP counts by manually enumerating through each tensor operation. Table\u00a03 shows that SBM-Transformer is comparably efficient across all tasks except for Text, where SBM-Transformer showed the largest average mask density. Note that while the cost of other baselines are fixed after initialization, the cost of SBM-Transformer is data-adaptive and can vary input-by-input. Further analysis and qualitative examples demonstrating the input-dependent attention mask densities can be found in Appendix C.", "We also compare the densities of masks sampled at each layer of SBM-Transformer during test time to examine whether our model is capable of diversifying sparsity across layers for better performance. Recall that this allows models to gather information in different levels, as seen in pretrained BERT where lower layers focus on the overall content via dense attention while upper layers gather syntactic information with tree-like patterns\u00a0[11]. For each of the five tasks, we pick two highest-performing models (one for unregularized and another for regularized) for measurement. Figure\u00a05 shows the average layer-wise mask densities of unregularized and regularized SBM-Transformers across different tasks. We find that under no regularization, the two layers can differ by more than 10% in tasks such as ListOps and Image. This may be due to the hierarchical and compositional structure of the two tasks. We also find that the variation is relatively low in Text with densities around 25%, indicating that the task requires broad attention overall. Lastly, the standard deviation is extremely large in upper layers for Pathfinder, showing that it samples a wide variety of masks depending on the input.", "To check whether its strong performance demonstrated in LRA extends to the downstream NLP setting as well, we evaluate SBM-Transformer against baselines on the GLUE benchmark\u00a0[43].", "We consider four NLP tasks in GLUE\u00a0[43]. SST-2\u00a0[37] consists of movie reviews the model must predict their positive or negative sentiments. For QQP\u00a0[8], the task is to determine whether one question is a paraphrase of the other given a pair of questions. MNLI\u00a0[47] consists of sentence pairs, each with a target label indicating whether the two sentences are connected through entailment, contradiction, or neither. QNLI\u00a0[33] consists of sentence-question pairs and the task is to determine whether the sentence contains an answer to the question. Each task is formulated as sequence classification, and we measure performance by F1 score on the respective validation sets.", "Following previous work\u00a0[48], we arrange a small variant of BERT\u00a0[14] with 4 layers, 8 attention heads, and 512 embedding dimensions. We replace full attention with each attention module used in previous experiments. For SBM-Transformer, we use k=128\ud835\udc58128k=128italic_k = 128 clusters without sparsity regularization (i.e. \u03bb=0\ud835\udf060\\lambda=0italic_\u03bb = 0). Here, we find that adding local attention significantly boosts performance, and thus fix a sliding window of size 64 to SBM-Transformer. We first pretrain each model under the masked language modeling objective for 50 epochs on a corpus with text from English Wikipedia, BookCorpus\u00a0[55], and RealNews\u00a0[52]. We then finetune each pretrained model for 5 epochs on the GLUE training sets. More details on the architecture and training setup can be found in Appendix C.", "Table\u00a04 reports the F1 scores of each method on different NLP tasks. SBM-Transformer performs competitively against full attention overall, and outperforms all baselines in SST-2 and QQP. We also find that the fine-tuned SBM-Transformer models use 13.5% dense attention masks on average across all tasks, showing that the model can encode useful information from input sentences effectively under highly sparse attention.", "We propose SBM-Transformer, an efficient Transformer that can data-adaptively choose its attention sparsity between sparse and full attention without the need to explicitly compute the full attention score matrix. Theoretically, we show that our model enjoys the same expressibility as the original Transformer due to the flexibility of the latent SBM. Empirical experiments on LRA and GLUE show that our model performs competitively against previous state-of-the-art efficient Transformers.", "Nonetheless, there are limitations due to sparse tensor operations being less optimized on GPU kernels. In the LRA experiments, we found that SBM-Transformer can result in longer runtimes compared to dense counterparts while its memory usage is much lower. While previous sparsity-based attention mechanisms with block-sparse attention are much more amenable for GPU computation\u00a0[51, 9, 3], our work requires an architecture with better workload balancing and acceleration under unstructured sparsity, for which there is ongoing work\u00a0[46, 54].", "We still believe this work is valuable as it is the first approach to induce per-example attention sparsity, allowing the model to adjust its computational cost based on the input. The cost being dependent on the number of edges also allows practitioners to easily impose constraints based on the available computational resources.We hope to see more GPU-friendly tensor operations optimized for fine-grained sparsity in the future, at which point the value of this work will increase even further. As we propose a foundational replacement for the scaled dot-product attention module in the Transformer architecture, we do not expect any immediate negative societal impact due to this work."], "figure_types": {"50790468a774f3ecd663e79932e6da4e813048aa/10-Table4-1.png": "table", "50790468a774f3ecd663e79932e6da4e813048aa/15-Figure6-1.png": "schematic", "50790468a774f3ecd663e79932e6da4e813048aa/16-Table5-1.png": "table", "50790468a774f3ecd663e79932e6da4e813048aa/16-Table6-1.png": "table", "50790468a774f3ecd663e79932e6da4e813048aa/17-Table7-1.png": "table", "50790468a774f3ecd663e79932e6da4e813048aa/17-Table8-1.png": "table", "50790468a774f3ecd663e79932e6da4e813048aa/17-Table9-1.png": "table", "50790468a774f3ecd663e79932e6da4e813048aa/18-Figure7-1.png": "plot", "50790468a774f3ecd663e79932e6da4e813048aa/19-Figure8-1.png": "photograph(s)", "50790468a774f3ecd663e79932e6da4e813048aa/2-Figure1-1.png": "schematic", "50790468a774f3ecd663e79932e6da4e813048aa/4-Figure2-1.png": "schematic", "50790468a774f3ecd663e79932e6da4e813048aa/6-Figure3-1.png": "schematic", "50790468a774f3ecd663e79932e6da4e813048aa/7-Figure4-1.png": "plot", "50790468a774f3ecd663e79932e6da4e813048aa/8-Table1-1.png": "table", "50790468a774f3ecd663e79932e6da4e813048aa/8-Table2-1.png": "table", "50790468a774f3ecd663e79932e6da4e813048aa/9-Figure5-1.png": "plot", "50790468a774f3ecd663e79932e6da4e813048aa/9-Table3-1.png": "table"}}, "2209.02939": {"paper_id": "paper_152", "title": "Grouping-matrix based Graph Pooling with Adaptive Number of Clusters", "arxiv_url": "https://arxiv.org/abs/2209.02939", "s2orc_url": "https://www.semanticscholar.org/paper/c7bbe399aad2da39ba8f988fec83129d60aa5d52", "all_figures_tables": {"c7bbe399aad2da39ba8f988fec83129d60aa5d52/13-Table3-1.png": "Table 3: Hyperparameters on Models", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/15-Table4-1.png": "Table 4: Hyperparameters on Dataset", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/16-Figure4-1.png": "Figure 4: Histograms of effective number of clusters. The X-axis indicates the number of eigenvalues that exceed the threshold above, and Y-axis indicates the corresponding frequency.", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/2-Figure1-1.png": "Figure 1: An illustration of our framework. Our method first forms a grouping matrix that encodes clustering similarities between each pair of nodes and then acquires the pooling matrix that coarsens the graph by decomposing the grouping matrix.", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/5-Table1-1.png": "Table 1: Dataset Summary", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/7-Figure2-1.png": "Figure 2: Examples of Grouping Matrices. Both X and Y axes are node indices. (a) is the initial state of grouping matrix, (b) and (c) show a grouping matrix for example molecules from PLQY and \u03bbmax datasets, respectively.", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/8-Table2-1.png": "Table 2: Test Results from Various Datasets", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/9-Figure3-1.png": "Figure 3: RMSE for PLQY prediction (a) over varying number of pooling layers in GMPool and (b) across different methods. For figure (a), GMPool does not require to set number of clusters, however to explore the effect, the number of clusters is set by force in the first place."}, "referred_figures_tables": [["c7bbe399aad2da39ba8f988fec83129d60aa5d52/2-Figure1-1.png"], ["c7bbe399aad2da39ba8f988fec83129d60aa5d52/2-Figure1-1.png"]], "question_id": [0, 5], "question": ["How does NGMPool work exactly? How is it different from GMPool?", "What makes GMPool and NGMPool novel compared to existing graph pooling methods?"], "question_section": ["1 Introduction", "2 Related Work"], "question_trigger_sentence": ["1 Introduction", "2 Related Work"], "question_type": ["Testing question", "Testing question"], "evidential_info": [[{"context": "To overcome this challenge, we propose GMPool, a general pooling framework that does not require an universal number of clusters as a user hyperparameter. Figure 1 depicts the overall framework of GMPool. The core intuition is that the product of a pooling matrix with itself forms a grouping matrix, where each (i,j)-th entry indicates the pairwise clustering similarity: whether the nodes i and j are pooled to the same clusters. For each graph, GMPool parameterizes the clustering similarities in its grouping matrix via a classification layer. Finally, we perform SVD on the grouping matrix to obtain the pooling matrix such that the overall rank represents the suitable number of clusters. We also test a single-pooling variant NGMPool that does not perform any decomposition, but rather uses the grouping matrix as is. In real-world molecular property prediction tasks, we show that our approach outperforms previous baselines, while successfully learning suitable clusters.", "rationale": "GMPool performs SVD on the grouping matrix to obtain the pooling matrix, whereas NGMPool utilizes the grouping matrix as is."}, {"context": "The main contributions of this paper are as follows:\u2022We design a grouping matrix-based pooling operator that does not require users to specify the number of clusters a priori.\u2022We propose GMPool and NGMPool. GMPool performs SVD on the grouping matrix to obtain the pooling matrix, whereas NGMPool utilizes the grouping matrix as is.\u2022We demonstrate the power of our methods both quantitatively and qualitatively on a wide range of real molecular property prediction tasks.", "rationale": "The paper also tests a single-pooling variant NGMPool that does not perform any decomposition, but rather uses the grouping matrix as is."}], [{"context": "In this section, we propose a novel differentiable pooling layer, GMPool, which obtains the pooling matrix by first building a grouping matrix that contains clustering similarities of pairwise nodes and then decomposing the matrix into its square-root form. We start the section with preliminary information, then outline the details of GMPool in later sections.", "rationale": "This paper proposes a novel differentiable pooling layer, GMPool, which obtains the pooling matrix by first building a grouping matrix that contains clustering similarities of pairwise nodes and then decomposing the matrix into its square-root form."}, {"context": "To overcome this challenge, we propose GMPool, a general pooling framework that does not require an universal number of clusters as a user hyperparameter. Figure 1 depicts the overall framework of GMPool. The core intuition is that the product of a pooling matrix with itself forms a grouping matrix, where each (i,j)-th entry indicates the pairwise clustering similarity: whether the nodes i and j are pooled to the same clusters. For each graph, GMPool parameterizes the clustering similarities in its grouping matrix via a classification layer. Finally, we perform SVD on the grouping matrix to obtain the pooling matrix such that the overall rank represents the suitable number of clusters. We also test a single-pooling variant NGMPool that does not perform any decomposition, but rather uses the grouping matrix as is. In real-world molecular property prediction tasks, we show that our approach outperforms previous baselines, while successfully learning suitable clusters.", "rationale": "GMPool and NGMPool do not require users to specify the number of clusters a priori."}, {"context": "The main contributions of this paper are as follows:\u2022We design a grouping matrix-based pooling operator that does not require users to specify the number of clusters a priori.\u2022We propose GMPool and NGMPool. GMPool performs SVD on the grouping matrix to obtain the pooling matrix, whereas NGMPool utilizes the grouping matrix as is.\u2022We demonstrate the power of our methods both quantitatively and qualitatively on a wide range of real molecular property prediction tasks.", "rationale": "GMPool is a general pooling framework that does not require an universal number of clusters as a user hyperparameter."}, {"context": "However, the pooling methods above all share a common limitation: the number of clusters must be predefined for each layer as hyperparameters. This limitation is especially detrimental in inductive settings such as molecular property prediction, where each graph can have varying numbers of useful sub-structures. https://doi.org/10.1111/cbdd.12952 ; doi:10.1021/acs.jmedchem.0c00754 ; GUVENCH20161928  Allowing the model to pool towards varying number of clusters based on data is expected to enhance performance, and our proposed GMPool allows such variation through the rank of the grouping matrix. To the best of our knowledge, GMPool is the first to achieve high performance without the need to manually adjust the number of clusters through additional hyperparameter tuning.", "rationale": "Existing pooling methods share a common limitation: the number of clusters must be predefined for each layer as hyperparameters."}]], "composition": ["NGMPool is a single-pooling variant of GMPool that does not perform SVD on the grouping matrix, but rather uses the grouping matrix as is.", "GMPool and NGMPool overcome the limitation of existing pooling frameworks that require a universal number of clusters as user parameter by first building a grouping matrix and decomposing the matrix into its square-root form."], "Is_figure_in_evidence": [true, true], "Is_table_in_evidence": [false, false], "question_key": ["465", "468"], "passages": ["Graph Neural Networks (GNNs) learn representations of individual nodes based on the connectivity structure of an input graph.For graph-level prediction tasks, the standard procedure globally pools all the node features into a single graph representation without weight difference, then feeds the representation to a final prediction layer. This process implies that information only propagates through node-to-node edges, rendering the model unable to hierarchically aggregate information efficiently beyond local convolution.", "However, a hierarchical structure can encode the global topology of graphs that is useful for effective learning of long range interactions. Therefore, designing a pooling architecture which respects the graph structure is crucial for downstream tasks such as social network analyses https://doi.org/10.48550/arxiv.1609.02907 ; https://doi.org/10.48550/arxiv.1706.02216  and molecule property predictions https://doi.org/10.48550/arxiv.1509.09292 ; https://doi.org/10.48550/arxiv.1606.09375 ; https://doi.org/10.48550/arxiv.1312.6203 ; doi:10.1021/acs.jcim.6b00601 ; 4700287 ; https://doi.org/10.48550/arxiv.1812.01070 .", "As an alternative to global pooling, DiffPool first proposed an end-to-end differentiable pooling by soft-classifying each node into a smaller number of clusters ying2018 . Later gPool gao2019  and SAGpool lee2019  incorporated the attention mechanism into pooling, while MinCutPool proposed grouping the nodes into clusters by minimizing the relaxed K\ud835\udc3eKitalic_K-way normalized minimum cut objective bianchi2019 .", "In most inductive settings, there is no single number of clusters that is suitable across all graphs in the dataset.Particularly in molecular graphs, the number of functional groups often determines useful characteristics and chemical behaviors, while varying significantly across different molecules.Nonetheless, existing pooling methods require the number of clusters as a hyperparameter, then operates under the assumption that all graphs share the same number of clusters\u00a0ranjan2020asap . This is often undesirable as it not only requires additional hyperparameter tuning, but also imposes a strong inductive bias that deteriorates downstream performance.", "To overcome this challenge, we propose GMPool, a general pooling framework that does not require an universal number of clusters as a user hyperparameter. Figure 1 depicts the overall framework of GMPool. The core intuition is that the product of a pooling matrix with itself forms a grouping matrix, where each (i,j)\ud835\udc56\ud835\udc57(i,j)( italic_i , italic_j )-th entry indicates the pairwise clustering similarity: whether the nodes i\ud835\udc56iitalic_i and j\ud835\udc57jitalic_j are pooled to the same clusters. For each graph, GMPool parameterizes the clustering similarities in its grouping matrix via a classification layer. Finally, we perform SVD on the grouping matrix to obtain the pooling matrix such that the overall rank represents the suitable number of clusters. We also test a single-pooling variant NGMPool that does not perform any decomposition, but rather uses the grouping matrix as is. In real-world molecular property prediction tasks, we show that our approach outperforms previous baselines, while successfully learning suitable clusters.", "The main contributions of this paper are as follows:\u2022We design a grouping matrix-based pooling operator that does not require users to specify the number of clusters a priori.\u2022We propose GMPool and NGMPool. GMPool performs SVD on the grouping matrix to obtain the pooling matrix, whereas NGMPool utilizes the grouping matrix as is.\u2022We demonstrate the power of our methods both quantitatively and qualitatively on a wide range of real molecular property prediction tasks.", "GNN architectures have shown great performance in various fields such as social network data, authorship/citation networks, and molecular data that can naturally be interpreted as graphs. For graph convolution, several work have utilized the graph Laplacian in the spectral domain. However, sheer convolution in the spectral domain suffers from the non-locality problem, and various approaches have been introduced to overcome this limitation. https://doi.org/10.48550/arxiv.1706.02216 ; https://doi.org/10.48550/arxiv.1810.00826 ; https://doi.org/10.48550/arxiv.1710.10903 ; https://doi.org/10.48550/arxiv.1704.01212  One stream of work has embedded the attention architecture into GNN, inferring the interaction between nodes without using a diffusion-like picture. https://doi.org/10.48550/arxiv.1710.10903  Another line of work considered message passing networks, which ensures the signal to be localized and non-linearly weighted. https://doi.org/10.48550/arxiv.1704.01212  This architecture has been proven to be highly effective in molecular property prediction fields. doi:10.1021/acs.jcim.9b00237 ", "Graph pooling aims to utilize the hierarchical nature of graphs. Early work mainly focused on fixed axiomatic pooling methods such as minimum cut, k-means, and spectral clustering without any gradient-based optimization. https://doi.org/10.48550/arxiv.1312.6203 ; NIPS2011_6c1da886 ; 10.1016/j.patcog.2006.04.007 ; https://doi.org/10.48550/arxiv.0711.0189 ; 4302760 Although these pooling methods are effective on graphs without noise, the same heuristic often fails to work well on real datasets and tasks, especially due to a lack of differentiability that prohibits training under supervised signals.Since node representations and pooling strategies mutually affect each other during the training process, simultaneous optimization of whole components is crucial for avoiding local minima. Among many solutions, Diffpoolying2018  is the first to propose an end-to-end learnable pooling mechanism that learns an assignment matrix in which each entry represents the probability of a node being assigned to a cluster.", "gPool gao2019  and SAGPool lee2019  are ranking-based pooling methods that coarsen the input graph by ranking and downsampling a small subset of nodes. MinCutPool bianchi2019  leverages a continuous relaxation of the minimum-cut objective, enabling spectral clustering under full differentiability.", "However, the pooling methods above all share a common limitation: the number of clusters must be predefined for each layer as hyperparameters. This limitation is especially detrimental in inductive settings such as molecular property prediction, where each graph can have varying numbers of useful sub-structures. https://doi.org/10.1111/cbdd.12952 ; doi:10.1021/acs.jmedchem.0c00754 ; GUVENCH20161928  Allowing the model to pool towards varying number of clusters based on data is expected to enhance performance, and our proposed GMPool allows such variation through the rank of the grouping matrix. To the best of our knowledge, GMPool is the first to achieve high performance without the need to manually adjust the number of clusters through additional hyperparameter tuning.", "In this section, we propose a novel differentiable pooling layer, GMPool, which obtains the pooling matrix by first building a grouping matrix that contains clustering similarities of pairwise nodes and then decomposing the matrix into its square-root form. We start the section with preliminary information, then outline the details of GMPool in later sections.", "We assume an inductive graph-level prediction setting where our aim is to learn a function f\u03b8:\ud835\udca2\u2192\ud835\udcb4:subscript\ud835\udc53\ud835\udf03\u2192\ud835\udca2\ud835\udcb4f_{\\theta}:\\mathcal{G}\\to\\mathcal{Y}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT : caligraphic_G \u2192 caligraphic_Y that maps a graph G\u2208\ud835\udca2\ud835\udc3a\ud835\udca2G\\in\\mathcal{G}italic_G \u2208 caligraphic_G to a property label y\u2208\ud835\udcb4\ud835\udc66\ud835\udcb4y\\in\\mathcal{Y}italic_y \u2208 caligraphic_Y.Each graph G\ud835\udc3aGitalic_G with n\ud835\udc5bnitalic_n nodes is represented as a triplet G=(A,X,E)\ud835\udc3a\ud835\udc34\ud835\udc4b\ud835\udc38G=(A,X,E)italic_G = ( italic_A , italic_X , italic_E ) with graph adjacency A\u2208{0,1}n\u00d7n\ud835\udc34superscript01\ud835\udc5b\ud835\udc5bA\\in\\{0,1\\}^{n\\times n}italic_A \u2208 { 0 , 1 } start_POSTSUPERSCRIPT italic_n \u00d7 italic_n end_POSTSUPERSCRIPT, node features X\u2208\u211dn\u00d7dn\ud835\udc4bsuperscript\u211d\ud835\udc5bsubscript\ud835\udc51\ud835\udc5bX\\in\\mathbb{R}^{n\\times d_{n}}italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, and edge features E\u2208\u211dn\u00d7n\u00d7de\ud835\udc38superscript\u211d\ud835\udc5b\ud835\udc5bsubscript\ud835\udc51\ud835\udc52E\\in\\mathbb{R}^{n\\times n\\times d_{e}}italic_E \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_n \u00d7 italic_d start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_POSTSUPERSCRIPT. We use Xisubscript\ud835\udc4b\ud835\udc56X_{i}italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and Eijsubscript\ud835\udc38\ud835\udc56\ud835\udc57E_{ij}italic_E start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT to denote the features of node i\ud835\udc56iitalic_i and edge (i,j)\ud835\udc56\ud835\udc57(i,j)( italic_i , italic_j ), respectively.", "As our backbone GNN, we adopt the Directed Message Passing Neural Network (DMPNN) doi:10.1021/acs.jcim.9b00237  which aggregates messages through directed edges. Note that while we chose DMPNN due to its superior performance over GNN architectures, our pooling layer is module-agnostic and can be combined with any GNN as long as node representations are returned as output.Given a graph, DMPNN first initializes the hidden state of each edge (i,j)\ud835\udc56\ud835\udc57(i,j)( italic_i , italic_j ) based on its feature Eijsubscript\ud835\udc38\ud835\udc56\ud835\udc57E_{ij}italic_E start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT and the source-node\u2019s feature Xisubscript\ud835\udc4b\ud835\udc56X_{i}italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. At each timestep t\ud835\udc61titalic_t, each directional edge gathers hidden states from incident edges into a message mijt+1superscriptsubscript\ud835\udc5a\ud835\udc56\ud835\udc57\ud835\udc611m_{ij}^{t+1}italic_m start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT and updates its own hidden state to hijt+1superscriptsubscript\u210e\ud835\udc56\ud835\udc57\ud835\udc611h_{ij}^{t+1}italic_h start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT as followsmijt+1=\u2211k\u2208\ud835\udca9(i)\u2216jhkitsuperscriptsubscript\ud835\udc5a\ud835\udc56\ud835\udc57\ud835\udc611subscript\ud835\udc58\ud835\udca9\ud835\udc56\ud835\udc57superscriptsubscript\u210e\ud835\udc58\ud835\udc56\ud835\udc61\\displaystyle m_{ij}^{t+1}=\\sum_{k\\in\\mathcal{N}(i)\\setminus j}h_{ki}^{t}italic_m start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT = \u2211 start_POSTSUBSCRIPT italic_k \u2208 caligraphic_N ( italic_i ) \u2216 italic_j end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT(1)hijt+1=\ud835\ude81\ud835\ude8e\ud835\ude7b\ud835\ude84(hij0+Wemijt+1)superscriptsubscript\u210e\ud835\udc56\ud835\udc57\ud835\udc611\ud835\ude81\ud835\ude8e\ud835\ude7b\ud835\ude84superscriptsubscript\u210e\ud835\udc56\ud835\udc570subscript\ud835\udc4a\ud835\udc52superscriptsubscript\ud835\udc5a\ud835\udc56\ud835\udc57\ud835\udc611\\displaystyle h_{ij}^{t+1}=\\texttt{ReLU}(h_{ij}^{0}+W_{e}m_{ij}^{t+1})italic_h start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT = ReLU ( italic_h start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT + italic_W start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT )(2)Here, \ud835\udca9(i)\ud835\udca9\ud835\udc56\\mathcal{N}(i)caligraphic_N ( italic_i ) denotes the set of neighboring nodes of node i\ud835\udc56iitalic_i and Wesubscript\ud835\udc4a\ud835\udc52W_{e}italic_W start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT a learnable weight. The hidden states of nodes are updated by aggregating the hidden states of incident edges into message mit+1superscriptsubscript\ud835\udc5a\ud835\udc56\ud835\udc611m_{i}^{t+1}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT, and passing its concatenation with the node feature Xisubscript\ud835\udc4b\ud835\udc56X_{i}italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into a linear layer followed by ReLU non-linearitymit+1=\u2211j\u2208\ud835\udca9(i)hijtsuperscriptsubscript\ud835\udc5a\ud835\udc56\ud835\udc611subscript\ud835\udc57\ud835\udca9\ud835\udc56superscriptsubscript\u210e\ud835\udc56\ud835\udc57\ud835\udc61\\displaystyle m_{i}^{t+1}=\\sum_{j\\in\\mathcal{N}(i)}h_{ij}^{t}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT = \u2211 start_POSTSUBSCRIPT italic_j \u2208 caligraphic_N ( italic_i ) end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT(3)hit+1=\ud835\ude81\ud835\ude8e\ud835\ude7b\ud835\ude84(Wn\ud835\ude8c\ud835\ude98\ud835\ude97\ud835\ude8c\ud835\ude8a\ud835\ude9d(Xi,mit+1))superscriptsubscript\u210e\ud835\udc56\ud835\udc611\ud835\ude81\ud835\ude8e\ud835\ude7b\ud835\ude84subscript\ud835\udc4a\ud835\udc5b\ud835\ude8c\ud835\ude98\ud835\ude97\ud835\ude8c\ud835\ude8a\ud835\ude9dsubscript\ud835\udc4b\ud835\udc56superscriptsubscript\ud835\udc5a\ud835\udc56\ud835\udc611\\displaystyle h_{i}^{t+1}=\\texttt{ReLU}(W_{n}\\texttt{concat}(X_{i},m_{i}^{t+1}))italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT = ReLU ( italic_W start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT concat ( italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT ) )(4)Similarly, Wnsubscript\ud835\udc4a\ud835\udc5bW_{n}italic_W start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT denotes a learnable weight. Assuming DMPNN runs for T\ud835\udc47Titalic_T timesteps, we use (Xout,Eout)=\ud835\ude76\ud835\ude7d\ud835\ude7d(A,X,E)subscript\ud835\udc4b\ud835\udc5c\ud835\udc62\ud835\udc61subscript\ud835\udc38\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\ude76\ud835\ude7d\ud835\ude7d\ud835\udc34\ud835\udc4b\ud835\udc38(X_{out},E_{out})=\\texttt{GNN}(A,X,E)( italic_X start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT ) = GNN ( italic_A , italic_X , italic_E ) to denote the output representation matrices containing hidden states of all nodes and edges, respectively (i.e., Xout,i=hiTsubscript\ud835\udc4b\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc56superscriptsubscript\u210e\ud835\udc56\ud835\udc47X_{out,i}=h_{i}^{T}italic_X start_POSTSUBSCRIPT italic_o italic_u italic_t , italic_i end_POSTSUBSCRIPT = italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and Eout,ij=hijTsubscript\ud835\udc38\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc56\ud835\udc57superscriptsubscript\u210e\ud835\udc56\ud835\udc57\ud835\udc47E_{out,ij}=h_{ij}^{T}italic_E start_POSTSUBSCRIPT italic_o italic_u italic_t , italic_i italic_j end_POSTSUBSCRIPT = italic_h start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT).", "For graph-level prediction, the node representations after the final GNN layer are typically sum-pooled to obtain a single graph representation hG=\u2211ihisubscript\u210e\ud835\udc3asubscript\ud835\udc56subscript\u210e\ud835\udc56h_{G}=\\sum_{i}h_{i}italic_h start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which is then passed to a FFN prediction layer. Note that this approach only allows features to propagate locally and is hence unable to learn long-range dependencies and hierarchical structures within graphs.", "Our goal is to learn a pooling operator to coarsen the input graph after the GNN in each hierarchical layer. In each hierarchical layer, the GNN constructs node representations and then the pooling layer forms a coarsened graph, which is used as input to the next hierarchical layer. More formally, given the representations from the l\ud835\udc59litalic_l-th layer as (Xout(l),Eout(l))=\ud835\ude76\ud835\ude7d\ud835\ude7d(A(l),X(l),E(l))subscriptsuperscript\ud835\udc4b\ud835\udc59\ud835\udc5c\ud835\udc62\ud835\udc61subscriptsuperscript\ud835\udc38\ud835\udc59\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\ude76\ud835\ude7d\ud835\ude7dsuperscript\ud835\udc34\ud835\udc59superscript\ud835\udc4b\ud835\udc59superscript\ud835\udc38\ud835\udc59(X^{(l)}_{out},E^{(l)}_{out})=\\texttt{GNN}(A^{(l)},X^{(l)},E^{(l)})( italic_X start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT , italic_E start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT ) = GNN ( italic_A start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT , italic_X start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT , italic_E start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ), the pooling layer yields an assignment matrix S(l)\u2208\u211dnl\u00d7nl+1superscript\ud835\udc46\ud835\udc59superscript\u211dsubscript\ud835\udc5b\ud835\udc59subscript\ud835\udc5b\ud835\udc591S^{(l)}\\in\\mathbb{R}^{n_{l}\\times n_{l+1}}italic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT \u00d7 italic_n start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT pooling nlsubscript\ud835\udc5b\ud835\udc59n_{l}italic_n start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT nodes into nl+1subscript\ud835\udc5b\ud835\udc591n_{l+1}italic_n start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT clusters. Then, the graph G(l)=(A(l),X(l),E(l))superscript\ud835\udc3a\ud835\udc59superscript\ud835\udc34\ud835\udc59superscript\ud835\udc4b\ud835\udc59superscript\ud835\udc38\ud835\udc59G^{(l)}=(A^{(l)},X^{(l)},E^{(l)})italic_G start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT = ( italic_A start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT , italic_X start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT , italic_E start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ) is coarsened into G(l+1)=(A(l+1),X(l+1),E(l+1))=(S(l)TA(l)S(l),S(l)TXout(l),S(l)TEout(l)S(l))superscript\ud835\udc3a\ud835\udc591superscript\ud835\udc34\ud835\udc591superscript\ud835\udc4b\ud835\udc591superscript\ud835\udc38\ud835\udc591superscript\ud835\udc46superscript\ud835\udc59\ud835\udc47superscript\ud835\udc34\ud835\udc59superscript\ud835\udc46\ud835\udc59superscript\ud835\udc46superscript\ud835\udc59\ud835\udc47subscriptsuperscript\ud835\udc4b\ud835\udc59\ud835\udc5c\ud835\udc62\ud835\udc61superscript\ud835\udc46superscript\ud835\udc59\ud835\udc47subscriptsuperscript\ud835\udc38\ud835\udc59\ud835\udc5c\ud835\udc62\ud835\udc61superscript\ud835\udc46\ud835\udc59G^{(l+1)}=(A^{(l+1)},X^{(l+1)},E^{(l+1)})=(S^{(l)^{T}}A^{(l)}S^{(l)},S^{(l)^{T}}X^{(l)}_{out},S^{(l)^{T}}E^{(l)}_{out}S^{(l)})italic_G start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT = ( italic_A start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT , italic_X start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT , italic_E start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT ) = ( italic_S start_POSTSUPERSCRIPT ( italic_l ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT italic_A start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT , italic_S start_POSTSUPERSCRIPT ( italic_l ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT italic_X start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT , italic_S start_POSTSUPERSCRIPT ( italic_l ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT italic_E start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT italic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ). This hierarchical process can be utilized iteratively depending on the task at hand.", "When looking into the relation between pairs of nodes, the grouping task becomes rather simple. While most previous models focus on classifying each node to a predefined number of clusters, our idea simplifies the task into classifying whether each pair of nodes is in the same group. Thus, setting the number of clusters a priori becomes unnecessary. This classification will go through every pair of combinations of nodes to ensure permutation invariance.Mij(l)=Softmax(Clf(f(Xi,Xj)))\u2200i,j\u2208# of Nodesformulae-sequencesubscriptsuperscript\ud835\udc40\ud835\udc59\ud835\udc56\ud835\udc57SoftmaxClf\ud835\udc53subscript\ud835\udc4b\ud835\udc56subscript\ud835\udc4b\ud835\udc57for-all\ud835\udc56\ud835\udc57# of NodesM^{(l)}_{ij}=\\textrm{Softmax}(\\textrm{Clf}(f(X_{i},X_{j})))\\qquad\\forall\\,\\,i,j\\in\\textrm{\\# of Nodes}italic_M start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = Softmax ( Clf ( italic_f ( italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ) ) \u2200 italic_i , italic_j \u2208 # of Nodes(5)where M(l)\u2208\u211dN\u00d7Nsuperscript\ud835\udc40\ud835\udc59superscript\u211d\ud835\udc41\ud835\udc41M^{(l)}\\in\\mathbb{R}^{N\\times N}italic_M start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_N \u00d7 italic_N end_POSTSUPERSCRIPT and f\ud835\udc53fitalic_f is a commutative functionf:X\u2295X\u2192YwhereX,Y\u2208\u211dN:\ud835\udc53formulae-sequence\u2192direct-sum\ud835\udc4b\ud835\udc4b\ud835\udc4cwhere\ud835\udc4b\ud835\udc4csuperscript\u211d\ud835\udc41f:X\\oplus X\\rightarrow Y\\qquad\\textrm{where}\\,\\,X,Y\\in\\mathbb{R}^{N}italic_f : italic_X \u2295 italic_X \u2192 italic_Y where italic_X , italic_Y \u2208 blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT(6)that maps two input vectors into one output vector. While there exist many available choices for f\ud835\udc53fitalic_f, we use Euclidean distance between input vectors to simplify the classification task. Each matrix index corresponds to the node number and each element contains probability values for each pair of nodes whether they are in the same group.", "As an illustrative example, consider a set of disjoint clusters with no overlapping nodes. In such case, the grouping matrix not only contains 0,1010,10 , 1 as its elements, but also can be reformed into a block diagonal form. The number of blocks corresponds to the number of groups after pooling and nodes assigned to the same blocks corresponds to a same group. For instance, if there are three different groups and each group size are k1,k2,k3subscript\ud835\udc581subscript\ud835\udc582subscript\ud835\udc583k_{1},k_{2},k_{3}italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT,M(l)=[1k1\u00d7k10001k2\u00d7k20001k3\u00d7k3]superscript\ud835\udc40\ud835\udc59matrixsubscript1subscript\ud835\udc581subscript\ud835\udc581000subscript1subscript\ud835\udc582subscript\ud835\udc582000subscript1subscript\ud835\udc583subscript\ud835\udc583M^{(l)}=\\begin{bmatrix}\\framebox{$1_{k_{1}\\times k_{1}}$}&0&0\\\\0&\\framebox{$1_{k_{2}\\times k_{2}}$}&0\\\\0&0&\\framebox{$1_{k_{3}\\times k_{3}}$}\\\\\\end{bmatrix}italic_M start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT = [ start_ARG start_ROW start_CELL 1 start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u00d7 italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 1 start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u00d7 italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 1 start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT \u00d7 italic_k start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ](7)One can easily see that the corresponding pooling operator is as followsS(l)=[1k1\u00d7100\u22ef001k2\u00d710\u22ef0001k3\u00d71\u22ef0]superscript\ud835\udc46\ud835\udc59matrixsubscript1subscript\ud835\udc581100\u22ef00subscript1subscript\ud835\udc58210\u22ef000subscript1subscript\ud835\udc5831\u22ef0S^{(l)}=\\begin{bmatrix}\\framebox{$1_{k_{1}\\times 1}$}&0&0\\quad&\\cdots&\\quad 0\\\\0&\\framebox{$1_{k_{2}\\times 1}$}&0\\quad&\\cdots&\\quad 0\\\\0&0&\\framebox{$1_{k_{3}\\times 1}$}\\quad&\\cdots&\\quad 0\\\\\\end{bmatrix}italic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT = [ start_ARG start_ROW start_CELL 1 start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u00d7 1 end_POSTSUBSCRIPT end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL \u22ef end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 1 start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u00d7 1 end_POSTSUBSCRIPT end_CELL start_CELL 0 end_CELL start_CELL \u22ef end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL start_ARG 1 start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT \u00d7 1 end_POSTSUBSCRIPT end_ARG end_CELL start_CELL \u22ef end_CELL start_CELL 0 end_CELL end_ROW end_ARG ](8)In general, each element of the grouping matrix (in eq. 26) is a continuous number within [0,1]01[0,1][ 0 , 1 ], which allows soft-clustering with overlapping nodes. For detailed computation, see appendix.", "However, the grouping matrix itself has a limited role in pooling operations. Therefore, extracting pooling operators from the grouping matrix is crucial. Our strategy to form a pooling operator is rather simple. It can be acquired by decomposing a grouping matrix into square-root form. There are numerous known methods which can be utilized, yet we will introduce two representative methods in the following subsection.", "While the grouping matrix cannot be used for pooling as is, it encodes how similarly each pair of nodes are pooled as it equals the product of the pooling operator with its transpose.The (i,j)\ud835\udc56\ud835\udc57(i,j)( italic_i , italic_j )-th entry of the grouping matrix equals \u27e8Si(l),Sj(l)\u27e9=1superscriptsubscript\ud835\udc46\ud835\udc56\ud835\udc59superscriptsubscript\ud835\udc46\ud835\udc57\ud835\udc591\\langle S_{i}^{(l)},S_{j}^{(l)}\\rangle=1\u27e8 italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT , italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT \u27e9 = 1 if the nodes are exactly pooled to the same clusters, \u27e8Si(l),Sj(l)\u27e9=0superscriptsubscript\ud835\udc46\ud835\udc56\ud835\udc59superscriptsubscript\ud835\udc46\ud835\udc57\ud835\udc590\\langle S_{i}^{(l)},S_{j}^{(l)}\\rangle=0\u27e8 italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT , italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT \u27e9 = 0 if they are pooled orthogonally to different clusters. Therefore, if we can decompose the grouping matrix into square-root form, it can be interpreted as a pooling operator for the model.S(l)S(l)T=M(l)superscript\ud835\udc46\ud835\udc59superscript\ud835\udc46\ud835\udc59\ud835\udc47superscript\ud835\udc40\ud835\udc59S^{(l)}S^{(l)T}=M^{(l)}italic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT ( italic_l ) italic_T end_POSTSUPERSCRIPT = italic_M start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT(9)The pooling operator S\u2208\u211dnl\u00d7nl+1\ud835\udc46superscript\u211dsubscript\ud835\udc5b\ud835\udc59subscript\ud835\udc5b\ud835\udc591S\\in\\mathbb{R}^{n_{l}\\times n_{l+1}}italic_S \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT \u00d7 italic_n start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT is a matrix where nl+1\u2264nlsubscript\ud835\udc5b\ud835\udc591subscript\ud835\udc5b\ud835\udc59n_{l+1}\\leq n_{l}italic_n start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT \u2264 italic_n start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT. Note that by multiplying pooling operator S\ud835\udc46Sitalic_S in reverse order, the degree matrix D\u2208\u211dnl+1\u00d7nl+1\ud835\udc37superscript\u211dsubscript\ud835\udc5b\ud835\udc591subscript\ud835\udc5b\ud835\udc591D\\in\\mathbb{R}^{n_{l+1}\\times n_{l+1}}italic_D \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT \u00d7 italic_n start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT of pooling space can be obtained.S(l)TS(l)=D(l)superscript\ud835\udc46\ud835\udc59\ud835\udc47superscript\ud835\udc46\ud835\udc59superscript\ud835\udc37\ud835\udc59S^{(l)T}S^{(l)}=D^{(l)}italic_S start_POSTSUPERSCRIPT ( italic_l ) italic_T end_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT = italic_D start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT(10)From eq. 9, it is obvious that the pooling operator completely reconstructs grouping matrix by interacting pooling indices. Moreover, S\ud835\udc46Sitalic_S can be interpreted as a weighted matrix for each node to form appropriate sub-structures.", "Eigen decomposition is one of the basic decomposition schemes one can consider. It is widely used to decompose a given matrix into orthonormal basis O\u2208\u211dnl\u00d7nl\ud835\udc42superscript\u211dsubscript\ud835\udc5b\ud835\udc59subscript\ud835\udc5b\ud835\udc59O\\in\\mathbb{R}^{n_{l}\\times n_{l}}italic_O \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT \u00d7 italic_n start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and eigen value \u039b\u2208\u211dnl\u00d7nl\u039bsuperscript\u211dsubscript\ud835\udc5b\ud835\udc59subscript\ud835\udc5b\ud835\udc59\\Lambda\\in\\mathbb{R}^{n_{l}\\times n_{l}}roman_\u039b \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT \u00d7 italic_n start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_POSTSUPERSCRIPT.M(l)=O\u039bOTsuperscript\ud835\udc40\ud835\udc59\ud835\udc42\u039bsuperscript\ud835\udc42\ud835\udc47M^{(l)}=O\\Lambda O^{T}italic_M start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT = italic_O roman_\u039b italic_O start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT(11)This particular decomposition scheme always works unless the determinant of a given matrix is equal to 0. From eq. 11, one can rearrange RHS of the equation to become a square form of pooling operator if we set nl+1=nlsubscript\ud835\udc5b\ud835\udc591subscript\ud835\udc5b\ud835\udc59n_{l+1}=n_{l}italic_n start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT = italic_n start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT.M(l)=O\u039b\u039bOT\u2261S(l)S(l)Tsuperscript\ud835\udc40\ud835\udc59\ud835\udc42\u039b\u039bsuperscript\ud835\udc42\ud835\udc47superscript\ud835\udc46\ud835\udc59superscript\ud835\udc46\ud835\udc59\ud835\udc47M^{(l)}=O\\sqrt{\\Lambda}\\sqrt{\\Lambda}O^{T}\\equiv S^{(l)}S^{(l)T}italic_M start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT = italic_O square-root start_ARG roman_\u039b end_ARG square-root start_ARG roman_\u039b end_ARG italic_O start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT \u2261 italic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT ( italic_l ) italic_T end_POSTSUPERSCRIPT(12)The pooling operator S\ud835\udc46Sitalic_S is a square matrix with size of nl\u00d7nlsubscript\ud835\udc5b\ud835\udc59subscript\ud835\udc5b\ud835\udc59n_{l}\\times n_{l}italic_n start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT \u00d7 italic_n start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT, yet the eigen value \u039b\u039b\\Lambdaroman_\u039b suppresses useless ranks in the matrix by multiplying 00 to each column of orthonormal basis.Also, eigen decomposition works for any matrix with non-zero determinants, and so it performs perfectly fine in real world situations. Furthermore, any symmetric and real matrix are guaranteed to have real eigen values as well as vectors. Therefore, the square-root of the grouping matrix is ensured to be interpreted as a transformation operator forming sub-groups from nodes. These continuous real valued elements have the advantage that nodes can be soft-clustered to sub-groups.In conventional clustering, it is hard to cluster these structures properly. However, since soft clustering is naturally embedded in the algorithm, linker structures can be dealt with ease.", "After acquiring the pooling operator, the pooling process becomes obvious. Nodes are in fundamental representation while edge features and adjacency matrix are in adjoint representation. Which leads to the following transformation rules.Xi(l+1)=S(l)Xi(l)superscriptsubscript\ud835\udc4b\ud835\udc56\ud835\udc591superscript\ud835\udc46\ud835\udc59superscriptsubscript\ud835\udc4b\ud835\udc56\ud835\udc59\\displaystyle X_{i}^{(l+1)}=S^{(l)}X_{i}^{(l)}italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT = italic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT(13)Eij(l+1)=S(l)Eij(l)S(l)Tsuperscriptsubscript\ud835\udc38\ud835\udc56\ud835\udc57\ud835\udc591superscript\ud835\udc46\ud835\udc59superscriptsubscript\ud835\udc38\ud835\udc56\ud835\udc57\ud835\udc59superscript\ud835\udc46\ud835\udc59\ud835\udc47\\displaystyle E_{ij}^{(l+1)}=S^{(l)}E_{ij}^{(l)}S^{(l)T}italic_E start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT = italic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT italic_E start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT ( italic_l ) italic_T end_POSTSUPERSCRIPT(14)Aij(l+1)=S(l)Aij(l)S(l)Tsuperscriptsubscript\ud835\udc34\ud835\udc56\ud835\udc57\ud835\udc591superscript\ud835\udc46\ud835\udc59superscriptsubscript\ud835\udc34\ud835\udc56\ud835\udc57\ud835\udc59superscript\ud835\udc46\ud835\udc59\ud835\udc47\\displaystyle A_{ij}^{(l+1)}=S^{(l)}A_{ij}^{(l)}S^{(l)T}italic_A start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT = italic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT ( italic_l ) italic_T end_POSTSUPERSCRIPT(15)If grouping is properly done, 00 (or close to 00) components will appear in the decomposed eigen value matrix. These zero eigenvalues arise naturally and play a role in disregarding group information; those are ineffective towards prediction. However, zero elements in the eigen values causes a major problem in the decomposition process since the matrix might carry a singular determinant.Eigen decomposition is based on an iterative approximation algorithm which includes unbounded terms if any two eigen values are small or close. One can see clearly about this matter in DBLP:journals/corr/IonescuVS15 .(\u2202l\u2202A)=U(KT\u2299(UT\u2202l\u2202U)+(\u2202l\u2202\u039b)diag)(UT)\ud835\udc59\ud835\udc34\ud835\udc48direct-productsuperscript\ud835\udc3e\ud835\udc47superscript\ud835\udc48\ud835\udc47\ud835\udc59\ud835\udc48subscript\ud835\udc59\u039bdiagsuperscript\ud835\udc48\ud835\udc47\\Big{(}\\frac{\\partial{l}}{\\partial{A}}\\Big{)}=U\\big{(}K^{T}\\odot(U^{T}\\frac{\\partial{l}}{\\partial{U}})+(\\frac{\\partial{l}}{\\partial{\\Lambda}})_{\\textrm{diag}})(U^{T})( divide start_ARG \u2202 italic_l end_ARG start_ARG \u2202 italic_A end_ARG ) = italic_U ( italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT \u2299 ( italic_U start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT divide start_ARG \u2202 italic_l end_ARG start_ARG \u2202 italic_U end_ARG ) + ( divide start_ARG \u2202 italic_l end_ARG start_ARG \u2202 roman_\u039b end_ARG ) start_POSTSUBSCRIPT diag end_POSTSUBSCRIPT ) ( italic_U start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT )(16)Here, \u2299direct-product\\odot\u2299 denotes element-wise product. Off-diagonal components of K=1/(\u03bbi\u2212\u03bbj)\ud835\udc3e1subscript\ud835\udf06\ud835\udc56subscript\ud835\udf06\ud835\udc57K=1/(\\lambda_{i}-\\lambda_{j})italic_K = 1 / ( italic_\u03bb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_\u03bb start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) causes the problem, since the value blows up to the infinity if any two eigen values are close or very small. However, there are some solutions for this matter by approximating gradient in different ways DBLP:journals/corr/abs-1906-09023 ; 9400752 ; DBLP:journals/corr/abs-2105-02498 . Those methods are developed further to achieve higher speed in the calculation DBLP:journals/corr/abs-2201-08663 . They claim that the method is noticeably faster, over 8888 times, than the standard SVD which has the time complexity \ud835\udcaa(n3)\ud835\udcaasuperscript\ud835\udc5b3\\mathcal{O}(n^{3})caligraphic_O ( italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ). Thus, we utilized this method in our work to stabilize and accelerate the learning process. However, since the algorithm achieves the higher speed by approximating gradients, the error compared to standard SVD grows bigger as the size of the matrix grows. Therefore, this method might not be valid with large sized graph data.", "Another decomposition scheme we are introducing has a rather different approach. Since computing the square root of a given matrix is not an easy task, here we focus on the square of the pooling operator, which is nothing but the grouping matrix itself, and formulate a pooling-like effect by multiplying the grouping matrix. The key idea is to retain pooling depth to one and use a weighted aggregation vector in pooling space as an aggregation basis. The weighted aggregation vector is transformed Euclidean one vector by acting a pooling matrix obtained by decomposing the grouping matrix.1i(l+1)=S(l)1i(l)superscriptsubscript1\ud835\udc56\ud835\udc591superscript\ud835\udc46\ud835\udc59superscriptsubscript1\ud835\udc56\ud835\udc59\\displaystyle 1_{i}^{(l+1)}=S^{(l)}1_{i}^{(l)}1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT = italic_S start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT 1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT(17)The final form of the transformation can be expressed as follows.Xi(l+1)\u223cM(l)Xi(l)similar-tosuperscriptsubscript\ud835\udc4b\ud835\udc56\ud835\udc591superscript\ud835\udc40\ud835\udc59superscriptsubscript\ud835\udc4b\ud835\udc56\ud835\udc59\\displaystyle X_{i}^{(l+1)}\\sim M^{(l)}X_{i}^{(l)}italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT \u223c italic_M start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT(18)Eij(l+1)\u223cM(l)Eij(l)M(l)similar-tosuperscriptsubscript\ud835\udc38\ud835\udc56\ud835\udc57\ud835\udc591superscript\ud835\udc40\ud835\udc59superscriptsubscript\ud835\udc38\ud835\udc56\ud835\udc57\ud835\udc59superscript\ud835\udc40\ud835\udc59\\displaystyle E_{ij}^{(l+1)}\\sim M^{(l)}E_{ij}^{(l)}M^{(l)}italic_E start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT \u223c italic_M start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT italic_E start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT italic_M start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT(19)This pooling scheme is simpler to use and more scalable (with \ud835\udcaa(n2)\ud835\udcaasuperscript\ud835\udc5b2\\mathcal{O}(n^{2})caligraphic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) cost) than GMPool since the method circumvents SVD computation. Yet there are two mathematical ambiguities. One is that it is only valid for single depth pooling cases. If one tries to perform multiple sequential pooling operations, the pooling operators are no more available to be reduced into the grouping matrix, since two different pooling operators are not Abelian. The other ambiguity is that most activation functions commonly used are not equivariant with pooling operators. However, since many of them are based on element-wise operations with monotonic functions, we can presume that the anomaly are not dominant in most cases. We find that this approach performs comparably to GMPool for small sized molecules where a single pooling depth suffice.", "We arrange a total of five datasets to test our algorithms: two are open datasets collected from MoleculeNet Ramsundar-et-al-2019  and Binding DB 10.2174/1386207013330670 ; 10.1093/bioinformatics/18.1.130 ; 10.1002/bip.10076 ; 10.1093/nar/gkl999 ; 10.1093/nar/gkv1072 , three are manually collected and arranged from different literatures including scientific articles and patents.\u2022PLQY includes experimentally measured values of photoluminescence quantum yield (PLQY) for fluorescence molecules.\u2022\u03bbmaxsubscript\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65\\lambda_{max}italic_\u03bb start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT Solvents contains measured \u03bbmaxsubscript\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65\\lambda_{max}italic_\u03bb start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT, wavelength that shows maximum intensity for emission of a fluorescence molecule, under the solvent condition.\u2022\u03bbmaxsubscript\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65\\lambda_{max}italic_\u03bb start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT Films consists of \u03bbmaxsubscript\ud835\udf06\ud835\udc5a\ud835\udc4e\ud835\udc65\\lambda_{max}italic_\u03bb start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT values measured after spin coating of fluorescence molecules on films doped with host materials.\u2022pIC50 contains the negative log of the IC50 values for ATP receptor. IC50 implies minimum concentration of certain molecule needed for inhibiting half of activity of the target proteins. The IC50 values are optained from the BindingDB (https://www.bindingdb.org/bind/index.jsp).\u2022Tox21 consists of results of 12 types of toxicity screening tests. We labeled a molecule \u2018toxic\u2019 if the molecule failed in any of screening type. Data were originated from Tox21 challenge (2014). Since there are molecules without graph structure information in the dataset, we selected 7,83178317,8317 , 831 molecules that have the graph structure information.For proper evaluation of pooling approaches, each graph in the data must have at least two or more effective groups.However, Tox21 and pIC50 data contains molecules too small to contain multiple groups and thus we drop molecules with less than 20 nodes from the datasets.In addition, we drop molecules with more than 40 nodes from Tox21 and pIC50 datasets to accelerate the whole training process under dense matrix computations: the largest molecule in each respective dataset has 86 and 132 nodes, but the ratio of molecules with size over 40 in the dataset is only 3.4%percent3.43.4\\%3.4 % and 3.6%percent3.63.6\\%3.6 %. Especially for pIC50 dataset, the proportion of molecules with less than 20 nodes are 0.4%percent0.40.4\\%0.4 %. Lastly, the Tox21 task has been simplified to a single classification task by setting a positive label if any of the 12 tasks are positive in the original dataset. Details can be found in Table 1 and appendix section.Every experiments are tested under five-fold settings with uniform sampling and 10% of dedicated test set to secure the results, and single RTX 3090 is used for the experiments.", "For empirical evaluation, we compare the performance of GMPool and NGMPool against that of five other pooling approaches. We run all experiments via a pipeline with a fixed DMPNN backbone, while exchanging the pooling layers only. Here we provide brief descriptions of each baselines used: Top-k\u00a0gao2019  and SAGPool\u00a0lee2019  retain nodes with the highest scoring based on the projections of node features and self-attention scores, respectively. DiffPool\u00a0ying2018  uses an additional GNN to learn soft-assignment matrices that mix nodes into clusters. ASAPool\u00a0ranjan2020asap  clusters local subgraphs together through scoring and selection of clusters. MemPool\u00a0mempool  incorporates memory layers that jointly coarsen and transform input node representations. Note that we reimplemented the DMPNN backbone, Top-k pooling, and DiffPool. Implementations of other pooling baselines are borrowed from the pytorch-geometric library.", "For the backbone of the model, DMPNN, we use the same hidden size of 200 across all three independent layers: the initial edge features with dimension desubscript\ud835\udc51\ud835\udc52d_{e}italic_d start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT and node features with dimension dnsubscript\ud835\udc51\ud835\udc5bd_{n}italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT are passed through layers of dimension de\u00d7200subscript\ud835\udc51\ud835\udc52200d_{e}\\times 200italic_d start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT \u00d7 200 and dn\u00d7200subscript\ud835\udc51\ud835\udc5b200d_{n}\\times 200italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT \u00d7 200, respectively with ReLU activation. The initial node and edge embeddings are determined by features generated in RDKit. The message passing module passes node embeddings through a linear layer with dimension 200\u00d7200200200200\\times 200200 \u00d7 200, followed by ReLU activation and 0.150.150.150.15 dropout layer. For graph representation we use a global average pooling scheme. GMPool and NGMPool construct the grouping matrix via a 200\u00d712001200\\times 1200 \u00d7 1 linear layer and sigmoid activation without any parameters related to cluster numbers or thresholds. We use a batch size of 80808080 and Adam optimizer for all model training.", "For baseline pooling methods that require the cluster size as a hyperparameter, we perform grid search across candidates following previous work, and present best results.However, we fix the final pooling size to 10 as the average size of most common 40404040 functional groups in bioactive molecules is 4.254.254.254.25\u00a0ertl2020most , indicating that molecules under concern (statistics shown in Table\u00a01) can have up to 10101010 clusters.The specific hyperparameter setups used for pooling baselines can be found in appendix.", "The grouping matrix starts from randomized initial state and is optimized to gather effective functional groups in the molecules (Figures 1(b) and 1(c)).Furthermore, since our algorithm fully enjoys the soft clustering concept, the result shows continuous weights for each group. This characteristic ensures the model can gather information from distant nodes if necessary. However, sometimes the grouping matrix shows unfamiliar forms, since the effective functional groups should vary due to the downstream task itself. For instance, for some simple tasks such as PLQY prediction, the grouping is rather intuitive as shown in Figure 1(b), yet for complicated tasks like \u03bbmaxsubscript\ud835\udf06max\\lambda_{\\textrm{max}}italic_\u03bb start_POSTSUBSCRIPT max end_POSTSUBSCRIPT prediction, the effective functional groups are also complicated as in Figure 1(c).", "We tested various combinations of models and dataset to check the validity of our algorithm. We selected GCN, DMPNN, Top-k, SAGPool, DiffPool, ASAPool and MemPool algorithm to set a benchmark score to compare with. As it is shown in the table 2, majority of the cases, our models outperform conventional methods. However, for some tasks (i.e. \u03bbmaxsubscript\ud835\udf06max\\lambda_{\\textrm{max}}italic_\u03bb start_POSTSUBSCRIPT max end_POSTSUBSCRIPT datasets), our model is gaining only a small margin of the performance. This is caused by the underlying mechanism of the chemical effect. Since some tasks are strongly related to the effective groups of the molecule, yet others are not. In those cases, sub-structures are not intuitive and might appear in very complicated forms, as shown in Figure 1(c). If the grouping becomes complicated, the rank of the pooling matrix should be larger to cover all degrees of freedom for the data. However, conventional models, which shared predefined numbers as universal grouping numbers, force to collect groups and reduce it to the low-rank form, which might not have enough degree of freedom. This will cause information loss or blend which compromises the prediction result. Therefore, one can check that in \u03bbmaxsubscript\ud835\udf06max\\lambda_{\\textrm{max}}italic_\u03bb start_POSTSUBSCRIPT max end_POSTSUBSCRIPT prediction test, conventional pooling algorithms show inferior result than simple massage passing scheme. Yet our model is not designed to reduce the physical rank of the matrix during the pooling process, and there is always enough degree of freedom to carry the information throughout learning. Hence, even for the \u03bbmaxsubscript\ud835\udf06max\\lambda_{\\textrm{max}}italic_\u03bb start_POSTSUBSCRIPT max end_POSTSUBSCRIPT case, our model outperforms the others. Furthermore, for other tasks, it is clear that our model improves performance by 5\u223c10%psimilar-to5percent10\ud835\udc5d5\\sim 10\\%p5 \u223c 10 % italic_p.", "One crucial hyperparameter to be explored in pooling models is the number of clusters. Even though our model does not require to fix the number of clusters in the first place, one can set the parameter by force. One can easily see in the Figure 2(a) that the number of clusters can be set to the number of nodes without compromising performance of the model. Further, Figure 2(b) shows that our model outperforms Top k algorithms with various cluster numbers and original DMPNN as well. This is one of the powerful features of our model, since the model automatically splits groups and determines the appropriate number of sub-structures for each individual graph. One can also force the number of clusters and share through all graphs in an equal manner; however, it is not effective for the following reasons. In real world data, one can not esteem the exact number of clusters for individual graphs. This might be problematic if one sets the number of clusters less than it requires, the models\u2019 performance will be compromised due to the information loss. Another problem is caused by the mathematical structure of the decomposition scheme. Using SVD method will cause ambiguity since collecting only top k eigen values from the decomposed matrix might not reconstruct the original grouping matrix due to lack of information. It is even worse in the initial stage of the learning as the weight is almost in the random state and the top k eigen values are not precisely representing the appropriate clusters. Thus, as it is depicted in the above figure, it is best to leave the cluster number to be determined automatically by the model itself.", "We have introduced a novel pooling architecture with adaptive number of clusters based on a second order pooling operator, namely the grouping matrix. The grouping matrix is based on clustering similarities between every possible pairs of nodes, ensuring permutation invariance. We have shown that our model is valid for chemical property prediction and outperforms conventional methods in real-world datasets.", "While our model is useful and effective, there is still room for improvement. First of all, despite leveraging a method to decompose the grouping matrix with stable gradient computations, there exist corner cases with a small eigengap at which the model fails to converge. This event seldom happens (about 0.00018%percent0.000180.00018\\%0.00018 % in our experiments), but can be non-negligible when one needs to learn with a large number of data points. Hence, one future direction would be to impose proper constraints on the loss to avoid such gradient blowup in the grouping matrix.", "Another future direction would be to enhance scalability of our methods to improve applicability to large-scale graphs. Since the grouping matrix decomposition step via SVD is the main computational bottleneck of GMPool, incorporating faster decomposition modules such as randomized approximation\u00a0halko2011finding ; DBLP:journals/corr/abs-1710-02812  methods can lead to faster inference. However, this is likely to incur loss in predictive performance, and as the focus of this work lies in allowing variation in the number of clusters in small molecular graphs where scalability is not an issue, we defer improving the scalability to future work.", "Lastly, generalizing the second order grouping matrix towards higher-order grouping tensors can allow further expressive power. We have introduced a pairwise structure; yet it is not obliged to be fixed into the pairwise form. If we consider higher-order form of node combinations, i.e. k-form where k<N\ud835\udc58\ud835\udc41k<Nitalic_k < italic_N and N\ud835\udc41Nitalic_N is total node number, the grouping matrix can be generalized into the higher rank tensor. Based on the tensor-form, the transformation rule can be written as", "M~\u03bc1\u22ef\u03bck=S\u03bc1\u03bd1\u22efS\u03bck\u03bdkM\u03bd1\u22ef\u03bdksubscript~\ud835\udc40subscript\ud835\udf071\u22efsubscript\ud835\udf07\ud835\udc58superscriptsubscript\ud835\udc46subscript\ud835\udf071subscript\ud835\udf081\u22efsuperscriptsubscript\ud835\udc46subscript\ud835\udf07\ud835\udc58subscript\ud835\udf08\ud835\udc58subscript\ud835\udc40subscript\ud835\udf081\u22efsubscript\ud835\udf08\ud835\udc58\\tilde{M}_{\\mu_{1}\\cdots\\mu_{k}}=S_{\\mu_{1}}^{\\phantom{\\mu_{1}}\\nu_{1}}\\cdots S_{\\mu_{k}}^{\\phantom{\\mu_{k}}\\nu_{k}}M_{\\nu_{1}\\cdots\\nu_{k}}over~ start_ARG italic_M end_ARG start_POSTSUBSCRIPT italic_\u03bc start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u22ef italic_\u03bc start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_S start_POSTSUBSCRIPT italic_\u03bc start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bd start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT \u22ef italic_S start_POSTSUBSCRIPT italic_\u03bc start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bd start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT italic_\u03bd start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u22ef italic_\u03bd start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT(20)", "Note that to satisfy the above transformation rule, the following conditions are required. One is that selecting nodes combination should be as same as selecting nodes set and size of the set should fixed into the number of nodes in a group. The other is that the classification result of the node set should be retained the same for any subset in the node set. This concept may have a connection to hypergraph configurations. However if we raise the nodes numbers above 2222, required computation power increases by a huge amount, since the combination number grows exponentially until the number of nodes hits N/2\ud835\udc412N/2italic_N / 2. Therefore, practically it is a difficult task to test the higher rank version of our algorithm, yet it could be useful for learning datasets with higher order connections."], "figure_types": {"c7bbe399aad2da39ba8f988fec83129d60aa5d52/13-Table3-1.png": "table", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/15-Table4-1.png": "table", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/16-Figure4-1.png": "plot", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/2-Figure1-1.png": "schematic", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/5-Table1-1.png": "table", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/7-Figure2-1.png": "plot", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/8-Table2-1.png": "table", "c7bbe399aad2da39ba8f988fec83129d60aa5d52/9-Figure3-1.png": "plot"}}, "2210.01504": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "arxiv_url": "https://arxiv.org/abs/2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "all_figures_tables": {"91fb2254c5942048425e642c8a6c8d400006150e/10-Table4-1.png": "Table 4: Unlearning GPT-NEO 1.3B on token sequences sampled from 8 different domains. We fix the epoch to 10, set s = 8, and show the result of the average of 5 random samplings. Italicized () denotes the \u2206 from INITIAL.", "91fb2254c5942048425e642c8a6c8d400006150e/15-Figure4-1.png": "Figure 4: Varying the learning rate for unlearning the GPT-NEO 1.3B with s = 32. We report the average of 3 random samplings and display the standard deviations as the shaded regions. Red dotted lines denote the memorization accuracy forgetting threshold of the 1.3B model reported in Table 1.", "91fb2254c5942048425e642c8a6c8d400006150e/16-Figure5-1.png": "Figure 5: Additional results of sequential unlearning for GPT-NEO 125M, 1.3B, and 2.7B. Red dotted lines denote the memorization accuracy forgetting threshold reported of each model in Table 1.", "91fb2254c5942048425e642c8a6c8d400006150e/17-Table5-1.png": "Table 5: All of the individual runs for the Main Results", "91fb2254c5942048425e642c8a6c8d400006150e/18-Table6-1.png": "Table 6: All of the individual runs for the Domain Analysis Results for GPT-NEO 1.3B LM.", "91fb2254c5942048425e642c8a6c8d400006150e/18-Table7-1.png": "Table 7: All of the individual runs for s = 32 for the dialogue tasks in the Main Results.", "91fb2254c5942048425e642c8a6c8d400006150e/19-Table8-1.png": "Table 8: Measuring perplexity on Pile and Wikitext corpora for the main unlearning experiments (Table 2).", "91fb2254c5942048425e642c8a6c8d400006150e/19-Table9-1.png": "Table 9: Training compute comparison of methods mitigating privacy risks in LMs for sizes 125M, 1.3B, and 2.7B measured via FLOPs.", "91fb2254c5942048425e642c8a6c8d400006150e/2-Figure1-1.png": "Figure 1: Comparison of previous approaches and knowledge unlearning when an individual practices his/her Right-To-Be-Forgotten (RTBF).", "91fb2254c5942048425e642c8a6c8d400006150e/20-Table10-1.png": "Table 10: Examples from each of the 8 domains from the Pile corpora.", "91fb2254c5942048425e642c8a6c8d400006150e/21-Table11-1.png": "Table 11: More examples performing extraction attacks on token sequences, showing knowledge unlearning guarantees protection against extraction attacks. Blue denotes the model generated text given the prefix of length 100 as input. For the extraction attack, we utilize a na\u0131\u0308ve greedy decoding strategy.", "91fb2254c5942048425e642c8a6c8d400006150e/21-Table12-1.png": "Table 12: Forgetting Threshold for GPT-NEO LMs for varying n.", "91fb2254c5942048425e642c8a6c8d400006150e/7-Table1-1.png": "Table 1: Forgetting Threshold for GPT-NEO LMs", "91fb2254c5942048425e642c8a6c8d400006150e/7-Table2-1.png": "Table 2: Main Results showing the average of 5 random sampling of s = 32 (forgetting 32 samples at once). OPT represents the LM with deduplication applied. NEO denotes the initial GPT-NEO LM, NEO + DPD+ represents applying the DP Decoding strategy by varying the \u03bb to match the forgetting criteria, NEO + UL represents performing unlearning on the initial NEO until it provides stronger security for the target sequences than OPT, NEO + UL+ represents performing unlearning on GPT-NEO until target sequences match the forgetting criteria, LM Avg. denotes the average accuracy of the 9 classification datasets, and Dialogue Avg. denotes the average F1 score of the 4 dialogue datasets. The best comparable performances are bolded and second best underlined.", "91fb2254c5942048425e642c8a6c8d400006150e/8-Figure2-1.png": "Figure 2: Average LM performance on the 9 benchmarks when varying the total number of samples forgotten at once is shown in (a) and the average LM performance when the 128 samples are divided into 4 chunks and are forgotten sequentially is shown in (b). The lines denote the average performances of 5 random samplings and the standard deviation is shown as the shaded regions. The dotted lines in (b) denote the s = 128 performance in (a) for comparison purposes.", "91fb2254c5942048425e642c8a6c8d400006150e/9-Figure3-1.png": "Figure 3: Performance on the LM benchmarks as we perform 10 different unlearning runs on GPT-NEO 1.3B where s = 1.", "91fb2254c5942048425e642c8a6c8d400006150e/9-Table3-1.png": "Table 3: An example extracting the suffix of a token sequence from BOOKS3 domain from GPT-NEO 1.3B showing the effect of knowledge unlearning. Model generated text given a prefix of length 100 are shown in Blue."}, "referred_figures_tables": [["91fb2254c5942048425e642c8a6c8d400006150e/8-Figure2-1.png", "91fb2254c5942048425e642c8a6c8d400006150e/8-Figure2-1.png", "91fb2254c5942048425e642c8a6c8d400006150e/8-Figure2-1.png"], ["91fb2254c5942048425e642c8a6c8d400006150e/21-Table12-1.png"]], "question_id": [2, 1], "question": ["Only a small number of examples (32) are randomly selected to be unlearned. Have the authors tried unlearning much larger portions of the training data and observing the effect on the resulting model?", "How much does the success of the EL metric vary depending on which n tokens are used as a prompt for this metric?"], "question_section": ["Section 4.2", "Section 4.1"], "question_trigger_sentence": ["Section 4.2", "Section 4.1"], "question_type": ["Testing question", "Testing question"], "evidential_info": [[{"context": "We show the effect of varying s (the # of data instances to be forgotten at once) in Figure 2a across model scales. We denote this approach as batch unlearning. As shown by the s=128 results, it is harder to forget more samples at once, resulting in substantial degradation of average LM performance regardless of how large the LM is. Since s\\leq 32 does not show much degradation, we explore if sequentially unlearning can be a solution. In Figure 2b, we show the result of dividing the 128 samples into 4 chunks of 32 and performing sequential unlearning; we unlearn each chunk at a time until the chunk reaches the forgetting threshold. Surprisingly, as shown by the performance gap at s=128 between the dotted lines (the s=128 performance of Figure 2a) and straight lines, the end result is vastly different even though exactly the same instances were forgotten. Sequential unlearning shows almost no degradation of average LM performance. In Appendix G, we show that chunks once forgotten stay forgotten and that later chunks are forgotten much faster compared to the initial chunk. This result hints at the generalization of unlearning, which we do not further explore in the scope of this work. The result also suggests that knowledge unlearning can be continually applied to LMs when needed.", "rationale": "Results show that forgetting 128 samples at once results in a severe degradation of general LM performance while forgetting 32 samples does not."}], [{"context": "First, we show the Extraction Likelihood (EL) Forgetting Threshold values for n=[5,10,20,40] by measuring the value on the 10,000 validation instances unseen during training in Table 12. Next, we show the average LM performance (on the 9 classification benchmarks) where we perform unlearning on the LM on 32 samples until the target token sequences are forgotten (the EL & MA value are both lower than the threshold values) in Table 13. Performance shows the average of 5 random samplings.", "rationale": "The average LM perfomance of varying n for the EL metric is shown in Table 13."}]], "composition": ["Results show that forgetting 128 samples at once results in a severe degradation of general LM performance while forgetting 32 samples does not.", "The average LM perfomance of varying n for the EL metric is shown in Table 13."], "Is_figure_in_evidence": [true, false], "Is_table_in_evidence": [false, true], "question_key": ["474", "475"], "passages": ["Recent work has shown that an adversary can extract training data from Pretrained Language Models (LMs) including Personally Identifiable Information (PII) such as names, phone numbers, and email addresses, and other information such as licensed code, private clinical notes, and 128-bit UUIDs\u00a0(Carlini et\u00a0al., 2021; Lee et\u00a0al., 2022; Huang et\u00a0al., 2022; Lehman et\u00a0al., 2021). In 2021, an AI chatbot Iruda became the first AI system to be sued for violating the Personal Information Protection Act after generating the exact home addresses and bank account numbers of actual individuals unintentionally\u00a0(Park, 2021). \u00a0Heikkil\u00e4 (2022) has also shown that GPT-3\u00a0(Brown et\u00a0al., 2020), one of the most well known LM currently in commercial use, offered detailed private information about the Editor-in-Chief of MIT Technology Review including his family members, work address, and phone number. Considering findings that show extracting training data gets easier as LMs scale to larger sizes\u00a0(Carlini et\u00a0al., 2022) and that it is common practice for practitioners to release billion parameter pretrained LMs for public use\u00a0(Gao et\u00a0al., 2020; Black et\u00a0al., 2021; Zhang et\u00a0al., 2022), it has become important to provide privacy guarantees for large LMs.", "Practitioners are required to delete personal information from the LMs by individuals\u2019 request because each individual has the \u201cRight To Be Forgotten (RTBF)\u201d\u00a0(Mantelero, 2013; Graves et\u00a0al., 2021) and can limit the direct and indirect commercial use of their personal information\u00a0(Villaronga et\u00a0al., 2018). Previous methods addressing privacy risks for language models attempt to remove all private information from the training data (data preprocessing)\u00a0(Aura et\u00a0al., 2006; Dernoncourt et\u00a0al., 2017; Lison et\u00a0al., 2021; Kandpal et\u00a0al., 2022) or attempt to design algorithms that ensure differential privacy (DP)\u00a0(Dwork, 2008; Dwork et\u00a0al., 2006; Abadi et\u00a0al., 2016; Anil et\u00a0al., 2021; Li et\u00a0al., 2022; Yu et\u00a0al., 2022). Both approaches require retraining the underlying LM every time individuals want to practice their RTBF, which makess them inadequate for large LMs that are extremely costly to retrain. Furthermore, as pointed out by Brown et\u00a0al. (2022), data preprocessing methods assume private information to be easily identifiable, specified, and removed and DP algorithms can only guarantee protection for information that has clear privacy borders, which make them inadequate in the real-world scenarios where the standard of privacy might differ by each individuals.", "To this end, we propose knowledge unlearning (Figure 1) as an efficient solution that can be applied with just a few parameter updates instead of pretraining the underlying LM again. We perform experiments on GPT-Neo LMs (125M, 1.3B, 2.7B)\u00a0(Black et\u00a0al., 2021) and show that simply changing the gradient descent to the opposite direction during language modeling (which can also be seen as maximizing instead of minimizing the loss function) is effective at protecting target sequences from extraction attacks with little to no performance degradation on the initial LM capabilities measured via 9 common NLP benchmarks: Hellaswag\u00a0(Zellers et\u00a0al., 2019), Lambada\u00a0(Paperno et\u00a0al., 2016), Winogrande\u00a0(Sakaguchi et\u00a0al., 2021), COPA\u00a0(Gordon et\u00a0al., 2012), ARC-Easy\u00a0(Clark et\u00a0al., 2018), ARC-Challenge\u00a0(Clark et\u00a0al., 2018), Piqa\u00a0(Bisk et\u00a0al., 2020), MathQA\u00a0(Amini et\u00a0al., 2019), and PubmedQA\u00a0(Jin et\u00a0al., 2019). For some cases, knowledge unlearning unexpectedly shows significant improvements in LM performance for some of the benchmarks.", "We compare our approach with data deduplication method\u00a0(Kandpal et\u00a0al., 2022) which is known to mitigate privacy risks, and show the effectiveness of knowledge unlearning by providing a stronger privacy guarantee while being much more efficient. We also provide a general guideline that can be used to quantify the memorization and extraction likelihood of target token sequences and suggest when we can empirically consider them to have been \u201cforgotten\u201d. Specifically, we introduce a novel metric that measures the extraction likelihood by varying the prefix length of the target token sequence and quantifying how much of the suffix is actually extracted from the LM.", "Surprisingly, for knowledge unlearning, we find that it is easier to forget a chunk of instances sequentially rather than trying to forget them all at once. We provide further analysis and show that the difficulty of knowledge unlearning depends heavily on the target data being forgotten, especially the domain of the target data. We also provide empirical examples of performing extraction attacks and how exactly knowledge unlearning provides a privacy guarantee for the LM.", "To summarize, our main contributions are fourfold:\u2022We compare knowledge unlearning with a data preprocessing approach and show that our approach results in little to no performance degradation of general language modeling performance (sometimes resulting in improvement) while providing stronger empirical privacy guarantees in situations individuals practice their RTBF and being 3,500,000x more computationally efficient than the compared approach.\u2022We perform additional experiments to determine which factors contribute to the difficulty of knowledge unlearning and find that (1) trying to forget many samples at once results in substantial LM performance degradation which can be mitigated by sequentially forgetting chunks of data and that (2) the domain of the target data (Code, License, Wikipedia, etc.) plays a critical role in determining how hard they are to forget.\u2022We provide a novel metric and a general guideline for quantifying the privacy risks for LMs and determine when they should be considered to have \u201cforgotten\u201d a given target sequence.\u2022Knowledge unlearning surprisingly seems to make LMs stronger where the extreme cases bring +8.0% (37.6% \u2192\u2192\\rightarrow\u2192 45.6%), +10.1% (57.4% \u2192\u2192\\rightarrow\u2192 67.5%), and +7.9% (62.2% \u2192\u2192\\rightarrow\u2192 70.1%) improvements on Lambada for GPT-Neo 125M, 1.3B, and 2.7B, respectively.", "Prior work that tries to mitigate privacy risks for LMs can be divided mainly into data pre/post-processing methods and differential privacy methods.", "Data preprocessing aims to sanitize the training data; it aims to get rid of all data that might violate any kind of privacy from the training data prior to training. These methods mostly utilize measures such as parsers and classification models that try to identify and predict patterns that constitute private information. This is effective at identifying well-formatted private information such as social security numbers or special forms of medical notes\u00a0(Aura et\u00a0al., 2006; Dernoncourt et\u00a0al., 2017; Lison et\u00a0al., 2021; Kandpal et\u00a0al., 2022). However, as pointed out by Brown et\u00a0al. (2022), considering that private information is mostly context-dependent and sometimes in a non-specific format, data preprocessing methods cannot fully claim that they provide privacy guarantees, especially guarantees that match each individual\u2019s standards. Methods that attempt to utilize post-processing methods such as applying censorship to the LM outputs still face the same limitations.", "In this work, we compare our proposed method with a data preprocessing approach proposed by Kandpal et\u00a0al. (2022) which shows that deduplicating the training corpora before pretraining helps pretrain LMs that show stronger robustness against extraction attacks than an LM pretrained under the same circumstances without deduplicating the pretraining corpora. However, we highlight that this approach, which may still be effective at mitigating the overall privacy risks, is not the most suitable approach when considering a realistic scenario of individuals requesting the removal of their information from the implicit parameters of the LMs.", "Differential Privacy (DP) aims to guarantee that the effect of an individual input on the output of a specific function is bounded\u00a0(Dwork, 2008; Dwork et\u00a0al., 2006). In the context of deep neural networks, DP, which needs to be applied during the training phase, aims to construct models that can ensure that the individual information within the training data cannot be inferred\u00a0(Abadi et\u00a0al., 2016). While DP has shown to be surprisingly effective at fine-tuning LMs\u00a0(Li et\u00a0al., 2022; Yu et\u00a0al., 2022), pretraining LMs with DP still suffers from substantial performance gap, expensive computation, and slow convergence\u00a0(Anil et\u00a0al., 2021). Furthermore, as pointed out by Brown et\u00a0al. (2022), DP can only provide limited guarantees for LMs because DP requires a unified definition for privacy boundaries, which is inherently impossible for natural language data. Furthermore, in a realistic scenario where individuals may practice their Right-To-Be-Forgotten (RTBF) dynamically after model deployment, it is extremely difficult to define a notion of privacy that matches the requirements of each individual beforehand, which is required for training an LM with DP.", "Machine unlearning has received attention as an alternative approach to overcome data privacy issues in machine learning\u00a0(Cao & Yang, 2015; Ginart et\u00a0al., 2019; Bourtoule et\u00a0al., 2021; Graves et\u00a0al., 2021). Several studies attempt to explore machine unlearning for deep neural networks\u00a0(Golatkar et\u00a0al., 2020; Mehta et\u00a0al., 2022). However, they mostly focus on proposing algorithms for image classification models where they aim to forget a whole class; that is, achieve random performance for specific image classes such as \u201ccats\u201d or \u201cships\u201d.We are the first, to the best of our knowledge, to explore unlearning a specific sequence of tokens for LMs which is a quite different set-up from traditional image classification models (\u223csimilar-to\\sim\u223ctens of image classes vs. a sequence of tokens that can each be classified into V\u2208\u211d\u223c50,000\ud835\udc49superscript\u211dsimilar-toabsent50000V\\in\\mathbb{R}^{\\sim 50,000}italic_V \u2208 blackboard_R start_POSTSUPERSCRIPT \u223c 50 , 000 end_POSTSUPERSCRIPT). In this work, we coin this approach as knowledge unlearning since we are more focused on forgetting specific knowledge represented by sequences of tokens.", "Zhou et\u00a0al. (2022) focus on how forgetting can be leveraged to improve the performance of the underlying model. They propose \u201cforget-and-relearn\u201d that unifies existing iterative training algorithms by selectively removing undesirable information and re-learning good features, helping boost performance for the task of image classification and multi-agent emergence communication. The underlying assumption is that it is often easier to define and stop unwanted behavior than to teach good behavior. We also show this phenomenon in Section 4 where we unintentionally find unlearning just a few sequences of tokens sometimes boosts general LM capabilities.", "Previous work that explores to which extent LMs have memorized their training data approach the phenomenon with two different viewpoints. Some work view memorization of LMs simply as a threat to individual privacy\u00a0(Carlini et\u00a0al., 2021; 2022; Jagielski et\u00a0al., 2022) and utilize metrics that quantify how much the LMs are susceptible to adversarial attacks. These metrics are mostly dependent on the specific types of attacks such as the membership inference attack\u00a0(Shokri et\u00a0al., 2017) and measure the privacy risks of LMs by quantifying the success rate of these attacks.", "Another line of work simply quantifies how much knowledge is accumulated and forgotten during pretraining by extracting relational knowledge about the world\u00a0(Petroni et\u00a0al., 2019; Lazaridou et\u00a0al., 2021; Jang et\u00a0al., 2022b; a). This line of work does not view memorization as a negative trait, but as a positive one that can be leveraged to extract world knowledge from its implicit parameters and perform knowledge-intensive tasks such as question answering or training knowledgeable conversation agents.", "Our work is highly related to Jagielski et\u00a0al. (2022)\u2019s work where they also assert that forgetting can be a relaxed version of differential privacy. However, there are two main differences between our work and theirs. First, they only analyze forgetting as a passive form of mitigating privacy, asserting that data seen early in large-scale training obtain privacy benefits, whereas we suggest a more active form of forgetting. Second, they only show analysis results with image classification and audio generation models while we specifically focus on large LMs.", "We propose simply negating the original training objective of minimizing the negative log-likelihood of the token sequences as our main method of knowledge unlearning in LMs. Specifically, given a sequence of tokens \ud835\udc99=(x1,\u2026,xT)\ud835\udc99subscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc47\\bm{x}=(x_{1},...,x_{T})bold_italic_x = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ), our unlearning training objective is simply minimizing the following loss function:", "\u2112UL(f\u03b8,\ud835\udc99)=\u2212\u2211t=1Tlog(1\u2212p\u03b8(xt|x<t))subscript\u2112\ud835\udc48\ud835\udc3fsubscript\ud835\udc53\ud835\udf03\ud835\udc99superscriptsubscript\ud835\udc611\ud835\udc47log1subscript\ud835\udc5d\ud835\udf03conditionalsubscript\ud835\udc65\ud835\udc61subscript\ud835\udc65absent\ud835\udc61\\mathcal{L}_{UL}(f_{\\theta},\\bm{x})=-\\sum_{t=1}^{T}\\text{log}(1-p_{\\theta}(x_{t}|x_{<t}))caligraphic_L start_POSTSUBSCRIPT italic_U italic_L end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT , bold_italic_x ) = - \u2211 start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT log ( 1 - italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT ) )(1)", "where x<tsubscript\ud835\udc65absent\ud835\udc61x_{<t}italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT denotes the token sequence x=(x1,\u2026,xt\u22121)\ud835\udc65subscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc611x=(x_{1},...,x_{t-1})italic_x = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ) and p\u03b8(xt|x<t)subscript\ud835\udc5d\ud835\udf03conditionalsubscript\ud835\udc65\ud835\udc61subscript\ud835\udc65absent\ud835\udc61p_{\\theta}(x_{t}|x_{<t})italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT ) denotes the conditional probability of predicting the next token to be xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT when given x<tsubscript\ud835\udc65absent\ud835\udc61x_{<t}italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT to an LM f\ud835\udc53fitalic_f with parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8.", "Prior work refer to this training objective as unlikelihood training and combines it together with the original loss of minimizing the negative log-likelihood for the final objective of enhancing language modeling quality\u00a0(Welleck et\u00a0al., 2020) and few-shot learning for downstream NLP tasks\u00a0(Tam et\u00a0al., 2021). In contrast, we simply optimize the unlikelihood training objective since we are only concerned with forgetting. While this method seems simple, it is highly effective at forgetting specific token sequences without affecting the overall LM capabilities as shown in Section 4.", "In this subsection, we introduce two metrics we use to quantify the privacy risks given a specific token sequence and how we empirically define the token sequence to be forgotten.", "We first introduce a new metric, EL. Given a sequence of tokens \ud835\udc99=(x1,\u2026,xT)\ud835\udc99subscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc47\\bm{x}=(x_{1},...,x_{T})bold_italic_x = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) and an LM f\ud835\udc53fitalic_f with pre-trained parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8, we define EL to be as follows:", "ELn(\ud835\udc99)=\u2211t=1T\u2212nOverlapn(f\u03b8(x<t),x\u2265t)T\u2212nsubscriptEL\ud835\udc5b\ud835\udc99superscriptsubscript\ud835\udc611\ud835\udc47\ud835\udc5bsubscriptOverlap\ud835\udc5bsubscript\ud835\udc53\ud835\udf03subscript\ud835\udc65absent\ud835\udc61subscript\ud835\udc65absent\ud835\udc61\ud835\udc47\ud835\udc5b\\textsc{EL}_{n}(\\bm{x})=\\dfrac{\\sum_{t=1}^{T-n}\\textsc{Overlap}_{n}(f_{\\theta}(x_{<t}),x_{\\geq t})}{T-n}EL start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( bold_italic_x ) = divide start_ARG \u2211 start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T - italic_n end_POSTSUPERSCRIPT Overlap start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT ) , italic_x start_POSTSUBSCRIPT \u2265 italic_t end_POSTSUBSCRIPT ) end_ARG start_ARG italic_T - italic_n end_ARG(2)Overlapn(\ud835\udc82,\ud835\udc83)=\u2211c\u2208n-grams(\ud835\udc82)\ud835\udfd9{c\u2208n-grams(\ud835\udc83)}|n-grams(\ud835\udc82)|subscriptOverlap\ud835\udc5b\ud835\udc82\ud835\udc83subscript\ud835\udc50\ud835\udc5b-\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5a\ud835\udc60\ud835\udc821\ud835\udc50\ud835\udc5b-\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5a\ud835\udc60\ud835\udc83\ud835\udc5b-\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5a\ud835\udc60\ud835\udc82\\textsc{Overlap}_{n}(\\bm{a},\\bm{b})=\\dfrac{\\sum_{c\\in n\\mbox{-}grams(\\bm{a})}\\mathbbm{1}\\{c\\in n\\mbox{-}grams(\\bm{b})\\}}{|n\\mbox{-}grams(\\bm{a})|}Overlap start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( bold_italic_a , bold_italic_b ) = divide start_ARG \u2211 start_POSTSUBSCRIPT italic_c \u2208 italic_n - italic_g italic_r italic_a italic_m italic_s ( bold_italic_a ) end_POSTSUBSCRIPT blackboard_1 { italic_c \u2208 italic_n - italic_g italic_r italic_a italic_m italic_s ( bold_italic_b ) } end_ARG start_ARG | italic_n - italic_g italic_r italic_a italic_m italic_s ( bold_italic_a ) | end_ARG(3)", "where n-grams()\ud835\udc5b-\ud835\udc54\ud835\udc5f\ud835\udc4e\ud835\udc5a\ud835\udc60n\\mbox{-}grams()italic_n - italic_g italic_r italic_a italic_m italic_s ( ) denotes the list of n\ud835\udc5bnitalic_n-grams in the given token sequence and f\u03b8(x<t)subscript\ud835\udc53\ud835\udf03subscript\ud835\udc65absent\ud835\udc61f_{\\theta}(x_{<t})italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT ) denotes the output token sequences from the LM f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT when given x<tsubscript\ud835\udc65absent\ud835\udc61x_{<t}italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT as input that can have max lengths |x\u2265t|subscript\ud835\udc65absent\ud835\udc61|x_{\\geq t}|| italic_x start_POSTSUBSCRIPT \u2265 italic_t end_POSTSUBSCRIPT | but may be shorter when the EOS (end-of-sequence) token is generated beforehand.", "The process of varying the prefix length |x<t|subscript\ud835\udc65absent\ud835\udc61|x_{<t}|| italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT | can be seen as varying the strength of adversarial attacks. This is based on the assumption that the more prior information is provided about the target token sequence, the easier the LM will be able to extract it. Overall, EL can be seen as estimating the general extraction likelihood since we are measuring the average success rate of varying extraction attacks quantified via getting the n-gram overlap of generated and target token sequences. While previous metrics quantifying the privacy risks of LMs are dependent on specific adversarial attacks, this characteristic of EL allows it to quantify the general likelihood of extraction without any dependency on specific extraction attacks.", "We regard n\ud835\udc5bnitalic_n to be a hyper-parameter that can be varied depending on the stringency of privacy standards. The higher n\ud835\udc5bnitalic_n is set, the stricter we set the standard for a successful extraction attack.", "We define Memorization Accuracy (MA) as follows:", "MA(\ud835\udc99)=\u2211t=1T\u22121\ud835\udfd9{\u00a0argmax(p\u03b8(\u22c5|x<t))=xt}T\u22121\\textsc{MA}(\\bm{x})=\\dfrac{\\sum_{t=1}^{T-1}\\mathbbm{1}\\{\\text{argmax}(p_{\\theta}(\\cdot|x_{<t}))=x_{t}\\}}{T-1}MA ( bold_italic_x ) = divide start_ARG \u2211 start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T - 1 end_POSTSUPERSCRIPT blackboard_1 { argmax ( italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( \u22c5 | italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT ) ) = italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } end_ARG start_ARG italic_T - 1 end_ARG(4)", "MA quantifies how much f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT has memorized the given token sequences and was proposed by Tirumala et\u00a0al. (2022) to analyze the training dynamics of large LMs.", "By utilizing both ELnsubscriptEL\ud835\udc5b\\textsc{EL}_{n}EL start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and MA, we empirically define a specific token sequence \ud835\udc99\ud835\udc99\\bm{x}bold_italic_x to be forgotten and is no longer susceptible to extraction attacks when the following conditions are met:", "ELn(\ud835\udc99)\u22641|D\u2032|\u2211\ud835\udc99\u2032\u2208D\u2032ELn(\ud835\udc99\u2032)\u00a0and\u00a0MA(\ud835\udc99)\u22641|D\u2032|\u2211\ud835\udc99\u2032\u2208D\u2032MA(\ud835\udc99\u2032)subscriptEL\ud835\udc5b\ud835\udc991superscript\ud835\udc37\u2032subscriptsuperscript\ud835\udc99\u2032superscript\ud835\udc37\u2032subscriptEL\ud835\udc5bsuperscript\ud835\udc99\u2032\u00a0and\u00a0MA\ud835\udc991superscript\ud835\udc37\u2032subscriptsuperscript\ud835\udc99\u2032superscript\ud835\udc37\u2032MAsuperscript\ud835\udc99\u2032\\textsc{EL}_{n}(\\bm{x})\\leq\\dfrac{1}{|D^{\\prime}|}\\sum_{\\bm{x}^{\\prime}\\in D^{\\prime}}\\textsc{EL}_{n}(\\bm{x}^{\\prime})\\text{ and }\\textsc{MA}(\\bm{x})\\leq\\dfrac{1}{|D^{\\prime}|}\\sum_{\\bm{x}^{\\prime}\\in D^{\\prime}}\\textsc{MA}(\\bm{x}^{\\prime})EL start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( bold_italic_x ) \u2264 divide start_ARG 1 end_ARG start_ARG | italic_D start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT | end_ARG \u2211 start_POSTSUBSCRIPT bold_italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 italic_D start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT EL start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) and smallcaps_MA ( bold_italic_x ) \u2264 divide start_ARG 1 end_ARG start_ARG | italic_D start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT | end_ARG \u2211 start_POSTSUBSCRIPT bold_italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 italic_D start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT MA ( bold_italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT )(5)", "where D\u2032superscript\ud835\udc37\u2032D^{\\prime}italic_D start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT represents a validation corpora not seen during training. In other words, we define \ud835\udc99\ud835\udc99\\bm{x}bold_italic_x to be forgotten when the ELnsubscriptEL\ud835\udc5b\\textsc{EL}_{n}EL start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT(\ud835\udc99\ud835\udc99\\bm{x}bold_italic_x) and MA(\ud835\udc99\ud835\udc99\\bm{x}bold_italic_x) reach a value that is lower than the average ELnsubscriptEL\ud835\udc5b\\textsc{EL}_{n}EL start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and MA on token sequences that were not seen during training.", "For the experiments, we use the GPT-Neo (125M, 1.3B, 2.7B) LMs\u00a0(Black et\u00a0al., 2021) initially pretrained on all of the Pile corpora (825GB)\u00a0(Gao et\u00a0al., 2020), and the OPT (125M, 1.3B, 2.7B) LMs\u00a0(Zhang et\u00a0al., 2022), pretrained on a subset of the deduplicated version of the Pile as well as other corpora from different domains. For the experiments, we perform unlearning the GPT-Neo LMs and quantify the privacy risks of the target data compared to the OPT LMs to measure how effective our proposed approach is in contrast to deduplicating the training corpora before pretraining the underlying LM\u00a0Kandpal et\u00a0al. (2022). We do not use the exact LMs from Kandpal et\u00a0al. (2022) because the LMs were not open-sourced, and thus use the OPT LMs instead.", "For the actual target data used to quantify the privacy risks of the LMs, we sample instances from the Training Data Extraction Challenge\u00a0111https://github.com/google-research/lm-extraction-benchmark where 15,000 examples (each are 200 token sequences long) from 16 different domains of the Pile corpora that are identified to be somewhat easy-to-extract are provided. For our experiments, we randomly sample s samples from the 15,000 examples and make the underlying LM forget the s samples at once. As a default, we show the average results of 5 random samplings of s samples for all of our experimental settings. We only provide the average of the 5 samplings and do not separately report the standard deviation. Instead, we provide the results of each individual run in Appendix A.", "Guaranteeing stronger privacy for LMs may become meaningless if it requires sacrificing their original capabilities.Thus, while quantifying the privacy risks of LMs, we also quantify the original LM capabilities by evaluating the LMs on 9 different tasks: Hellaswag\u00a0(Zellers et\u00a0al., 2019) and Lambada\u00a0(Paperno et\u00a0al., 2016) benchmarks to measure linguistic reasoning abilities, Winogrande\u00a0(Sakaguchi et\u00a0al., 2021) and COPA\u00a0(Gordon et\u00a0al., 2012) to measure commonsense reasoning abilities, and ARC-Easy\u00a0(Clark et\u00a0al., 2018), ARC-Challenge\u00a0(Clark et\u00a0al., 2018), Piqa\u00a0(Bisk et\u00a0al., 2020), MathQA\u00a0(Amini et\u00a0al., 2019), PubmedQA\u00a0(Jin et\u00a0al., 2019) benchmarks to measure the scientific reasoning abilities. We use the test set for Lambada and the validation set for the rest of the benchmarks. We also show the results of measuring the perplexity on the validation corpora of Pile and Wikitext in Appendix B. We do not include measuring perplexity as one of the main evaluations because perplexity might not be the most suitable metric for quantifying general LM performance, especially in the case of unlearning\u00a0222We give further explanation of why we think perplexity is not a good proxy for quantifying the general LM capability in our scenario in Appendix B..", "For the learning rate, we set it to 5e-5.We show the effect of varying learning rates in Appendix D. We use a constant learning rate scheduling throughout the run. We fix the global batch size to be the same as s (how many samples are forgotten at once) because having global batch sizes smaller than s\ud835\udc60sitalic_s proved to degrade general LM capabilities\u00a0333In Section 4.3, We show that s\ud835\udc60sitalic_s plays a critical role in determining how much the unlearning will degrade in general capabilities of the LM since s=128\ud835\udc60128s=128italic_s = 128 shows to result in much degradation. Method to mitigate this is proposed in Section 4.3 as well.. For ELnsubscriptEL\ud835\udc5b\\textsc{EL}_{n}EL start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, we set n=10 which means EL measures the extraction likelihood of extracting n consecutive tokens of varying extraction attack. For calculating EL10subscriptEL10\\textsc{EL}_{10}EL start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT and MA, we use a na\u00efve greedy decoding strategy. We set both the dropout and weight decay rates to 0. Lastly, while we provide a guideline of empirically deciding a single token sequence to be forgotten in Section 3.2, for considering a chunk of s\ud835\udc60sitalic_s token sequences to be forgotten, we use the average EL10subscriptEL10\\textsc{EL}_{10}EL start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT and MA as an approximation of the individual EL10subscriptEL10\\textsc{EL}_{10}EL start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT and MA.", "First, we show how we get the Forgetting Threshold for EL10subscriptEL10\\textsc{EL}_{10}EL start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT and MA, the values where we consider the token sequence to be forgotten and unsusceptible from extraction attacks, for all model sizes of GPT-Neo LMs in Table 1. For D\u2032superscript\ud835\udc37\u2032D^{\\prime}italic_D start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT, we perform weighted sampling (same domain distribution as the Pile training corpora) of 10,000 instances each with token lengths 200 from the Pile validation corpora and measure the average EL10subscriptEL10\\textsc{EL}_{10}EL start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT and MA (Equation 5), which are empirically set as the Forgetting Threshold values.", "Table 2 shows the main results of performing unlearning on LMs of varying sizes. In the table, Neo denotes the GPT-Neo LMs. Neo + UL denotes applying unlearning on the GPT-Neo LM until it reaches lower EL10subscriptEL10\\textsc{EL}_{10}EL start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT and MA than OPT. Lastly, Neo + UL+superscriptNeo + UL\\textsc{Neo + UL}^{+}Neo + UL start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT denotes performing unlearning on the GPT-Neo until the target token sequences are forgotten (EL10subscriptEL10\\textsc{EL}_{10}EL start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT and MA value reach below the Forgetting Threshold). While we provide the average performances of the 5 random samplings in Table 2, we provide each individual runs in Appendix A for reference.", "We highlight four main points regarding the results. (1) OPT LMs show a much lower EL10subscriptEL10\\textsc{EL}_{10}EL start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT and MA than GPT-Neo LMs, confirming that deduplicating the pretraining corpora is indeed helpful for mitigating privacy risks. (2) Neo + UL+superscriptNeo + UL\\textsc{Neo + UL}^{+}Neo + UL start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT results in degradation of average LM performance for the 125M LM while retaining most of its previous capabilities for the 1.3B and 2.7B LMs. Interestingly, for some benchmarks such as Lambada and ARC-Challenge, it actually results in a boost of performance. (3) While the LMs scale to larger sizes, it takes fewer epochs for the target sequences to be forgotten. Together with (2), this implies that larger LMs are strong unlearners. (4) While Neo + UL+superscriptNeo + UL\\textsc{Neo + UL}^{+}Neo + UL start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT provides a stronger privacy guarantee than OPT without sacrificing average LM performance from Neo, it is much more computationally efficient (3,500,000x) than re-training the underlying LM, which is required for all data preprocessing approaches\u00a0444Computational efficiency is measured via FLOPs which is calculated by (6 x Total Training Tokens x Parameter Size) as in Brown et\u00a0al. (2020). FLOPs for OPT LMs were estimated using information from Zhang et\u00a0al. (2022). We provide the FLOPs for the methods in Appendix C.. This characteristic makes knowledge unlearning very advantageous in scenarios where people might practice their RTBF and thus dynamic forgetting of specific token sequences are required.", "Overall, results show unlearning to be an effective approach to providing a stronger empirical privacy guarantee than existing LMs measured via EL10subscriptEL10\\textsc{EL}_{10}EL start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT and MA while retaining and sometimes even improving general LM capabilities.", "We show the effect of varying s\ud835\udc60sitalic_s (the # of data instances to be forgotten at once) in Figure 2a across model scales. We denote this approach as batch unlearning. As shown by the s=128\ud835\udc60128s=128italic_s = 128 results, it is harder to forget more samples at once, resulting in substantial degradation of average LM performance regardless of how large the LM is. Since s\u226432\ud835\udc6032s\\leq 32italic_s \u2264 32 does not show much degradation, we explore if sequentially unlearning can be a solution. In Figure 2b, we show the result of dividing the 128 samples into 4 chunks of 32 and performing sequential unlearning; we unlearn each chunk at a time until the chunk reaches the forgetting threshold. Surprisingly, as shown by the performance gap at s=128\ud835\udc60128s=128italic_s = 128 between the dotted lines (the s=128\ud835\udc60128s=128italic_s = 128 performance of Figure 2a) and straight lines, the end result is vastly different even though exactly the same instances were forgotten. Sequential unlearning shows almost no degradation of average LM performance. In Appendix G, we show that chunks once forgotten stay forgotten and that later chunks are forgotten much faster compared to the initial chunk. This result hints at the generalization of unlearning, which we do not further explore in the scope of this work. The result also suggests that knowledge unlearning can be continually applied to LMs when needed.", "To show exactly what happens to the LM during knowledge unlearning, we show how the performance of each of the LM benchmarks changes as we perform 10 runs of unlearning to the GPT-Neo (1.3B) model (each run with s=1\ud835\udc601s=1italic_s = 1) in Figure 3. As shown in the figure, the LM performance for each benchmark varies tremendously on which sample is chosen to be forgotten. Furthermore, the ending time of each run is different, indicating that some samples are forgotten faster than others.", "To provide a better intuition of exactly how knowledge unlearning guarantees privacy, we perform an extraction attack with a token sequence sample in Table 3 where we show the model-generated text from the extraction attack before and after applying knowledge unlearning. While the extraction attack is extremely successful at extracting the rest of the suffix before unlearning (100% of the token sequence), only a small portion (\u223csimilar-to\\sim\u223c3% of the token sequence) of the suffix is extracted after applying unlearning.", "To measure why some instances are harder to forget, we perform 5 random samplings of s=8\ud835\udc608s=8italic_s = 8 from 8 different domains from the Training Data Extraction Challenge555https://github.com/google-research/lm-extraction-benchmark and perform unlearning on the GPT-Neo 1.3B LM. We also show the results of each individual run in Appendix A. As shown in Table 4, despite undergoing the same number of token updates (10 epochs of unlearning), different domains result in vastly different outcomes; enron emails results in the average LM performance degradation of only -0.4% while uspto backgrounds results in -4.5% degradation. Furthermore, the final EL10subscriptEL10\\textsc{EL}_{10}EL start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT varies depending on the domain, suggesting that some domains (e.g., Freelaw) are harder to forget than others. Lastly, domains that are more structured, which means the data consists of some kind of patterns such as a list of emails (enron emails) or code (github (code)), seem to result in less degradation of LM performance in contrast to domains that are more unstructured, which means the data consist of mostly raw English text such as a review for journal submission (pubmed central). We provide examples from each domain in Appendix E.", "In this paper, we propose knowledge unlearning as a method for mitigating privacy risks in LMs that is not only orders of magnitude more efficient than previous methods, but also provides a stronger empirical privacy guarantee with little to no degradation of general LM capabilities measured by evaluating on 9 common LM benchmarks. As large LMs expand their use cases, potentially affecting the daily lives of people, the research community should make sure that the privacy of individuals is not violated intentionally or unintentionally by the knowledge stored in the implicit parameters of large LMs. Since it is inherently impossible to prevent and predict all future privacy concerns prior to pretraining the LM, we suggest the community consider knowledge unlearning for ensuring privacy upon individuals\u2019 requests post hoc pretraining\u00a0666We provide some limitations of our work in Appendix H..", "We thank Hanseok Oh, Minsu Kim, James Thorne, and Hyunji Lee for useful discussion and feedback while preparing the paper draft. This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program(KAIST))."], "figure_types": {"91fb2254c5942048425e642c8a6c8d400006150e/10-Table4-1.png": "table", "91fb2254c5942048425e642c8a6c8d400006150e/15-Figure4-1.png": "plot", "91fb2254c5942048425e642c8a6c8d400006150e/16-Figure5-1.png": "plot", "91fb2254c5942048425e642c8a6c8d400006150e/17-Table5-1.png": "table", "91fb2254c5942048425e642c8a6c8d400006150e/18-Table6-1.png": "table", "91fb2254c5942048425e642c8a6c8d400006150e/18-Table7-1.png": "table", "91fb2254c5942048425e642c8a6c8d400006150e/19-Table8-1.png": "table", "91fb2254c5942048425e642c8a6c8d400006150e/19-Table9-1.png": "table", "91fb2254c5942048425e642c8a6c8d400006150e/2-Figure1-1.png": "schematic", "91fb2254c5942048425e642c8a6c8d400006150e/20-Table10-1.png": "table", "91fb2254c5942048425e642c8a6c8d400006150e/21-Table11-1.png": "table", "91fb2254c5942048425e642c8a6c8d400006150e/21-Table12-1.png": "table", "91fb2254c5942048425e642c8a6c8d400006150e/7-Table1-1.png": "table", "91fb2254c5942048425e642c8a6c8d400006150e/7-Table2-1.png": "table", "91fb2254c5942048425e642c8a6c8d400006150e/8-Figure2-1.png": "plot", "91fb2254c5942048425e642c8a6c8d400006150e/9-Figure3-1.png": "plot", "91fb2254c5942048425e642c8a6c8d400006150e/9-Table3-1.png": "table"}}, "2007.08294": {"paper_id": "paper_162", "title": "Self-supervised auxiliary learning with meta-paths for heterogeneous graphs", "arxiv_url": "https://arxiv.org/abs/2007.08294", "s2orc_url": "https://www.semanticscholar.org/paper/4ccdd1fe6cc9c896e910582bea2c33c19317c5ca", "all_figures_tables": {"4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/12-Table4-1.png": "Table 4: Link prediction performance (AUC-score) of GNNs", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/13-Table5-1.png": "Table 5: Datasets on heterogeneous graphs.", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/13-Table6-1.png": "Table 6: The average of the task-specific weighted loss on Last-FM and Book-Crossing datasets.", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/14-Figure4-1.png": "Figure 4: Weightinf function V(\u00b7) learnt by SELAR+HintNet on Last-FM on GCN, GAT, GIN and SGC.", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/4-Figure1-1.png": "Figure 1: The SELAR framework for self-supervised auxiliary learning. Our framework learns how to balance (or softly select) auxiliary tasks to improve the primary task via meta-learning. In this paper, the primary task is link prediction (or node classification) and auxiliary tasks are meta-path predictions to capture rich information of a heterogeneous graph.", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/5-Figure2-1.png": "Figure 2: HintNet helps the learner network to learn even with challenging and remotely relevant auxiliary tasks. As our framework selects effective auxiliary tasks, our framework with HintNet learns VH to decide to use hint y\u0302H in the orange line from HintNet or not via meta-learning. y\u0302 in the blue line denotes the prediction from the learner network.", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/7-Table1-1.png": "Table 1: Link prediction performance (AUC) of GNNs trained by various learning strategies.", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/7-Table2-1.png": "Table 2: Node classification performance (F1-score) of GNNs trained by various learning schemes.", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/8-Figure3-1.png": "Figure 3: Weighting function V(\u00b7) learnt by SELAR+HintNet. V(\u00b7) gives overall high weights to the primary task positive samples (red) in (a). V(\u00b7) decreases the weights of easy samples with a loss ranged from 0 to 1. In (b), the adjusted cross entropy, i.e., \u2212V(\u03be; \u0398) log(y\u0302), by V(\u00b7) acts like the focal loss, which focuses on hard examples by \u2212(1\u2212 pt)\u03b3 log(y\u0302).", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/8-Table3-1.png": "Table 3: Comparison between 1-fold and 3-fold as meta-data on Last-FM datasets."}, "referred_figures_tables": [["4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/5-Figure2-1.png"], ["4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/5-Figure2-1.png"], ["4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/5-Figure2-1.png"]], "question_id": [4, 5, 8], "question": ["What do challenging auxiliary tasks mean?", " How can Hint Network help with challenging auxiliary tasks?", "What is a meta-path? Please explain with examples."], "question_section": ["Introduction", "Introduction", "Method"], "question_trigger_sentence": ["We develop Hint Network that helps the learner network to benefit from challenging auxiliary tasks.", "We develop Hint Network that helps the learner network to benefit from challenging auxiliary tasks.", "The meta-paths capture diverse and meaningful relations between nodes on heterogeneous graphs [46]."], "question_type": ["Testing question", "Deep/complex question", "Shallow question"], "evidential_info": [[{"context": "Meta-path prediction is generally more challenging than link prediction and node classification since it requires the understanding of long-range relations across heterogeneous nodes. The meta-path prediction gets more difficult when mini-batch training is inevitable due to the size of datasets or models. Within a mini-batch, important nodes and edges for meta-paths are not available. Also, a small learner network, e.g., two-layer GNNs, with a limited receptive field, inherently cannot capture long-range relations. The challenges can hinder representation learning and damage the generalization of the primary task. We proposed a Hint Network (HintNet) which makes the challenge tasks more solvable by correcting the answer with more information at the learner\u2019s need. Specifically, in our experiments, the HintNet corrects the answer of the learner with its own answer from the augmented graph with hub nodes, see Fig. 2.", "rationale": "Meta-path prediction is generally more challenging than link prediction and node classification since it requires the understanding of long-range relations across heterogeneous nodes. The meta-path prediction gets more difficult when mini-batch training is inevitable due to the size of datasets or models. Within a mini-batch, important nodes and edges for meta-paths are not available. Also, a small learner network, e.g., two-layer GNNs, with a limited receptive field, inherently cannot capture long-range relations."}], [{"context": "The amount of help (correction) by HintNet is optimized maximizing the learner\u2019s gain.Let \\mathcal{V}_{H}(\\cdot) and \\Theta_{H} be a weight function to determine the amount of hint and its parameters which are optimized by meta-learning. Then, our formulation with HintNet is given as\\displaystyle\\min_{\\mathbf{w},\\Theta}\\sum_{i=1}^{M_{0}}\\frac{1}{M_{0}}\\ell^{0}(y_{i}^{(0,meta)},f(x_{i}^{(0,meta)};\\mathbf{w}^{\\ast}(\\Theta,\\Theta_{H})))(10)\\displaystyle\\text{s.t. }\\mathbf{w}^{\\ast}(\\Theta)=\\operatorname*{\\arg\\!\\min}_{\\mathbf{w}}\\sum_{t=0}^{T}\\sum_{i=1}^{N_{t}}\\frac{1}{N_{t}}\\mathcal{V}(\\xi^{(t,train)}_{i},\\ell^{t};\\Theta)\\ell^{t}(y_{i}^{(t,train)},\\hat{y}_{i}^{(t,train)}(\\Theta_{H})),(11)where \\hat{y}_{i}^{(t,train)}(\\Theta_{H}) denotes the convex combination of the learner\u2019s answer and HintNet\u2019s answer, i.e., \\mathcal{V}_{H}(\\xi^{(t,train)}_{i};\\Theta_{H})f^{t}(x_{i}^{(t,train)};\\mathbf{w})+(1-\\mathcal{V}_{H}(\\xi^{(t,train)}_{i};\\Theta_{H}))f_{H}^{t}(x_{i}^{(t,train)};\\mathbf{w}). The sample embedding is\\xi^{(t,train)}_{i}=\\left[\\ell^{t};\\ell^{t}_{H};e_{t};y_{i}^{(t,train)}\\right]\\in\\textbf{R}^{T+3}.", "rationale": "The amount of help (correction) by HintNet is optimized maximizing the learner\u2019s gain.Let \\mathcal{V}_{H}(\\cdot) and \\Theta_{H} be a weight function to determine the amount of hint and its parameters which are optimized by meta-learning."}, {"context": "Meta-path prediction is generally more challenging than link prediction and node classification since it requires the understanding of long-range relations across heterogeneous nodes. The meta-path prediction gets more difficult when mini-batch training is inevitable due to the size of datasets or models. Within a mini-batch, important nodes and edges for meta-paths are not available. Also, a small learner network, e.g., two-layer GNNs, with a limited receptive field, inherently cannot capture long-range relations. The challenges can hinder representation learning and damage the generalization of the primary task. We proposed a Hint Network (HintNet) which makes the challenge tasks more solvable by correcting the answer with more information at the learner\u2019s need. Specifically, in our experiments, the HintNet corrects the answer of the learner with its own answer from the augmented graph with hub nodes, see Fig. 2.", "rationale": "We proposed a Hint Network (HintNet) which makes the challenge tasks more solvable by correcting the answer with more information at the learner\u2019s need. Specifically, in our experiments, the HintNet corrects the answer of the learner with its own answer from the augmented graph with hub nodes, see Fig. 2."}], [{"context": "Meta-Path\u00a0HAN ; sun2011pathsim  is a path on a heterogeneous graph G that a sequence of nodes connected with heterogeneous edges, i.e., {v}_{1}\\xrightarrow{t_{1}}{v}_{2}\\xrightarrow{t_{2}}\\ldots\\xrightarrow{t_{l}}{v}_{l+1},where t_{l}\\in\\mathcal{T}^{e} denotes an l-th edge type of the meta-path.The meta-path can be viewed as a composite relation R=t_{1}\\circ t_{2}\\ldots\\circ t_{l} between node {v}_{1} and {v}_{l+1}, where R_{1}\\circ R_{2} denotes the composition of relation R_{1} and R_{2}.The definition of meta-path generalizes multi-hop connections and is shown to be useful to analyze heterogeneous graphs.For instance, in Book-Crossing dataset, \u2018user-item-written.series-item-user\u2019 indicates that a meta-path that connects users who like the same book series.", "rationale": "The definition of meta-path generalizes multi-hop connections and is shown to be useful to analyze heterogeneous graphs.For instance, in Book-Crossing dataset, \u2018user-item-written.series-item-user\u2019 indicates that a meta-path that connects users who like the same book series."}, {"context": "Meta-path prediction is similar to link prediction but meta-paths allow heterogeneous composite relations.The meta-path prediction can be achieved in the same manner as link prediction.If two nodes u and v are connected by a meta-path p with the heterogeneous edges (t_{1},t_{2},\\ldots t_{\\ell}), then y_{u,v}^{p}=1, otherwise y_{u,v}^{p}=0. The labels can be generated from a heterogeneous graph without any manual labeling.They can be obtained by A_{p}=A_{t_{l}}\\ldots A_{t_{2}}A_{t_{1}}, where A_{t} is the adjacency matrix of edge type t. The binarized value at (u,v) in A_{p} indicates whether u and v are connected with the meta-path p.In this paper, we use meta-path prediction as a self-supervised auxiliary task.", "rationale": "Meta-path prediction is generally more challenging than link prediction and node classification since it requires the understanding of long-range relations across heterogeneous nodes."}, {"context": "Meta-path prediction is generally more challenging than link prediction and node classification since it requires the understanding of long-range relations across heterogeneous nodes.The meta-path prediction gets more difficult when mini-batch training is inevitable due to the size of datasets or models.Within a mini-batch, important nodes and edges for meta-paths are not available.Also, a small learner network, e.g., two-layer GNNs, with a limited receptive field, inherently cannot capture long-range relations.The challenges can hinder representation learning and damage the generalization of the primary task.We proposed a Hint Network (HintNet) which makes the challenge tasks more solvable by correcting the answer with more information at the learner\u2019s need.Specifically, in our experiments, the HintNet corrects the answer of the learner with its own answer from the augmented graph with hub nodes, see Fig. \u00a02.", "rationale": "Meta-path prediction is similar to link prediction but meta-paths allow heterogeneous composite relations.The meta-path prediction can be achieved in the same manner as link prediction."}]], "composition": ["Challenging auxiliary tasks refer to tasks that are difficult for the model to learn, which can negatively impact the performance of the primary task. In the case of meta-path prediction, it is considered more challenging than link prediction and node classification because it requires the understanding of long-range relations across heterogeneous nodes. The task becomes even more difficult when mini-batch training is necessary due to the large size of datasets or models, as important nodes and edges for meta-paths may not be available within a mini-batch.", "The HintNet is designed to make challenging tasks more solvable by providing the model with additional information at the point of need, specifically by correcting the answer of the learner with its own answer from an augmented graph with hub nodes. The amount of help (correction) provided by the HintNet is optimized to maximize the learner's gain, and the help is determined by weighting functions for HintNet, which are optimized by meta-learning.", "A meta-path is a sequence of node types and edge types in a graph that describes a specific type of relationship between nodes. An example is in a recommendation system, a meta-path could be \"user-item-written.series-item-user\" which describes a relationship between users who like the same book series."], "Is_figure_in_evidence": [true, true, true], "Is_table_in_evidence": [false, false, false], "question_key": ["520", "521", "524"], "passages": ["Graph neural networks\u00a0hamilton2017representation  have been proven effective to learn representations for various tasks such as node classification GCN , link prediction link , and graph classification DiffPool .The powerful representation yields state-of-the-art performance in a variety of applications including social network analysis\u00a0social1 , citation network analysis\u00a0GCN ,visual understanding\u00a0yang2018graph ; HwangRTKCS18 , recommender systems\u00a0recom1 , physics \u00a0battaglia2016interaction , and drug discovery\u00a0wu2018moleculenet .Despite the wide operating range of graph neural networks, employing auxiliary (pre-text) tasks has been less explored for further improving graph representation learning.", "Pre-training with an auxiliary task is a common technique for deep neural networks.Indeed, it is the de facto standard step in natural language processing and computer vision to learn a powerful backbone networks such as BERT\u00a0devlin2018bert  and ResNet\u00a0he2016deep  leveraging large datasets such as BooksCorpus zhu2015aligning , English Wikipedia, and ImageNet\u00a0deng2009imagenet .The models trained on the auxiliary task are often beneficial for the primary (target) task of interest.Despite the success of pre-training, few approaches have been generalized to graph-structured data due to their fundamental challenges.First, graph structure (e.g., the number of nodes/edges, and diameter) and its meaning can significantly differ between domains. So the model trained on an auxiliary task can harm generalization on the primary task, i.e., negative transfer\u00a0pan2009survey .Also, many graph neural networks are transductive approaches. This often makes transfer learning between datasets inherently infeasible.So, pre-training on the target dataset has been proposed using auxiliary tasks: graph kernel \u00a0navarin2018pre , graph reconstruction\u00a0zhang2020graph , and attribute masking \u00a0hu2020strategies . These assume that the auxiliary tasks for pre-training are carefully selected with substantial domain knowledge and expertise in graph characteristics to assist the primary task.Since most graph neural networks operate on homogeneous graphs, which have a single type of nodes and edges, the previous pre-training/auxiliary tasksare not specifically designed for heterogeneous graphs, which have multiple types of nodes and edges.Heterogeneous graphs commonly occur in real-world applications, for instance, a music dataset has multiple types of nodes (e.g., user, song, artist) and multiple types of relations (e.g., user-artist, song-film, song-instrument).", "In this paper, we proposed a framework to train a graph neural networks with automatically selected auxiliary self-supervised tasks which assist the target task without additional data and labels.Our approach first generates meta-paths from heterogeneous graphs without manual labeling and train a model with meta-path prediction to assist the primary task such as link prediction and node classification. This can be formulated as a meta-learning problem. Furthermore, our method can be adopted to existing GNNs in a plug-in manner, enhancing the model performance.", "Our contribution is threefold: (i) We propose a self-supervised learning method on a heterogeneous graph via meta-path prediction without additional data. (ii) Our framework automatically selects meta-paths (auxiliary tasks) to assist the primary task via meta-learning.(iii) We develop Hint Network that helps the learner network to benefit from challenging auxiliary tasks.To the best of our knowledge, this is the first auxiliary task with meta-paths specifically designed for leveraging heterogeneous graph structure.Our experiment shows that meta-path prediction improves the representational power and the gain can be further improved to explicitly optimize the auxiliary tasks for the primary task via meta-learning and the Hint Network, built on various state-of-the-art GNNs.", "Graph Neural Networkshave provided promising results for various tasks\u00a0GCN ; social1 ; yang2018graph ; HwangRTKCS18 ; recom1 ; battaglia2016interaction ; wu2018moleculenet .Bruna et al.\u00a0spectral  proposed a neural network that performs convolution on the graph domain using the Fourier basis from spectral graph theory. In contrast, non-spectral (spatial) approaches have been developed\u00a0GCN ; GAT ; graphsage2017 ; xu2018powerful ; wu2019simplifying ; yun2019graph ; MengAKFS18 .Inspired by self-supervised learning\u00a0ss1 ; ss2 ; ss3 ; ss4  and pre-training\u00a0devlin2018bert ; donahue2014decaf  in computer vision and natural language processing, pre-training for GNNs has been recently proposed\u00a0navarin2018pre ; hu2020strategies .Recent works show promising results that self-supervised learning can be effective for GNNs\u00a0navarin2018pre ; zhang2020graph ; hu2020strategies ; you2020does .Hu et al.\u00a0hu2020strategies  have introduced several strategies for pre-training GNNs such as attribute masking and context prediction.Separated from the pre-training and fine-tuning strategy, you2020does  has studied multi-task learning and analyzed why the pretext tasks are useful for GNNs.However, one problem with both pre-training and multi-task learning strategies is that all the auxiliary tasks are not beneficial for the downstream applications.So, we studied auxiliary learning for GNNs that explicitly focuses on the primary task.", "Auxiliary Learning is a learning strategy to employ auxiliary tasks to assist the primary task.It is similar to multi-task learning, but auxiliary learning cares only the performance of the primary task.A number of auxiliary learning methods are proposed in a wide range of tasks\u00a0auxlearning1 ; Unaux1 ; Unaux2 .AC-GAN\u00a0acgan  proposed an auxiliary classifier for generative models.Recently, Meta-Auxiliary Learning\u00a0maxl  proposes an elegant solution to generate new auxiliary tasks by collapsing existing classes.However, it cannot be applicable to some tasks such as link prediction which has only one positive class.Our approach generates meta-paths on heterogeneous graphs to make new labels and trains models to predict meta-paths as auxiliary tasks.", "Meta-learning aims at learning to learn models efficiently and effectively, and generalizes the learning strategy to new tasks.Meta-learning includes black-box methods to approximate gradients without any information about models\u00a0santoro2016meta ; DBLP:conf/iclr/RaviL17 ,optimization-based methods to learn an optimal initialization for adapting new tasks\u00a0MAML ; han2018coteaching ; layerwise ,learning loss functions\u00a0han2018coteaching ; learnwhere  andmetric-learning or non-parametric methods for few-shot learning Koch2015SiameseNN ; DBLP:conf/nips/SnellSZ17 ; DBLP:conf/cvpr/SungYZXTH18 .In contrast to classical learning algorithms that generalize across samples, meta-learning generalizes across tasks.In this paper, we use meta-learning to learn a concept across tasks and transfer the knowledge from auxiliary tasks to the primary task.", "The goal of our framework is to learn with multiple auxiliary tasks to improve the performance of the primary task.In this work, we demonstrate our framework with meta-path predictions as auxiliary tasks. But our framework could be extended to include other auxiliary tasks.The meta-paths capture diverse and meaningful relations between nodes on heterogeneous graphs\u00a0HAN .However, learning with auxiliary tasks has multiple challenges: identifying useful auxiliary tasks, balancing the auxiliary tasks with the primary task, and converting challenging auxiliary tasks into solvable (and relevant) tasks.To address the challenges, we propose SELf-supervised Auxiliary LeaRning (SELAR).Our framework consists of two main components:1) learning weight functions to softly select auxiliary tasks and balance them with the primary task via meta-learning, and2) learning Hint Networks to convert challenging auxiliary tasks into more relevant and solvable tasks to the primary task learner.", "Most existing graph neural networks have been studied focusing on homogeneous graphs that have a single type of nodes and edges.However, in real-world applications, heterogeneous graphs\u00a0heterogeneous , which have multiple types of nodes and edges, commonly occur.Learning models on the heterogeneous graphs requires different considerations to effectively represent their node and edge heterogeneity.", "Heterogeneous graph\u00a0shi2016survey . Let G=(V,E)\ud835\udc3a\ud835\udc49\ud835\udc38G=(V,E)italic_G = ( italic_V , italic_E ) be a graph with a set of nodes V\ud835\udc49Vitalic_V and edges E\ud835\udc38Eitalic_E.A heterogeneous graph is a graph equipped with a node type mapping function fv:V\u2192\ud835\udcafv:subscript\ud835\udc53\ud835\udc63\u2192\ud835\udc49superscript\ud835\udcaf\ud835\udc63f_{v}:V\\rightarrow\\mathcal{T}^{v}italic_f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT : italic_V \u2192 caligraphic_T start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT and an edge type mapping function fe:E\u2192\ud835\udcafe:subscript\ud835\udc53\ud835\udc52\u2192\ud835\udc38superscript\ud835\udcaf\ud835\udc52f_{e}:E\\rightarrow\\mathcal{T}^{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT : italic_E \u2192 caligraphic_T start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT, where \ud835\udcafvsuperscript\ud835\udcaf\ud835\udc63\\mathcal{T}^{v}caligraphic_T start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT is a set of node types and \ud835\udcafesuperscript\ud835\udcaf\ud835\udc52\\mathcal{T}^{e}caligraphic_T start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT is a set of edge types.Each node vi\u2208Vsubscript\ud835\udc63\ud835\udc56\ud835\udc49v_{i}\\in Vitalic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 italic_V (and edge eij\u2208Esubscript\ud835\udc52\ud835\udc56\ud835\udc57\ud835\udc38e_{ij}\\in Eitalic_e start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT \u2208 italic_E resp.) has one node type, i.e., fv(vi)\u2208\ud835\udcafvsubscript\ud835\udc53\ud835\udc63subscript\ud835\udc63\ud835\udc56superscript\ud835\udcaf\ud835\udc63f_{v}(v_{i})\\in\\mathcal{T}^{v}italic_f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) \u2208 caligraphic_T start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT, (and one edge type fe(eij)\u2208\ud835\udcafesubscript\ud835\udc53\ud835\udc52subscript\ud835\udc52\ud835\udc56\ud835\udc57superscript\ud835\udcaf\ud835\udc52f_{e}(e_{ij})\\in\\mathcal{T}^{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) \u2208 caligraphic_T start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT resp.). In this paper, we consider the heterogeneous graphs with |\ud835\udcafe|>1superscript\ud835\udcaf\ud835\udc521|\\mathcal{T}^{e}|>1| caligraphic_T start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT | > 1 or |\ud835\udcafv|>1superscript\ud835\udcaf\ud835\udc631|\\mathcal{T}^{v}|>1| caligraphic_T start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT | > 1. When |\ud835\udcafe|=1superscript\ud835\udcaf\ud835\udc521|\\mathcal{T}^{e}|=1| caligraphic_T start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT | = 1 and |\ud835\udcafv|=1superscript\ud835\udcaf\ud835\udc631|\\mathcal{T}^{v}|=1| caligraphic_T start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT | = 1, it becomes a homogeneous graph.", "Meta-Path\u00a0HAN ; sun2011pathsim  is a path on a heterogeneous graph G\ud835\udc3aGitalic_G that a sequence of nodes connected with heterogeneous edges, i.e., v1\u2192t1v2\u2192t2\u2026\u2192tlvl+1subscript\ud835\udc611\u2192subscript\ud835\udc631subscript\ud835\udc632subscript\ud835\udc612\u2192\u2026subscript\ud835\udc61\ud835\udc59\u2192subscript\ud835\udc63\ud835\udc591{v}_{1}\\xrightarrow{t_{1}}{v}_{2}\\xrightarrow{t_{2}}\\ldots\\xrightarrow{t_{l}}{v}_{l+1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_ARROW start_OVERACCENT italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_OVERACCENT \u2192 end_ARROW italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_ARROW start_OVERACCENT italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_OVERACCENT \u2192 end_ARROW \u2026 start_ARROW start_OVERACCENT italic_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_OVERACCENT \u2192 end_ARROW italic_v start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT,where tl\u2208\ud835\udcafesubscript\ud835\udc61\ud835\udc59superscript\ud835\udcaf\ud835\udc52t_{l}\\in\\mathcal{T}^{e}italic_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT \u2208 caligraphic_T start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT denotes an l\ud835\udc59litalic_l-th edge type of the meta-path.The meta-path can be viewed as a composite relation R=t1\u2218t2\u2026\u2218tl\ud835\udc45subscript\ud835\udc611subscript\ud835\udc612\u2026subscript\ud835\udc61\ud835\udc59R=t_{1}\\circ t_{2}\\ldots\\circ t_{l}italic_R = italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2218 italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2026 \u2218 italic_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT between node v1subscript\ud835\udc631{v}_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and vl+1subscript\ud835\udc63\ud835\udc591{v}_{l+1}italic_v start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT, where R1\u2218R2subscript\ud835\udc451subscript\ud835\udc452R_{1}\\circ R_{2}italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2218 italic_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT denotes the composition of relation R1subscript\ud835\udc451R_{1}italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and R2subscript\ud835\udc452R_{2}italic_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.The definition of meta-path generalizes multi-hop connections and is shown to be useful to analyze heterogeneous graphs.For instance, in Book-Crossing dataset, \u2018user-item-written.series-item-user\u2019 indicates that a meta-path that connects users who like the same book series.", "We introduce meta-path prediction as a self-supervised auxiliary task to improve the representational power of graph neural networks.To our knowledge, the meta-path prediction has not been studied in the context of self-supervised learning for graph neural networks in the literature.", "Meta-path prediction is similar to link prediction but meta-paths allow heterogeneous composite relations.The meta-path prediction can be achieved in the same manner as link prediction.If two nodes u\ud835\udc62uitalic_u and v\ud835\udc63vitalic_v are connected by a meta-path p\ud835\udc5dpitalic_p with the heterogeneous edges (t1,t2,\u2026t\u2113)subscript\ud835\udc611subscript\ud835\udc612\u2026subscript\ud835\udc61\u2113(t_{1},t_{2},\\ldots t_{\\ell})( italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 italic_t start_POSTSUBSCRIPT roman_\u2113 end_POSTSUBSCRIPT ), then yu,vp=1superscriptsubscript\ud835\udc66\ud835\udc62\ud835\udc63\ud835\udc5d1y_{u,v}^{p}=1italic_y start_POSTSUBSCRIPT italic_u , italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT = 1, otherwise yu,vp=0superscriptsubscript\ud835\udc66\ud835\udc62\ud835\udc63\ud835\udc5d0y_{u,v}^{p}=0italic_y start_POSTSUBSCRIPT italic_u , italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT = 0. The labels can be generated from a heterogeneous graph without any manual labeling.They can be obtained by Ap=Atl\u2026At2At1subscript\ud835\udc34\ud835\udc5dsubscript\ud835\udc34subscript\ud835\udc61\ud835\udc59\u2026subscript\ud835\udc34subscript\ud835\udc612subscript\ud835\udc34subscript\ud835\udc611A_{p}=A_{t_{l}}\\ldots A_{t_{2}}A_{t_{1}}italic_A start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_A start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2026 italic_A start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT, where Atsubscript\ud835\udc34\ud835\udc61A_{t}italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the adjacency matrix of edge type t\ud835\udc61titalic_t. The binarized value at (u,v)\ud835\udc62\ud835\udc63(u,v)( italic_u , italic_v ) in Apsubscript\ud835\udc34\ud835\udc5dA_{p}italic_A start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT indicates whether u\ud835\udc62uitalic_u and v\ud835\udc63vitalic_v are connected with the meta-path p\ud835\udc5dpitalic_p.In this paper, we use meta-path prediction as a self-supervised auxiliary task.", "Let \ud835\udc17\u2208\ud835\udc11|V|\u00d7d\ud835\udc17superscript\ud835\udc11\ud835\udc49\ud835\udc51\\mathbf{X}\\in\\textbf{R}^{|V|\\times d}bold_X \u2208 R start_POSTSUPERSCRIPT | italic_V | \u00d7 italic_d end_POSTSUPERSCRIPT and \ud835\udc19\u2208\ud835\udc11|V|\u00d7d\u2032\ud835\udc19superscript\ud835\udc11\ud835\udc49superscript\ud835\udc51\u2032\\mathbf{Z}\\in\\textbf{R}^{|V|\\times d^{\\prime}}bold_Z \u2208 R start_POSTSUPERSCRIPT | italic_V | \u00d7 italic_d start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT be input features and their hidden representations learnt by GNN f\ud835\udc53fitalic_f, i.e., \ud835\udc19=f(\ud835\udc7f;\ud835\udc30,\ud835\udc68)\ud835\udc19\ud835\udc53\ud835\udc7f\ud835\udc30\ud835\udc68\\mathbf{Z}=f(\\boldsymbol{X};\\mathbf{w},\\boldsymbol{A})bold_Z = italic_f ( bold_italic_X ; bold_w , bold_italic_A ), where \ud835\udc30\ud835\udc30\\mathbf{w}bold_w is the parameter for f\ud835\udc53fitalic_f, and \ud835\udc00\u2208\ud835\udc11|V|\u00d7|V|\ud835\udc00superscript\ud835\udc11\ud835\udc49\ud835\udc49\\mathbf{A}\\in\\textbf{R}^{|V|\\times|V|}bold_A \u2208 R start_POSTSUPERSCRIPT | italic_V | \u00d7 | italic_V | end_POSTSUPERSCRIPT is the adjacency matrix. Then link prediction and meta-path prediction are obtained by a simple operation asy^u,vt=\u03c3(\u03a6t(zu)\u22a4\u03a6t(zv)),superscriptsubscript^\ud835\udc66\ud835\udc62\ud835\udc63\ud835\udc61\ud835\udf0esubscript\u03a6\ud835\udc61superscriptsubscript\ud835\udc67\ud835\udc62topsubscript\u03a6\ud835\udc61subscript\ud835\udc67\ud835\udc63\\displaystyle\\hat{y}_{u,v}^{t}=\\sigma(\\Phi_{t}(z_{u})^{\\top}\\Phi_{t}(z_{v})),over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_u , italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = italic_\u03c3 ( roman_\u03a6 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT roman_\u03a6 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) ) ,(1)where \u03a6tsubscript\u03a6\ud835\udc61\\Phi_{t}roman_\u03a6 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the task-specific network for task t\u2208\ud835\udcaf\ud835\udc61\ud835\udcaft\\in\\mathcal{T}italic_t \u2208 caligraphic_T and zusubscript\ud835\udc67\ud835\udc62z_{u}italic_z start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT and zvsubscript\ud835\udc67\ud835\udc63z_{v}italic_z start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT are the node embeddings of node u\ud835\udc62uitalic_u and v\ud835\udc63vitalic_v. e.g., \u03a60subscript\u03a60\\Phi_{0}roman_\u03a6 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT (and \u03a61subscript\u03a61\\Phi_{1}roman_\u03a6 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT resp.) for link prediction (and the first type of meta-path prediction resp.).", "The architecture is shown in Fig. \u00a01.To optimize the model, as the link prediction, cross entropy is used.The graph neural network f\ud835\udc53fitalic_f is shared by the link prediction and meta-path predictions.As any auxiliary learning methods, the meta-paths (auxiliary tasks) should be carefully chosen and properly weighted so that the meta-path prediction does not compete with link prediction especially when the capacity of GNNs is limited.To address these issues, we propose our framework that automatically selects meta-paths and balances them with the link prediction via meta-learning.", "Our framework SELAR is learning to learn a primary task with multiple auxiliary tasks to assist the primary task.This can be formally written asmin\ud835\udc30,\u0398\u2061\ud835\udd3c[\u2112pr(\ud835\udc30\u2217(\u0398))](x,y)\u223cDpr\u00a0s.t.\u00a0\ud835\udc30\u2217(\u0398)=arg\u2061min\ud835\udc30\u2061\ud835\udd3c[\u2112pr+au(\ud835\udc30;\u0398)](x,y)\u223cDpr+au,\\displaystyle\\min_{\\mathbf{w},\\Theta}\\;\\;\\underset{(x,y)\\sim D^{pr}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}{\\text{\\large$\\mathbb{E}$}\\;\\;\\left[\\;\\;\\mathcal{L}^{pr}(\\mathbf{w}^{\\ast}(\\Theta))\\;\\;\\right]}\\;\\;\\text{ s.t. }\\;\\;\\mathbf{w}^{\\ast}(\\Theta)=\\operatorname*{\\arg\\!\\min}_{\\mathbf{w}}\\underset{(x,y)\\sim D^{pr+au}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}{\\;\\;\\mathbb{E}\\;\\;\\left[\\;\\;\\mathcal{L}^{pr+au}(\\mathbf{w};\\Theta)\\;\\;\\right]},roman_min start_POSTSUBSCRIPT bold_w , roman_\u0398 end_POSTSUBSCRIPT start_UNDERACCENT ( italic_x , italic_y ) \u223c italic_D start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT end_UNDERACCENT start_ARG blackboard_E [ caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT ( bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 ) ) ] end_ARG s.t. bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_w end_POSTSUBSCRIPT start_UNDERACCENT ( italic_x , italic_y ) \u223c italic_D start_POSTSUPERSCRIPT italic_p italic_r + italic_a italic_u end_POSTSUPERSCRIPT end_UNDERACCENT start_ARG blackboard_E [ caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r + italic_a italic_u end_POSTSUPERSCRIPT ( bold_w ; roman_\u0398 ) ] end_ARG ,(2)where \u2112pr(\u22c5)superscript\u2112\ud835\udc5d\ud835\udc5f\u22c5\\mathcal{L}^{pr}(\\cdot)caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT ( \u22c5 ) is the primary task loss function to evaluate the trained model f(x;\ud835\udc30\u2217(\u0398))\ud835\udc53\ud835\udc65superscript\ud835\udc30\u2217\u0398f(x;\\mathbf{w}^{\\ast}(\\Theta))italic_f ( italic_x ; bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 ) ) on meta-data (a validation for meta-learning\u00a0han2018coteaching ) Dprsuperscript\ud835\udc37\ud835\udc5d\ud835\udc5fD^{pr}italic_D start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT and \u2112pr+ausuperscript\u2112\ud835\udc5d\ud835\udc5f\ud835\udc4e\ud835\udc62\\mathcal{L}^{pr+au}caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r + italic_a italic_u end_POSTSUPERSCRIPT is the loss function to train a model on training data Dpr+ausuperscript\ud835\udc37\ud835\udc5d\ud835\udc5f\ud835\udc4e\ud835\udc62D^{pr+au}italic_D start_POSTSUPERSCRIPT italic_p italic_r + italic_a italic_u end_POSTSUPERSCRIPT with the primary and auxiliary tasks. To avoid cluttered notation, f\ud835\udc53fitalic_f, x\ud835\udc65xitalic_x, and y\ud835\udc66yitalic_y are omitted. Each task \ud835\udcaftsubscript\ud835\udcaf\ud835\udc61\\mathcal{T}_{t}caligraphic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT has Ntsubscript\ud835\udc41\ud835\udc61N_{t}italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT samples and \ud835\udcaf0subscript\ud835\udcaf0\\mathcal{T}_{0}caligraphic_T start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and {\ud835\udcaft}t=1Tsuperscriptsubscriptsubscript\ud835\udcaf\ud835\udc61\ud835\udc611\ud835\udc47\\{\\mathcal{T}_{t}\\}_{t=1}^{T}{ caligraphic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT denote the primary and auxiliary tasks respectively.The proposed formulation in Eq.\u00a0(2) learns how to assist the primary task by optimizing \u0398\u0398\\Thetaroman_\u0398 via meta-learning. The nested optimization problem given \u0398\u0398\\Thetaroman_\u0398 is a regular training with properly adjusted loss functions to balance the primary and auxiliary tasks. The formulation can be more specifically written asmin\ud835\udc30,\u0398subscript\ud835\udc30\u0398\\displaystyle\\min_{\\mathbf{w},\\Theta}roman_min start_POSTSUBSCRIPT bold_w , roman_\u0398 end_POSTSUBSCRIPT\u2211i=1M01M0\u21130(yi(0,meta),f(xi(0,meta);\ud835\udc30\u2217(\u0398))\\displaystyle\\sum_{i=1}^{M_{0}}\\frac{1}{M_{0}}\\ell^{0}(y_{i}^{(0,meta)},f(x_{i}^{(0,meta)};\\mathbf{w}^{\\ast}(\\Theta))\u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG roman_\u2113 start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 , italic_m italic_e italic_t italic_a ) end_POSTSUPERSCRIPT , italic_f ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 , italic_m italic_e italic_t italic_a ) end_POSTSUPERSCRIPT ; bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 ) )(3)s.t.\ud835\udc30\u2217(\u0398)=arg\u2061min\ud835\udc30\u2211t=0T\u2211i=1Nt1Nt\ud835\udcb1(\u03bei(t,train);\u0398)\u2113t(yi(t,train),ft(xi(t,train);\ud835\udc30)),superscript\ud835\udc30\u2217\u0398subscript\ud835\udc30superscriptsubscript\ud835\udc610\ud835\udc47superscriptsubscript\ud835\udc561subscript\ud835\udc41\ud835\udc611subscript\ud835\udc41\ud835\udc61\ud835\udcb1subscriptsuperscript\ud835\udf09\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc56\u0398superscript\u2113\ud835\udc61superscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bsuperscript\ud835\udc53\ud835\udc61superscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc30\\displaystyle\\mathbf{w}^{\\ast}(\\Theta)=\\operatorname*{\\arg\\!\\min}_{\\mathbf{w}}\\sum_{t=0}^{T}\\sum_{i=1}^{N_{t}}\\frac{1}{N_{t}}\\mathcal{V}(\\xi^{(t,train)}_{i};\\Theta)\\ell^{t}(y_{i}^{(t,train)},f^{t}(x_{i}^{(t,train)};\\mathbf{w})),bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_w end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG caligraphic_V ( italic_\u03be start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; roman_\u0398 ) roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT , italic_f start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT ; bold_w ) ) ,(4)where \u2113tsuperscript\u2113\ud835\udc61\\ell^{t}roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT and ftsuperscript\ud835\udc53\ud835\udc61f^{t}italic_f start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT denote the loss function and the model for task t\ud835\udc61titalic_t. We overload \u2113tsuperscript\u2113\ud835\udc61\\ell^{t}roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT with its function value, i.e., \u2113t=\u2113t(yi(t,train),ft(xi(t,train);\ud835\udc30))superscript\u2113\ud835\udc61superscript\u2113\ud835\udc61superscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bsuperscript\ud835\udc53\ud835\udc61superscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc30\\ell^{t}=\\ell^{t}(y_{i}^{(t,train)},f^{t}(x_{i}^{(t,train)};\\mathbf{w}))roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT , italic_f start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT ; bold_w ) ). \u03bei(t,train)subscriptsuperscript\ud835\udf09\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc56\\xi^{(t,train)}_{i}italic_\u03be start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the embedding vector of ithsubscript\ud835\udc56\ud835\udc61\u210ei_{th}italic_i start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT sample for task t\ud835\udc61titalic_t. It is the concatenation of one-hot representation of task types, the label of the sample (positive/negative), and its loss value, i.e., \u03bei(t,train)=[\u2113t;et;yi(t,train)]\u2208\ud835\udc11T+2subscriptsuperscript\ud835\udf09\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc56superscript\u2113\ud835\udc61subscript\ud835\udc52\ud835\udc61superscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bsuperscript\ud835\udc11\ud835\udc472\\xi^{(t,train)}_{i}=\\left[\\ell^{t};e_{t};y_{i}^{(t,train)}\\right]\\in\\textbf{R}^{T+2}italic_\u03be start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ; italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT ] \u2208 R start_POSTSUPERSCRIPT italic_T + 2 end_POSTSUPERSCRIPT.To derive our learning algorithm,we first shorten the objective function in Eq.\u00a0(3) and Eq.\u00a0(4) as \u2112pr(\ud835\udc30\u2217(\u0398))superscript\u2112\ud835\udc5d\ud835\udc5fsuperscript\ud835\udc30\u2217\u0398\\mathcal{L}^{pr}(\\mathbf{w}^{\\ast}(\\Theta))caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT ( bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 ) ) and \u2112pr+au(\ud835\udc30;\u0398)superscript\u2112\ud835\udc5d\ud835\udc5f\ud835\udc4e\ud835\udc62\ud835\udc30\u0398\\mathcal{L}^{pr+au}(\\mathbf{w};\\Theta)caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r + italic_a italic_u end_POSTSUPERSCRIPT ( bold_w ; roman_\u0398 ).This is equivalent to Eq.\u00a0(2) without expectation.Then, our formulation is given asmin\ud835\udc30,\u0398\u2061\u2112pr(\ud835\udc30\u2217(\u0398))\u00a0s.t.\u00a0\ud835\udc30\u2217(\u0398)=arg\u2061min\ud835\udc30\u2061\u2112pr+au(\ud835\udc30;\u0398),subscript\ud835\udc30\u0398superscript\u2112\ud835\udc5d\ud835\udc5fsuperscript\ud835\udc30\u2217\u0398\u00a0s.t.\u00a0superscript\ud835\udc30\u2217\u0398subscript\ud835\udc30superscript\u2112\ud835\udc5d\ud835\udc5f\ud835\udc4e\ud835\udc62\ud835\udc30\u0398\\min_{\\mathbf{w},\\Theta}\\mathcal{L}^{pr}(\\mathbf{w}^{\\ast}(\\Theta))\\;\\;\\text{ s.t. }\\mathbf{w}^{\\ast}(\\Theta)=\\operatorname*{\\arg\\!\\min}_{\\mathbf{w}}\\mathcal{L}^{pr+au}(\\mathbf{w};\\Theta),roman_min start_POSTSUBSCRIPT bold_w , roman_\u0398 end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT ( bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 ) ) s.t. bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_w end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r + italic_a italic_u end_POSTSUPERSCRIPT ( bold_w ; roman_\u0398 ) ,(5)To circumvent the difficulty of the bi-level optimization, as previous works\u00a0MAML ; han2018coteaching  in meta-learning we approximate it with the updated parameters \ud835\udc30^^\ud835\udc30\\hat{\\mathbf{w}}over^ start_ARG bold_w end_ARG using the gradient descent update as\ud835\udc30\u2217(\u0398)\u2248\ud835\udc30^k(\u0398k)=\ud835\udc30k\u2212\u03b1\u2207\ud835\udc30\u2112pr+au(\ud835\udc30k;\u0398k),superscript\ud835\udc30\u2217\u0398superscript^\ud835\udc30\ud835\udc58superscript\u0398\ud835\udc58superscript\ud835\udc30\ud835\udc58\ud835\udefcsubscript\u2207\ud835\udc30superscript\u2112\ud835\udc5d\ud835\udc5f\ud835\udc4e\ud835\udc62superscript\ud835\udc30\ud835\udc58superscript\u0398\ud835\udc58\\displaystyle\\mathbf{w}^{\\ast}(\\Theta)\\approx\\hat{\\mathbf{w}}^{k}(\\Theta^{k})=\\mathbf{w}^{k}-\\alpha\\nabla_{\\mathbf{w}}\\mathcal{L}^{pr+au}(\\mathbf{w}^{k};\\Theta^{k}),bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 ) \u2248 over^ start_ARG bold_w end_ARG start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( roman_\u0398 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) = bold_w start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT - italic_\u03b1 \u2207 start_POSTSUBSCRIPT bold_w end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r + italic_a italic_u end_POSTSUPERSCRIPT ( bold_w start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ; roman_\u0398 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) ,(6)where \u03b1\ud835\udefc\\alphaitalic_\u03b1 is the learning rate for \ud835\udc30\ud835\udc30\\mathbf{w}bold_w.We do not numerically evaluate \ud835\udc30^k(\u0398)superscript^\ud835\udc30\ud835\udc58\u0398\\hat{\\mathbf{w}}^{k}(\\Theta)over^ start_ARG bold_w end_ARG start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( roman_\u0398 ) instead we plug the computational graph of \ud835\udc30^ksuperscript^\ud835\udc30\ud835\udc58\\hat{\\mathbf{w}}^{k}over^ start_ARG bold_w end_ARG start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT in \u2112pr(\ud835\udc30\u2217(\u0398))superscript\u2112\ud835\udc5d\ud835\udc5fsuperscript\ud835\udc30\u2217\u0398\\mathcal{L}^{pr}(\\mathbf{w}^{\\ast}(\\Theta))caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT ( bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 ) ) to optimize \u0398\u0398\\Thetaroman_\u0398.Let \u2207\u0398\u2112pr(\ud835\udc30\u2217(\u0398k))subscript\u2207\u0398superscript\u2112\ud835\udc5d\ud835\udc5fsuperscript\ud835\udc30\u2217superscript\u0398\ud835\udc58\\nabla_{\\Theta}\\mathcal{L}^{pr}(\\mathbf{w}^{\\ast}(\\Theta^{k}))\u2207 start_POSTSUBSCRIPT roman_\u0398 end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT ( bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) ) be the gradient evaluated at \u0398ksuperscript\u0398\ud835\udc58\\Theta^{k}roman_\u0398 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT.Then updating parameters \u0398\u0398\\Thetaroman_\u0398 is given as\u0398k+1=\u0398k\u2212\u03b2\u2207\u0398\u2112pr(\ud835\udc30^k(\u0398k)),superscript\u0398\ud835\udc581superscript\u0398\ud835\udc58\ud835\udefdsubscript\u2207\u0398superscript\u2112\ud835\udc5d\ud835\udc5fsuperscript^\ud835\udc30\ud835\udc58superscript\u0398\ud835\udc58\\displaystyle\\Theta^{k+1}=\\Theta^{k}-\\beta\\nabla_{\\Theta}\\mathcal{L}^{pr}(\\hat{\\mathbf{w}}^{k}(\\Theta^{k})),roman_\u0398 start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT = roman_\u0398 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT - italic_\u03b2 \u2207 start_POSTSUBSCRIPT roman_\u0398 end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT ( over^ start_ARG bold_w end_ARG start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( roman_\u0398 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) ) ,(7)where \u03b2\ud835\udefd\\betaitalic_\u03b2 is the learning rate for \u0398\u0398\\Thetaroman_\u0398. This update allows softly selecting useful auxiliary tasks (meta-paths) and balance them with the primary task to improve the performance of the primary task. Without balancing tasks with the weighting function \ud835\udcb1(\u22c5;\u0398)\ud835\udcb1\u22c5\u0398\\mathcal{V}(\\cdot;\\Theta)caligraphic_V ( \u22c5 ; roman_\u0398 ), auxiliary tasks can dominate training and degrade the performance of the primary task.", "The model parameters \ud835\udc30ksuperscript\ud835\udc30\ud835\udc58\\mathbf{w}^{k}bold_w start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT for tasks can be updated with optimized \u0398k+1superscript\u0398\ud835\udc581\\Theta^{k+1}roman_\u0398 start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT in (7) as\ud835\udc30k+1=\ud835\udc30k\u2212\u03b1\u2207\ud835\udc30\u2112pr+au(\ud835\udc30k;\u0398k+1).superscript\ud835\udc30\ud835\udc581superscript\ud835\udc30\ud835\udc58\ud835\udefcsubscript\u2207\ud835\udc30superscript\u2112\ud835\udc5d\ud835\udc5f\ud835\udc4e\ud835\udc62superscript\ud835\udc30\ud835\udc58superscript\u0398\ud835\udc581\\displaystyle\\mathbf{w}^{k+1}=\\mathbf{w}^{k}-\\alpha\\nabla_{\\mathbf{w}}\\mathcal{L}^{pr+au}(\\mathbf{w}^{k};\\Theta^{k+1}).bold_w start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT = bold_w start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT - italic_\u03b1 \u2207 start_POSTSUBSCRIPT bold_w end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r + italic_a italic_u end_POSTSUPERSCRIPT ( bold_w start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ; roman_\u0398 start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT ) .(8)Remarks. The proposed formulation can suffer from the meta-overfitting antoniou2018train ; zintgraf2018fast  meaning that the parameters \u0398\u0398\\Thetaroman_\u0398 to learn weights for softly selecting meta-paths and balancing the tasks with the primary task can overfit to the small meta-dataset.In our experiment, we found that the overfitting can be alleviated by meta-validation sets antoniou2018train .To learn \u0398\u0398\\Thetaroman_\u0398 that is generalizable across meta-training sets, we optimize \u0398\u0398\\Thetaroman_\u0398 across k\ud835\udc58kitalic_k different meta-datasets like k\ud835\udc58kitalic_k-fold cross validation using the following equation:\u0398k+1=\u0398k\u2212\u03b2\ud835\udd3c[\u2207\u0398\u2112pr(\ud835\udc30^k(\u0398k))],Dpr(meta)\u223cCV\\displaystyle\\Theta^{k+1}\\;=\\;\\underset{D^{pr(meta)}\\sim CV\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}{\\Theta^{k}\\;-\\;\\;\\beta\\;\\;\\mathbb{E}\\left[\\;\\nabla_{\\Theta}\\mathcal{L}^{pr}(\\hat{\\mathbf{w}}^{k}(\\Theta^{k}))\\;\\right],}roman_\u0398 start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT = start_UNDERACCENT italic_D start_POSTSUPERSCRIPT italic_p italic_r ( italic_m italic_e italic_t italic_a ) end_POSTSUPERSCRIPT \u223c italic_C italic_V end_UNDERACCENT start_ARG roman_\u0398 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT - italic_\u03b2 blackboard_E [ \u2207 start_POSTSUBSCRIPT roman_\u0398 end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT ( over^ start_ARG bold_w end_ARG start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( roman_\u0398 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) ) ] , end_ARG(9)where Dpr(meta)\u223cCVsimilar-tosuperscript\ud835\udc37\ud835\udc5d\ud835\udc5f\ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc36\ud835\udc49D^{pr(meta)}\\sim CVitalic_D start_POSTSUPERSCRIPT italic_p italic_r ( italic_m italic_e italic_t italic_a ) end_POSTSUPERSCRIPT \u223c italic_C italic_V is a meta-dataset from cross validation. We used 3-fold cross validation and the gradients of \u0398\u0398\\Thetaroman_\u0398 w.r.t different meta-datasets are averaged to update \u0398ksuperscript\u0398\ud835\udc58\\Theta^{k}roman_\u0398 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, see Algorithm 1. The cross validation is crucial to alleviate meta-overfitting and more discussion is Section 4.3.", "Meta-path prediction is generally more challenging than link prediction and node classification since it requires the understanding of long-range relations across heterogeneous nodes.The meta-path prediction gets more difficult when mini-batch training is inevitable due to the size of datasets or models.Within a mini-batch, important nodes and edges for meta-paths are not available.Also, a small learner network, e.g., two-layer GNNs, with a limited receptive field, inherently cannot capture long-range relations.The challenges can hinder representation learning and damage the generalization of the primary task.We proposed a Hint Network (HintNet) which makes the challenge tasks more solvable by correcting the answer with more information at the learner\u2019s need.Specifically, in our experiments, the HintNet corrects the answer of the learner with its own answer from the augmented graph with hub nodes, see Fig. \u00a02.", "The amount of help (correction) by HintNet is optimized maximizing the learner\u2019s gain.Let \ud835\udcb1H(\u22c5)subscript\ud835\udcb1\ud835\udc3b\u22c5\\mathcal{V}_{H}(\\cdot)caligraphic_V start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( \u22c5 ) and \u0398Hsubscript\u0398\ud835\udc3b\\Theta_{H}roman_\u0398 start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT be a weight function to determine the amount of hint and its parameters which are optimized by meta-learning. Then, our formulation with HintNet is given asmin\ud835\udc30,\u0398\u2211i=1M01M0\u21130(yi(0,meta),f(xi(0,meta);\ud835\udc30\u2217(\u0398,\u0398H)))subscript\ud835\udc30\u0398superscriptsubscript\ud835\udc561subscript\ud835\udc4001subscript\ud835\udc400superscript\u21130superscriptsubscript\ud835\udc66\ud835\udc560\ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc53superscriptsubscript\ud835\udc65\ud835\udc560\ud835\udc5a\ud835\udc52\ud835\udc61\ud835\udc4esuperscript\ud835\udc30\u2217\u0398subscript\u0398\ud835\udc3b\\displaystyle\\min_{\\mathbf{w},\\Theta}\\sum_{i=1}^{M_{0}}\\frac{1}{M_{0}}\\ell^{0}(y_{i}^{(0,meta)},f(x_{i}^{(0,meta)};\\mathbf{w}^{\\ast}(\\Theta,\\Theta_{H})))roman_min start_POSTSUBSCRIPT bold_w , roman_\u0398 end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG roman_\u2113 start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 , italic_m italic_e italic_t italic_a ) end_POSTSUPERSCRIPT , italic_f ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 , italic_m italic_e italic_t italic_a ) end_POSTSUPERSCRIPT ; bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 , roman_\u0398 start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) ) )(10)s.t.\u00a0\ud835\udc30\u2217(\u0398)=arg\u2061min\ud835\udc30\u2211t=0T\u2211i=1Nt1Nt\ud835\udcb1(\u03bei(t,train),\u2113t;\u0398)\u2113t(yi(t,train),y^i(t,train)(\u0398H)),s.t.\u00a0superscript\ud835\udc30\u2217\u0398subscript\ud835\udc30superscriptsubscript\ud835\udc610\ud835\udc47superscriptsubscript\ud835\udc561subscript\ud835\udc41\ud835\udc611subscript\ud835\udc41\ud835\udc61\ud835\udcb1subscriptsuperscript\ud835\udf09\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc56superscript\u2113\ud835\udc61\u0398superscript\u2113\ud835\udc61superscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bsuperscriptsubscript^\ud835\udc66\ud835\udc56\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bsubscript\u0398\ud835\udc3b\\displaystyle\\text{s.t. }\\mathbf{w}^{\\ast}(\\Theta)=\\operatorname*{\\arg\\!\\min}_{\\mathbf{w}}\\sum_{t=0}^{T}\\sum_{i=1}^{N_{t}}\\frac{1}{N_{t}}\\mathcal{V}(\\xi^{(t,train)}_{i},\\ell^{t};\\Theta)\\ell^{t}(y_{i}^{(t,train)},\\hat{y}_{i}^{(t,train)}(\\Theta_{H})),s.t. bold_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( roman_\u0398 ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_w end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG caligraphic_V ( italic_\u03be start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ; roman_\u0398 ) roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT ( roman_\u0398 start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) ) ,(11)where y^i(t,train)(\u0398H)superscriptsubscript^\ud835\udc66\ud835\udc56\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bsubscript\u0398\ud835\udc3b\\hat{y}_{i}^{(t,train)}(\\Theta_{H})over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT ( roman_\u0398 start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) denotes the convex combination of the learner\u2019s answer and HintNet\u2019s answer, i.e., \ud835\udcb1H(\u03bei(t,train);\u0398H)ft(xi(t,train);\ud835\udc30)+(1\u2212\ud835\udcb1H(\u03bei(t,train);\u0398H))fHt(xi(t,train);\ud835\udc30)subscript\ud835\udcb1\ud835\udc3bsubscriptsuperscript\ud835\udf09\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc56subscript\u0398\ud835\udc3bsuperscript\ud835\udc53\ud835\udc61superscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc301subscript\ud835\udcb1\ud835\udc3bsubscriptsuperscript\ud835\udf09\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc56subscript\u0398\ud835\udc3bsuperscriptsubscript\ud835\udc53\ud835\udc3b\ud835\udc61superscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc30\\mathcal{V}_{H}(\\xi^{(t,train)}_{i};\\Theta_{H})f^{t}(x_{i}^{(t,train)};\\mathbf{w})+(1-\\mathcal{V}_{H}(\\xi^{(t,train)}_{i};\\Theta_{H}))f_{H}^{t}(x_{i}^{(t,train)};\\mathbf{w})caligraphic_V start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_\u03be start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; roman_\u0398 start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) italic_f start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT ; bold_w ) + ( 1 - caligraphic_V start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_\u03be start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; roman_\u0398 start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) ) italic_f start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT ; bold_w ). The sample embedding is\u03bei(t,train)=[\u2113t;\u2113Ht;et;yi(t,train)]\u2208\ud835\udc11T+3subscriptsuperscript\ud835\udf09\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\ud835\udc56superscript\u2113\ud835\udc61subscriptsuperscript\u2113\ud835\udc61\ud835\udc3bsubscript\ud835\udc52\ud835\udc61superscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bsuperscript\ud835\udc11\ud835\udc473\\xi^{(t,train)}_{i}=\\left[\\ell^{t};\\ell^{t}_{H};e_{t};y_{i}^{(t,train)}\\right]\\in\\textbf{R}^{T+3}italic_\u03be start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ; roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ; italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t , italic_t italic_r italic_a italic_i italic_n ) end_POSTSUPERSCRIPT ] \u2208 R start_POSTSUPERSCRIPT italic_T + 3 end_POSTSUPERSCRIPT.", "We evaluate our proposed methods on four public benchmark datasets on heterogeneous graphs.Our experiments answer the following research questions: Q1. Is meta-path prediction effective for representation learning on heterogeneous graphs?Q2. Can the meta-path prediction be further improved by the proposed methods (e.g., SELAR, HintNet)?Q3. Why are the proposed methods effective, any relation with hard negative mining?", "Datasets. We use two public benchmark datasets from different domains for link prediction: Music dataset Last-FM and Book dataset Book-Crossing, released by KGNN-LS\u00a0wang2019knowledge , RippleNet\u00a0wang2018ripplenet .We use two datasets for node classification: citation network datasets ACM and Movie dataset IMDB, used by HAN\u00a0HAN  for node classification tasks.ACM has three types nodes (Paper(P), Author(A), Subject(S)), four types of edges (PA, AP, PS, SP) and labels (categories of papers).IMDB contains three types of nodes (Movie (M), Actor (A), Director (D)), four types (MA, AM, MD, DM) of edges and labels (genres of movies).ACM and IMDB have node features, which are bag-of-words of keywords and plots.Dataset details are in the supplement.", "Baselines. We evaluate our methods with five graph neural networks : GCN\u00a0GCN , GAT\u00a0GAT , GIN\u00a0xu2018powerful , SGConv\u00a0wu2019simplifying  and GTN\u00a0yun2019graph . Our methods can be applied to both homogeneous graphs and heterogeneous graphs. We compare four learning strategies: Vanilla, standard training of base models only with the primary task samples; w/o meta-path, learning a primary task with sample weighting function \ud835\udcb1(\u03be;\u0398)\ud835\udcb1\ud835\udf09\u0398\\mathcal{V}(\\xi;\\Theta)caligraphic_V ( italic_\u03be ; roman_\u0398 ); w/ meta-path, training with the primary task and auxiliary tasks (meta-path prediction) with a standard loss function; SELAR proposed in Section 3.2, learning the primary task with optimized auxiliary tasks by meta-learning; SELAR+Hint introduced in Section 3.3.In all the experiments, we report the mean performance of three independent runs.Implementation details are in the supplement. Our experiments were mainly performed based on NAVER Smart Machine Learning platform (NSML)\u00a0sung2017nsml ; kim2018nsml .", "We used five types of meta-paths of length 2 to 4 for auxiliary tasks.Table\u00a01 shows that our methods consistently improve link prediction performance for all the GNNs, compared to the Vanilla and the method using Meta-Weight-Net\u00a0han2018coteaching  only without meta-paths (denoted as w/o meta-path).Overall, a standard training with meta-paths shows 1.1% improvement on average on both Last-FM and Book-Crossing whereas meta-learning that learns sample weights degrades on average on Last-FM and improves only 0.6% on average on Book-Crossing, e.g., GCN, SGC and GTN on Last-FM and GCN and SGC on Book-Crossing, show degradation 0.2% compared to the standard training (Vanilla).As we expected, SELAR and SELAR with HintNet provide more optimized auxiliary learning resulting in 1.9% and 2.0% absolute improvement on Last-FM and 2.6% and 2.7% on the Book-Crossing dataset.Further, in particular, GIN on Book-crossing, SELAR and SELAR+Hint provide \u223csimilar-to\\sim\u223c5.5% and \u223csimilar-to\\sim\u223c5.3% absolute improvement compared to the vanilla algorithm.", "Similar to link prediction above, our SELAR consistently enhances node classification performance of all the GNN models and the improvements are more significant on IMDB which is larger than the ACM dataset.We believe that ACM dataset is already saturated and the room for improvement is limited. However, our methods still show small yet consistent improvement over all the architecture on ACM.We conjecture that the efficacy of our proposed methods differs depending on graph structures.However, it is worth noting that introducing meta-path prediction as auxiliary tasks remarkably improves the performance of primary tasks such as link and node prediction with consistency compared to the existing methods. \u201cw/o meta-path\u201d, the meta-learning to learn sample weight function on a primary task shows marginal degradation in five out of eight settings.Remarkably, SELAR improved the F1-score of GAT on the IMDB by (4.46%) compared to the vanilla learning scheme.", "The effectiveness of meta-path prediction and the proposed learning strategies are answered above.To address the last research question Q3. why the proposed method is effective, we provide analysis on the weighting function \ud835\udcb1(\u03be;\u0398)\ud835\udcb1\ud835\udf09\u0398\\mathcal{V}(\\xi;\\Theta)caligraphic_V ( italic_\u03be ; roman_\u0398 ) learned by our framework. Also, we show the evidence that meta-overfitting occurs and can be addressed by cross-validation as in Algorithm 1.", "Weighting function. Our proposed methods can automatically balance multiple auxiliary tasks to improve the primary task.To understand the ability of our method, we analyze the weighting function and the adjusted loss function by the weighting function, i.e.,\ud835\udcb1(\u03be;\u0398)\ud835\udcb1\ud835\udf09\u0398\\mathcal{V}(\\xi;\\Theta)caligraphic_V ( italic_\u03be ; roman_\u0398 ), \ud835\udcb1(\u03be;\u0398)\u2113t(y,y^)\ud835\udcb1\ud835\udf09\u0398superscript\u2113\ud835\udc61\ud835\udc66^\ud835\udc66\\mathcal{V}(\\xi;\\Theta)\\ell^{t}(y,\\hat{y})caligraphic_V ( italic_\u03be ; roman_\u0398 ) roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_y , over^ start_ARG italic_y end_ARG ).The positive and negative samples are solid and dash lines respectively.We present the weighting function learnt by SELAR+HintNet for GAT which is the best-performing construction on Last-FM. The weighting function is from the epoch with the best validation performance.Fig.\u00a03 shows that the learnt weighting function attends to hard examples more than easy ones with a small loss range from 0 to 1.", "Also, the primary task-positive samples are relatively less down weighted than auxiliary tasks even when the samples are easy (i.e., the loss is ranged from 0 to 1). Our adjusted loss \ud835\udcb1(\u03be;\u0398)\u2113t(y,y^)\ud835\udcb1\ud835\udf09\u0398superscript\u2113\ud835\udc61\ud835\udc66^\ud835\udc66\\mathcal{V}(\\xi;\\Theta)\\ell^{t}(y,\\hat{y})caligraphic_V ( italic_\u03be ; roman_\u0398 ) roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_y , over^ start_ARG italic_y end_ARG ) is closely related to the focal loss, \u2212(1\u2212pt)\u03b3log\u2061(pt)superscript1subscript\ud835\udc5d\ud835\udc61\ud835\udefesubscript\ud835\udc5d\ud835\udc61-(1-p_{t})^{\\gamma}\\log(p_{t})- ( 1 - italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_\u03b3 end_POSTSUPERSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ). When \u2113tsuperscript\u2113\ud835\udc61\\ell^{t}roman_\u2113 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT is the cross-entropy, it becomes \ud835\udcb1(\u03be;\u0398)log\u2061(pt)\ud835\udcb1\ud835\udf09\u0398subscript\ud835\udc5d\ud835\udc61\\mathcal{V}(\\xi;\\Theta)\\log(p_{t})caligraphic_V ( italic_\u03be ; roman_\u0398 ) roman_log ( italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), where p\ud835\udc5dpitalic_p is the model\u2019s prediction for the correct class and ptsubscript\ud835\udc5d\ud835\udc61p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is defined as p\ud835\udc5dpitalic_p if y=1\ud835\udc661y=1italic_y = 1, otherwise 1\u2212p1\ud835\udc5d1-p1 - italic_p as lin2017focal .The weighting function differentially evolves over iterations. At the early stage of training, it often focuses on easy examples first and then changes its focus over time. Also, the adjusted loss values by the weighting function learnt by our method differ across tasks. To analyze the contribution of each task, we calculate the average of the task-specific weighted loss on the Last-FM and Book-Crossing datasets. Especially, on the Book-Crossing, our method has more attention to \u2019user-item\u2019 (primary task) and \u2018user-item-literary.series.item-user\u2019 (auxiliary task) which is a meta-path that connects users who like a book series. This implies that two users who like a book series likely have a similar preference.More results and discussion are available in the supplement.", "Meta cross-validation, i.e., cross-validation for meta-learning, helps to keep weighting function from over-fitting on meta data.Table\u00a03 evidence that our algorithms as other meta-learning methods can overfit to meta-data.As in Algorithm 1, our proposed methods, both SELAR and SELAR with HintNet, with cross-validation denoted as \u20183-fold\u2019 alleviates the meta-overfitting problem and provides a significant performance gain, whereas without meta cross-validation denoted as \u20181-fold\u2019 the proposed method can underperform the vanilla training strategy.", "We proposed meta-path prediction as self-supervised auxiliary tasks on heterogeneous graphs.Our experiments show that the representation learning on heterogeneous graphscan benefit from meta-path prediction which encourages to capture rich semantic information.The auxiliary tasks can be further improved by our proposed method SELAR, which automatically balances auxiliary tasks to assist the primary task via a form of meta-learning.The learnt weighting function identifies more beneficial meta-paths for the primary tasks.Within a task, the weighting function can adjust the cross entropy like the focal loss, which focuses on hard examples by decreasing weights for easy samples.Moreover, when it comes to challenging and remotely relevant auxiliary tasks,our HintNet helps the learner by correcting the learner\u2019s answer dynamically and further improves the gain from auxiliary tasks.Our framework based on meta-learning provides learning strategies to balance primary task and auxiliary tasks, and easy/hard (and positive/negative) samples.Interesting future directions include applying our framework to other domains and various auxiliary tasks.Our code is publicly available at https://github.com/mlvlab/SELAR.", "Acknowledgements.\u2003This work was partly supported by NAVER Corp. and Institute for Information & communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT): the Regional Strategic Industry Convergence Security Core Talent Training Business (No.2019-0-01343) and the ICT Creative Consilience Program (IITP-2020-0-01819)."], "figure_types": {"4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/12-Table4-1.png": "table", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/13-Table5-1.png": "table", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/13-Table6-1.png": "table", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/14-Figure4-1.png": "plot", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/4-Figure1-1.png": "schematic", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/5-Figure2-1.png": "schematic", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/7-Table1-1.png": "table", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/7-Table2-1.png": "table", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/8-Figure3-1.png": "plot", "4ccdd1fe6cc9c896e910582bea2c33c19317c5ca/8-Table3-1.png": "table"}}, "2110.05379": {"paper_id": "paper_163", "title": "Point cloud augmentation with weighted local transformations", "arxiv_url": "https://arxiv.org/abs/2110.05379", "s2orc_url": "https://www.semanticscholar.org/paper/4e6bbeee3e8810de2a5b12941f81920866a6b38b", "all_figures_tables": {"4e6bbeee3e8810de2a5b12941f81920866a6b38b/1-Figure1-1.png": "Figure 1. Locally augmented point clouds using PointWOLF. In each row, the left most sample is the original, and the remaining samples are its locally transformed results (brighter regions indicate stronger local transformations). PointWOLF can locally transform objects while preserving the original shape identity.", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/3-Figure2-1.png": "Figure 2. PointWOLF Framework Illustration. Given an original sample, PointWOLF has multiple local transformations at each anchor point (red). PointWOLF produces smoothly varying non-rigid deformations based on the weighted local transformations.", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/6-Table2-1.png": "Table 2. Overall accuracy on ScanObjectNN.", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/7-Table3-1.png": "Table 3. Overall mean IoU ( mI oU ) on ShapeNetPart.", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/7-Table4-1.png": "Table 4. PointWOLF Ablation. R: rotate, S: scale, T: translate.", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/7-Table6-1.png": "Table 6. Interpolation space for AugTune.", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/8-Figure3-1.png": "Figure 3. Illustration of Local and Global Corruption. (a) and (b) are local corruptions while (c) and (d) are global corruptions.", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/8-Figure4-1.png": "Figure 4. Advanced Deformations by PointWOLF. In each transformations, the locally transformed samples (right) are generated from original samples (left).", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/8-Table7-1.png": "Table 7. Robustness to Corruption."}, "referred_figures_tables": [["4e6bbeee3e8810de2a5b12941f81920866a6b38b/1-Figure1-1.png"]], "question_id": [1], "question": ["What does it mean the realistic sample?"], "question_section": ["Method"], "question_trigger_sentence": ["The smoothly varying weights based on the distance to the anchor points allow spatially continuous augmentation and generate realistic samples."], "question_type": ["Shallow question"], "evidential_info": [[{"context": "Smooth deformations are key to generate realistic and locally transformed samples.A na\u00efve application of a random local transformation within its finite neighborhood may result in a discontinuous shape and an overlap of different parts. It has a high chance to lose discriminative structures.Instead, we employ the Nadaraya-Watson kernel regression\u00a0[27, 28] to smoothly interpolate the local transformations in the 3D space.Given M local transformations \\{T_{j}\\}_{j=1}^{M}, our smoothly varying transformation at an arbitrary point \\mathbf{p}_{i} is given as:\\small\\hat{T}(\\mathbf{p}_{i})=\\frac{\\sum_{j=1}^{M}{K_{h}}(\\mathbf{p}_{i},\\mathbf{p}^{\\mathcal{A}}_{j})T_{j}}{\\sum_{k=1}^{M}{K_{h}}(\\mathbf{p}_{i},\\mathbf{p}^{\\mathcal{A}}_{k})},(3)where K_{h}(\\cdot,\\cdot) is a kernel function with bandwidth h, and T_{j} is the local transformation in (2) centered at \\mathbf{p}^{\\mathcal{A}}_{j}.To define \\hat{T}(\\mathbf{p}_{i}) at any point in the 3D space, we use a kernel function that has a strictly positive value for any pair of points,i.e., K_{h}(\\mathbf{p}_{i},\\mathbf{p}_{j})>0 for \\forall\\mathbf{p}_{i},\\forall\\mathbf{p}_{j}.The following proposition theoretically guarantees that our augmentation is a smooth transformation under mild conditions. The proof is in the supplement.", "rationale": "Smooth deformations are key to generate realistic and locally transformed samples.A na\u00efve application of a random local transformation within its finite neighborhood may result in a discontinuous shape and an overlap of different parts."}, {"context": "Thus, CDA is simply a similarity transformation with small jittering that cannot simulate diverse shapes and deformable objects.Unlike synthetic datasets like ModelNet\u00a0[14] and ShapeNet\u00a0[26], a real-world dataset like ScanObjectNN\u00a0[1] further necessitates the generation of sophisticated deformations such as a mixture of local transformations.These are exemplified in Figure 1: airplanes with varying lengths and directions of wings and body, guitars in varying sizes and aspect ratios, and people with different heights and postures (e.g., crossing legs).", "rationale": "These are exemplified in Figure 1: airplanes with varying lengths and directions of wings and body, guitars in varying sizes and aspect ratios, and people with different heights and postures (e.g., crossing legs)."}]], "composition": ["A realistic sample in this context refers to a 3D object that has undergone a smooth deformation, meaning that the shape of the object changes gradually rather than abruptly. The realistic samples that the authors aim to generate are those that resemble real-world objects with diverse shapes and deformations, such as airplanes with varying wing lengths and directions, guitars with different sizes and aspect ratios, and people with different heights and postures."], "Is_figure_in_evidence": [true], "Is_table_in_evidence": [false], "question_key": ["528"], "passages": ["Modern deep learning techniques, which established their popularity on structured data, began showing success on point clouds.Unlike images with clear lattice structures, each point cloud is an unordered set of points with no inherent structures that globally represent various 3D objects.Recent deep learning efforts have focused on enabling neural networks to operate on point clouds.While several point cloud datasets appeared, a particular dataset of scanned real-world objects [1] required a much greater understanding of the point cloud structures to identify highly complex real-world objects.In response, the approaches have evolved from extracting point-wise information with no structural information [2] to explicitly encoding the local structure [3].These works on network development have been making steady progress despite the scarcity of point cloud data.", "Our interest lies in data augmentation, which is extensively utilized in other machine learning pipelines for solving the data scarcity issue.Interestingly, despite its prevalence in other data modalities, data augmentation (DA) on point clouds is relatively less explored.For instance, conventional data augmentation (CDA) [2, 3], which consists of global rotation, scaling, translation, and small point-wise noise, is commonly applied to point cloud datasets.Recently, PointAugment [4] proposes to learn the transformation matrix with an augmentor network to produce augmentations.PointMixup [5] finds linearly interpolated point cloud samples and their classes (e.g., Mixup [6]).Despite their efforts to elevate the previous CDA, they still have apparent limitations.Specifically, PointAugment relies on a single (thus global) transformation matrix/vector for each sample, and PointMixup mixes up the samples globally without explicitly considering each sample\u2019s local structures.Thus, the need for a point cloud augmentation approach capable of producing diverse samples that accurately depict real-world local variability (e.g., airplanes with varying lengths of wings and body) still remains.", "In this work, we propose a novel point cloud augmentation PointWOLF that satisfies such needs. PointWOLF generates diverse and realistic local deformations such as a person with varying postures (see Figure\u00a01).Our approach systematically enables local deformation by first considering multiple local transformations with respect to anchor points and carefully combining them in smoothly varying manners.Furthermore, we present AugTune to adaptively control DA strength in a sample-wise manner.AugTune produces consistent and beneficial samples during training with a single hyperparameter which alleviates the known dependence on hyperparameter selection of augmentation.We believe our method can further resolve this common dependence issue in general data augmentation.", "Our contributions is fourfold:(i) We propose a powerful point cloud transformation approach capable of generating diverse and realistic augmented samples by deforming local structures.(ii) Our framework adaptively adjusts the strength of augmentation with only a single hyperparameter.(iii) We demonstrate that our framework brings consistent improvements over existing state-of-the-art augmentation methods on both synthetic and real-world datasets in point cloud shape classification and part segmentation tasks.(iv) Our framework improves the robustness of models against various local and global corruptions.", "Deep Learning on Point Clouds.Early deep learning works on point cloud have focused on enabling existing CNNs to operate on point clouds.These include multi-view based methods like \u00a0[7, 8, 9, 10] where they project the 3D point cloud to 2D space through bird\u2019s-eye view or multi-view where 2D convolution becomes feasible.Similarly, other works\u00a0[11, 12, 13, 14, 15] voxelize the point cloud to directly apply 3D convolution on the voxelized point cloud.To preserve the original structure of point clouds, point-based methods have emerged.PointNet\u00a0[2] considers each point cloud as an unordered set and derives point-wise features with multi-layer perceptron and max pooling.However, a symmetric function such as pooling cannot characterize the local structure of point clouds, thus, PointNet++\u00a0[3] appeared which utilizes local information through hierarchical sampling and grouping.Other related studies\u00a0[16, 17, 18, 19, 20, 21] also rely on grouping to identify the relationship between points and extract local structure.DGCNN\u00a0[20] explicitly leverages the graph-like structure of the point clouds in the feature space rather than the 3D space.Interestingly, despite these network-wise efforts to exploit the local structure, only a few works have looked for solutions outside the networks, e.g., data augmentation.", "Data Augmentation.Data augmentation (DA) has become a necessity for modern machine learning model training to improve the generalization power.For point clouds, global similarity transformations such as rotation, scaling, and translation with point-wise jittering\u00a0[2, 3] are conventionally used.However, such conventional DAs (CDA) do not augment local structures which many successful works mentioned above find beneficial and explicitly leverage.In light of this, only a few recent studies proposed more advanced DAs on point cloud.PointAugment\u00a0[4] learns an augmentor network to augment samples to be more difficult to classify than the original sample.PointMixup\u00a0[5], enables Mixup\u00a0[6, 22] technique to point cloud, specifically by interpolating between two point cloud samples and predicting the ratio of the mixed classes with a soft label.While these works enable augmentations beyond simple similarity transformations, the transformations are fundamentally global: PointAugment learns a sample-wise global transformations matrix and PointMixup globally interpolates between samples.Thus, they often do not produce augmentations that are truly local and realistic. In response to this need, we propose a novel augmentation method PointWOLF which locally transforms samples as in Figure\u00a01.", "Searching for Optimal DA.In practice, identifying strong candidate transformations and optimal parameters for DA lacks intuitive conventions and heuristics thus requires extensive searching process.Several works address this, for instance, AutoAugment\u00a0[23] and Fast AutoAugment\u00a0[24] dynamically search for the best transformation policy via costly solvers such as reinforcement learning or bayesian optimization.RandAug\u00a0[25] has drastically reduced the search space by binding multiple augmentation parameters as a single hyperparameter.In this paper, we present AugTune that efficiently controls the sample-wise DA strength with a single parameter using the target confidence score.", "We first briefly describe the conventional DA for point clouds.Then, we describe PointWOLF, which aims to generate augmented point clouds. Unlike previous works that perform a global transformation and point-wise jittering, our framework augments the point clouds by locally weighted multiple transformations.We generate diverse and realistic augmented samples by deforming local structures.Also, to reduce the dependence on optimal hyperparameters of DA frameworks,we present AugTune, adaptively modulates the augmentation strength with a single parameter.", "A point cloud \ud835\udcab\u2208\u211d(3+C)\u00d7N\ud835\udcabsuperscript\u211d3\ud835\udc36\ud835\udc41\\mathcal{P}\\in\\mathbb{R}^{(3+C)\\times N}caligraphic_P \u2208 blackboard_R start_POSTSUPERSCRIPT ( 3 + italic_C ) \u00d7 italic_N end_POSTSUPERSCRIPT in 3D is a set of points {\ud835\udc291,\u22ef,\ud835\udc29N}subscript\ud835\udc291\u22efsubscript\ud835\udc29\ud835\udc41\\{\\mathbf{p}_{1},\\cdots,\\mathbf{p}_{N}\\}{ bold_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , bold_p start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }.Each point is represented as a vector \ud835\udc29i\u2208\u211d3+Csubscript\ud835\udc29\ud835\udc56superscript\u211d3\ud835\udc36\\mathbf{p}_{i}\\in\\mathbb{R}^{3+C}bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT 3 + italic_C end_POSTSUPERSCRIPT which is a concatenation of its coordinates (i.e., [x,y,z]) and C\ud835\udc36Citalic_C dimensional input features such as color and a surface normal.Since the problem of our interest only focuses on the point cloud structure, we assume that no additional input features are given, i.e., \ud835\udcab\u2208\u211d3\u00d7N\ud835\udcabsuperscript\u211d3\ud835\udc41\\mathcal{P}\\in\\mathbb{R}^{3\\times N}caligraphic_P \u2208 blackboard_R start_POSTSUPERSCRIPT 3 \u00d7 italic_N end_POSTSUPERSCRIPT.A conventional data augmentation (CDA) [2, 3] for point clouds applies a global similarity transformation (e.g., scaling, rotation and translation) and point-wise jittering.A resulting augmented point cloud \ud835\udcab\u2032\u2208\u211d3\u00d7Nsuperscript\ud835\udcab\u2032superscript\u211d3\ud835\udc41{\\mathcal{P}^{\\prime}}\\in\\mathbb{R}^{3\\times N}caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT 3 \u00d7 italic_N end_POSTSUPERSCRIPT is given as follows:\ud835\udcab\u2032=s\ud835\udc11\ud835\udcab+\ud835\udc01,superscript\ud835\udcab\u2032\ud835\udc60\ud835\udc11\ud835\udcab\ud835\udc01\\small\\mathcal{P}^{\\prime}=s\\mathbf{R}\\mathcal{P}+\\mathbf{B},caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = italic_s bold_R caligraphic_P + bold_B ,(1)where s>0\ud835\udc600s>0italic_s > 0 is a scaling factor, \ud835\udc11\ud835\udc11\\mathbf{R}bold_R is a 3D rotation matrix, and \ud835\udc01\u2208\u211d3\u00d7N\ud835\udc01superscript\u211d3\ud835\udc41\\mathbf{B}\\in\\mathbb{R}^{3\\times N}bold_B \u2208 blackboard_R start_POSTSUPERSCRIPT 3 \u00d7 italic_N end_POSTSUPERSCRIPT is a translation matrix with global translation and point-wise jittering.Typically, \ud835\udc11\ud835\udc11\\mathbf{R}bold_R is an extrinsic rotation parameterized by a uniformly drawn Euler angle for up-axis orientation.Scaling and translation factors are uniformly drawn from an interval, and point-wise jittering vectors are sampled from a truncated Gaussian distribution.", "Thus, CDA is simply a similarity transformation with small jittering that cannot simulate diverse shapes and deformable objects.Unlike synthetic datasets like ModelNet\u00a0[14] and ShapeNet\u00a0[26], a real-world dataset like ScanObjectNN\u00a0[1] further necessitates the generation of sophisticated deformations such as a mixture of local transformations.These are exemplified in Figure 1: airplanes with varying lengths and directions of wings and body, guitars in varying sizes and aspect ratios, and people with different heights and postures (e.g., crossing legs).", "We present a simple yet effective point cloud augmentation with weighted local transformations (PointWOLF).Our method generates deformation for point clouds by a convex combination of multiple transformations with smoothly varying weights.PointWOLF first selects several anchor points and locates random local transformations (e.g., similarity transformations) at the anchor points.Based on the distance from a point in the input to the anchor points, our method differentially applies the local transformations.The smoothly varying weights based on the distance to the anchor points allow spatially continuous augmentation and generate realistic samples.Our framework can be viewed as a kernel regression with transformations.", "Sampling anchor points is the first step of our framework to locate multiple local transformations.To minimize the redundancy between local transformations, the anchor points \ud835\udcab\ud835\udc9c\u2282\ud835\udcabsuperscript\ud835\udcab\ud835\udc9c\ud835\udcab\\mathcal{P}^{\\mathcal{A}}\\subset\\mathcal{P}caligraphic_P start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT \u2282 caligraphic_P are selected by the Farthest Point Sampling (FPS) algorithm.FPS randomly chooses the first point and then sequentially chooses the farthest points from previous points.This maximizes the coverage of anchor points and allows diverse transformations.", "Local transformations in our framework are centered at the anchor points.At each anchor point, we randomly sample a local transformation that includes scaling from the anchor point, changing aspect ratios, translation, and rotation around the anchor point.This subsumes the global transformation in (1).Given an anchor point \ud835\udc29j\ud835\udc9csubscriptsuperscript\ud835\udc29\ud835\udc9c\ud835\udc57\\mathbf{p}^{\\mathcal{A}}_{j}bold_p start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT in \ud835\udcab\ud835\udc9csuperscript\ud835\udcab\ud835\udc9c\\mathcal{P}^{\\mathcal{A}}caligraphic_P start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT, the local transformation for an input point \ud835\udc29isubscript\ud835\udc29\ud835\udc56\\mathbf{p}_{i}bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT can be written as:\ud835\udc29ij=\ud835\udc12j\ud835\udc11j(\ud835\udc29i\u2212\ud835\udc29j\ud835\udc9c)+\ud835\udc1bj+\ud835\udc29j\ud835\udc9c,subscriptsuperscript\ud835\udc29\ud835\udc57\ud835\udc56subscript\ud835\udc12\ud835\udc57subscript\ud835\udc11\ud835\udc57subscript\ud835\udc29\ud835\udc56subscriptsuperscript\ud835\udc29\ud835\udc9c\ud835\udc57subscript\ud835\udc1b\ud835\udc57subscriptsuperscript\ud835\udc29\ud835\udc9c\ud835\udc57\\small\\mathbf{p}^{j}_{i}=\\mathbf{S}_{j}\\mathbf{R}_{j}(\\mathbf{p}_{i}-\\mathbf{p}^{\\mathcal{A}}_{j})+\\mathbf{b}_{j}+\\mathbf{p}^{\\mathcal{A}}_{j},bold_p start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_R start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_p start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) + bold_b start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + bold_p start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ,(2)where \ud835\udc11jsubscript\ud835\udc11\ud835\udc57\\mathbf{R}_{j}bold_R start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, \ud835\udc12jsubscript\ud835\udc12\ud835\udc57\\mathbf{S}_{j}bold_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT and \ud835\udc1bjsubscript\ud835\udc1b\ud835\udc57\\mathbf{b}_{j}bold_b start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT are rotation matrix, scaling matrix and translation vector \ud835\udc1bjsubscript\ud835\udc1b\ud835\udc57\\mathbf{b}_{j}bold_b start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT respectively which specifically correspond to \ud835\udc29j\ud835\udc9csubscriptsuperscript\ud835\udc29\ud835\udc9c\ud835\udc57\\mathbf{p}^{\\mathcal{A}}_{j}bold_p start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT.\ud835\udc12\ud835\udc12\\mathbf{S}bold_S is a diagonal matrix with three positive real values, i.e., \ud835\udc12=diag(sx,sy,sz)\ud835\udc12diagsubscript\ud835\udc60\ud835\udc65subscript\ud835\udc60\ud835\udc66subscript\ud835\udc60\ud835\udc67\\mathbf{S}=\\text{diag}(s_{x},s_{y},s_{z})bold_S = diag ( italic_s start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT ) to allow different scaling factors for different axes.In order to change the aspect ratios along arbitrary directions, a randomly rotated scaling matrix, e.g., \ud835\udc12j\u2032=\ud835\udc11\u22121\ud835\udc12j\ud835\udc11superscriptsubscript\ud835\udc12\ud835\udc57\u2032superscript\ud835\udc111subscript\ud835\udc12\ud835\udc57\ud835\udc11\\mathbf{S}_{j}^{\\prime}=\\mathbf{R}^{-1}\\mathbf{S}_{j}\\mathbf{R}bold_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = bold_R start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_R, can be used.Since many commonly used datasets are pre-aligned in a standard space (e.g., airplanes facing the same direction), we may assume sensible object orientations.In practice, we see that scaling with reasonable rotations as in (2) is sufficient.", "Smooth deformations are key to generate realistic and locally transformed samples.A na\u00efve application of a random local transformation within its finite neighborhood may result in a discontinuous shape and an overlap of different parts. It has a high chance to lose discriminative structures.Instead, we employ the Nadaraya-Watson kernel regression\u00a0[27, 28] to smoothly interpolate the local transformations in the 3D space.Given M\ud835\udc40Mitalic_M local transformations {Tj}j=1Msuperscriptsubscriptsubscript\ud835\udc47\ud835\udc57\ud835\udc571\ud835\udc40\\{T_{j}\\}_{j=1}^{M}{ italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT, our smoothly varying transformation at an arbitrary point \ud835\udc29isubscript\ud835\udc29\ud835\udc56\\mathbf{p}_{i}bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is given as:T^(\ud835\udc29i)=\u2211j=1MKh(\ud835\udc29i,\ud835\udc29j\ud835\udc9c)Tj\u2211k=1MKh(\ud835\udc29i,\ud835\udc29k\ud835\udc9c),^\ud835\udc47subscript\ud835\udc29\ud835\udc56superscriptsubscript\ud835\udc571\ud835\udc40subscript\ud835\udc3e\u210esubscript\ud835\udc29\ud835\udc56subscriptsuperscript\ud835\udc29\ud835\udc9c\ud835\udc57subscript\ud835\udc47\ud835\udc57superscriptsubscript\ud835\udc581\ud835\udc40subscript\ud835\udc3e\u210esubscript\ud835\udc29\ud835\udc56subscriptsuperscript\ud835\udc29\ud835\udc9c\ud835\udc58\\small\\hat{T}(\\mathbf{p}_{i})=\\frac{\\sum_{j=1}^{M}{K_{h}}(\\mathbf{p}_{i},\\mathbf{p}^{\\mathcal{A}}_{j})T_{j}}{\\sum_{k=1}^{M}{K_{h}}(\\mathbf{p}_{i},\\mathbf{p}^{\\mathcal{A}}_{k})},over^ start_ARG italic_T end_ARG ( bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = divide start_ARG \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_K start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_p start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_K start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_p start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG ,(3)where Kh(\u22c5,\u22c5)subscript\ud835\udc3e\u210e\u22c5\u22c5K_{h}(\\cdot,\\cdot)italic_K start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( \u22c5 , \u22c5 ) is a kernel function with bandwidth h\u210ehitalic_h, and Tjsubscript\ud835\udc47\ud835\udc57T_{j}italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is the local transformation in (2) centered at \ud835\udc29j\ud835\udc9csubscriptsuperscript\ud835\udc29\ud835\udc9c\ud835\udc57\\mathbf{p}^{\\mathcal{A}}_{j}bold_p start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT.To define T^(\ud835\udc29i)^\ud835\udc47subscript\ud835\udc29\ud835\udc56\\hat{T}(\\mathbf{p}_{i})over^ start_ARG italic_T end_ARG ( bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) at any point in the 3D space, we use a kernel function that has a strictly positive value for any pair of points,i.e., Kh(\ud835\udc29i,\ud835\udc29j)>0subscript\ud835\udc3e\u210esubscript\ud835\udc29\ud835\udc56subscript\ud835\udc29\ud835\udc570K_{h}(\\mathbf{p}_{i},\\mathbf{p}_{j})>0italic_K start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) > 0 for \u2200\ud835\udc29i,\u2200\ud835\udc29jfor-allsubscript\ud835\udc29\ud835\udc56for-allsubscript\ud835\udc29\ud835\udc57\\forall\\mathbf{p}_{i},\\forall\\mathbf{p}_{j}\u2200 bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , \u2200 bold_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT.The following proposition theoretically guarantees that our augmentation is a smooth transformation under mild conditions. The proof is in the supplement.", "In our experiments, we use the Gaussian kernel with Euclidean distance after projection. Our kernel function isKh(\ud835\udc29i,\ud835\udc29j\ud835\udc9c;\u03a0j)=exp(\u2212\u2225\u03a0j(\ud835\udc29i\u2212\ud835\udc29j\ud835\udc9c)\u2225222h2),subscript\ud835\udc3e\u210esubscript\ud835\udc29\ud835\udc56subscriptsuperscript\ud835\udc29\ud835\udc9c\ud835\udc57subscript\u03a0\ud835\udc57expsubscriptsuperscriptdelimited-\u2225\u2225subscript\u03a0\ud835\udc57subscript\ud835\udc29\ud835\udc56subscriptsuperscript\ud835\udc29\ud835\udc9c\ud835\udc57222superscript\u210e2\\small K_{h}(\\mathbf{p}_{i},\\mathbf{p}^{\\mathcal{A}}_{j};\\Pi_{j})=\\text{exp}\\left(\\frac{-\\lVert\\Pi_{j}(\\mathbf{p}_{i}-\\mathbf{p}^{\\mathcal{A}}_{j})\\rVert^{2}_{2}}{2h^{2}}\\right),italic_K start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_p start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ; roman_\u03a0 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) = exp ( divide start_ARG - \u2225 roman_\u03a0 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_p start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 2 italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) ,(4)where h\u210ehitalic_h is the bandwidth and \u03a0j\u2208\u211d3\u00d73subscript\u03a0\ud835\udc57superscript\u211d33\\Pi_{j}\\in\\mathbb{R}^{3\\times 3}roman_\u03a0 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT 3 \u00d7 3 end_POSTSUPERSCRIPT is a projection matrix.\u03a0=diag(\u03c0x,\u03c0y,\u03c0z)\u03a0diagsubscript\ud835\udf0b\ud835\udc65subscript\ud835\udf0b\ud835\udc66subscript\ud835\udf0b\ud835\udc67\\Pi=\\text{diag}(\\pi_{x},\\pi_{y},\\pi_{z})roman_\u03a0 = diag ( italic_\u03c0 start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_\u03c0 start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , italic_\u03c0 start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT ) is constructed with \u03c0x,\u03c0y,\u03c0z\u223cBernoulli(\u03b2)similar-tosubscript\ud835\udf0b\ud835\udc65subscript\ud835\udf0b\ud835\udc66subscript\ud835\udf0b\ud835\udc67Bernoulli\ud835\udefd\\pi_{x},\\pi_{y},\\pi_{z}\\sim\\text{Bernoulli}(\\beta)italic_\u03c0 start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_\u03c0 start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , italic_\u03c0 start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT \u223c Bernoulli ( italic_\u03b2 ) for \u03b2\u2208(0,1)\ud835\udefd01\\beta\\in(0,1)italic_\u03b2 \u2208 ( 0 , 1 )111To prevent the projection matrix from zero-matrix, we resample \u03a0\u03a0\\Piroman_\u03a0 if (0,0,0) is selected., which acts as a \u201cbinary mask\u201d to measure distances with respect to a random subset of the coordinates.For instance, a kernel function with \u03a0j=diag(0,0,1)subscript\u03a0\ud835\udc57diag001\\Pi_{j}=\\text{diag}(0,0,1)roman_\u03a0 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = diag ( 0 , 0 , 1 ) attenuates the influence of local transformation Tjsubscript\ud835\udc47\ud835\udc57T_{j}italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT based on the distance from \ud835\udc29j\ud835\udc9csubscriptsuperscript\ud835\udc29\ud835\udc9c\ud835\udc57\\mathbf{p}^{\\mathcal{A}}_{j}bold_p start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT along the z\ud835\udc67zitalic_z-axis, and this allows more diverse and realistic transformations such as shearing and torsion (Section 4.4) by a combination of local similarity transformations.Similar to the scaling matrix \ud835\udc12\ud835\udc12\\mathbf{S}bold_S in (2), in our experiments we use the projections onto the canonical axes/planes instead of an arbitrary subspace. Our preliminary experiments show that they are sufficient for pre-aligned point clouds.", "We have introduced our framework from a kernel regression perspective.Figure\u00a02 shows a pipeline of our framework where the Augmented Sample is obtained by combining local transformations as a smooth transformation T^^\ud835\udc47\\hat{T}over^ start_ARG italic_T end_ARG and applying it on the Original Sample.Interestingly, at a high-level, we may also view our framework as an adaptive interpolation of multiple globally transformed point clouds resulting from applying different (local) transformations (e.g., T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, T3subscript\ud835\udc473T_{3}italic_T start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT in Figure\u00a02) on the Original Sample.Thus, our framework can be implemented in two ways: (1) Transforming each point once by a smoothly varying transformation T^^\ud835\udc47\\hat{T}over^ start_ARG italic_T end_ARG in Eq.\u00a0(3) and (2) Transforming each point M\ud835\udc40Mitalic_M times by the local transformations {Tj}j=1Msuperscriptsubscriptsubscript\ud835\udc47\ud835\udc57\ud835\udc571\ud835\udc40\\{T_{j}\\}_{j=1}^{M}{ italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT and interpolate these M\ud835\udc40Mitalic_M augmented points by the adaptive weights K(\ud835\udc29,\ud835\udc29j\ud835\udc9c)/\u2211kK(\ud835\udc29,\ud835\udc29k\ud835\udc9c)\ud835\udc3e\ud835\udc29subscriptsuperscript\ud835\udc29\ud835\udc9c\ud835\udc57subscript\ud835\udc58\ud835\udc3e\ud835\udc29subscriptsuperscript\ud835\udc29\ud835\udc9c\ud835\udc58K(\\mathbf{p},\\mathbf{p}^{\\mathcal{A}}_{j})/\\sum_{k}K(\\mathbf{p},\\mathbf{p}^{\\mathcal{A}}_{k})italic_K ( bold_p , bold_p start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) / \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_K ( bold_p , bold_p start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ).Although both approaches require O(MN)\ud835\udc42\ud835\udc40\ud835\udc41O(MN)italic_O ( italic_M italic_N ) complexity if we mainly consider M\ud835\udc40Mitalic_M anchor points and N\ud835\udc41Nitalic_N points, the second approach is slightly more efficient in practice since this only involves operations on points (vector) while the first approach involves operations on transformation matrices.Thus, we show the pseudocode of the second approach in Algorithm 1 and show the first approach\u2019s pseudocode in the supplement.", "The keys to effective data augmentation are strong candidate transformations and the optimal strength of the augmentation.We introduced PointWOLF that generates more diverse and smooth nonlinear transformations.Now, we present an efficient scheme to adaptively adjust the strength of data augmentation during training with a single hyperparameter.We believe that our scheme benefits not only our framework but also any classical data augmentation methods that heavily rely on an exhaustive grid search with a huge number of hyperparameters.", "AugTune.We present AugTune described in Algorithm 2 to control the strength of augmentation.AugTune adjusts the strength of data augmentation by mixing the augmentation proposal \ud835\udcab\u2032superscript\ud835\udcab\u2032\\mathcal{P^{\\prime}}caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT from PointWOLF and the original sample \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P.Given a classifier f(\u22c5;\ud835\udc30)\ud835\udc53\u22c5\ud835\udc30f(\\cdot;\\mathbf{w})italic_f ( \u22c5 ; bold_w ) and a sample \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P, let \ud835\udc32^\ud835\udcabsubscript^\ud835\udc32\ud835\udcab\\mathbf{\\hat{y}}_{\\mathcal{P}}over^ start_ARG bold_y end_ARG start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT and c\ud835\udcabsubscript\ud835\udc50\ud835\udcabc_{\\mathcal{P}}italic_c start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT denoteits prediction and confidence score, i.e., \ud835\udc32^\ud835\udcab=f(\ud835\udcab,\ud835\udc30)subscript^\ud835\udc32\ud835\udcab\ud835\udc53\ud835\udcab\ud835\udc30\\mathbf{\\hat{y}}_{\\mathcal{P}}=f(\\mathcal{P},\\mathbf{w})over^ start_ARG bold_y end_ARG start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT = italic_f ( caligraphic_P , bold_w ) and c\ud835\udcab=\ud835\udc32^\ud835\udcab\u22a4\ud835\udc32subscript\ud835\udc50\ud835\udcabsubscriptsuperscript^\ud835\udc32top\ud835\udcab\ud835\udc32c_{\\mathcal{P}}=\\mathbf{\\hat{y}}^{\\top}_{\\mathcal{P}}\\mathbf{y}italic_c start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT = over^ start_ARG bold_y end_ARG start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT bold_y, where \ud835\udc32\ud835\udc32\\mathbf{y}bold_y is the ground truth label represented in one-hot encoding.\ud835\udc32^\ud835\udcab\u2032subscript^\ud835\udc32superscript\ud835\udcab\u2032\\mathbf{\\hat{y}}_{\\mathcal{P}^{\\prime}}over^ start_ARG bold_y end_ARG start_POSTSUBSCRIPT caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT and c\ud835\udcab\u2032subscript\ud835\udc50superscript\ud835\udcab\u2032c_{\\mathcal{P}^{\\prime}}italic_c start_POSTSUBSCRIPT caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT are similarly defined for \ud835\udcab\u2032superscript\ud835\udcab\u2032\\mathcal{P^{\\prime}}caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.Note that all the confidence scores c\ud835\udcabsubscript\ud835\udc50\ud835\udcabc_{\\mathcal{P}}italic_c start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT, c\ud835\udcab\u2032subscript\ud835\udc50superscript\ud835\udcab\u2032c_{\\mathcal{P}^{\\prime}}italic_c start_POSTSUBSCRIPT caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT are obtained on the fly while training the model, i.e., an extra pretrained classifier is not required.To adjust the strength of augmentation, given a difficulty coefficient \u03bb\u2208(0,1]\ud835\udf0601\\lambda\\in(0,1]italic_\u03bb \u2208 ( 0 , 1 ],AugTune first computes a target confidence score c\ud835\udc50citalic_c for each sample byc=max\u2061(c\ud835\udcab\u2032,(1\u2212\u03bb)c\ud835\udcab).\ud835\udc50subscript\ud835\udc50superscript\ud835\udcab\u20321\ud835\udf06subscript\ud835\udc50\ud835\udcabc=\\max(c_{\\mathcal{P^{\\prime}}},(1-\\lambda)c_{\\mathcal{P}}).italic_c = roman_max ( italic_c start_POSTSUBSCRIPT caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , ( 1 - italic_\u03bb ) italic_c start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT ) .(5)Assuming the augmented \ud835\udcab\u2032superscript\ud835\udcab\u2032\\mathcal{P}^{\\prime}caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is difficult than the original \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P, i.e., c\ud835\udcab\u2032<c\ud835\udcabsubscript\ud835\udc50superscript\ud835\udcab\u2032subscript\ud835\udc50\ud835\udcabc_{\\mathcal{P^{\\prime}}}<c_{\\mathcal{P}}italic_c start_POSTSUBSCRIPT caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT < italic_c start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT, as \u03bb\ud835\udf06\\lambdaitalic_\u03bb gets close to 0, it implies that AugTune generates samples similar to the original sample \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P.Conversely, when \u03bb=1\ud835\udf061\\lambda=1italic_\u03bb = 1, c=c\ud835\udcab\u2032\ud835\udc50subscript\ud835\udc50superscript\ud835\udcab\u2032c=c_{\\mathcal{P^{\\prime}}}italic_c = italic_c start_POSTSUBSCRIPT caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, AugTune uses the augmentation proposal \ud835\udcab\u2032superscript\ud835\udcab\u2032\\mathcal{P^{\\prime}}caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT without any adjustment.To generate an augmented sample with the target confidence score, we use the linear interpolation of two samples \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P and \ud835\udcab\u2032superscript\ud835\udcab\u2032\\mathcal{P^{\\prime}}caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.Then, the problem is reduced to finding \u03b1*superscript\ud835\udefc\\alpha^{*}italic_\u03b1 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT defined by\u03b1*=argmin\u03b1\u2016c\u2212f(\u03b1\ud835\udcab+(1\u2212\u03b1)\ud835\udcab\u2032)\u20162.superscript\ud835\udefcsubscriptargmin\ud835\udefcsuperscriptnorm\ud835\udc50\ud835\udc53\ud835\udefc\ud835\udcab1\ud835\udefcsuperscript\ud835\udcab\u20322\\alpha^{*}=\\mathop{\\mathrm{argmin}}_{\\alpha}\\parallel c-f(\\alpha\\mathcal{P}+(1-\\alpha)\\mathcal{P}^{\\prime})\\parallel^{2}.italic_\u03b1 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT = roman_argmin start_POSTSUBSCRIPT italic_\u03b1 end_POSTSUBSCRIPT \u2225 italic_c - italic_f ( italic_\u03b1 caligraphic_P + ( 1 - italic_\u03b1 ) caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .(6)However, solving (6) directly by optimization algorithms or grid search is still computationally expensive.Thus, we approximate \u03b1\u2217superscript\ud835\udefc\u2217\\alpha^{\\ast}italic_\u03b1 start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT by \u03b1~=c\u2212c\ud835\udcab\u2032c\ud835\udcab\u2212c\ud835\udcab\u2032~\ud835\udefc\ud835\udc50subscript\ud835\udc50superscript\ud835\udcab\u2032subscript\ud835\udc50\ud835\udcabsubscript\ud835\udc50superscript\ud835\udcab\u2032\\tilde{\\alpha}=\\frac{c-c_{\\mathcal{P}^{{}^{\\prime}}}}{c_{\\mathcal{P}}-c_{\\mathcal{P}^{{}^{\\prime}}}}over~ start_ARG italic_\u03b1 end_ARG = divide start_ARG italic_c - italic_c start_POSTSUBSCRIPT caligraphic_P start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT \u2032 end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG italic_c start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT - italic_c start_POSTSUBSCRIPT caligraphic_P start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT \u2032 end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_ARG which is the solution to", "\u03b1c\ud835\udcab+(1\u2212\u03b1)c\ud835\udcab\u2032=c.\ud835\udefcsubscript\ud835\udc50\ud835\udcab1\ud835\udefcsubscript\ud835\udc50superscript\ud835\udcab\u2032\ud835\udc50\\alpha c_{\\mathcal{P}}+(1-\\alpha)c_{\\mathcal{P}^{\\prime}}=c.italic_\u03b1 italic_c start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT + ( 1 - italic_\u03b1 ) italic_c start_POSTSUBSCRIPT caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = italic_c .(7)Our experiments show this approximation does not cause degradation in the target tasks (see the supplement).The final augmented sample \ud835\udcab\u2217superscript\ud835\udcab\u2217\\mathcal{P}^{\\ast}caligraphic_P start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT is a convex combination of \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P and \ud835\udcab\u2032superscript\ud835\udcab\u2032\\mathcal{P}^{\\prime}caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT with \u03b1~~\ud835\udefc\\tilde{\\alpha}over~ start_ARG italic_\u03b1 end_ARG, i.e., \ud835\udcab\u2217=\u03b1~\ud835\udcab+(1\u2212\u03b1~)\ud835\udcab\u2032superscript\ud835\udcab\u2217~\ud835\udefc\ud835\udcab1~\ud835\udefcsuperscript\ud835\udcab\u2032\\mathcal{P}^{\\ast}=\\tilde{\\alpha}\\mathcal{P}+(1-\\tilde{\\alpha})\\mathcal{P}^{\\prime}caligraphic_P start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT = over~ start_ARG italic_\u03b1 end_ARG caligraphic_P + ( 1 - over~ start_ARG italic_\u03b1 end_ARG ) caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT, then the model parameter \ud835\udc30\ud835\udc30\\mathbf{w}bold_w is updated as \ud835\udc30\u2190\ud835\udc30\u2212\u03b3\u2207\ud835\udc30\u2112(f(\ud835\udcab\u2217,\ud835\udc30),\ud835\udc32)\u2190\ud835\udc30\ud835\udc30\ud835\udefesubscript\u2207\ud835\udc30\u2112\ud835\udc53superscript\ud835\udcab\u2217\ud835\udc30\ud835\udc32\\mathbf{w}\\;\\leftarrow\\;\\mathbf{w}-\\gamma\\nabla_{\\mathbf{w}}\\mathcal{L}(f(\\mathcal{P}^{\\ast},\\mathbf{w}),\\mathbf{y})bold_w \u2190 bold_w - italic_\u03b3 \u2207 start_POSTSUBSCRIPT bold_w end_POSTSUBSCRIPT caligraphic_L ( italic_f ( caligraphic_P start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , bold_w ) , bold_y ), where \u03b3\ud835\udefe\\gammaitalic_\u03b3 is a learning rate.Note that since the correspondence between \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P and \ud835\udcab\u2032superscript\ud835\udcab\u2032\\mathcal{P}^{\\prime}caligraphic_P start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT are known by construction, the interpolation of two point clouds can be obtained by a simple point-wise interpolation given as \ud835\udc29\u2217=\u03b1~\ud835\udc29+(1\u2212\u03b1~)\ud835\udc29\u2032superscript\ud835\udc29\u2217~\ud835\udefc\ud835\udc291~\ud835\udefcsuperscript\ud835\udc29\u2032\\mathbf{p}^{\\ast}=\\tilde{\\alpha}\\mathbf{p}+(1-\\tilde{\\alpha})\\mathbf{p}^{\\prime}bold_p start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT = over~ start_ARG italic_\u03b1 end_ARG bold_p + ( 1 - over~ start_ARG italic_\u03b1 end_ARG ) bold_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.Moreover, AugTune works as a safeguard to preserve the shape identity for the final \ud835\udcab\u2217superscript\ud835\udcab\u2217\\mathcal{P}^{\\ast}caligraphic_P start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT.So, we rarely observed unrealistic augmented samples with reasonable hyperparameters (see the supplement for visualizations).", "Remarks. As we viewed our framework as the kernel regression on local transformations, AugTune is directly applicable to the transformation (parameter) space.In other words, instead of the point-wise interpolation, we may simply interpolate the local transformation parameters: scaling matrix \ud835\udc12j\u2032=\u03b1\ud835\udc08+(1\u2212\u03b1)\ud835\udc12jsubscriptsuperscript\ud835\udc12\u2032\ud835\udc57\ud835\udefc\ud835\udc081\ud835\udefcsubscript\ud835\udc12\ud835\udc57\\mathbf{S}^{\\prime}_{j}=\\alpha\\mathbf{I}+(1-\\alpha)\\mathbf{S}_{j}bold_S start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_\u03b1 bold_I + ( 1 - italic_\u03b1 ) bold_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, rotation \u03b8j\u2032=(1\u2212\u03b1)\u03b8jsubscriptsuperscript\ud835\udf03\u2032\ud835\udc571\ud835\udefcsubscript\ud835\udf03\ud835\udc57\\theta^{\\prime}_{j}=(1-\\alpha)\\theta_{j}italic_\u03b8 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = ( 1 - italic_\u03b1 ) italic_\u03b8 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, and translation \ud835\udc1bj\u2032=(1\u2212\u03b1)\ud835\udc1bjsubscriptsuperscript\ud835\udc1b\u2032\ud835\udc571\ud835\udefcsubscript\ud835\udc1b\ud835\udc57\\mathbf{b}^{\\prime}_{j}=(1-\\alpha)\\mathbf{b}_{j}bold_b start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = ( 1 - italic_\u03b1 ) bold_b start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT.However, due to its slightly higher computational cost and inferior performance, we applied AugTune on the input data space, see Section 4.2.", "In this section, we demonstrate the effectiveness of our PointWOLF on both synthetic and real-world datasets.We begin by describing the datasets, baselines, and experimental setup.Then, we evaluate our framework for shape classification and part segmentation (Section 4.1), followed by ablation studies and analyses (Section 4.2).We conduct the experiments to show whether our method improves the robustness of models against both local and global corruptions leveraging diverse locally-augmented samples (Section 4.3).Lastly, we provide qualitative analysis of the augmented samples by PointWOLF (Section 4.4).", "Datasets.We use both synthetic and real-world datasets for shape classification to evaluate our framework. ModelNet40 (MN40)\u00a0[14] is a widely used synthetic benchmark dataset containing 9,840 CAD models in the training set and 2,468 CAD models in the validation set with total 40 classes of common object categories.As in\u00a0[5], we also use the reduced version of MN40 (ReducedMN40) to simulate data scarcity.ScanObjectNN (SONN) \u00a0[1] is a recent point cloud object dataset constructed from the real-world indoor datasets such as SceneNN\u00a0[29] and ScanNet\u00a0[30].We use the following versions of SONN: (1) OBJ_ONLY which has 2,309 and 581 scanned objects for the training and validation sets respectively and (2) PB_T50_RS which is a perturbed version with 11,416 and 2,882 scanned objects for the training and validation sets respectively. Both have 15 classes.For part segmentation we adopt ShapeNetPart\u00a0[26] which is a synthetic dataset contains 14,007 and 2,874 samples for training and validation sets.ShapeNetPart consists of 16 classes with 50 part labels. Each class has 2 to 6 parts.", "Implementation Details.All models and experiments are implemented in PyTorch.For PointNet and PointNet++, the PyTorch implementation by\u00a0[31] was used with a minimum modification.With DGCNN, we use the official PyTorch code by the authors.We train each model with a batch size of 32 for 250 epochs.Note that for maximal fairness and consistency, we reproduced the numbers for every baselines except for PointMixup\u00a0[5] and followed the evaluation protocol of [5] for every case.For our framework, the augmentation strength of PointWOLF was controlled by AugTune.Indeed in our framework, the difficulty coefficient \u03bb\ud835\udf06\\lambdaitalic_\u03bb is the only hyperparameter to tune. We used \u03bb=0.1\ud835\udf060.1\\lambda=0.1italic_\u03bb = 0.1 for synthetic datasets and \u03bb=0.3\ud835\udf060.3\\lambda=0.3italic_\u03bb = 0.3 for all real-world datasets. For more details, see the supplement.", "Baselines.We compare our framework (PointWOLF with AugTune) with the following data augmentation methods: (1) A conventional DA (CDA) that performs the global similarity transformation (e.g., rotation along the up-axis, scaling, and translation) with point-wise jittering as\u00a0[3].(2) PointAugment [4] performs shape-wise transformation and point-wise displacement by learning an augmentor network.For datasets on which the models have not been evaluated in the literature, we use the authors\u2019 official implementation of [4].(3) PointMixup [5] uses the interpolated sample between two point clouds.", "We evaluate our methods on shape classification using a synthetic dataset (MN40) and a real-world dataset (SONN).Also we conduct experiments on a synthetic dataset (ShapeNetPart) for part segmentation.", "Shape Classification.Table 1 shows that our PointWOLF achieves consistent improvements in overall accuracy on both MN40 and ReducedMN40 with all three models compared to other augmentation methods (CDA, PointAugment, and PointMixup).Observe that MN40 and ReducedMN40 are pre-aligned synthetic datasets and interestingly CDA without rotation denoted by CDA (w/o R) outperforms CDA.Despite the saturated datasets, PointWOLF improves overall accuracy by 1.6 % compared to the best performing baseline on ReducedMN40 with PointNet.", "Next, Table\u00a02 shows the experimental results on SONN that is a more challengingand diverse real-world dataset.As expected, diverse and realistic augmented samples from PointWOLF significantly improve the performance on both OBJ_ONLY and PB_T50_RS with all three models.Specifically, on PB_T50_RS with PointNet++,the performance gains are 4.7%, 6.2%, and 3.5% compared to CDA, PointAugment, and PointMixup, respectively.Our PointWOLF benefits the models more on the challenging cases with real-world data.", "Part Segmentation.Given a point cloud \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P, part segmentation isa point-wise classification where a model predicts a label for each point \ud835\udc29isubscript\ud835\udc29\ud835\udc56\\mathbf{p}_{i}bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.In part segmentation, to derive the mixing ratio \u03b1~~\ud835\udefc\\tilde{\\alpha}over~ start_ARG italic_\u03b1 end_ARG in (7) at the object-level, we simply used the average of the pixel-wise confidence scores for our AugTune, i.e., c\ud835\udcab=\u2211ic\ud835\udc29i/|\ud835\udcab|subscript\ud835\udc50\ud835\udcabsubscript\ud835\udc56subscript\ud835\udc50subscript\ud835\udc29\ud835\udc56\ud835\udcabc_{\\mathcal{P}}=\\sum_{i}c_{\\mathbf{p}_{i}}/|\\mathcal{P}|italic_c start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT / | caligraphic_P |.Our experiments in Table\u00a03 show that on ShapeNetPart\u00a0[26],PointWOLF consistently improves mean IoU (mIoU) over baselines (0.3% over PointNet, 0.4% over PointNet++ and DGCNN), demonstrating the applicability of PointWOLF to point-wise tasks.", "We conduct ablation studies and analyses on SONN (OBJ_ONLY) dataset with PointNet++ to analyze the significance of each component of PointWOLF and AugTune.", "Local Transformation Ablations.Table\u00a04 reports the ablations on three types of local transformations in PointWOLF: rotation (R), scaling (S), and translation (T).PointWOLF with no local transformations is equivalent to PointNet++ [3] with CDA.All three types of local transformations contribute to the accuracy gain.The best performance is obtained by +RST which utilizes all three local transformations, providing 3.5% improvement over the baseline with no local transformations denoted by \u2018None\u2019.", "AugTune Ablations.We evaluate how effectively AugTune controls the augmentation strengths given suboptimal augmentation ranges.We set the augmentation ranges S\ud835\udc46Sitalic_S = (\u03c1rsubscript\ud835\udf0c\ud835\udc5f\\rho_{r}italic_\u03c1 start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT=15\u2218superscript1515^{\\circ}15 start_POSTSUPERSCRIPT \u2218 end_POSTSUPERSCRIPT, \u03c1ssubscript\ud835\udf0c\ud835\udc60\\rho_{s}italic_\u03c1 start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT=2222, \u03c1tsubscript\ud835\udf0c\ud835\udc61\\rho_{t}italic_\u03c1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT=1111)and use the multiples of the augmentation ranges: kS\ud835\udc58\ud835\udc46kSitalic_k italic_S=(k\u03c1r\ud835\udc58subscript\ud835\udf0c\ud835\udc5fk\\rho_{r}italic_k italic_\u03c1 start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, k\u03c1s\ud835\udc58subscript\ud835\udf0c\ud835\udc60k\\rho_{s}italic_k italic_\u03c1 start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, k\u03c1t\ud835\udc58subscript\ud835\udf0c\ud835\udc61k\\rho_{t}italic_k italic_\u03c1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT).Table\u00a05 shows that PointWOLF w/ AugTune outperforms PointWOLF w/o AugTune by 0.4 % \u223csimilar-to\\sim\u223c 1.9 %.Our AugTune simplifies and accelerates the augmentation strength tuning with one difficulty coefficient \u03bb\ud835\udf06\\lambdaitalic_\u03bb.Our AugTune also benefits other augmentation methods, e.g., CDA, (see the supplement).", "Interpolation Space for AugTune.Two interpolation spaces can be considered for AugTune: the input data space and the transformation (parameter) space.Although directly tuning the transformation parameters seems natural, we have experimentally shown that AugTune in the input data space is a sensible choice.Table\u00a06 shows the superiority of AugTune in the input data space regarding both performance and computational efficiency.For N\ud835\udc41Nitalic_N points and M\ud835\udc40Mitalic_M anchor points, AugTune in the transformation (parameter) space requires computing a new transformation for each point and each anchor point in O(MN)\ud835\udc42\ud835\udc40\ud835\udc41O(MN)italic_O ( italic_M italic_N ).Contrarily, AugTune in the input data space simply interpolates the points (i.e., \u03b1\ud835\udc29+(1\u2212\u03b1)\ud835\udc29\u2032\ud835\udefc\ud835\udc291\ud835\udefcsuperscript\ud835\udc29\u2032\\alpha\\mathbf{p}+(1-\\alpha)\\mathbf{p}^{\\prime}italic_\u03b1 bold_p + ( 1 - italic_\u03b1 ) bold_p start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT for each \ud835\udc29\ud835\udc29\\mathbf{p}bold_p) in O(N)\ud835\udc42\ud835\udc41O(N)italic_O ( italic_N ).", "Additional studies demonstrate our PointWOLF improves the robustness of models against various corruptions as shown in Figure\u00a03.First, we consider two local corruptions: (1) LocalDrop drops \ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C local clusters and (2) LocalAdd adds \ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C local clusters where a cluster consists of K\ud835\udc3eKitalic_K nearest points from a randomly selected cluster center point.We used K=50\ud835\udc3e50K=50italic_K = 50 in both cases.Second, to examine the general robustness to global corruption, we perform random point-wise (3) Dropout with a dropout rate r\u2208{0.25,0.5,0.75}\ud835\udc5f0.250.50.75r\\in\\{0.25,0.5,0.75\\}italic_r \u2208 { 0.25 , 0.5 , 0.75 } and (4) Noise perturbation by offsets drawn from a Gaussian distribution with standard deviation \u03c3\u2208{0.01,0.03,0.05}\ud835\udf0e0.010.030.05\\sigma\\in\\{0.01,0.03,0.05\\}italic_\u03c3 \u2208 { 0.01 , 0.03 , 0.05 }.", "We trained PointNet++ with CDA (baseline) and PointWOLF and evaluated them on corrupted samples by the local and global corruptions above.Experimental results on MN40 in Table\u00a07 show that compared to CDA, PointWOLF consistently and significantly improves the robustness against various corruptions.Importantly, the gain over the baseline significantly increases as the amount of corruptions increases: 7.2% for LocalDrop (\ud835\udc9e=7\ud835\udc9e7\\mathcal{C}=7caligraphic_C = 7), 13.1% for LocalAdd (\ud835\udc9e=7\ud835\udc9e7\\mathcal{C}=7caligraphic_C = 7), 31.3% for Dropout (r=0.75\ud835\udc5f0.75r=0.75italic_r = 0.75), and 22.2% for Noise (\u03c3=0.05\ud835\udf0e0.05\\sigma=0.05italic_\u03c3 = 0.05).We believe that the diverse samples augmented by locally weighted transformations in PointWOLF help models to learn more robust features against both \u2018local\u2019 and \u2018global\u2019 corruptions.", "Although PointWOLF essentially makes use of simple transformations such as rotation, scaling, and translation, we interestingly find that PointWOLF often mimics highly advanced yet realistic global deformations like torsion and shearing which cannot trivially be applied to point clouds.We achieve this by (1) projecting the transformations to random subsets of the axes and (2) allowing AugTune to identify \u201cbeneficial\u201d cases which interestingly turn out to be a set of realistic deformations.Figure\u00a04 displays several such examples.For instance, when two anchor points are located at the top and bottom of the stool in Figure\u00a04(a), a torsion occurs when it rotates only along the up-axis while preserving the near-anchor shapes of bright regions.", "Similarly, a combination of local scaling and translation produce shearing or partial scaling.In fact, many advanced deformations that naturally preserve the shape identity are commonly defined by combinations of simpler transformations.In this sense, PointWOLF can adaptively allow a set of local transformations that often mimic advanced deformations.Importantly, seeing how these visually explainable augmentations from local transformations also bring empirical benefits, understanding and exploiting local structures are crucial for successful DA on point cloud.", "We propose a novel point cloud augmentation method, PointWOLF, which augments point clouds by weighted local transformations.Our method generates diverse and realistic augmented samples with smoothly varying deformations formulated as a kernel regression and brings significant improvements on point cloud tasks across several datasets.Moreover, to find an optimal augmentation in an expansive search space, our AugTune adaptively controls the strength of augmentation during training with a single hyperparameter.Our findings show that the augmentations we produce are not only visually realistic but also beneficial to the models, further validating the importance of understanding the local structure of point clouds.", "This work was supported by ICT Creative Consilience program(IITP-2021-2020-0-01819) supervised by the IITP, Research on CPU vulnerability detection and validation (No. 2019-0-00533), the National Research Council of Science & Technology (NST) grant by the Korea government (MSIT) (No. CAP-18-03-ETRI), and Samsung Electronics."], "figure_types": {"4e6bbeee3e8810de2a5b12941f81920866a6b38b/1-Figure1-1.png": "other", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/3-Figure2-1.png": "schematic", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/6-Table2-1.png": "table", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/7-Table3-1.png": "table", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/7-Table4-1.png": "table", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/7-Table6-1.png": "table", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/8-Figure3-1.png": "schematic", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/8-Figure4-1.png": "schematic", "4e6bbeee3e8810de2a5b12941f81920866a6b38b/8-Table7-1.png": "table"}}, "2210.12302": {"paper_id": "paper_17", "title": "What do Large Language Models Learn beyond Language?", "arxiv_url": "https://arxiv.org/abs/2210.12302", "s2orc_url": "https://www.semanticscholar.org/paper/5efab88c0cdb11c795fa8f44a5d31b40e2a1c261", "all_figures_tables": {"5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/1-Figure1-1.png": "Figure 1: We investigate the effect of pretraining of languages models on learning non-linguistic tasks using three task paradigms involving symbolic reasoning.", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/12-Table4-1.png": "Table 4: Statistical significance values (paired t-test) between non-pretrained model and other baseline BERT models trained on different datasets.", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/12-Table5-1.png": "Table 5: Statistical significance values (paired t-test) between non-pretrained model and other baseline DeBERTA models trained on different datasets.", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/13-FigureA.1-1.png": "Figure A.1: Performance comparison of pretrained and non-pretrained models of DeBERTa and BERT large on four quantitative computation tasks (odd classification, even classification, odd even classification and decimal operation).", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/13-FigureA.2-1.png": "Figure A.2: Performance comparison of pretrained and non-pretrained models of DeBERTa and BERT large on four quantitative tasks (mean, median, mode, decimal & word operation).", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/13-FigureA.3-1.png": "Figure A.3: Performance comparison of pretrained and non-pretrained models of DeBERTa, and BERT large on regular expression tasks (AA*BB*CC*DD*EE* and recognize {0,1,2}*02*).", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/13-FigureA.4-1.png": "Figure A.4: Performance comparison of pretrained and non-pretrained models of DeBERTa and BERT large on four string reasoning (palindrome, anagram, isogram and tautonym classification).", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/14-FigureA.5-1.png": "Figure A.5: Performance comparison of pretrained and non-pretrained models of DeBERTa and BERT large on five string reasoning tasks (length of a string, maximum frequent character, vowels classification, parity check and count of unique character).", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/14-Table6-1.png": "Table 6: Statistical significance values (paired t-test) between pretrained and non-pretrained model on all the tasks.", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/2-Table1-1.png": "Table 1: Description of the non-linguistic tasks with input and output examples. Classes are the class labels for each task. Input range denotes the range of the input operands in each task.", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/4-Figure2-1.png": "Figure 2: Performance comparison of pretrained and non-pretrained models of BERT small, and ELMO on four quantitative computation tasks (odd classification, even classification, odd even classification and decimal operation).", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/4-Figure3-1.png": "Figure 3: Performance comparison of pretrained and non-pretrained models of BERT small, and ELMO on four quantitative computation tasks (mean, median, mode and decimal & word operation tasks).", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/5-Figure4-1.png": "Figure 4: Performance comparison of pretrained and non-pretrained models of BERT small, and ELMO on regular expression tasks (AA*BB*CC*DD*EE* and recognize {0,1,2}*02*).", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/6-Figure5-1.png": "Figure 5: Effect of model size on non-pretrained models. NP denotes a non-pretrained model and PT denotes the pretrained model. Mid-sized non-pretrained models outperform bigger and smaller variants, but still perform significantly lower than pretrained LM models. Results are the average of six representative tasks: palindrome classification, anagram classification, isogram classification, tautonym classification, mean and median.", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/7-Figure6-1.png": "Figure 6: Performance comparison of pretrained and non-pretrained models of BERT small, and ELMO on four string reasoning tasks (palindrome, anagram, isogram and tautonym classification).", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/7-Figure7-1.png": "Figure 7: Performance comparison of pretrained and non-pretrained models of BERT small, and ELMO on five string reasoning tasks (length of a string, maximum frequent character, vowels classification, parity check and count of unique character).", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/7-Table2-1.png": "Table 2: Average accuracy scores of different pretrained BERT representations on six representative non-linguistic tasks: palindrome, anagram, isogram, tautonym, mean, and median. The results are rounded to the nearest percentage point. All models except Synthetic Vocabulary (Syn Voc). show statistically significant improvements (p &lt; 0.05) over the non-pretrained models.", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/8-Table3-1.png": "Table 3: Average accuracy scores of different pretrained DeBERTA representations on six representative nonlinguistic tasks: palindrome, anagram isogram, tautonym, mean, and median. The results are rounded to the nearest percentage point. All models except Synthetic Vocabulary (Syn Voc). show statistically significant improvements (p &lt; 0.05) over the non-pretrained models."}, "referred_figures_tables": [["5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/1-Figure1-1.png", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/1-Figure1-1.png", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/2-Table1-1.png", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/7-Table2-1.png"], ["5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/1-Figure1-1.png", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/1-Figure1-1.png", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/2-Table1-1.png"], ["5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/5-Figure4-1.png"]], "question_id": [7, 15, 20], "question": ["What does non-linguistic means?", "Explain the motivation of this paper", "Look Figure 4.  Give your one observation by comparing (a) and (b), or pretrained and non-pretrained. Reason them."], "question_section": ["Introduction", " Introduction", "5. comparative Evaluation"], "question_trigger_sentence": ["On the other hand, little work so far has explored the abilities of pretrained LMs for learning non-linguistic tasks.", "In this paper, we explore whether pretraining on text is inherently about learning language, or if pre- training also imbues LMs with skills for symbolic manipulation and non-linguistic reasoning", "Figure 4: Performance comparison of pretrained and non-pretrained models of BERT small, and ELMO on regular expression tasks (AABBCCDDEE* and recognize {0,1,2}02)."], "question_type": ["Testing question", "Deep/complex question", "Deep/complex question"], "evidential_info": [[{"context": "In this paper, we explore whether pretraining on text is inherently about learning language, or if pretraining also imbues LMs with skills for symbolic manipulation and non-linguistic reasoning (for example, performing quantitative computation such as finding the median of a set of numbers, recognizing regular expressions, or identifying whether a string is a palindrome, as shown in Figure\u00a01). In other words, we investigate whether and how pretraining develops helpful inductive biases for non-linguistic reasoning. For this analysis, we create a set of 19 tasks from three categories of task paradigms: quantitative computation (\u00a73.1), recognizing regular expressions (\u00a73.2), and string reasoning (\u00a73.3). Figure 1 shows an example for each category, and the full list of tasks is described in the table 1. We experiment with transformer and RNN based LMs (\u00a74) for learning these tasks, and perform a comparative analysis with (non-pretrained) neural model variants from the perspective of learning metrics such as accuracy and sample efficiency.", "rationale": "quantitative computation, recognizing regular expressions, and identifying whether a string is a palindrome are examples of non linguistic tasks."}, {"context": "Tables 2 and 3 shows the average accuracy of six non-linguistic tasks (palindrome classification, isogram classification, tautonym classification, odd even classification, decimal operation and median) fine-tuned using different BERT and DeBERTA representations respectively. We note that the models pretrained on all three domains outperformed the non-pretrained model (NP). This suggests that the results of experiments in Section 5 generalize to new text corpora for pretraining, and do not rely on having access to text on specific topics during pretraining. This is a non-trivial result, since it suggests for example, that the higher performance of pretrained models on tasks such as palindrome and anagram classification is not due to the pretrained models having seen information about such concepts during pretraining. This is especially so since the results even generalize to ROC stories, which contain no information on such technical concepts. ", "rationale": "There are six non-linguistic tasks, palindrome classification, isogram classification, tautonym classification, odd even classification, decimal operation and median."}], [{"context": "In this paper, we explore whether pretraining on text is inherently about learning language, or if pretraining also imbues LMs with skills for symbolic manipulation and non-linguistic reasoning (for example, performing quantitative computation such as finding the median of a set of numbers, recognizing regular expressions, or identifying whether a string is a palindrome, as shown in Figure\u00a01). In other words, we investigate whether and how pretraining develops helpful inductive biases for non-linguistic reasoning. For this analysis, we create a set of 19 tasks from three categories of task paradigms: quantitative computation (\u00a73.1), recognizing regular expressions (\u00a73.2), and string reasoning (\u00a73.3). Figure 1 shows an example for each category, and the full list of tasks is described in the table 1. We experiment with transformer and RNN based LMs (\u00a74) for learning these tasks, and perform a comparative analysis with (non-pretrained) neural model variants from the perspective of learning metrics such as accuracy and sample efficiency.", "rationale": "They explore whether pretraining on text is inherently about learning language or if pretraining inject non-linguisitc reasoning to LMs."}], [{"context": "Recognizing regular expressions: Figure 4 shows the comparative performance of pretrained LMs on non-pretrained models on the two tasks involving recognizing regular expressions. For both tasks, we note that the pretrained LMs can perfectly learn the tasks with many fewer labeled examples compared to the non-pretrained models. In both cases, the non-pretrained Transformer-based models eventually reach optimal performance as well. However, curiously the ELMO based non-pretrained models struggle with learning both tasks.", "rationale": "pretrained LMs can perfectly learn the tasks with many fewer labeled examples, compared to the non-pretrained models in both tasks."}]], "composition": ["Non-linguistic is something which is not related to linguistic information, and it includes the tasks such as quantitative computation and decimal operation.", "The motivation of this paper is analyzing whether pretraining on text is inherently about learning language or if pretraining inject non-linguisitc reasoning to LMs.", "pretrained LMs can perfectly learn the tasks with many fewer labeled examples, compared to the non-pretrained models in both tasks."], "Is_figure_in_evidence": [true, true, true], "Is_table_in_evidence": [false, false, false], "question_key": ["532", "547", "552"], "passages": ["Pretrained Language Models (LMs) have shown singular succcess on a range of natural language understandings tasks, to the extent that they have become foundational for contemporary NLP systems. Several works have investigated why pretraining works so well Warstadt et\u00a0al. (2019); Zhao et\u00a0al. (2020). In particular, studies have shown that the pretrained LMs like BERT capture linguistic knowledge about syntax Lin et\u00a0al. (2019); Wu et\u00a0al. (2020), semantics Vuli\u0107 et\u00a0al. (2020b, a) and morphology Hofmann et\u00a0al. (2020, 2021). In fact, Tenney et\u00a0al. (2019) demonstrated that learned representations in pretrained LMs even internally reflect the classical NLP pipeline. Since most NLP benchmarks such as SuperGLUE Wang et\u00a0al. (2019) naturally are focused on tasks such as textual entailment and reading comprehension that require linguistic knowledge and reasoning, it is unsurprising that LMs have achieved strong results on these tasks. On the other hand, little work so far has explored the abilities of pretrained LMs for learning non-linguistic tasks.", "In this paper, we explore whether pretraining on text is inherently about learning language, or if pretraining also imbues LMs with skills for symbolic manipulation and non-linguistic reasoning (for example, performing quantitative computation such as finding the median of a set of numbers, recognizing regular expressions, or identifying whether a string is a palindrome, as shown in Figure\u00a01). In other words, we investigate whether and how pretraining develops helpful inductive biases for non-linguistic reasoning. For this analysis, we create a set of 19 tasks from three categories of task paradigms: quantitative computation (\u00a73.1), recognizing regular expressions (\u00a73.2), and string reasoning (\u00a73.3). Figure 1 shows an example for each category, and the full list of tasks is described in the table 1. We experiment with transformer and RNN based LMs (\u00a74) for learning these tasks, and perform a comparative analysis with (non-pretrained) neural model variants from the perspective of learning metrics such as accuracy and sample efficiency.", "Our experiments (\u00a75) reveal that pretrained models overall perform substantially better and are more sample efficient on most tasks. However, there are significant differences and patterns in performance between task types, as well as variance between different LM architectures. Since non-pretrained models do not have the benefit of regularization that comes from pretraining, a plausible reason for the discrepancy between them and pretrained LMs might be underfitting of the non-pretrained models when trained on comparatively small dataset sizes. To account for this, we also comprehensively explore the effect of model size (\u00a76) of non-pretrained models for both transformer and RNN architectures. We find that the discrepancy in performance remains even for smaller neural models, indicating that the differences are not simply due to a mismatch in model and data sizes.", "Finally, we investigate the role that pretraining data plays in influencing task performance on non-linguistic tasks (\u00a77). We experiment with pretraining on different domains of text, pretraining on perturbed representations of natural language text (such as shuffled word order), pretraining on text of computer programs (no linguistic properties of natural languages), pretraining on multi-lingual and non-English text, and pretraining with synthetic text (data sampled from synthetic distributions). Our analysis reveals that the advantages of pretraining surprisingly persist with various degrees across these variations, suggesting hithertho unexplored connections between pretraining and the learning abilities of language models. Our contributions are:", "\u2022We compare a range of pretrained LMs and non-pretrained models on a carefully designed suite of 19 classifications tasks that require non-linguistic reasoning.\u2022We comprehensively explore the role of the pretraining data by experimenting with models pretrained from texts with different provenances.\u2022We establish that the positive effects of pretraining are not simply due to better model regularization by experimenting with neural models with different complexities and architectures.", "A body of work has investigated contextual word embeddings to determine whether they capture aspects of mathematical meaning for numbers Naik et\u00a0al. (2019).Wallace et\u00a0al. (2019) probed numerical supremacy on token embeddings of contextual language models such as ELMO and BERT. Thawani et\u00a0al. (2021) surveyed numerical understanding in NLP models using 7 sub-tasks such as measurement estimation and word problems. Our work diverges from these in exploring a richer set of tasks including harder tasks such as set operations. Further, previous methods explore mathematical reasoning tasks posed as language problems, which conflates the problems of language and mathematical learning and also makes the datasets susceptible to biases due to data collection. Our analysis circumvents both these issues by design.", "Some previous works have explored the ability of RNN and Transformer architectures for learning regular languages Weiss et\u00a0al. (2018); Sennhauser and Berwick (2018); Suzgun et\u00a0al. (2019b); Bhattamishra et\u00a0al. (2020), closing brackets Skachkova et\u00a0al. (2018), and dynamic counting Suzgun et\u00a0al. (2019a). However, they focus on the learnability of these tasks with specific architectures, and do not look at pretrained LMs, which are our focus here.", "Finally, in our discussion, we conceptually stretch the notion of inductive bias. The idea of inductive bias is usually associated with specific model types McCoy et\u00a0al. (2020); Kharitonov and Chaabouni (2021), architectures Xu et\u00a0al. (2021); Brutzkus and Globerson (2021) and regularization approaches Helmbold and Long (2015). We believe that extending this to refer to learning tasks with pretrained LMs is both reasonable and useful.", "In this section, we describe the tasks used for our analysis, which we refer to as NILM (measuring Non-linguistic Inductive bias in Language Models). The tasks correspond to three task paradigms: (1) quantitative computation, (2) regular expressions, and (3) string reasoning. Each task in NILM is posed as a classification task. The descriptions for all the tasks with input and output examples, class labels and the input range are shown in Table\u00a01. Each task has a synthetically generated dataset with train/dev/test splits222The training set size for all tasks is 10K, dev set size is 1K and test set size is 1K, except for tasks on recognizing regular expressions, where the test set size is 2K following previous work Bhattamishra et\u00a0al. (2020).. To avoid biases in the datasets, relevant numbers and strings in individual examples are uniformly sampled from the appropriate ranges.", "This task paradigm focuses on tasks involving arithmetic and set statistics. Odd classification.Classify if a number is odd. Even classification.Classify if a number is even. Odd even classification.For a given number N\ud835\udc41Nitalic_N and a string \u201ceven\u201d or \u201codd\u201d, classify if the number satisfies the string condition. Decimal operation. Subtract or divide two numbers. Operands are represented in decimal notation. Decimal & word operation. Subtract or divide two numbers. Operands are represented in decimal or word notation. Mean. Given a set of numbers, output the mean.Median. Given a set, output the median. Mode. Given a set of numbers, output the mode.", "This task paradigm focuses on recognizing regular expressions. The training data consists of positive and negative examples of strings matching a regular expression Bhattamishra et\u00a0al. (2020). Recognize {0,1,2}*02*.Recognize if a pattern matches {0,1,2}*02*. The maximum length of the patterns is 20.Recognize AA*BB*CC*DD*EE*.Recognize if a pattern matches AA*BB*CC*DD*EE*. The maximum length of the patterns is 30.", "This task paradigm focuses on reasoning tasks over individual strings or pairs of strings. Palindrome classification.A string is a palindrome if it reads the same forward and backward. The task is to classify whether a given string is a palindrome. The string length ranges from 1 to 15.Anagram classification.Two strings are anagrams if one is formed by rearranging letters from the other. The task is to classify if a pair of strings are anagrams. The string length ranges from 2 to 15.Isogram classification.A string is an isogram if it has no repeating characters. The task is to classify whether a given string is an isogram. The string length ranges from 1 to 52.Tautonym classification.A tautonym is a word which can be broken down into two identical parts, with the same spelling. The task is to classify whether a given string is a tautonym. The string length ranges from 1 to 10. Length of a string.Output the length of a given string. The string length ranges from 1 to 10.Count of unique characters.Given a string, count the number of unique characters in it. The string lengths ranges from 10 to 30.Parity check.Given a binary string, output if the counts of ones and zeros are the same. The maximum length of the binary string is 20.Vowels classification.Given a string, classify if the string contains only vowel characters. The string length ranges from 3 to 10. Maximum frequent character.Given a string, output the character with the maximum frequency. The string length ranges from 5 to 30.", "Next, we describe the LMs and their variants used in NILM. We experiment with four language models, based on both Transformer and RNN architectures. BERT small.This is the bert-base-uncased model with 12 transformer encoder layers and the dimension of the representations is 768. BERT tokenizer is based on the WordPiece model Wu et\u00a0al. (2016). BERT large.This is the bert-large-uncased model which has 24 transformer encoders and representations have 1024 dimensions. DeBERTa.This is a transformer based language model and its tokenizer is built using Byte Pair Encoding Sennrich et\u00a0al. (2016). We consider the DeBERTa base model. It has 12 transformer encoder layers and representations have 768 dimensions. ELMO.This is an LSTM based language model Peters et\u00a0al. (2018).It has 3 layers and the output representations have 1024 dimensions.", "Our experiments are based on pretrained and non-pretrained variants of these architectures. For pretrained variants, the weights are initialized with the pretrained weights. The tokenization on the training data is performed using the pre-built vocabulary. For the non-pretrained neural models, the weights are initialized randomly and updated during training. The tokenizer used is the same as in the pretrained variant.", "All the models are trained with varying training data of sizes 10, 20, 40, 80, 160, 320, 640, 1280, 2560, 5120, 6000, 7000, 8000, 9000 and 10000. For training set sizes of less than 1000 samples, we report the average of 10 runs. For training set sizes greater than 1000, all reported numbers are averages of 5 runs. In the next section, we present a comparative analysis of pretrained and non-pretrained models.", "Next, we compare the performance of pretrained and non-pretrained models on tasks in NILM 333Details, including statistical significance results with the paired t-value test, are included in Appendix 6.", "Quantitative computation: Figure 2 shows results on odd classification, even classification, odd even classification and decimal operation tasks. We find that pretrained LMs outperformed non-pretrained model for all of these tasks. Further, Transformer-based LMs outperformed the RNN-based ELMO models in all the tasks444We will focus on BERT small as representative of transformer models. Results for BERT large and DeBERTa follow similar trends, and are included in the supplementary material. We note that for the relatively easy tasks such as odd and even classifications, the pretrained LMs show more stable training. However, for harder tasks such as Decimal operations (where the baseline performance is around 10%), no models are able to learn the task well even with 10K labeled examples.", "Figure 3 shows results on median, mean, mode and decimal & word operation tasks. The median task requires complex reasoning (sorting numbers and computing the middle element), and shows significantly lower performance than the mean and mode tasks for the non-pretrained models even with the maximum training set size. The pretrained LM models show little eventual difference in performance between these three tasks. On the other hand, for the easiest of these tasks (mode), non-pretrained models actually show higher performance than pretrained LMs in the low data regime.", "Recognizing regular expressions: Figure 4 shows the comparative performance of pretrained LMs on non-pretrained models on the two tasks involving recognizing regular expressions. For both tasks, we note that the pretrained LMs can perfectly learn the tasks with many fewer labeled examples compared to the non-pretrained models. In both cases, the non-pretrained Transformer-based models eventually reach optimal performance as well. However, curiously the ELMO based non-pretrained models struggle with learning both tasks.", "String reasoning: Figures 6 show the results on Palindrome, Anagram, Isogram and Tautonym classification. These tasks require character comparison within the string or with another string. Again, the pretrained variants consistently outperformed non-pretrained models variants in all of these tasks. In particular, the non-pretrained models completely fail to learn the Anagram and Palindrome tasks even for the largest training set size. Again, Transformer based LMs outperform LSTM based LMs.", "Figure 7 shows the results on vowels classification, maximum frequent character, length of a string and parity check tasks. These tasks don\u2019t require intra-string comparisons. We see that most Transformer-based variants eventually achieve optimal performance. For these simpler tasks, we again observe several instances where the Transformer-based non-pretrained models actually outperform pretrained LMs in the low data regime.", "As previously mentioned, a possible explanation for the underperformance of non-pretrained models ise that the large number of parameters of the architecture relative to the sizes of the training data might be leading to under-fitting. To test this, we experiment with smaller Transformer-based models with varying numbers of parameters.", "Figure 5 illustrates the effect of model sizes of non-pretrained model. The original 110 million parameter model has 12 encoder layers, 12 attention heads, and 768 dimensional representations. The 42 million parameter model has 8 encoder layers, 8 attention heads and 512 dimensional representations. The 29 million parameter model has 4 encoder layers, 8 attention heads and 512 dimensional representations. The 11 million parameter model has 4 encoder layers, 4 attention heads and 256 dimensional representations. The smallest 4 million parameter model has 2 encoder layers, 2 attention heads and 128 dimensional representations.", "As seen in the figure, reducing the model size significantly improves the average performance of the non-pretrained models over 6 representative tasks. However, the smallest models show a performance drop. Most significantly, even the best performing intermediate-sized architectures are significantly worse than the pretrained LM models. This strongly suggests that the discrepancy between pretrained and non-pretrained models is not simply due to a mismatch between model and data sizes.", "We observe that pretrained LMs consistently performed better than non-pretrained models. This leads to the natural question of what role the text data used for pretraining plays in the process. Next, we investigate this in depth by experimenting with language models pretrained on different types of text. For this, we pretrain models using the BERT-small and DeBERTa architectures and an MLM objective on different text datasets, and evaluate the performance of these models on NILM tasks.", "We first explore models pretrained on three different domains of text.", "SNLI. We pretrained BERT small from scratch on SNLI data Bowman et\u00a0al. (2015). It has 1000k sentences (570k pairs of text and hypothesis). Amazon reviews. We selected 500k movies and tv reviews from the larger Amazon reviews dataset He and McAuley (2016) and used for pretraining. Since reviews are in a free-text format, and their collection was not tailored with a NLP task in mind, they might be more representative of the complexity of real-world language use than SNLI.ROC. ROC is a corpora of 100K children stories, each made up of five sentences Mostafazadeh et\u00a0al. (2017). The language in ROC is relatively simple in both vocabulary and sentence structure.", "Tables 2 and 3 shows the average accuracy of six non-linguistic tasks (palindrome classification, isogram classification, tautonym classification, odd even classification, decimal operation and median) fine-tuned using different BERT and DeBERTA representations respectively. We note that the models pretrained on all three domains outperformed the non-pretrained model (NP). This suggests that the results of experiments in Section 5 generalize to new text corpora for pretraining, and do not rely on having access to text on specific topics during pretraining. This is a non-trivial result, since it suggests for example, that the higher performance of pretrained models on tasks such as palindrome and anagram classification is not due to the pretrained models having seen information about such concepts during pretraining. This is especially so since the results even generalize to ROC stories, which contain no information on such technical concepts.", "Next, we experiment with perturbing the text used for pretraining by changing the order of words in the text. We explore the following models:", "SNLI sort. The words in the sentences of SNLI dataset are sorted based on alphabetical order. SNLI shuffle. We randomly shuffle words in sentences in the SNLI dataset. Amazon reviews sort. Similar to SNLI sort, the words in sentences are alphabetically sorted. Amazon reviews shuffle. We randomly shuffle words in sentences in the Amazon reviews dataset.", "We observe that models pretrained with perturbed text also significantly outperformed non-pretrained models, and perform comparably to the original pretrained LMs. For the SNLI dataset, there is 3% drop in best performance when pretrained on SNLI sort and 2% drop in performance when pretrained on SNLI shuffle for BERT (Table 2). In fact, for DeBERTa, SNLI shuffle outperformed the standard SNLI by 2% (Table 3). Similarly, the Amazon sort and Amazon shuffle versions outperformed or achieved similar performance as the standard Amazon data version. A likely explanation for this is that, even though syntactic word order is disturbed by shuffling, distributional information over sentence contexts is still preserved in the perturbed data. We describe experiments with text data having no distributional information in later sections.", "A possible rationale for explaining the beneficial effect of pretraining for non-linguistic tasks is that irrespective of whether the tasks require non-linguistic reasoning, their format is in language, and hence language models should be able to learn these tasks with fewer examples. To test this hypothesis, we also experiment with models pretrained on text from languages different from English, as well as models pretrained on computer code. These include the following models: Multilingual BERT.Multilingual BERT is pretrained on text from 102 different languages. About 21% of the pretraining text is English.Chinese BERT.Chinese BERT is a BERT model pretrained on Chinese text. Code BERT.CodeBERT Feng et\u00a0al. (2020) is pretrained on code from six programming languages.", "In Table\u00a02, we note that all three non-English pretrained LMs significantly outperformed non-pretrained models, with the best performance being comparable or marginally lower than English versions. In fact, Code-BERT surprisingly surpasses ROC by 5%. These findings strongly indicate that the advantages from pretraining have little to do with the format of the tasks, since they persist for scenarios with little shared linguistic structure.", "Finally, to investigate what happens if we weaken the distributional properties that hold even in the perturbed text versions from Section 6.2, we experiment with pretraining models on synthetic text sampled from simple probability distributions:", "Zipf distribution. We select 30k words (types) from the Amazon reviews dataset. Words are picked with a unigram probability that follows Zipf\u2019s word frequency law, which all natural languages empirically follow\u00a0Piantadosi (2014). For the Zipf distribution, we chose \u03b1\ud835\udefc\\alphaitalic_\u03b1=1 and \u03b2\ud835\udefd\\betaitalic_\u03b2=2.7, to match the parameters of most natural languages. The text does not follow any word order.Uniform distribution. In this dataset, words are sampled from the same vocabulary as in \u2018Zipf distribution\u2019, but with a uniform unigram probability. The text does not follow any word order.Synthetic Vocabulary. Words are selected with uniform distribution from a vocabulary to form sentences. However, instead of a vocabulary of English words, the words in the vocabulary are also synthetically generated (3 letter combinations of lower-case alphabets). In this text, the words do not possess morphology in addition to no syntax.", "In Tables \u00a02 and \u00a03, we note that surprisingly, even models pretrained on Zipfian and uniform distribution text continue to outperform the non-pretrained models. In fact, the Zipf version\u2019s best accuracy is 3% higher than the standard Amazon data version and 2% compared to perturbed Amazon shuffled data version in case of BERT. Zipf outperforms standard amazon data by 1% and lags behind amazon shuffle by 3% for DeBERTA. The Uniform distribution version lags behind Zipf by 9% and 2% for BERT and DeBERTa respectively. We note that the Zipf and Uniform versions still use the prebuilt vocabulary from the Amazon data, and hence this text maintains morphological structure. However, the gains finally disappear for the Synthetic vocabulary model, which cannot leverage morphological structure in the text, and its performance is similar to the non-pretrained models.", "We explore the non-linguistic inductive biases of pretrained LMs. While the general trend (that pretraining helps) is unsurprising, our analysis with models pretrained on different text corporashows that this is not due to the model seeing related topics during pretraining. We find that these gains persist even in absence of any shared linguistic structure (in cross-lingual settings). Our observation that this behavior is seen even when pretraining on synthetically generated languages is intriguing and can be explored further by future work. "], "figure_types": {"5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/1-Figure1-1.png": "schematic", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/12-Table4-1.png": "table", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/12-Table5-1.png": "table", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/13-FigureA.1-1.png": "plot", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/13-FigureA.2-1.png": "plot", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/13-FigureA.3-1.png": "plot", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/13-FigureA.4-1.png": "plot", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/14-FigureA.5-1.png": "plot", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/14-Table6-1.png": "table", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/2-Table1-1.png": "table", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/4-Figure2-1.png": "plot", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/4-Figure3-1.png": "plot", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/5-Figure4-1.png": "plot", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/6-Figure5-1.png": "plot", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/7-Figure6-1.png": "plot", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/7-Figure7-1.png": "plot", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/7-Table2-1.png": "table", "5efab88c0cdb11c795fa8f44a5d31b40e2a1c261/8-Table3-1.png": "table"}}, "2206.03715": {"paper_id": "paper_180", "title": "Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning", "arxiv_url": "https://arxiv.org/abs/2206.03715", "s2orc_url": "https://www.semanticscholar.org/paper/6f9aa703c1dea0bd316ff7c758381199b321a3ba", "all_figures_tables": {"6f9aa703c1dea0bd316ff7c758381199b321a3ba/11-Table3-1.png": "Table 3: Prefixes used for synthetic QA dataset", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/11-Table4-1.png": "Table 4: Synthetic QA dataset statistics. Whole represents the combination of AT,CN,WD and WN.", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/11-Table5-1.png": "Table 5: Synthetic QA examples. We use templates to convert a question (ehead, r) into a natural language.", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/11-Table6-1.png": "Table 6: Statistics of the dataset for zero-shot fusion", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/12-Figure7-1.png": "Figure 7: Illustration of the zero-shot fusion architecture with parameters. Each colored circle represents expert adapters, except the black circle which denotes KG-Classifier adapter. \u2217 indicates the fixed layer.", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/12-Table7-1.png": "Table 7: Notations and their meanings", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/12-Table8-1.png": "Table 8: KG-Classification examples from synthetic QA dataset of each KG", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/14-Table10-1.png": "Table 10: STL-Adapter and zero-shot fusion w/ KG-C adapter performance across five commonsense tasks in various combination of KGs. AT, CN, WD and WN represent ATOMIC, ConceptNet, WikiData and WordNet, respectively. Whole represents the combination of AT, CN, WD and WN. We run our experiment with seed 42.", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/14-Table9-1.png": "Table 9: STL-PLM and MTL performance across five commonsense tasks in various combination of KGs. AT, CN, WD and WN represent ATOMIC, ConceptNet, WikiData and WordNet, respectively. We run our experiment with seed 42.", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/3-Table1-1.png": "Table 1: Synthetic QA examples. We use templates to convert (ehead, r) into a natural language sentence.", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/4-Figure1-1.png": "Figure 1: Illustration of the proposed modularized framework for zero-shot commonsense reasoning. Each colored square represents different KGs. Not only for KG modularization, we re-use a set of synthetic QA datasets from the multiple KGs for the purpose of KG classification and zero-shot fusion, which enables better knowledge aggregation.", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/5-Figure2-1.png": "Figure 2: Illustration of the zero-shot fusion architecture with KG-Classifier adapter. Each colored circle represents expert adapters, except the black circle which denotes KG-Classifier adapter. \u2217 indicates the fixed layer. Details are in Appendix F", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/6-Table2-1.png": "Table 2: Zero-shot evaluation results with different combinations of models and knowledge sources across five commonsense tasks. AT, CN, WD and WN represent ATOMIC, ConceptNet, WikiData and WordNet, respectively. Whole represents the combination of AT, CN, WD and WN. Bold text indicates the best performance on each benchmark. RoBERTa-L (MR) used the synthetic dataset after filtering, while we use the raw version. SMLM (*) used different KG which has strong alignment with each task (e.g.AT for SIQA).", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/7-Figure3-1.png": "Figure 3: t-SNE visualization of the hidden representation from (a) PLM and (b) KG-C adapter. Each color denotes the five different benchmark samples.", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/7-Figure4-1.png": "Figure 4: Comparison of attention probability between zero-shot fusion with/without KG-C adapter. The xand y-axis indicate expert adapters and the fusion layer number in RoBERTa-L, respectively. The darker color indicates higher attention probability in fusion layer.", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/8-Figure5-1.png": "Figure 5: Interference ratio of multi-KG models on five benchmarks. The lower indicates less interference.", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/8-Figure6-1.png": "Figure 6: Relative improvement upon the STL on five benchmarks. The x- and y-axis indicate the benchmark and the combination of the KGs, respectively. The value of each cell indicates the relative performance improvement of using multiple KGs over the highest performance among STLs. The green and red colors denote the improvement or decrease of relative performance, respectively."}, "referred_figures_tables": [["6f9aa703c1dea0bd316ff7c758381199b321a3ba/5-Figure2-1.png"], ["6f9aa703c1dea0bd316ff7c758381199b321a3ba/8-Figure5-1.png"], ["6f9aa703c1dea0bd316ff7c758381199b321a3ba/5-Figure2-1.png"], ["6f9aa703c1dea0bd316ff7c758381199b321a3ba/14-Table9-1.png", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/14-Table10-1.png"], ["6f9aa703c1dea0bd316ff7c758381199b321a3ba/7-Figure4-1.png"], ["6f9aa703c1dea0bd316ff7c758381199b321a3ba/8-Figure6-1.png"], ["6f9aa703c1dea0bd316ff7c758381199b321a3ba/8-Figure6-1.png"]], "question_id": [5, 15, 7, 12, 14, 16, 22], "question": ["Why is KG Modularization needed?", "How does the author show the mitigation of interference?", "What is the difference between zero-shot fusion and original AdapterFusion?", "What does STL stand for?", "How does KG-Classifier affect zero-shot fusion?", "What is the correlation between the number of KGs and the performance when using zero-shot fusion?", "Why is there decrease of the performance of the zeor-shot fusion without ATOMIC?"], "question_section": ["3.1 KG Modularization", "4.4 Mitigating Interference", "3.2 Zero-shot Fusion", "4.1.2. Baselines", "4.3 Impact of the KG-Classifier Adapter", "4.5 Visualization of Knowledge Aggregation", "4.5. Visualization of Knowledge Aggregation"], "question_trigger_sentence": ["Considering the importance of using a suitable and well-aligned KG (Ma et al., 2019, 2021) on a downstream task, the subtle difference between each KG should be learned by the model without any interference from each other.", "Using the interference ratio, we can precisely compare the negative effects of multi-KG models on knowledge aggregation since the only reason to get the correct samples wrong is the interference caused by learning with additional KGs.", "We present a novel fu- sion strategy as shown in Figure 2, which is referred to as the zero-shot fusion. In contrast to Adapter- Fusion (Pfeiffer et al., 2021) where the focus is learning to transfer knowledge to a specific target task, our zero-shot fusion aims to generalize this transfer to any arbitrary target task.", "Single-Task Learning. The model is pre-trained on a synthetic QA dataset gener- ated from a single KG.", "We can observe that zero-shot fusion with KG-C adapter fuses the knowledge from different experts with a subtle difference rather than focusing on a single expert severely. This implies that KG-C adapter enables the delicate balancing between multiple knowledge sources based on the KG-alignment awareness, which leads to performance improvements in commonsense reasoning tasks.", "In Figure 6, while the MTL tends to show the decrease of the performance when more KGs are utilized for training, our method obtains relative performance improvement across most of benchmarks.", "In both framework, the slightly degraded performance of the combination of KGs without ATOMIC could be due to the strong alignment between ATOMIC and SIQA."], "question_type": ["Deep/complex question", "Testing question", "Testing question", "Testing question", "Testing question", "Testing question", "Deep/complex question"], "evidential_info": [[{"context": "First, we modularize the KGs to preserve their intrinsic knowledge. Considering the importance of using a suitable and well-aligned KG (Ma et al., 2019, 2021) on a downstream task, the subtle difference between each KG should be learned by the model without any interference from each other. Accordingly, we adopt the adapter module (Houlsby et al., 2019) which repurposes a pre- trained language model (PLM) to incorporate each KG as tiny modules in between Transformer blocks. Specifically, as illustrated in Figure 2 (except for green area), the adapter training strategy involves injecting new layers (parameterized by \u03a6) into the original PLM (parameterized by \u03b8). The weights of the original PLM are untouched, while the new adapter layers are initialized at random. Formally, we call each adapter trained with DkQA as an expert adapter for KG k, parameterized by \u03a6kQA.", "rationale": "First, we modularize the KGs to preserve their intrinsic knowledge. Considering the importance of using a suitable and well-aligned KG (Ma et al., 2019, 2021) on a downstream task, the subtle difference between each KG should be learned by the model without any interference from each other."}], [{"context": "Using the interference ratio, we can precisely compare the negative effects of multi-KG models on knowledge aggregation since the only reason to get the correct samples wrong is the interference caused by learning with additional KGs. We present the interference ratio of the models on five benchmark datasets in Figure 5. This figure shows that MTL has the higher interference ratio than the competing models across all benchmarks. Our method achieves a substantially better ratio, especially when KG-C adapter is used. This demonstrates the efficacy of our framework in mitigating interference between knowledge, which is one of the major problems of MTL.", "rationale": "Using the interference ratio, we can precisely compare the negative effects of multi-KG models on knowledge aggregation since the only reason to get the correct samples wrong is the interference caused by learning with additional KGs."}], [{"context": "Once the expert adapters are learned, we combine the knowledge from each expert adapter using an attention-like mechanism. We present a novel fusion strategy as shown in Figure 2, which is referred to as the zero-shot fusion. In contrast to AdapterFusion (Pfeiffer et al., 2021) where the focus islearning to transfer knowledge to a specific targettask, our zero-shot fusion aims to generalize this transfer to any arbitrary target task. Specifically, the zero-shot fusion parameters \u03a8 learn to combine fixed expert adapters which are parameterized by \u03a6_1 QA, ..., \u03a6 K QA. In each Transformer layer l of PLM with the injected fusion layer, the zero-shot fusion parameters \u03a8QA consist of query, key, and value matrices, denoted by WQ_l, WK_l, and WV_l respectively. These parameters are used to learn the balancing between the representation of each expert adapters through attention-like mechanism. While fixing both the parameters \u03b8 and all expert adapters \u03a6_1 QA, ..., \u03a6_K QA, the only trainable weights \u03a8QA on the fusion layer learns to combine the knowledge from different K expert adapters by using the subset of {Dk QA} K k=1 by random sampling. Here, we balance the ratio between the K knowledge-driven datasets as N samples (details are in Appendix D).", "rationale": "In contrast to AdapterFusion (Pfeiffer et al., 2021) where the focus is learning to transfer knowledge to a specific target task, our zero-shot fusion aims to generalize this transfer to any arbitrary target task."}], [{"context": "Single-Task Learning (STL): The model is pre-trained on a synthetic QA dataset generated from a single KG. Specifically, we experiment two architectural choices: PLM (STLPLM) and PLM with adapters (STL-Adapter). For each architecture, there are four STL models for each of synthetic QA datasets derived from ATOMIC, ConceptNet, WikiData, and WordNet. We note that the trained STLAdapter is an expert adapter from a specific KG in our framework. The performance of each STL baseline is shown in Appendix I Table 9 and Table 10.", "rationale": "Single-Task Learning (STL): The model is pre-trained on a synthetic QA dataset generated from a single KG."}], [{"context": "Further, we explore how the KG-C adapter affects zero-shot fusion which is based on an attention-like mechanism (Pfeiffer et al., 2021) compared to zero-shot fusion without KG-C adapter. Here, while zero-shot fusion without KGC adapter simply uses the representation of PLM as a query, zero-shot fusion with KG-C adapter leverages the representation of KG-C adapter. To illustrate this strength, we visualize the attention probability of [CLS] token from each fusion layer as a representative in Figure 4. The column of the darker cell indicates the adapter that has the bigger influence on the fused representation. We can observe that zero-shot fusion with KG-C adapter fuses the knowledge from different experts with a subtle difference rather than focusing on a single expert severely. This implies that KG-C adapter enables the delicate balancing between multiple knowledge sources based on the KG-alignment awareness, which leads to performance improvements in commonsense reasoning tasks. Interestingly, both cases have the ability not to focus on the expert adapter based on WikiData, which can be seen as a redundant expert.4 This observation would benefit from the further study that explores the optimal combination of KGs by expert selection or rejection.", "rationale": "We can observe that zero-shot fusion with KG-C adapter fuses the knowledge from different experts with a subtle difference rather than focusing on a single expert severely. This implies that KG-C adapter enables the delicate balancing between multiple knowledge sources based on the KG-alignment awareness, which leads to performance improvements in commonsense reasoning tasks."}], [{"context": "In Figure 6, while the MTL tends to show the decrease of the performance when more KGs are utilized for training, our method obtains relative performance improvement across most of benchmarks. In both framework, the slightly degraded performance of the combination of KGs without ATOMIC could be due to the strong alignment between ATOMIC and SIQA. Except for the above case, we can observe that as more KGs are leveraged, the color of the cell gets greener, which implies that our method gains more advantages for better performance. This demonstrates that our method enables knowledge aggregation for multiple KGs synergetically.", "rationale": "n Figure 6, while the MTL tends to show the decrease of the performance when more KGs are utilized for training, our method obtains relative performance improvement across most of benchmarks."}], [{"context": "In Figure 6, while the MTL tends to show the decrease of the performance when more KGs are utilized for training, our method obtains relative performance improvement across most of benchmarks. In both framework, the slightly degraded performance of the combination of KGs without ATOMIC could be due to the strong alignment between ATOMIC and SIQA. Except for the above case, we can observe that as more KGs are leveraged, the color of the cell gets greener, which implies that our method gains more advantages for better performance. This demonstrates that our method enables knowledge aggregation for multiple KGs synergetically.", "rationale": "In both framework, the slightly degraded performance of the combination of KGs without ATOMIC could be due to the strong alignment between ATOMIC and SIQA."}]], "composition": ["KG modularization is crucial for maintaining the intrinsic knowledge of each individual KG. As the selection and alignment of an appropriate KG has been shown to have a significant impact on downstream tasks, it is important that the model is able to learn the subtle differences between each KG without any interference from other KGs.", "Use interference ratio.", "In contrast to AdapterFusion where the focus is learning to transfer knowledge to a specific target task, our zero-shot fusion aims to generalize this transfer to any arbitrary target task.", "Single-Task Learning (STL): The model is pre-trained on a synthetic QA dataset generated from a single KG.", "zero-shot fusion with KG-C adapter fuses the knowledge from different experts with a subtle difference rather than focusing on a single expert severely.", "Zero-shot fusion obtains relative performance improvement across most of benchmark when more KGs are utilized for training.", "In both framework, the slightly degraded performance of the combination of KGs without ATOMIC could be due to the strong alignment between ATOMIC and SIQA."], "Is_figure_in_evidence": [true, true, true, false, true, true, true], "Is_table_in_evidence": [false, false, false, true, false, false, false], "question_key": ["620", "622", "623", "628", "630", "631", "637"], "passages": ["The ability to understand natural language through commonsense reasoning is one of the core focuses in the field of natural language processing. To measure and study the different aspects of commonsense reasoning, several datasets are developed, such as SocialIQA\u00a0(Sap et\u00a0al., 2019b), CommonsenseQA\u00a0(Talmor et\u00a0al., 2018), and PhysicalIQA\u00a0(Bisk et\u00a0al., 2020), each requiring different type of commonsense knowledge (e.g., social, taxonomic, causal, declarative, etc) to select the correct answer. While large-scale neural systems\u00a0(Devlin et\u00a0al., 2018; Yang et\u00a0al., 2019; Liu et\u00a0al., 2019b) have shown human-level accuracy on these benchmarks, recent studies\u00a0(Mitra et\u00a0al., 2019) also criticize that these models solve individual datasets, rather than learning how to perform general semantic reasoning. To this end, Ma et\u00a0al. (2021) suggested zero-shot evaluation as a genuine measure for the reasoning capability of the machine.", "Inspired by this new metric, in this work, we focus on building unsupervised zero-shot multiple-choice QA systems. That is, we target an arbitrary commonsense reasoning task where conventional approaches (that rely heavily on task-specific supervision) are not applicable to such zero-shot learning scenarios. To learn QA models without expensive annotation efforts, recent works\u00a0(Ma et\u00a0al., 2021; Banerjee and Baral, 2020; Malaviya et\u00a0al., 2020) propose to generate a synthetic QA dataset using a commonsense KG such as ATOMIC\u00a0(Sap et\u00a0al., 2019a) and ConceptNet\u00a0(Speer et\u00a0al., 2017). Such an approach mostly focuses only on one specific type of reasoning relations (e.g., if-then relation, or declarative relation), neglecting the fact that real-world QA systems require simultaneously considering different types of reasoning abilities (e.g., declarative and social, or causal and physical reasoning; Ilievski et\u00a0al., 2021; Chang et\u00a0al., 2021).", "To consider different types of reasoning, this paper extends ideas from the aforementioned zero-shot learning to the multi-source case such that it benefits from different types of commonsense knowledge on individual KGs. For example, ATOMIC\u00a0(Sap et\u00a0al., 2019a) focuses on social commonsense while ConceptNet\u00a0(Speer et\u00a0al., 2017) contains conceptual knowledge. A practical approach is multi-task learning (MTL; Caruana, 1997; Liu et\u00a0al., 2019a), which learns a shared encoder for different synthetic QA datasets from multiple KGs. Despite its effectiveness, MTL scheme suffers from interference among different KGs, which results in forgetting previously learned knowledge when trained on new KG which has different kinds of knowledge\u00a0(Pilault et\u00a0al., 2021; Pfeiffer et\u00a0al., 2021; Wang et\u00a0al., 2021a; Wu et\u00a0al., 2020).", "To address these limitations, we propose a novel, modularized framework that aims to learn multiple expert models for KGs, then conduct zero-shot fusion to allow collaboration among KGs. For this purpose, we leverage AdapterFusion\u00a0(Pfeiffer et\u00a0al., 2021) where multiple tiny modules between Transformer blocks called adapters\u00a0(Houlsby et\u00a0al., 2019) can be combined after independent training, thus allowing a continual integration of the adapters without retraining the entire framework. Specifically, we treat the adapters as different KG-specific experts, and combine them using an attention-like fusion module. To improve the fusion of adapters, we suggest a KG-alignment adapter that guides to the apt expert adapters. Here, we use KGs in three different synthetic supervision training: (1) KG-specific QA datasets to train the KG-specific expert adapters, (2) a KG classification datasets to train the KG-alignment adapter, and (3) a balanced mixture of KG-specific QA datasets to train the fusion module. Our modularized method alleviates the interference between different KGs, which is the pitfall of MTL from our empirical observation, and thus combines multiple KGs into a synergetic zero-shot framework.", "Our contributions are: (1) We suggest a simple, yet effective KG modularization strategy for the use of multiple KGs in commonsense reasoning. (2) We then explore the use of AdapterFusion\u00a0(Pfeiffer et\u00a0al., 2021) for better knowledge aggregation based on the KG modularization in zero-shot setting. We believe that such modularized transfer learning is critical to using different knowledge sources synergetically against interference between them. (3) In extensive experiments on various commonsense reasoning benchmarks, our framework achieves significant improvements over baselines using a single KG, even using multiple KGs, which implies the robustness in commonsense reasoning.", "Many researchers have recently focused on building unsupervised models without any benchmark supervisions (i.e., zero-shot learning). In such zero-shot setting, KGs are often used as an external resource for improving model prior (e.g., continually learned from pre-trained language models)\u00a0(Banerjee and Baral, 2020; Bosselut and Choi, 2019; Ma et\u00a0al., 2021), especially for commonsense reasoning, as much existing work couples language models with neural/symbolic commonsense KGs.", "However, most of existing work are either assuming the existence of the alignment information between tasks and KGs\u00a0(Banerjee and Baral, 2020) or an integrated KG\u00a0(Ma et\u00a0al., 2021). For example, \ud835\ude70\ud835\ude83\ud835\ude7e\ud835\ude7c\ud835\ude78\ud835\ude722020subscriptsuperscript\ud835\ude70\ud835\ude83\ud835\ude7e\ud835\ude7c\ud835\ude78\ud835\ude722020\\texttt{ATOMIC}^{20}_{20}ATOMIC start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 20 end_POSTSUBSCRIPT\u00a0(Hwang et\u00a0al., 2021), a commonsense KG which incorporates tuples from ConceptNet and ATOMIC with new relations and further crowdsourcing, combines multiple KGs into a new integrated KG, but as widely known\u00a0(Ilievski et\u00a0al., 2020; Hwang et\u00a0al., 2021), heterogeneous schema between different KGs may limit triplets that can be integrated.111Only 172K tuples of the 3.4M tuples and 5 relations of 36 relations in ConceptNet are integrated into \ud835\ude70\ud835\ude83\ud835\ude7e\ud835\ude7c\ud835\ude78\ud835\ude722020subscriptsuperscript\ud835\ude70\ud835\ude83\ud835\ude7e\ud835\ude7c\ud835\ude78\ud835\ude722020\\texttt{ATOMIC}^{20}_{20}ATOMIC start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 20 end_POSTSUBSCRIPT. Rather than such symbolic KG integration with the inevitable loss of knowledge, in this work, we explore the neural KG integration leveraging the multiple KGs without additional processing and alignment information between KG and task.", "The idea of having specialized parameters, or so-called experts, has been widely studied to integrate multiple sources of knowledge via transfer learning. The adapter module\u00a0(Rebuffi et\u00a0al., 2017; Houlsby et\u00a0al., 2019) has been explored as one of such approaches, introducing a small number of task-specific parameters at every layer of pre-trained language model (PLM) while sharing the parameters of underlying PLM which is fixed. To address the limitations of transfer learning due to high re-training cost, many works utilize the multiple adapter modules for individual tasks with different domains\u00a0(Puigcerver et\u00a0al., 2020; Bapna et\u00a0al., 2019; R\u00fcckl\u00e9 et\u00a0al., 2020; Madotto et\u00a0al., 2021) considering each adapter to be an expert of each domain. Similar to our work, K-Adapter\u00a0(Wang et\u00a0al., 2021a) encodes factual and linguistic knowledge to each adapter, but in this paper, we further explore how to mitigate catastrophic forgetting or interference among multiple adapters for better knowledge transfer in zero-shot setting.", "MTL\u00a0(Liu et\u00a0al., 2019a; Zhang and Yang, 2017; Caruana, 1997) learns a shared representation while aggregating knowledge across multiple learning tasks, often leading to better generalization ability of a model. However, parametric aggregation of knowledge with MTL has following limitations: (1) retraining the full model when adding new tasks\u00a0(Houlsby et\u00a0al., 2019; Pfeiffer et\u00a0al., 2021, 2020b) (2) catastrophic forgetting and interference between tasks leading to difficulties of solving each task equally well\u00a0(Pilault et\u00a0al., 2021; Wu et\u00a0al., 2020; Yu et\u00a0al., 2020) and (3) inconsistent effect\u00a0(Lourie et\u00a0al., 2021). To deal with these challenges, Mixture-of-Experts (MoE) is a parameterized generalization of ensembling techniques, which has been adapted for MTL with gating network trained to optimize each task\u00a0(Ma et\u00a0al., 2018). However, simple linear gating networks are too shallow and thus may destruct task knowledge for commonsense reasoning.", "To address this problem, AdapterFusion\u00a0(Pfeiffer et\u00a0al., 2021) has been proposed to fuse task specific parameters called adapters for the given target task leveraging attention-like mechanism. AdapterFusion aggregates adapters, which is trained independently for each task, in a non-destructive manner mitigating aforementioned MTL problems such as forgetting and interference between tasks. Recently, it has been used for zero-shot cross-lingual transfer framework\u00a0(Pfeiffer et\u00a0al., 2020c; Wang et\u00a0al., 2021b), which motivates our work to transfer multi-source knowledge with less interference for zero-shot commonsense reasoning.", "In our setup, we repurpose synthetic QA generation\u00a0(Ma et\u00a0al., 2021) for the task of knowledge-driven zero-shot learning for commonsense reasoning, i.e., we transform a KG into multiple (Qi,Ai)subscript\ud835\udc44\ud835\udc56subscript\ud835\udc34\ud835\udc56(Q_{i},A_{i})( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) pairs where Qisubscript\ud835\udc44\ud835\udc56Q_{i}italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is a natural language question and Ai={Ai,1,\u2026,Ai,m}subscript\ud835\udc34\ud835\udc56subscript\ud835\udc34\ud835\udc561\u2026subscript\ud835\udc34\ud835\udc56\ud835\udc5aA_{i}=\\{A_{i,1},...,A_{i,m}\\}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_A start_POSTSUBSCRIPT italic_i , 1 end_POSTSUBSCRIPT , \u2026 , italic_A start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT } is the set of options with m\ud835\udc5amitalic_m answer candidates. Specifically, given a triple (ehead,r,etail)superscript\ud835\udc52\u210e\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc5fsuperscript\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc56\ud835\udc59(e^{head},r,e^{tail})( italic_e start_POSTSUPERSCRIPT italic_h italic_e italic_a italic_d end_POSTSUPERSCRIPT , italic_r , italic_e start_POSTSUPERSCRIPT italic_t italic_a italic_i italic_l end_POSTSUPERSCRIPT ) in a KG, where eheadsuperscript\ud835\udc52\u210e\ud835\udc52\ud835\udc4e\ud835\udc51e^{head}italic_e start_POSTSUPERSCRIPT italic_h italic_e italic_a italic_d end_POSTSUPERSCRIPT, etailsuperscript\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc56\ud835\udc59e^{tail}italic_e start_POSTSUPERSCRIPT italic_t italic_a italic_i italic_l end_POSTSUPERSCRIPT and r\ud835\udc5fritalic_r denote head/tail entity and relation respectively, we transform eheadsuperscript\ud835\udc52\u210e\ud835\udc52\ud835\udc4e\ud835\udc51e^{head}italic_e start_POSTSUPERSCRIPT italic_h italic_e italic_a italic_d end_POSTSUPERSCRIPT and r\ud835\udc5fritalic_r into a natural language question Qisubscript\ud835\udc44\ud835\udc56Q_{i}italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT using templates. For the option set Aisubscript\ud835\udc34\ud835\udc56A_{i}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we use the combination of the correct answer etailsuperscript\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc56\ud835\udc59e^{tail}italic_e start_POSTSUPERSCRIPT italic_t italic_a italic_i italic_l end_POSTSUPERSCRIPT and m\u22121\ud835\udc5a1m-1italic_m - 1 distractors which are tail entities from other triples sampled randomly\u00a0(Ma et\u00a0al., 2021). Details are described in Appendix\u00a0B."], "figure_types": {"6f9aa703c1dea0bd316ff7c758381199b321a3ba/11-Table3-1.png": "table", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/11-Table4-1.png": "table", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/11-Table5-1.png": "table", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/11-Table6-1.png": "table", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/12-Figure7-1.png": "schematic", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/12-Table7-1.png": "table", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/12-Table8-1.png": "table", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/14-Table10-1.png": "table", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/14-Table9-1.png": "table", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/3-Table1-1.png": "table", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/4-Figure1-1.png": "schematic", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/5-Figure2-1.png": "schematic", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/6-Table2-1.png": "table", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/7-Figure3-1.png": "plot", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/7-Figure4-1.png": "plot", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/8-Figure5-1.png": "plot", "6f9aa703c1dea0bd316ff7c758381199b321a3ba/8-Figure6-1.png": "plot"}}, "2102.01373": {"paper_id": "paper_20", "title": "An Improved Baseline for Sentence-level Relation Extraction", "arxiv_url": "https://arxiv.org/abs/2102.01373", "s2orc_url": "https://www.semanticscholar.org/paper/11baa9cc02d6158edd9cb1f299579dad7828e162", "all_figures_tables": {"11baa9cc02d6158edd9cb1f299579dad7828e162/4-Table1-1.png": "Table 1: Test F1 (in %) of different entity representation techniques on TACRED. For each technique, we also provide the processed input of an example text \u201cBill was born in Seattle\u201d. Typed entity markers (original and punct) significantly outperforms others.", "11baa9cc02d6158edd9cb1f299579dad7828e162/4-Table2-1.png": "Table 2: F1 (in %) on the test sets. * marks reimplemented results from Alt et al. (2020). \u2020 marks reimplemented results from Stoica et al. (2021). \u2021 marks our re-implemented results.", "11baa9cc02d6158edd9cb1f299579dad7828e162/7-Table3-1.png": "Table 3: Statistics of datasets.", "11baa9cc02d6158edd9cb1f299579dad7828e162/7-Table4-1.png": "Table 4: Test F1 on the filtered test sets. The typed entity marker consistently outperforms the entity mask, showing that knowledge from entity names can generalize to unseen entities.", "11baa9cc02d6158edd9cb1f299579dad7828e162/7-Table5-1.png": "Table 5: Test F1 on the clean test set of TACRED. The gain on the clean test set is smaller than on TACRED and TACREV."}, "referred_figures_tables": [["11baa9cc02d6158edd9cb1f299579dad7828e162/4-Table1-1.png"]], "question_id": [9], "question": ["Author said that they achieved to make SOTA RE models. Give an evidences for this statement."], "question_section": ["conclusions"], "question_trigger_sentence": ["In this paper, we present a simple yet strong RE baseline that offers new SOTA performance"], "question_type": ["Deep/complex question"], "evidential_info": [[{"context": "We first provide an analysis on different entity representation techniques. In this analysis, we use the base and large versions of BERT\u00a0Devlin et\u00a0al. (2019) and the large version of RoBERTa\u00a0Liu et\u00a0al. (2019) as the encoder.Table\u00a01 shows the performance of the PLMs incorporated with different entity representation techniques.For each technique, we also provide an example of the processed text.We have several observations from the results.First, the typed entity marker and its variants outperform untyped entity representation techniques by a notable margin.Especially, the RoBERTa model achieves an F_{1} score of 74.6\\% using the typed entity marker (punct), which is significantly higher than the SOTA result of 72.7\\% by LUKE\u00a0Yamada et\u00a0al. (2020).This shows that representing all categories of entity information is helpful to the RE task.It also shows that keeping entity names in the input improves the performance of RE models.Second, symbols used in entity markers have an obvious impact on the performance of RE models.Although the original and punct versions of entity representation techniques represent the same categories of entity information, they do lead to a difference in model performance.Particularly, introducing new special tokens hinders the model performance drastically on RoBERTa.On RoBERTa{}_{\\text{LARGE}}, the entity marker underperforms the entity marker (punct) by 0.7\\%, the typed entity marker underperforms the typed entity marker (punct) by 3.6\\%, while the entity mask gets a much worse result of 60.9\\%.", "rationale": "Their improved RE baseline achieved SOTA performance on the RE-TACRED dataset with f1 score of 91.1%."}, {"context": "In this paper, we present a simple yet strong RE baseline that offers new SOTA performance, along with a comprehensive study to understand its prediction generalizability and robustness.Specifically, we revisit two technical problems in sentence-level RE, namely entity representation and noisy or ill-defined labels.We propose an improved entity representation technique, which significantly outperforms existing sentence-level RE models.Especially, our improved RE baseline achieves an F_{1} score of 91.1\\% on the Re-TACRED dataset, showing that PLMs already achieve satisfactory performance on this task.We hope the proposed techniques and analyses can benefit future research on RE.", "rationale": "Using RoBERTa Liu et al. (2019) as the backbone, they improved baseline model on TACRED and TACREV with f1 score 74.6% and 83.2%, respectively."}, {"context": "We evaluate our model on TACRED\u00a0Zhang et\u00a0al. (2017), TACREV\u00a0Alt et\u00a0al. (2020), and Re-TACRED\u00a0Stoica et\u00a0al. (2021).Using RoBERTa\u00a0Liu et\u00a0al. (2019) as the backbone, our improved baseline model achieves an F_{1} of 74.6\\% and 83.2\\% on TACRED and TACREV, respectively, significantly outperforming various SOTA RE models.Particularly, our baseline model achieves an F_{1} of 91.1\\% on Re-TACRED, demonstrating that PLMs can achieve much better results on RE than shown in previous work.222This work first appeared as a technical report on arXiv in Feb 2021 Zhou and Chen (2021).Since then, the proposed techniques have been incorporated into several follow-up works Chen et\u00a0al. (2022); Wang et\u00a0al. (2022b, a); Lu et\u00a0al. (2022); Han et\u00a0al. (2021); Kulkarni et\u00a0al. (2022) that are published before this version of the paper.", "rationale": "The RoBERTa model achieves 1.9% higher f1 score than the SOTA model LUKE Yamada et al."}]], "composition": ["Their improved RE baseline achieved SOTA performance on the RE-TACRED dataset with f1 score of 91.1%. Moreover, Using RoBERTa Liu et al. (2019) as the backbone, they improved baseline model on TACRED and TACREV with f1 score 74.6% and 83.2%, respectively. The RoBERTa model achieves 1.9% higher f1 score than the SOTA model LUKE Yamada et al."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["687"], "passages": ["As one of the fundamental information extraction (IE) tasks,relation extraction\u00a0(RE) aims at identifying the relationship(s) between two entities in a given piece of text from a pre-defined set of relationships of interest.For example, given the sentence \u201cBill Gates founded Microsoft together with his friend Paul Allen in 1975\u201d and an entity pair (\u201cBill Gates\u201d, \u201cMicrosoft\u201d), the RE model is expected to predict the relation ORG:FOUNDED_BY.On this task, SOTA models based on PLMs Devlin et\u00a0al. (2019); Joshi et\u00a0al. (2020) have gained significant success.", "Recent work on sentence-level RE can be divided into two lines.One focuses on injecting external knowledge into PLMs.Methods of such, including ERNIE\u00a0Zhang et\u00a0al. (2019) and KnowBERT\u00a0Peters et\u00a0al. (2019), take entity embedding pretrained from knowledge graphs as inputs to the Transformer.Similarly, K-Adapter\u00a0Wang et\u00a0al. (2020) introduces a plug-in neural adaptor that injects factual and linguistic knowledge into the language model.LUKE\u00a0Yamada et\u00a0al. (2020) further extends the pretraining objective of masked language modeling to entities and proposes an entity-aware self-attention mechanism.The other line of work focuses on continually pretraining PLMs on text with linked entities using relation-oriented objectives.Specifically, BERT-MTB\u00a0Baldini\u00a0Soares et\u00a0al. (2019) proposes a matching-the-blanks objective that decides whether two relation instances share the same entities.Despite extensively studied, existing RE models still perform far from perfect.On the commonly-used benchmark TACRED\u00a0Zhang et\u00a0al. (2017), the SOTA F1subscript\ud835\udc391F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT result only increases from 70.1%percent70.170.1\\%70.1 %\u00a0(BERTLARGELARGE{}_{\\text{LARGE}}start_FLOATSUBSCRIPT LARGE end_FLOATSUBSCRIPT) to 72.7%percent72.772.7\\%72.7 %\u00a0(LUKE) after applying PLMs to this task.It is unclear what building block is missing to constitute a promising RE system.", "In this work, we discuss two obstacles that have hindered the performance of existing RE models.First, the RE task provides a structured input of both the raw texts and side information of the entities, such as entity names, spans, and types (typically provided by NER models), which are shown important to the performance of RE models\u00a0Peng et\u00a0al. (2020).However, existing methods fall short of representing the entity information comprehensively in the text, leading to limited characterization of the entities.Second, human-labeled RE datasets\u00a0(e.g., TACRED), may contain a large portion of noisy or ill-defined labels, causing the model performance to be misestimated.Alt et\u00a0al. (2020) relabeled the development and test set of TACRED and found that 6.62%percent6.626.62\\%6.62 % of labels are incorrect.Stoica et\u00a0al. (2021) refined many ill-defined relation types and further re-annotated the TACRED dataset using an improved annotation strategy to ensure high-quality labels.To this end, we propose an improved RE baseline, where we introduce the typed entity marker to sentence-level RE, which leads to promising improvement of performance over existing RE models.", "We evaluate our model on TACRED\u00a0Zhang et\u00a0al. (2017), TACREV\u00a0Alt et\u00a0al. (2020), and Re-TACRED\u00a0Stoica et\u00a0al. (2021).Using RoBERTa\u00a0Liu et\u00a0al. (2019) as the backbone, our improved baseline model achieves an F1subscript\ud835\udc391F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT of 74.6%percent74.674.6\\%74.6 % and 83.2%percent83.283.2\\%83.2 % on TACRED and TACREV, respectively, significantly outperforming various SOTA RE models.Particularly, our baseline model achieves an F1subscript\ud835\udc391F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT of 91.1%percent91.191.1\\%91.1 % on Re-TACRED, demonstrating that PLMs can achieve much better results on RE than shown in previous work.222This work first appeared as a technical report on arXiv in Feb 2021 Zhou and Chen (2021).Since then, the proposed techniques have been incorporated into several follow-up works Chen et\u00a0al. (2022); Wang et\u00a0al. (2022b, a); Lu et\u00a0al. (2022); Han et\u00a0al. (2021); Kulkarni et\u00a0al. (2022) that are published before this version of the paper.", "In this section, we first formally define the relation extraction task in Section\u00a02.1, and then present our model architecture and entity representation techniques in Section\u00a02.2 and Section\u00a02.3.", "In this paper, we focus on sentence-level RE.Specifically, given a sentence \ud835\udc99\ud835\udc99\\bm{x}bold_italic_x mentioning an entity pair (es,eo)subscript\ud835\udc52\ud835\udc60subscript\ud835\udc52\ud835\udc5c(e_{s},e_{o})( italic_e start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ), referred as the subject and object entities, respectively, the task of sentence-level RE is to predict the relationship r\ud835\udc5fritalic_r between essubscript\ud835\udc52\ud835\udc60e_{s}italic_e start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and eosubscript\ud835\udc52\ud835\udc5ce_{o}italic_e start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT from \u211b\u222a{NA}\u211bNA\\mathcal{R}\\cup\\{\\textsc{NA}\\}caligraphic_R \u222a { NA }, where \u211b\u211b\\mathcal{R}caligraphic_R is a pre-defined set of relationships of interest.If the text does not express any relation from \u211b\u211b\\mathcal{R}caligraphic_R, the entity pair will be accordingly labeled NA.", "Our RE classifier is an extension of previous PLM-based RE models\u00a0Baldini\u00a0Soares et\u00a0al. (2019).Given the input sentence \ud835\udc99\ud835\udc99\\bm{x}bold_italic_x, we first mark the entity spans and entity types using techniques presented in\u00a0Section\u00a02.3, then feed the processed sentence into a PLM to get its contextual embedding.Finally, we feed the hidden states of the subject and object entities in the language model\u2019s last layer, i.e., \ud835\udc89subjsubscript\ud835\udc89subj\\bm{h}_{\\text{subj}}bold_italic_h start_POSTSUBSCRIPT subj end_POSTSUBSCRIPT and \ud835\udc89objsubscript\ud835\udc89obj\\bm{h}_{\\text{obj}}bold_italic_h start_POSTSUBSCRIPT obj end_POSTSUBSCRIPT, into the softmax classifier:\ud835\udc9b\ud835\udc9b\\displaystyle\\bm{z}bold_italic_z=ReLU(\ud835\udc7eproj[\ud835\udc89subj,\ud835\udc89obj]),absentReLUsubscript\ud835\udc7eprojsubscript\ud835\udc89subjsubscript\ud835\udc89obj\\displaystyle=\\text{ReLU}\\left(\\bm{W}_{\\text{proj}}\\left[\\bm{h}_{\\text{subj}},\\bm{h}_{\\text{obj}}\\right]\\right),= ReLU ( bold_italic_W start_POSTSUBSCRIPT proj end_POSTSUBSCRIPT [ bold_italic_h start_POSTSUBSCRIPT subj end_POSTSUBSCRIPT , bold_italic_h start_POSTSUBSCRIPT obj end_POSTSUBSCRIPT ] ) ,P(r)P\ud835\udc5f\\displaystyle\\mathrm{P}(r)roman_P ( italic_r )=exp\u2061(\ud835\udc7er\ud835\udc9b+\ud835\udc83r)\u2211r\u2032\u2208\u211b\u222a{NA}exp\u2061(\ud835\udc7er\u2032\ud835\udc9b+\ud835\udc83r\u2032),absentsubscript\ud835\udc7e\ud835\udc5f\ud835\udc9bsubscript\ud835\udc83\ud835\udc5fsubscriptsuperscript\ud835\udc5f\u2032\u211bNAsubscript\ud835\udc7esuperscript\ud835\udc5f\u2032\ud835\udc9bsubscript\ud835\udc83superscript\ud835\udc5f\u2032\\displaystyle=\\frac{\\exp(\\bm{W}_{r}\\bm{z}+\\bm{b}_{r})}{\\sum_{r^{\\prime}\\in\\mathcal{R}\\cup\\{\\textsc{NA}\\}}\\exp(\\bm{W}_{r^{\\prime}}\\bm{z}+\\bm{b}_{r^{\\prime}})},= divide start_ARG roman_exp ( bold_italic_W start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT bold_italic_z + bold_italic_b start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_r start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 caligraphic_R \u222a { NA } end_POSTSUBSCRIPT roman_exp ( bold_italic_W start_POSTSUBSCRIPT italic_r start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT bold_italic_z + bold_italic_b start_POSTSUBSCRIPT italic_r start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) end_ARG ,where \ud835\udc7eproj\u2208\u211d2d\u00d7dsubscript\ud835\udc7eprojsuperscript\u211d2\ud835\udc51\ud835\udc51\\bm{W}_{\\text{proj}}\\in\\mathbb{R}^{2d\\times d}bold_italic_W start_POSTSUBSCRIPT proj end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT 2 italic_d \u00d7 italic_d end_POSTSUPERSCRIPT, \ud835\udc7er,\ud835\udc7er\u2032\u2208\u211dd,\ud835\udc83r,\ud835\udc83r\u2032\u2208\u211dformulae-sequencesubscript\ud835\udc7e\ud835\udc5fsubscript\ud835\udc7esuperscript\ud835\udc5f\u2032superscript\u211d\ud835\udc51subscript\ud835\udc83\ud835\udc5fsubscript\ud835\udc83superscript\ud835\udc5f\u2032\u211d\\bm{W}_{r},\\bm{W}_{{r^{\\prime}}}\\in\\mathbb{R}^{d},\\bm{b}_{r},\\bm{b}_{r^{\\prime}}\\in\\mathbb{R}bold_italic_W start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , bold_italic_W start_POSTSUBSCRIPT italic_r start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , bold_italic_b start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , bold_italic_b start_POSTSUBSCRIPT italic_r start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT \u2208 blackboard_R are model parameters.In inference, the classifier returns the relationship with the maximum probability as the predicted relationship.", "For sentence-level RE, the names, spans, and NER types of subject and object entities are provided in the structured input.Such composite entity information provides useful clues to the relation types.For example, the relationship ORG:FOUNDED_BY is more likely to hold when entity types of subject and object are ORGANIZATION and PERSON, respectively, and is less likely for instances where the entity types do not match.The entity information needs to be represented in the input text, allowing it to be captured by the PLMs.Such techniques have been studied in previous work\u00a0Zhang et\u00a0al. (2017); Baldini\u00a0Soares et\u00a0al. (2019); Wang et\u00a0al. (2020), while many of them fall short of capturing all types of the provided information.In this paper, we re-evaluate existing entity representation techniques and also seek to propose a better one.We evaluate the following techniques:", "\u2022Entity mask\u00a0Zhang et\u00a0al. (2017). This technique introduces new special tokens [SUBJ-TYPE] or [OBJ-TYPE] to mask the subject or object entities in the original text, where TYPE is substituted with the respective entity type.This technique was originally proposed in the PA-LSTM model\u00a0Zhang et\u00a0al. (2017), and was later adopted by PLMs such as SpanBERT\u00a0Joshi et\u00a0al. (2020).Zhang et\u00a0al. (2017) claim that this technique prevents the RE model from over-fitting specific entity names, leading to more generalizable inference.\u2022Entity marker\u00a0Zhang et\u00a0al. (2019); Baldini\u00a0Soares et\u00a0al. (2019). This technique introduces special tokens pairs [E1], [/E1] and [E2], [/E2] to enclose the subject and object entities, therefore modifying the input text to the format of \u201c[E1] subj [/E1] \u2026 [E2] obj [/E2]\u201d333subj and obj are respectively the original token spans of subject and object entities..\u2022Entity marker (punct)\u00a0Wang et\u00a0al. (2020); Zhou et\u00a0al. (2021). This technique is a variant of the previous technique that encloses entity spans using punctuation.It modifies the input text to \u201c@ subj @ \u2026 # obj #\u201d. The main difference from the previous technique is that this one does not introduce new special tokens into the model\u2019s reserved vocabulary.\u2022Typed entity marker\u00a0Zhong and Chen (2021). This technique further incorporates the NER types into entity markers.It introduces new special tokens \u27e8\u27e8\\langle\u27e8S:TYPE\u27e9normal-\u27e9\\rangle\u27e9, \u27e8\u27e8\\langle\u27e8/S:TYPE\u27e9normal-\u27e9\\rangle\u27e9, \u27e8\u27e8\\langle\u27e8O:TYPE\u27e9normal-\u27e9\\rangle\u27e9, \u27e8\u27e8\\langle\u27e8/O:TYPE\u27e9normal-\u27e9\\rangle\u27e9, where TYPE is the corresponding NER type given by a named entity tagger. The input text is accordingly modified to \u201c\u27e8\u27e8\\langle\u27e8S:TYPE\u27e9normal-\u27e9\\rangle\u27e9 subj \u27e8\u27e8\\langle\u27e8/S:TYPE\u27e9normal-\u27e9\\rangle\u27e9 \u2026 \u27e8\u27e8\\langle\u27e8O:TYPE\u27e9normal-\u27e9\\rangle\u27e9 obj \u27e8\u27e8\\langle\u27e8/O:TYPE\u27e9normal-\u27e9\\rangle\u27e9\u201d.\u2022Typed entity marker (punct). We propose a variant of the typed entity marker technique that marks the entity span and entity types without introducing new special tokens.This is to enclose the subject and object entities with \u201c@\u201d and \u201c#\u201d, respectively.We also represent the subject and object entity types using their label text, which is prepended to the entity spans and is enclosed by \u201c*\u201d for subjects or \u201c\u2227\\wedge\u2227\u201d for objects.The modified text is \u201c@ * subj-type * subj @ \u2026 # \u2227\\wedge\u2227 obj-type \u2227\\wedge\u2227 obj # \u201d, where subj-type and obj-type is the label text of NER types.", "The embedding of all new special tokens is randomly initialized and updated during fine-tuning.", "In this section, we evaluate the proposed techniques based on widely used RE benchmarks. The evaluation starts by first identifying the best-performing entity representation technique (Section\u00a03.2), which is further incorporated into our improved RE baseline to be compared against prior SOTA methods (Section\u00a03.3).Due to space limits, we study in the Appendix of how the incorporated techniques lead to varied generalizability on unseen entities (Appendix\u00a0B) and how they perform under annotation errors (Appendix\u00a0C).", "Datasets. The datasets we have used in the experiments include three versions of TACRED: the original TACRED\u00a0Zhang et\u00a0al. (2017), TACREV\u00a0Alt et\u00a0al. (2020), and Re-TACRED\u00a0Stoica et\u00a0al. (2021).Alt et\u00a0al. (2020) observed that the TACRED dataset contains about 6.62%percent6.626.62\\%6.62 % noisily-labeled instances and relabeled the development and test set.Stoica et\u00a0al. (2021) further refined the label definitions in TACRED and relabeled the whole dataset.We provide the statistics of the datasets in\u00a0Appendix\u00a0A.", "Compared methods.We compare with the following methods.PA-LSTM\u00a0Zhang et\u00a0al. (2017) adopts bi-directional LSTM\u00a0Hochreiter and Schmidhuber (1997) and positional-aware attention\u00a0Bahdanau et\u00a0al. (2015) to encode the text into an embedding, which is then fed into a softmax layer to predict the relation.C-GCN\u00a0Zhang et\u00a0al. (2018) is a graph-based model, which feeds the pruned dependency tree of the sentence into the graph convolutional network\u00a0Kipf and Welling (2017) to obtain the representation of entities.SpanBERT\u00a0Joshi et\u00a0al. (2020) is a PLM based on the Transformer\u00a0Vaswani et\u00a0al. (2017). It extends BERT\u00a0Devlin et\u00a0al. (2019) by incorporating a training objective of span prediction and achieves improved performance on RE.KnowBERT\u00a0Peters et\u00a0al. (2019) jointly trains a language model and an entity linker, which allows the subtokens to attend to entity embedding that is pretrained on knowledge bases.LUKE\u00a0Yamada et\u00a0al. (2020) pretrains the language model on both large text corpora and knowledge graphs. It adds frequent entities into the vocabulary and proposes an entity-aware self-attention mechanism.", "Model configurations.For the compared methods, we rerun their officially released code using the recommended hyperparameters in their papers.Our model is implemented based on HuggingFace\u2019s Transformers\u00a0Wolf et\u00a0al. (2020).Our model is optimized with Adam\u00a0Kingma and Ba (2015) using the learning rate of 5e\u221255e55\\mathrm{e}{-5}5 roman_e - 5 on BERTBASEBASE{}_{\\text{BASE}}start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT, and 3e\u221253e53\\mathrm{e}{-5}3 roman_e - 5 on BERTLARGELARGE{}_{\\text{LARGE}}start_FLOATSUBSCRIPT LARGE end_FLOATSUBSCRIPT and RoBERTaLARGELARGE{}_{\\text{LARGE}}start_FLOATSUBSCRIPT LARGE end_FLOATSUBSCRIPT, with a linear warm-up\u00a0Goyal et\u00a0al. (2017) of for the first 10%percent1010\\%10 % steps followed by a linear learning rate decay to 0.We use a batch size of 64 and fine-tune the model for 5 epochs on all datasets.For all experiments, we report the median F1subscript\ud835\udc391F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT of 5 runs of training using different random seeds.", "We first provide an analysis on different entity representation techniques. In this analysis, we use the base and large versions of BERT\u00a0Devlin et\u00a0al. (2019) and the large version of RoBERTa\u00a0Liu et\u00a0al. (2019) as the encoder.Table\u00a01 shows the performance of the PLMs incorporated with different entity representation techniques.For each technique, we also provide an example of the processed text.We have several observations from the results.First, the typed entity marker and its variants outperform untyped entity representation techniques by a notable margin.Especially, the RoBERTa model achieves an F1subscript\ud835\udc391F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score of 74.6%percent74.674.6\\%74.6 % using the typed entity marker (punct), which is significantly higher than the SOTA result of 72.7%percent72.772.7\\%72.7 % by LUKE\u00a0Yamada et\u00a0al. (2020).This shows that representing all categories of entity information is helpful to the RE task.It also shows that keeping entity names in the input improves the performance of RE models.Second, symbols used in entity markers have an obvious impact on the performance of RE models.Although the original and punct versions of entity representation techniques represent the same categories of entity information, they do lead to a difference in model performance.Particularly, introducing new special tokens hinders the model performance drastically on RoBERTa.On RoBERTaLARGELARGE{}_{\\text{LARGE}}start_FLOATSUBSCRIPT LARGE end_FLOATSUBSCRIPT, the entity marker underperforms the entity marker (punct) by 0.7%percent0.70.7\\%0.7 %, the typed entity marker underperforms the typed entity marker (punct) by 3.6%percent3.63.6\\%3.6 %, while the entity mask gets a much worse result of 60.9%percent60.960.9\\%60.9 %.", "The prior experiment has found RoBERTaLARGELARGE{}_{\\text{LARGE}}start_FLOATSUBSCRIPT LARGE end_FLOATSUBSCRIPT with the typed entity marker (punct) to be the best-performing RE model.We now compare our improved baseline with methods in prior studies.", "The experimental results are shown in Table\u00a02.We evaluate all methods on TACRED, TACREV, and Re-TACRED.Incorporated with the typed entity marker (punct) and using RoBERTaLARGELARGE{}_{\\text{LARGE}}start_FLOATSUBSCRIPT LARGE end_FLOATSUBSCRIPT as the backbone, our improved baseline model achieves new SOTA results over previous methods on all datasets.However, we observe that on Re-TACRED, the gain from the typed entity marker is much smaller compared to TACRED and TACREV, decreasing from 3.1\u22123.9%3.1percent3.93.1-3.9\\%3.1 - 3.9 % and 2.0\u22123.4%2.0percent3.42.0-3.4\\%2.0 - 3.4 % to 0.2\u22120.8%0.2percent0.80.2-0.8\\%0.2 - 0.8 % of F1subscript\ud835\udc391F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT.This observation could be attributed to the high noise rate in TACRED, in which the noisy labels are biased towards the side information of entities.", "To assess how the presented techniques contribute to robustness and generalizability of RE, we provide more analyses onvaried generalizability on unseen entities (Appendix\u00a0B) and the performance under annotation errors (Appendix\u00a0C) in the Appendix.", "In this paper, we present a simple yet strong RE baseline that offers new SOTA performance, along with a comprehensive study to understand its prediction generalizability and robustness.Specifically, we revisit two technical problems in sentence-level RE, namely entity representation and noisy or ill-defined labels.We propose an improved entity representation technique, which significantly outperforms existing sentence-level RE models.Especially, our improved RE baseline achieves an F1subscript\ud835\udc391F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score of 91.1%percent91.191.1\\%91.1 % on the Re-TACRED dataset, showing that PLMs already achieve satisfactory performance on this task.We hope the proposed techniques and analyses can benefit future research on RE."], "figure_types": {"11baa9cc02d6158edd9cb1f299579dad7828e162/4-Table1-1.png": "table", "11baa9cc02d6158edd9cb1f299579dad7828e162/4-Table2-1.png": "table", "11baa9cc02d6158edd9cb1f299579dad7828e162/7-Table3-1.png": "table", "11baa9cc02d6158edd9cb1f299579dad7828e162/7-Table4-1.png": "table", "11baa9cc02d6158edd9cb1f299579dad7828e162/7-Table5-1.png": "table"}}, "1911.03814": {"paper_id": "paper_21", "title": "Scalable Zero-shot Entity Linking with Dense Entity Retrieval", "arxiv_url": "https://arxiv.org/abs/1911.03814", "s2orc_url": "https://www.semanticscholar.org/paper/592a6691373f3936631bc4ac122f69df09c842bd", "all_figures_tables": {"592a6691373f3936631bc4ac122f69df09c842bd/2-Figure1-1.png": "Figure 1: High level description of our zero-shot entity linking solution.", "592a6691373f3936631bc4ac122f69df09c842bd/4-Figure2-1.png": "Figure 2: Top-k entity retrieval accuracy on validation dataset of Zero-shot EL dataset", "592a6691373f3936631bc4ac122f69df09c842bd/4-Table1-1.png": "Table 1: Recall@64 (%) on Zero-shot EL dataset", "592a6691373f3936631bc4ac122f69df09c842bd/4-Table2-1.png": "Table 2: Performance on test domains on the Zero-shot EL dataset. U.Acc. represents the unnormalized accuracy. \u2020 indicates model trained with domain adaptive pre-training on source and target domain. Average performance across a set of worlds is computed by macroaveraging.", "592a6691373f3936631bc4ac122f69df09c842bd/5-Table3-1.png": "Table 3: Normalized accuracy on validation and test set on Zero-shot EL, where the performance is evaluated on the subset of test instances for which the gold entity is among the top-k candidates retrieved during candidate generation. \u2020 indicates methods reimplemented by Logeswaran et al. (2019).", "592a6691373f3936631bc4ac122f69df09c842bd/5-Table4-1.png": "Table 4: Accuracy scores of our proposed model and models from prior work on TACKBP-2010. \u2020 indicates methods doing global resolution of all mentions in a document. Our work focuses on local resolution where each mention is modeled independently."}, "referred_figures_tables": [["592a6691373f3936631bc4ac122f69df09c842bd/2-Figure1-1.png"], ["592a6691373f3936631bc4ac122f69df09c842bd/2-Figure1-1.png", "592a6691373f3936631bc4ac122f69df09c842bd/5-Table4-1.png"], ["592a6691373f3936631bc4ac122f69df09c842bd/2-Figure1-1.png"]], "question_id": [0, 1, 8], "question": ["BLINK is Scalable. Is this true?", "BLINK have two different versions, bi-encoding version and cross-encoding version. Is this true?", "How BLINK can achieved zero-shot linking?"], "question_section": ["Introduction", "5.2. Evaluation setup and Results", "5.3 Knowledge Distillation"], "question_trigger_sentence": ["Scale is a key challenge for entity linking; there aremillions of possible entities to consider for eachmention.", "We experiment with both BERT-base and BERT-large (Devlin et al., 2019) for our bi-encoders andcross-encoders", "In this section, we present results on knowledgedistillation, using our cross-encoder as a teachermodel and bi-encoder as a student model."], "question_type": ["Shallow question", "Shallow question", "Deep/complex question"], "evidential_info": [[{"context": "Scale is a key challenge for entity linking; there are millions of possible entities to consider for each mention. To efficiently filter or rank the candidates, existing methods use different sources of external information, including manually curated mention tables\u00a0Ganea and Hofmann (2017), incoming Wikipedia link popularity\u00a0Yamada et\u00a0al. (2016), and gold Wikipedia entity categories\u00a0Gillick et\u00a0al. (2019). In this paper, we show that BERT-based models set new state-of-the-art performance levels for large scale entity linking when used in a zero shot setup, where there is no external knowledge and a short text description provides the only information we have for each entity. We also present an extensive evaluation of the accuracy-speed trade-off inherent to large pre-trained models, and show is possible to achieve very efficient linking with modest loss of accuracy.", "rationale": "Scalability is important in entity linking task since there are very large number (millions) of possible entities to consider for each mention. Different to existing methods, the proposed model showed state-of-the-art performance levels for large scale entity linking tasks only using short text description for each entitiy, without external knowledge source. The author also showed that the proposed model can achieve efficient linking considering accuracy-speed trade-off on large pre-trained models."}, {"context": "More specifically, we introduce a two stage approach for zero-shot linking (see Figure 1 for an overview), based on fine-tuned BERT architectures Devlin et\u00a0al. (2019). In the first stage, we do retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions\u00a0Humeau et\u00a0al. (2019); Gillick et\u00a0al. (2019).Each retrieved candidate is then examined more carefully with a cross-encoder that concatenates the mention and entity text, following\u00a0Logeswaran et\u00a0al. (2019). This overall approach is conceptually simple but highly effective, as we show through detailed experiments.", "rationale": "The author proposed the effective two-stage entity linking method which is simple and scalable. They evaluated their model on some zero-shot entity linking dataset, without using task-specific heuristics and external entity knowledge."}, {"context": "We proposed a conceptually simple, scalable, and highly effective two stage approach for entity linking. We show that our BERT-based model outperforms IR methods for entity retrieval, and achieved new state-of-the-art results on recently introduced zero-shot entity linking dataset, WikilinksNED Unseen-Mentions dataset, and the more established TACKBP-2010 benchmark, without any task-specific heuristics or external entity knowledge. We present evaluations of the accuracy-speed trade-off inherent to large pre-trained models, and show that it is possible to achieve efficient linking with modest loss of accuracy.Finally, we show that knowledge distillation can further improve bi-encoder model performance.Future work includes:\u2022Enriching entity representations by adding entity type and entity graph information;\u2022Modeling coherence by jointly resolving mentions in a document;\u2022Extending our work to other languages and other domains;\u2022Joint models for mention detection and entity linking.", "rationale": "The proposed methods contain two stages based on fine-tuned BERT architectures: candidates retrieval using a bi-encoder and examination using a cross-encoder. Experiments showed that this simple architecture can effectively works on the zero-shot linking task."}], [{"context": "As expected, the cross-encoder performs better than the bi-encoder on ranking. However, both models exceed state-of-the-art performance levels, demonstrating that the overall approach is highly effective. We observe that our model also performs well when we change the underlying Knowledgebase to full Wikipedia, and even without fine-tuning on the dataset. In Table 5 we show that our bi-encoder model is highly effective at retrieving relevant entities, where the underlying Knowledgebase is full Wikipedia.", "rationale": "The proposed model consists of two components: bi-encoder and cross-encoder. The bi-encoder encodes model context/mention and entity into dense vectors and scores each entity candidate with the dot product. The cross-encoder encodes entity candidates from the bi-encoder with context/mention in a single transformer and computes the final score for each pair."}, {"context": "In the first example, we see that the bi-encoder mistakenly links \u201cRonaldo\u201d to the Brazilian football player, while the cross-encoder is able to use context word \u201cJuventus\u201d to disambiguate. In the second example, the cross-encoder is able to identify from context that the sentence is describing art instead of fiction, where the bi-encoder failed. In the third example, the bi-encoder is able to find the correct entity \u201cAncient Greek,\u201d; where the cross-encoder mistakenly links it to the entity \u201cAncient Greek philosophy,\u201d likely because that the word \u201cphilosophers\u201d is in context. We observe that cross-encoder is often better at utilizing context information than bi-encoder, but can sometimes make mistakes because of misleading context cues.", "rationale": "The cross-encoder performs better than the bi-encoder model, while both of them reach state-of-the-art performance which shows the effectiveness of the proposed method."}, {"context": "Figure\u00a01 shows our overall approach. The bi-encoder uses two independent BERT transformers to encode model context/mention and entity into dense vectors, and each entity candidate is scored as the dot product of these vectors. The candidates retrieved by the bi-encoder are then passed to the cross-encoder for ranking. The cross-encoder encodes context/mention and entity in one transformer, and applies an additional linear layer to compute the final score for each pair.", "rationale": "The authors showed that the cross-encoder can better utilizing context information than bi-encoder, while failure cases also exists due to context misleading."}, {"context": "After training our model on Wikipedia, we fine-tune the model on the TACKBP-2010 training dataset. We use the top 100 candidates retrieved by the bi-encoder as training examples for the cross-encoder, and chose hyper-parameters based on cross validation. We report accuracy results in Table 4. For ablation studies, we also report the following versions of our model:", "rationale": "The authors performed the ablation studies for the proposed model, including bi-encoder only model that using bi-encoder for candidate ranking instead of cross-encoder."}], [{"context": "Scale is a key challenge for entity linking; there are millions of possible entities to consider for each mention. To efficiently filter or rank the candidates, existing methods use different sources of external information, including manually curated mention tables\u00a0Ganea and Hofmann (2017), incoming Wikipedia link popularity\u00a0Yamada et\u00a0al. (2016), and gold Wikipedia entity categories\u00a0Gillick et\u00a0al. (2019). In this paper, we show that BERT-based models set new state-of-the-art performance levels for large scale entity linking when used in a zero shot setup, where there is no external knowledge and a short text description provides the only information we have for each entity. We also present an extensive evaluation of the accuracy-speed trade-off inherent to large pre-trained models, and show is possible to achieve very efficient linking with modest loss of accuracy.", "rationale": "The authors introduced a two-stage approach for zero-shot linking based on fine-tuned BERT architectures. Specifically, the proposed approach first encodes the mention and entity independently does retrieve candidates in a dense space with the bi-encoder and is scored by the cross-encoder that concatenates the mention and entity."}, {"context": "More specifically, we introduce a two stage approach for zero-shot linking (see Figure 1 for an overview), based on fine-tuned BERT architectures Devlin et\u00a0al. (2019). In the first stage, we do retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions\u00a0Humeau et\u00a0al. (2019); Gillick et\u00a0al. (2019).Each retrieved candidate is then examined more carefully with a cross-encoder that concatenates the mention and entity text, following\u00a0Logeswaran et\u00a0al. (2019). This overall approach is conceptually simple but highly effective, as we show through detailed experiments.", "rationale": "The proposed approach utilizes cross-encoder and dense embeddings for entity ranking/candidate generation and showed that pre-trained zero-shot architectures are both highly accurate and computationally efficient at scale."}, {"context": "We proposed a conceptually simple, scalable, and highly effective two stage approach for entity linking. We show that our BERT-based model outperforms IR methods for entity retrieval, and achieved new state-of-the-art results on recently introduced zero-shot entity linking dataset, WikilinksNED Unseen-Mentions dataset, and the more established TACKBP-2010 benchmark, without any task-specific heuristics or external entity knowledge. We present evaluations of the accuracy-speed trade-off inherent to large pre-trained models, and show that it is possible to achieve efficient linking with modest loss of accuracy.Finally, we show that knowledge distillation can further improve bi-encoder model performance.Future work includes:\u2022Enriching entity representations by adding entity type and entity graph information;\u2022Modeling coherence by jointly resolving mentions in a document;\u2022Extending our work to other languages and other domains;\u2022Joint models for mention detection and entity linking.", "rationale": "The proposed approach could achieve new state-of-the-art results on WikilinksNED Unseen-Mentions and TACKBP-2010 dataset without any task-specific heuristics or external knowledge."}, {"context": "Two recent results are most closely related to our work. Logeswaran et\u00a0al. (2019) proposed the zero-shot entity linking task. They use cross-encoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on large scale benchmarks such as TACKBP.Gillick et\u00a0al. (2019) show that dense embeddings work well for candidate generation, but they did not do pre-training and included external category labels in their bi-encoder architectures, limiting their linking to entities in Wikipedia. Our approach can be seen as generalizing both of these lines of work, and showing for the first time that pre-trained zero-shot architectures are both highly accurate and computationally efficient at scale.", "rationale": "The authors showed that BERT-based models can achieve new state-of-the-art performance levels for large-scale entity linking when used in a zero-shot setup, where there is no external knowledge and only the short text description is available."}]], "composition": ["The paper shows the scalability of the proposed simple two-stage method with the experiments conducted on the zero-shot entity-linking dataset where external entity knowledge is not available, which enables the model to be used on various entity linking tasks that contain millions of possible entities to consider. The state-of-the-art result and the extensive evaluation of the accuracy-speed trade-off support that the proposed method is efficient and scalable.", "BLINK model is a two-stage method using two encoders: bi-encoder and cross-encoder. With the qualitative analysis, the authors compared the BLINK with its bi-encoding version which uses a bi-encoder for candidate ranking instead of a cross-encoder, and showed that the cross-encoding version utilizing context information better than the bi-encoding version. Therefore we can say that BLINK has two different versions.", "The BLINK model used a two-stage approach for entity linking based on fine-tuned BERT architectures that first encode the mention context and entity text with the bi-encoder for the candidate retrieval and utilize the cross-encoder to score and rank them. These pre-trained architectures are simple yet scalable and effective for entity link tasks without the help of task-specific heuristics or external knowledge. The authors showed that BLINK can achieve state-of-the-art performance for the large-scale entity linking on the dataset with a zero-shot setup. (WikilinksNED Unseen-Mentions and TACKBP-201)"], "Is_figure_in_evidence": [true, false, true], "Is_table_in_evidence": [false, true, false], "question_key": ["689", "690", "697"], "passages": ["Scale is a key challenge for entity linking; there are millions of possible entities to consider for each mention. To efficiently filter or rank the candidates, existing methods use different sources of external information, including manually curated mention tables\u00a0Ganea and Hofmann (2017), incoming Wikipedia link popularity\u00a0Yamada et\u00a0al. (2016), and gold Wikipedia entity categories\u00a0Gillick et\u00a0al. (2019). In this paper, we show that BERT-based models set new state-of-the-art performance levels for large scale entity linking when used in a zero shot setup, where there is no external knowledge and a short text description provides the only information we have for each entity. We also present an extensive evaluation of the accuracy-speed trade-off inherent to large pre-trained models, and show is possible to achieve very efficient linking with modest loss of accuracy.", "More specifically, we introduce a two stage approach for zero-shot linking (see Figure 1 for an overview), based on fine-tuned BERT architectures Devlin et\u00a0al. (2019). In the first stage, we do retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions\u00a0Humeau et\u00a0al. (2019); Gillick et\u00a0al. (2019).Each retrieved candidate is then examined more carefully with a cross-encoder that concatenates the mention and entity text, following\u00a0Logeswaran et\u00a0al. (2019). This overall approach is conceptually simple but highly effective, as we show through detailed experiments.", "Our two-stage approach achieves a new state-of-the-art result on TACKBP-2010, with an over 30% relative error reduction. By simply reading the provided text descriptions, we are able to outperform previous methods that included many extra cues such as entity name dictionaries and link popularity. We also improve the state of the art on existing zero-shot benchmarks, including a nearly 6 point absolute gain on the recently introduced Wikia corpus\u00a0Logeswaran et\u00a0al. (2019) and more than 7 point absolute gain on WikilinksNED Unseen-Mentions\u00a0Onoe and Durrett (2019).", "Finally, we do an extensive evaluation of the accuracy-speed trade-off inherent in our bi- and cross-encoder models.We show that the two stage methods scales well in a full Wikipedia setting, by linking against all the 5.9M Wikipedia entities for TACKBP-2010, while still outperforming existing model with much smaller candidate sets.We also show that bi-encoder linking is very fast with approximate nearest neighbor search (e.g. linking over 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation.We release our code and models, as well as a system to link entity mentions to all of Wikipedia (similar to TagME\u00a0Ferragina and Scaiella (2011)).111Our code and models are available at https://github.com/facebookresearch/BLINK", "We follow most recent work in studying entity linking with gold mentions.222Kolitsas et\u00a0al. (2018) study end-to-end linking. Our techniques should be applicable to this setting as well, but we leave this exploration to future work.The entity linking task can be broken into two steps: candidate generation and ranking. Prior work has used frequency information, alias tables and TF-IDF-based methods for candidate generation. For candidate ranking, He et\u00a0al. (2013), Sun et\u00a0al. (2015), Yamada et\u00a0al. (2016), Ganea and Hofmann (2017), and Kolitsas et\u00a0al. (2018) have established state-of-the-art results using neural networks to model context word, span and entity. There is also recent work demonstrating that fine-grained entity typing information helps linking\u00a0Raiman and Raiman (2018); Onoe and Durrett (2019); Khalife and Vazirgiannis (2018).", "Two recent results are most closely related to our work. Logeswaran et\u00a0al. (2019) proposed the zero-shot entity linking task. They use cross-encoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on large scale benchmarks such as TACKBP.Gillick et\u00a0al. (2019) show that dense embeddings work well for candidate generation, but they did not do pre-training and included external category labels in their bi-encoder architectures, limiting their linking to entities in Wikipedia. Our approach can be seen as generalizing both of these lines of work, and showing for the first time that pre-trained zero-shot architectures are both highly accurate and computationally efficient at scale.", "Humeau et\u00a0al. (2019) studied different architectures to use deep pre-trained bidirectional transformers and performed detailed comparison of three different architectures, namely bi-encoder, poly-encoder, cross-encoder on tasks of sentence selection in dialogues. Inspired by their work, we use similar architectures to the problem of entity linking, and in addition, demonstrate that bi-encoder can be a strong model for retrieval. Instead of using the poly-encoder as a trade-off between cross-encoder and bi-encoder, we propose to train a bi-encoder model with knowledge distillation\u00a0Buciluundefined et\u00a0al. (2006); Hinton et\u00a0al. (2015) from a cross-encoder model to further improve the bi-encoder\u2019s performances.", "Given an input text document \ud835\udc03={w1,\u2026,wr}\ud835\udc03subscript\ud835\udc641\u2026subscript\ud835\udc64\ud835\udc5f\\mathbf{D}=\\{w_{1},...,w_{r}\\}bold_D = { italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_w start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT } and a list of entity mentions \ud835\udc0c\ud835\udc03={m1,\u2026,mn}subscript\ud835\udc0c\ud835\udc03subscript\ud835\udc5a1\u2026subscript\ud835\udc5a\ud835\udc5b\\mathbf{M_{D}}=\\{m_{1},...,m_{n}\\}bold_M start_POSTSUBSCRIPT bold_D end_POSTSUBSCRIPT = { italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_m start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }, the output of an entity linking model is a list of mention-entity pairs {(mi,ei)}i\u2208[1,n]subscriptsubscript\ud835\udc5a\ud835\udc56subscript\ud835\udc52\ud835\udc56\ud835\udc561\ud835\udc5b\\{(m_{i},e_{i})\\}_{i\\in[1,n]}{ ( italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i \u2208 [ 1 , italic_n ] end_POSTSUBSCRIPTwhere each entity is an entry in a knowledge base (KB) (e.g. Wikipedia), e\u2208\u2130\ud835\udc52\u2130e\\in\\mathcal{E}italic_e \u2208 caligraphic_E. We assume that the title and description of the entities are available, which is a common setting in entity linking Ganea and Hofmann (2017); Logeswaran et\u00a0al. (2019).We also assume each mention has a valid gold entity in the KB, which is usually referred as in-KB evaluation. We leave the out-of-KB prediction (i.e. nil prediction) to future work.", "We also study zero-shot entity linking\u00a0Logeswaran et\u00a0al. (2019). Here the document setup is the same, but the knowledge base is separated in training and test time. Formally, denote \u2130trainsubscript\u2130\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\\mathcal{E}_{train}caligraphic_E start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT and \u2130testsubscript\u2130\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\\mathcal{E}_{test}caligraphic_E start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT to be the knowledge base in training and test, we require \u2130train\u2229\u2130test=\u2205subscript\u2130\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bsubscript\u2130\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\\mathcal{E}_{train}\\cap\\mathcal{E}_{test}=\\emptysetcaligraphic_E start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT \u2229 caligraphic_E start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT = \u2205.The set of text documents, mentions, and entity dictionary are separated in training and test so that the entities being linked at test time are unseen.", "Figure\u00a01 shows our overall approach. The bi-encoder uses two independent BERT transformers to encode model context/mention and entity into dense vectors, and each entity candidate is scored as the dot product of these vectors. The candidates retrieved by the bi-encoder are then passed to the cross-encoder for ranking. The cross-encoder encodes context/mention and entity in one transformer, and applies an additional linear layer to compute the final score for each pair.", "We use a bi-encoder architecture similar to the work of Humeau et\u00a0al. (2019) to model (mention, entity) pairs. This approach allows for fast, real-time inference, as the candidate representations can be cached. Both input context and candidate entity are encoded into vectors:\ud835\udc9a\ud835\udc8e=red(T1(\u03c4m))subscript\ud835\udc9a\ud835\udc8eredsubscript\ud835\udc471subscript\ud835\udf0f\ud835\udc5a\\displaystyle\\boldsymbol{y_{m}}=\\mathrm{red}(T_{1}(\\tau_{m}))bold_italic_y start_POSTSUBSCRIPT bold_italic_m end_POSTSUBSCRIPT = roman_red ( italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_\u03c4 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) )(1)\ud835\udc9a\ud835\udc86=red(T2(\u03c4e))subscript\ud835\udc9a\ud835\udc86redsubscript\ud835\udc472subscript\ud835\udf0f\ud835\udc52\\displaystyle\\boldsymbol{y_{e}}=\\mathrm{red}(T_{2}(\\tau_{e}))bold_italic_y start_POSTSUBSCRIPT bold_italic_e end_POSTSUBSCRIPT = roman_red ( italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_\u03c4 start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ) )(2)where \u03c4msubscript\ud835\udf0f\ud835\udc5a\\tau_{m}italic_\u03c4 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT and \u03c4esubscript\ud835\udf0f\ud835\udc52\\tau_{e}italic_\u03c4 start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT are input representations of mention and entity respectively, T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are two transformers. red(.)\\mathrm{red}(.)roman_red ( . ) is a function that reduces the sequence of vectors produced by the transformers into one vector. Following the experiments in Humeau et\u00a0al. (2019), we choose red(.)\\mathrm{red}(.)roman_red ( . ) to be the last layer of the output of the [CLS] token.", "The representation of context and mention \u03c4msubscript\ud835\udf0f\ud835\udc5a\\tau_{m}italic_\u03c4 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT is composed of the word-pieces of the context surrounding the mention and the mention itself. Specifically, we construct input of each mention example as:", "[CLS] ctxtl\ud835\udc59{}_{l}start_FLOATSUBSCRIPT italic_l end_FLOATSUBSCRIPT [Ms\ud835\udc60{}_{s}start_FLOATSUBSCRIPT italic_s end_FLOATSUBSCRIPT] mention [Me\ud835\udc52{}_{e}start_FLOATSUBSCRIPT italic_e end_FLOATSUBSCRIPT] ctxtr\ud835\udc5f{}_{r}start_FLOATSUBSCRIPT italic_r end_FLOATSUBSCRIPT [SEP]", "where mention, ctxtl\ud835\udc59{}_{l}start_FLOATSUBSCRIPT italic_l end_FLOATSUBSCRIPT, ctxtr\ud835\udc5f{}_{r}start_FLOATSUBSCRIPT italic_r end_FLOATSUBSCRIPT are the word-pieces tokens of the mention, context before and after the mention respectively, and [Ms\ud835\udc60{}_{s}start_FLOATSUBSCRIPT italic_s end_FLOATSUBSCRIPT], [Me\ud835\udc52{}_{e}start_FLOATSUBSCRIPT italic_e end_FLOATSUBSCRIPT] are special tokens to tag the mention. The maximum length of the input representation is a hyperparameter in our model, and we find that small value such as 32 works well in practice (see Appendix A). ", "The entity representation \u03c4esubscript\ud835\udf0f\ud835\udc52\\tau_{e}italic_\u03c4 start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT is also composed of word-pieces of the entity title and description (for Wikipedia entities, we use the first ten sentences as description).The input to our entity model is:", "[CLS] title [ENT] description [SEP]", "where title, description are word-pieces tokens of entity title and description, and [ENT] is a special token to separate entity title and description representation.", "The score of entity candidate eisubscript\ud835\udc52\ud835\udc56e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is given by the dot-product:s(m,ei)=\ud835\udc9a\ud835\udc8e\u22c5\ud835\udc9a\ud835\udc86\ud835\udc8a\ud835\udc60\ud835\udc5asubscript\ud835\udc52\ud835\udc56\u22c5subscript\ud835\udc9a\ud835\udc8esubscript\ud835\udc9asubscript\ud835\udc86\ud835\udc8a\\displaystyle s(m,e_{i})=\\boldsymbol{y_{m}}\\cdot\\boldsymbol{y_{e_{i}}}italic_s ( italic_m , italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = bold_italic_y start_POSTSUBSCRIPT bold_italic_m end_POSTSUBSCRIPT \u22c5 bold_italic_y start_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT bold_italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT(3)", "The network is trained to maximize the score of the correct entity with respect to the (randomly sampled) entities of the same batch\u00a0Lerer et\u00a0al. (2019); Humeau et\u00a0al. (2019). Concretely, for each training pair (mi,ei)subscript\ud835\udc5a\ud835\udc56subscript\ud835\udc52\ud835\udc56(m_{i},e_{i})( italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) in a batch of B\ud835\udc35Bitalic_B pairs, the loss is computed as:\u2112(mi,ei)=\u2212s(mi,ei)+log\u2211j=1Bexp\u2061(s(mi,ej))\u2112subscript\ud835\udc5a\ud835\udc56subscript\ud835\udc52\ud835\udc56\ud835\udc60subscript\ud835\udc5a\ud835\udc56subscript\ud835\udc52\ud835\udc56superscriptsubscript\ud835\udc571\ud835\udc35\ud835\udc60subscript\ud835\udc5a\ud835\udc56subscript\ud835\udc52\ud835\udc57\\displaystyle\\mathcal{L}(m_{i},e_{i})=-s(m_{i},e_{i})+\\log\\sum_{j=1}^{B}\\exp{(s(m_{i},e_{j}))}caligraphic_L ( italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = - italic_s ( italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + roman_log \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT roman_exp ( italic_s ( italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) )(4)", "Lerer et\u00a0al. (2019) presented a detailed analysis on speed and memory efficiency of using batched random negatives in large-scale systems. In addition to in-batch negatives, we follow Gillick et\u00a0al. (2019) by using hard negatives in training. The hard negatives are obtained by finding the top 10 predicted entities for each training example. We add these extra hard negatives to the random in-batch negatives.", "At inference time, the entity representation for all the entity candidates can be pre-computed and cached. The inference task is then reduced to finding maximum dot product between mention representation and entity candidate representations. In Section\u00a05.2.3 we present efficiency/accuracy trade-offs by exact and approximate nearest neighbor search using FAISS\u00a0Johnson et\u00a0al. (2019) in a large-scale setting.", "Our cross-encoder is similar to the ones described by Logeswaran et\u00a0al. (2019) and Humeau et\u00a0al. (2019). The input is the concatenation of the input context and mention representation and the entity representation described in Section\u00a04.1 (we remove the [CLS] token from the entity representation). This allows the model to have deep cross attention between the context and entity descriptions.Formally, we use ym,esubscript\ud835\udc66\ud835\udc5a\ud835\udc52y_{m,e}italic_y start_POSTSUBSCRIPT italic_m , italic_e end_POSTSUBSCRIPT to denote our context-candidate embedding:\ud835\udc9a\ud835\udc8e,\ud835\udc86=red(Tcross(\u03c4m,e))subscript\ud835\udc9a\ud835\udc8e\ud835\udc86redsubscript\ud835\udc47crosssubscript\ud835\udf0f\ud835\udc5a\ud835\udc52\\displaystyle\\boldsymbol{y_{m,e}}=\\mathrm{red}(T_{\\mathrm{cross}}(\\tau_{m,e}))bold_italic_y start_POSTSUBSCRIPT bold_italic_m bold_, bold_italic_e end_POSTSUBSCRIPT = roman_red ( italic_T start_POSTSUBSCRIPT roman_cross end_POSTSUBSCRIPT ( italic_\u03c4 start_POSTSUBSCRIPT italic_m , italic_e end_POSTSUBSCRIPT ) )(5)where \u03c4m,esubscript\ud835\udf0f\ud835\udc5a\ud835\udc52\\tau_{m,e}italic_\u03c4 start_POSTSUBSCRIPT italic_m , italic_e end_POSTSUBSCRIPT is the input representation of mention and entity, Tcrosssubscript\ud835\udc47\ud835\udc50\ud835\udc5f\ud835\udc5c\ud835\udc60\ud835\udc60T_{cross}italic_T start_POSTSUBSCRIPT italic_c italic_r italic_o italic_s italic_s end_POSTSUBSCRIPT is a transformer and red(.)red(.)italic_r italic_e italic_d ( . ) is the same function as defined in Section 4.1.", "To score entity candidates, a linear layer \ud835\udc7e\ud835\udc7e\\boldsymbol{W}bold_italic_W is applied to the embedding \ud835\udc9a\ud835\udc8e,\ud835\udc86subscript\ud835\udc9a\ud835\udc8e\ud835\udc86\\boldsymbol{y_{m,e}}bold_italic_y start_POSTSUBSCRIPT bold_italic_m bold_, bold_italic_e end_POSTSUBSCRIPT:scross(m,e)=\ud835\udc9a\ud835\udc8e,\ud835\udc86\ud835\udc7esubscript\ud835\udc60cross\ud835\udc5a\ud835\udc52subscript\ud835\udc9a\ud835\udc8e\ud835\udc86\ud835\udc7e\\displaystyle s_{\\mathrm{cross}}(m,e)=\\boldsymbol{y_{m,e}}\\boldsymbol{W}italic_s start_POSTSUBSCRIPT roman_cross end_POSTSUBSCRIPT ( italic_m , italic_e ) = bold_italic_y start_POSTSUBSCRIPT bold_italic_m bold_, bold_italic_e end_POSTSUBSCRIPT bold_italic_W(6)", "Similar to methods in Section\u00a04.1, the network is trained using a softmax loss to maximize scross(mi,ei)subscript\ud835\udc60crosssubscript\ud835\udc5a\ud835\udc56subscript\ud835\udc52\ud835\udc56s_{\\mathrm{cross}}(m_{i},e_{i})italic_s start_POSTSUBSCRIPT roman_cross end_POSTSUBSCRIPT ( italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) for the correct entity, given a set of entity candidates (same as in Equation 4).", "Due to its larger memory and compute footprint, we use the cross-encoder in a re-ranking stage, over a small set (\u2264100)\\leq 100)\u2264 100 ) of candidates retrieved with the bi-encoder. The cross-encoder is not suitable for retrieval or tasks that require fast inference.", "To better optimize the accuracy-speed trade-off, we also report knowledge distillation experiments that use a cross-encoder as a teacher for a bi-encoder model. We follow Hinton et\u00a0al. (2015) to use a softmax with temperature where the target distribution is based on the cross-encoder logits.", "Concretely, let z\ud835\udc67zitalic_z be a vector of logits for set of entity candidates and T\ud835\udc47Titalic_T a temperature, and \u03c3(z,T)\ud835\udf0e\ud835\udc67\ud835\udc47\\sigma(z,T)italic_\u03c3 ( italic_z , italic_T ) a (tempered) distribution over the entities with\u03c3(z,T)=exp\u2061(zi/T)\u2211jexp\u2061(zj/T).\ud835\udf0e\ud835\udc67\ud835\udc47subscript\ud835\udc67\ud835\udc56\ud835\udc47subscript\ud835\udc57subscript\ud835\udc67\ud835\udc57\ud835\udc47\\displaystyle\\sigma(z,T)=\\frac{\\exp{(z_{i}/T)}}{\\sum_{j}\\exp{(z_{j}/T)}}.italic_\u03c3 ( italic_z , italic_T ) = divide start_ARG roman_exp ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_T ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT roman_exp ( italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / italic_T ) end_ARG .(7)Then the overall loss function, incorporating both distillation and student losses, is calculated as\u2112dist=\u210b(\u03c3(zt;\u03c4),\u03c3(zs;\u03c4))subscript\u2112\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\u210b\ud835\udf0esubscript\ud835\udc67\ud835\udc61\ud835\udf0f\ud835\udf0esubscript\ud835\udc67\ud835\udc60\ud835\udf0f\\displaystyle\\mathcal{L}_{dist}=\\mathcal{H}(\\sigma(z_{t};\\tau),\\sigma(z_{s};\\tau))caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT = caligraphic_H ( italic_\u03c3 ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; italic_\u03c4 ) , italic_\u03c3 ( italic_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ; italic_\u03c4 ) )(8)\u2112st=\u210b(e,\u03c3(zs;1))subscript\u2112\ud835\udc60\ud835\udc61\u210b\ud835\udc52\ud835\udf0esubscript\ud835\udc67\ud835\udc601\\displaystyle\\mathcal{L}_{st}=\\mathcal{H}(e,\\sigma(z_{s};1))caligraphic_L start_POSTSUBSCRIPT italic_s italic_t end_POSTSUBSCRIPT = caligraphic_H ( italic_e , italic_\u03c3 ( italic_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ; 1 ) )(9)\u2112=\u03b1\u22c5\u2112st+(1\u2212\u03b1)\u22c5\u2112dist\u2112\u22c5\ud835\udefcsubscript\u2112\ud835\udc60\ud835\udc61\u22c51\ud835\udefcsubscript\u2112\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\\displaystyle\\mathcal{L}=\\alpha\\cdot\\mathcal{L}_{st}+(1-\\alpha)\\cdot\\mathcal{L}_{dist}caligraphic_L = italic_\u03b1 \u22c5 caligraphic_L start_POSTSUBSCRIPT italic_s italic_t end_POSTSUBSCRIPT + ( 1 - italic_\u03b1 ) \u22c5 caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t end_POSTSUBSCRIPT(10)where e\ud835\udc52eitalic_e is the ground truth label distribution with probability 1 for the gold entity, \u210b\u210b\\mathcal{H}caligraphic_H is the cross-entropy loss function, and \u03b1\ud835\udefc\\alphaitalic_\u03b1 is coefficient for mixing distillation and student loss \u2112stsubscript\u2112\ud835\udc60\ud835\udc61\\mathcal{L}_{st}caligraphic_L start_POSTSUBSCRIPT italic_s italic_t end_POSTSUBSCRIPT. The student logits zssubscript\ud835\udc67\ud835\udc60z_{s}italic_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT are the output of the bi-encoder scoring function s(m,ei)\ud835\udc60\ud835\udc5asubscript\ud835\udc52\ud835\udc56s(m,e_{i})italic_s ( italic_m , italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), the teacher logits the output of the cross-encoder scoring funcion scross(m,e)subscript\ud835\udc60cross\ud835\udc5a\ud835\udc52s_{\\mathrm{cross}}(m,e)italic_s start_POSTSUBSCRIPT roman_cross end_POSTSUBSCRIPT ( italic_m , italic_e ).", "In this section, we perform an empirical study of our model on three challenging datasets.", "was constructed by Logeswaran et\u00a0al. (2019) from Wikia.333https://www.wikia.com.The task is to link entity mentions in text to an entity dictionary with provided entity descriptions, in a set of domains. There are 49K, 10K, and 10K examples in the train, validation, test sets respectively.The entities in the validation and test sets are from different domains than the train set, allowing for evaluation of performance on entirely unseen entities. The entity dictionaries cover different domains and range in size from 10K to 100K entities.", "is widely used for evaluating entity linking systems\u00a0Ji et\u00a0al. (2010).444https://tac.nist.gov Following prior work, we measure in-KB accuracy (P@1). There are 1,074 and 1,020 annotated mention/entity pairs derived from 1,453 and 2,231 original news and web documents on training and evaluation dataset, respectively. All the entities are from the TAC Reference Knowledgebase which contains 818,741 entities with titles, descriptions and other meta info.", "was created by Onoe and Durrett (2019) from the original WikilinksNED dataset\u00a0Eshel et\u00a0al. (2017), which contains a diverse set of ambiguous entities spanning a variety of domains. In the Unseen-Mentions version, no mentions in the validation and test sets appear in the training set. The train, validation and test sets contain 2.2M, 10K, and 10K examples respectively. In this setting, the definition of unseen-mentions is different from that in zero-shot entity linking: entities in the test set can be seen in the training set. However, in both definitions no (mention, entity) pairs from test set are observed in the training set. In the unseen-mentions test set, about 25%percent2525\\%25 % of the entities appear in training set.", "We experiment with both BERT-base and BERT-large Devlin et\u00a0al. (2019) for our bi-encoders and cross-encoders. The details of training infrastructure and hyperparameters can be found in Appendix A.All models are implemented in PyTorch555https://pytorch.org and optimizied with Adam\u00a0Kingma and Ba (2014). We use (base) and (large) to indicate the version of our model where the underlying pretrained transformer model is BERT-base and BERT-large, respectively.", "First, we train our bi-encoder on the training set, initializing each encoder with pre-trained BERT base. Hyper-parameters are chosen based on Recall@64 on validation datase. For specifics, see Appendix A.2.Our bi-encoder achieves much higher recall than BM25, as shown in Figure 2.Following Logeswaran et\u00a0al. (2019), we use the top 64 retrieved candidates for the ranker, and we report Recall@64 on train, validation and test in Table\u00a01.", "After training the bi-encoder for candidate generation, we train our cross-encoder (initialized with pre-trained BERT) on the top 64 retrieved candidates from bi-encoder for each sample on the training set, and evaluate the cross-encoder on the test dataset. Overall, we are able to obtain a much better end-to-end accuracy, as shown in Table 2, largely due to the improvement on the retrieval stage.", "We also report cross-encoder performance on the same retrieval method (BM25) used by Logeswaran et\u00a0al. (2019) in Table\u00a03, where the performance is evaluated on the subset of test instances for which the gold entity is among the top 64 candidates retrieved by BM25. We observe that our cross-encoder obtains slightly better results than reported by Logeswaran et\u00a0al. (2019), likely due to implementation and hyper-parameter details.", "Following prior work Sun et\u00a0al. (2015); Cao et\u00a0al. (2018); Gillick et\u00a0al. (2019); Onoe and Durrett (2019), we pre-train our models on Wikipedia666https://www.wikipedia.org/ data. Data and model training details can be found in Appendix A.1. ", "After training our model on Wikipedia, we fine-tune the model on the TACKBP-2010 training dataset. We use the top 100 candidates retrieved by the bi-encoder as training examples for the cross-encoder, and chose hyper-parameters based on cross validation. We report accuracy results in Table\u00a04. For ablation studies, we also report the following versions of our model:1.bi-encoder only: we use bi-encoder for candidate ranking instead of cross-encoder.2.Full Wikipedia: we use 5.9M Wikipedia articles as our entity Knowlegebase, instead of TACKBP Reference Knowledgebase.3.Full Wikipedia w/o finetune: same as above, without fine-tuning on the TACKBP-2010 training set.", "As expected, the cross-encoder performs better than the bi-encoder on ranking. However, both models exceed state-of-the-art performance levels, demonstrating that the overall approach is highly effective. We observe that our model also performs well when we change the underlying Knowledgebase to full Wikipedia, and even without fine-tuning on the dataset. In Table 5 we show that our bi-encoder model is highly effective at retrieving relevant entities, where the underlying Knowledgebase is full Wikipedia.", "There are however many other cues that could potentially be added in future work. For example, Khalife and Vazirgiannis (2018) report 94.57%percent94.5794.57\\%94.57 % precision on the TACKBP-2010 dataset. However, their method is based on the strong assumption that a gold fine-grained entity type is given for each mention (and they do not attempt to do entity type prediction). Indeed, if fine-grained entity type information is given by an oracle at test time, then Raiman and Raiman (2018) reports 98.6%percent98.698.6\\%98.6 % accuracy on TACKBP-2010, indicating that improving fine-grained entity type prediction would likely improve entity linking. Our results is achieved without gold fine-grained entity type information. Instead, our model learns representations of context, mention and entities based on text only.", "Similarly to the approach described in Section 5.2.2, we train our bi-encoder and cross-encoder model first on Wikipedia examples, then fine-tune on the training data from this dataset. We also present our model trained on Wikipedia examples and applied directly on the test setas well as our model trained on this dataset directly without training on Wikipedia examples. We report our models\u2019 performance of accuracy on the test set in Table\u00a06, along with baseline models presented from\u00a0Onoe and Durrett (2019). We observe that our model out-performs all the baseline models.", "To illustrate the efficiency of our bi-encoder model, we profiled retrieval speed on a server with Intel Xeon CPU E5-2698 v4 @ 2.20GHz and 512GB memory. At inference time, we first compute all entity embeddings for the pool of 5.9M entities. This step is resource intensive but can be paralleled. On 8 Nvidia Volta v100 GPUs, it takes about 2.8 hours to compute all entity embeddings. Given a query of mention embedding, we use FAISS\u00a0Johnson et\u00a0al. (2019) IndexFlatIP index type (exact search) to obtain top 100 entity candidates. On the WikilinksNED Unseen-Mentions test dataset which contains 10K queries, it takes 9.2 ms on average to return top 100 candidates per query in batch mode.", "We also explore the approximate search options using FAISS. We choose the IndexHNSWFlat index type following Karpukhin et\u00a0al. (2020). It takes additional time in index construction while reduces the average time used per query. In Table\u00a07, we see that HNSW1\ud835\udc3b\ud835\udc41\ud835\udc46subscript\ud835\udc4a1HNSW_{1}italic_H italic_N italic_S italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT777Neighbors to store per node: 128, construction time search depth: 200, search depth: 256; construction time: 2.1h. reduces the average query time to 2.6 ms with less than 1.2% drop in accuracy and recall, and HNSW2\ud835\udc3b\ud835\udc41\ud835\udc46subscript\ud835\udc4a2HNSW_{2}italic_H italic_N italic_S italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT888Neighbors to store per node: 128, construction time search depth: 200, search depth: 128; construction time: 1.8h. further reduce the query time to 1.4 ms with less than 2.1% drop.", "In a two-stage entity linking systems, the choice of number of candidates retrieved influences the overall model performance. Prior work often used a fixed number of k\ud835\udc58kitalic_k candidates where k\ud835\udc58kitalic_k ranges from 5555 to 100100100100 (for instance, Yamada et\u00a0al. (2016) and Ganea and Hofmann (2017) choose k=30\ud835\udc5830k=30italic_k = 30, Logeswaran et\u00a0al. (2019) choose k=64\ud835\udc5864k=64italic_k = 64). When k\ud835\udc58kitalic_k is larger, the recall accuracy increases, however, the ranking stage accuracy is likely to decrease. Further, increasing k\ud835\udc58kitalic_k would often increase the run-time on the ranking stage. We explore different choices of k\ud835\udc58kitalic_k in our model, and present the recall@K\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59@\ud835\udc3erecall@Kitalic_r italic_e italic_c italic_a italic_l italic_l @ italic_K curve, ranking stage accuracy and overall accuracy in Figure 3. Based on the overall accuracy, we found that k=10\ud835\udc5810k=10italic_k = 10 is optimal.", "In this section, we present results on knowledge distillation, using our cross-encoder as a teacher model and bi-encoder as a student model.", "We experiment knowledge distillation on the TACKBP-2010 and the WikilinksNED Unseen-Mentions dataset.We use the bi-encoder pretrained on Wikipedia as the student model, and fine-tune it on each dataset with knowledge distillation from the teacher model, which is the best performing cross-encoder model pretrained on Wikipedia and fine-tuned on the dataset.", "We also fine-tune the student model in our experiments on each dataset, without the knowledge distillation component, as baseline models.As we can see in Table 9, the bi-encoder model trained with knowledge distillation from cross-encoder out-performs the bi-encoder without knowledge distillation, providing another point in the accuracy-speed trade-off curve for these architectures.", "Table\u00a08 presents some examples from our bi-encoder and cross-encoder model predictions, to provide intuition for how these two models consider context and mention for entity linking.", "In the first example, we see that the bi-encoder mistakenly links \u201cRonaldo\u201d to the Brazilian football player, while the cross-encoder is able to use context word \u201cJuventus\u201d to disambiguate. In the second example, the cross-encoder is able to identify from context that the sentence is describing art instead of fiction, where the bi-encoder failed. In the third example, the bi-encoder is able to find the correct entity \u201cAncient Greek,\u201d; where the cross-encoder mistakenly links it to the entity \u201cAncient Greek philosophy,\u201d likely because that the word \u201cphilosophers\u201d is in context. We observe that cross-encoder is often better at utilizing context information than bi-encoder, but can sometimes make mistakes because of misleading context cues.", "We proposed a conceptually simple, scalable, and highly effective two stage approach for entity linking. We show that our BERT-based model outperforms IR methods for entity retrieval, and achieved new state-of-the-art results on recently introduced zero-shot entity linking dataset, WikilinksNED Unseen-Mentions dataset, and the more established TACKBP-2010 benchmark, without any task-specific heuristics or external entity knowledge. We present evaluations of the accuracy-speed trade-off inherent to large pre-trained models, and show that it is possible to achieve efficient linking with modest loss of accuracy.Finally, we show that knowledge distillation can further improve bi-encoder model performance.Future work includes:\u2022Enriching entity representations by adding entity type and entity graph information;\u2022Modeling coherence by jointly resolving mentions in a document;\u2022Extending our work to other languages and other domains;\u2022Joint models for mention detection and entity linking."], "figure_types": {"592a6691373f3936631bc4ac122f69df09c842bd/2-Figure1-1.png": "schematic", "592a6691373f3936631bc4ac122f69df09c842bd/4-Figure2-1.png": "plot", "592a6691373f3936631bc4ac122f69df09c842bd/4-Table1-1.png": "table", "592a6691373f3936631bc4ac122f69df09c842bd/4-Table2-1.png": "table", "592a6691373f3936631bc4ac122f69df09c842bd/5-Table3-1.png": "table", "592a6691373f3936631bc4ac122f69df09c842bd/5-Table4-1.png": "table"}}, "1708.07747": {"paper_id": "paper_24", "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms", "arxiv_url": "https://arxiv.org/abs/1708.07747", "s2orc_url": "https://www.semanticscholar.org/paper/f9c602cc436a9ea2f9e7db48c77d924e09ce3c32", "all_figures_tables": {"f9c602cc436a9ea2f9e7db48c77d924e09ce3c32/2-Figure1-1.png": "Figure 1: Diagram of the conversion process used to generate Fashion-MNIST dataset. Two examples from dress and sandals categories are depicted, respectively. Each column represents a step described in section 2.", "f9c602cc436a9ea2f9e7db48c77d924e09ce3c32/2-Table1-1.png": "Table 1: Files contained in the Fashion-MNIST dataset.", "f9c602cc436a9ea2f9e7db48c77d924e09ce3c32/3-Table2-1.png": "Table 2: Class names and example images in Fashion-MNIST dataset.", "f9c602cc436a9ea2f9e7db48c77d924e09ce3c32/3-Table3-1.png": "Table 3: Benchmark on Fashion-MNIST (Fashion) and MNIST."}, "referred_figures_tables": [["f9c602cc436a9ea2f9e7db48c77d924e09ce3c32/3-Table3-1.png"]], "question_id": [12], "question": ["For a given benchmarking algorithm, did the authors try different hyper-parameters?"], "question_section": ["Abstract"], "question_trigger_sentence": ["We provide some classification results in Table 3 to form a benchmark on this data set"], "question_type": ["Shallow question"], "evidential_info": [[{"context": "We provide some classification results in Table\u00a03 to form a benchmark on this data set. All algorithms are repeated 5 times by shuffling the training data and the average accuracy on the test set is reported. The benchmark on the MNIST dataset is also included for a side-by-side comparison. A more comprehensive table with explanations on the algorithms can be found on https://github.com/zalandoresearch/fashion-mnist.", "rationale": "All algorithms are repeated 5 times by shuffling the training data and the average accuracy on the test set is reported. The benchmark on the MNIST dataset is also included for a side-by-side comparison."}]], "composition": ["Yes the author used hyper-parameter tuning."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["724"], "passages": ["The MNIST dataset comprising of 10-class handwritten digits, was first introduced by\u00a0LeCun et\u00a0al. (1998) in 1998. At that time one could not have foreseen the stellar rise of deep learning techniques and their performance. Despite the fact that today deep learning can do so much the simple MNIST dataset has become the most widely used testbed in deep learning, surpassing CIFAR-10\u00a0(Krizhevsky and Hinton, 2009) and ImageNet\u00a0(Deng et\u00a0al., 2009) in its popularity via Google trends111https://trends.google.com/trends/explore?date=all&q=mnist,CIFAR,ImageNet. Despite its simplicity its usage does not seem to be decreasing despite calls for it in the deep learning community.", "The reason MNIST is so popular has to do with its size, allowing deep learning researchers to quickly check and prototype their algorithms. This is also complemented by the fact that all machine learning libraries (e.g. scikit-learn) and deep learning frameworks (e.g. Tensorflow, Pytorch) provide helper functions and convenient examples that use MNIST out of the box.", "Our aim with this work is to create a good benchmark dataset which has all the accessibility of MNIST, namely its small size, straightforward encoding and permissive license. We took the approach of sticking to the 10101010 classes 70,0007000070,00070 , 000 grayscale images in the size of 28\u00d728282828\\times 2828 \u00d7 28 as in the original MNIST. In fact, the only change one needs to use this dataset is to change the URL from where the MNIST dataset is fetched. Moreover, Fashion-MNIST poses a more challenging classification task than the simple MNIST digits data, whereas the latter has been trained to accuracies above 99.7% as reported in\u00a0Wan et\u00a0al. (2013); Ciregan et\u00a0al. (2012).", "We also looked at the EMNIST dataset provided by\u00a0Cohen et\u00a0al. (2017), an extended version of MNIST that extends the number of classes by introducing uppercase and lowercase characters. However, to be able to use it seamlessly one needs to not only extend the deep learning framework\u2019s MNIST helpers, but also change the underlying deep neural network to classify these extra classes.", "Fashion-MNIST is based on the assortment on Zalando\u2019s website222Zalando is the Europe\u2019s largest online fashion platform. http://www.zalando.com. Every fashion product on Zalando has a set of pictures shot by professional photographers, demonstrating different aspects of the product, i.e. front and back looks, details, looks with model and in an outfit. The original picture has a light-gray background (hexadecimal color: #fdfdfd) and stored in 762\u00d710007621000762\\times 1000762 \u00d7 1000 JPEG format. For efficiently serving different frontend components, the original picture is resampled with multiple resolutions, e.g. large, medium, small, thumbnail and tiny.", "We use the front look thumbnail images of 70,0007000070,00070 , 000 unique products to build Fashion-MNIST. Those products come from different gender groups: men, women, kids and neutral. In particular, white-color products are not included in the dataset as they have low contrast to the background. The thumbnails (51\u00d773517351\\times 7351 \u00d7 73) are then fed into the following conversion pipeline, which is visualized in Figure\u00a01.", "1.Converting the input to a PNG image.2.Trimming any edges that are close to the color of the corner pixels.The \u201ccloseness\u201d is defined by the distance within 5%percent55\\%5 % of the maximum possible intensity in RGB space.3.Resizing the longest edge of the image to 28282828 by subsampling the pixels, i.e. some rows and columnsare skipped over.4.Sharpening pixels using a Gaussian operator of the radius and standard deviation of 1.01.01.01.0, with increasing effect near outlines.5.Extending the shortest edge to 28282828 and put the image to the center of the canvas.6.Negating the intensities of the image.7.Converting the image to 8-bit grayscale pixels.", "For the class labels, we use the silhouette code of the product. The silhouette code is manually labeled by the in-house fashion experts and reviewed by a separate team at Zalando. Each product contains only one silhouette code. Table\u00a02 gives a summary of all class labels in Fashion-MNIST with examples for each class.", "Finally, the dataset is divided into a training and a test set. The training set receives a randomly-selected 6,00060006,0006 , 000 examples from each class. Images and labels are stored in the same file format as the MNIST data set, which is designed for storing vectors and multidimensional matrices. The result files are listed in Table\u00a01. We sort examples by their labels while storing, resulting in smaller label files after compression comparing to the MNIST. It is also easier to retrieve examples with a certain class label. The data shuffling job is therefore left to the algorithm developer.", "We provide some classification results in Table\u00a03 to form a benchmark on this data set. All algorithms are repeated 5555 times by shuffling the training data and the average accuracy on the test set is reported. The benchmark on the MNIST dataset is also included for a side-by-side comparison. A more comprehensive table with explanations on the algorithms can be found on https://github.com/zalandoresearch/fashion-mnist.", "This paper introduced Fashion-MNIST, a fashion product images dataset intended to be a drop-in replacement of MNIST and whilst providing a more challenging alternative for benchmarking machine learning algorithm. The images in Fashion-MNIST are converted to a format that matches that of the MNIST dataset, making it immediately compatible with any machine learning package capable of working with the original MNIST dataset."], "figure_types": {"f9c602cc436a9ea2f9e7db48c77d924e09ce3c32/2-Figure1-1.png": "schematic", "f9c602cc436a9ea2f9e7db48c77d924e09ce3c32/2-Table1-1.png": "table", "f9c602cc436a9ea2f9e7db48c77d924e09ce3c32/3-Table2-1.png": "table", "f9c602cc436a9ea2f9e7db48c77d924e09ce3c32/3-Table3-1.png": "table"}}, "1703.06870": {"paper_id": "paper_29", "title": "Mask R-CNN", "arxiv_url": "https://arxiv.org/abs/1703.06870", "s2orc_url": "https://www.semanticscholar.org/paper/1a0912bb76777469295bb2c059faee907e7f3258", "all_figures_tables": {"1a0912bb76777469295bb2c059faee907e7f3258/1-Figure1-1.png": "Figure 1. The Mask R-CNN framework for instance segmentation.", "1a0912bb76777469295bb2c059faee907e7f3258/2-Figure2-1.png": "Figure 2. Mask R-CNN results on the COCO test set. These results are based on ResNet-101 [19], achieving a mask AP of 35.7 and running at 5 fps. Masks are shown in color, and bounding box, category, and confidences are also shown.", "1a0912bb76777469295bb2c059faee907e7f3258/4-Figure3-1.png": "Figure 3. Head Architecture: We extend two existing Faster RCNN heads [19, 27]. Left/Right panels show the heads for the ResNet C4 and FPN backbones, from [19] and [27], respectively, to which a mask branch is added. Numbers denote spatial resolution and channels. Arrows denote either conv, deconv, or fc layers as can be inferred from context (conv preserves spatial dimension while deconv increases it). All convs are 3\u00d73, except the output conv which is 1\u00d71, deconvs are 2\u00d72 with stride 2, and we use ReLU [30] in hidden layers. Left: \u2018res5\u2019 denotes ResNet\u2019s fifth stage, which for simplicity we altered so that the first conv operates on a 7\u00d77 RoI with stride 1 (instead of 14\u00d714 / stride 2 as in [19]). Right: \u2018\u00d74\u2019 denotes a stack of four consecutive convs.", "1a0912bb76777469295bb2c059faee907e7f3258/5-Figure4-1.png": "Figure 4. More results of Mask R-CNN on COCO test images, using ResNet-101-FPN and running at 5 fps, with 35.7 mask AP (Table 1).", "1a0912bb76777469295bb2c059faee907e7f3258/5-Table1-1.png": "Table 1. Instance segmentation mask AP on COCO test-dev. MNC [10] and FCIS [26] are the winners of the COCO 2015 and 2016 segmentation challenges, respectively. Without bells and whistles, Mask R-CNN outperforms the more complex FCIS+++, which includes multi-scale train/test, horizontal flip test, and OHEM [35]. All entries are single-model results.", "1a0912bb76777469295bb2c059faee907e7f3258/6-Figure5-1.png": "Figure 5. FCIS+++ [26] (top) vs. Mask R-CNN (bottom, ResNet-101-FPN). FCIS exhibits systematic artifacts on overlapping objects.", "1a0912bb76777469295bb2c059faee907e7f3258/6-Table2-1.png": "Table 2. Ablations for Mask R-CNN. We train on trainval35k, test on minival, and report mask AP unless otherwise noted.", "1a0912bb76777469295bb2c059faee907e7f3258/7-Table3-1.png": "Table 3. Object detection single-model results (bounding box AP), vs. state-of-the-art on test-dev. Mask R-CNN using ResNet-101FPN outperforms the base variants of all previous state-of-the-art models (the mask output is ignored in these experiments). The gains of Mask R-CNN over [27] come from using RoIAlign (+1.1 APbb), multitask training (+0.9 APbb), and ResNeXt-101 (+1.6 APbb).", "1a0912bb76777469295bb2c059faee907e7f3258/8-Figure6-1.png": "Figure 6. Keypoint detection results on COCO test using Mask R-CNN (ResNet-50-FPN), with person segmentation masks predicted from the same model. This model has a keypoint AP of 63.1 and runs at 5 fps.", "1a0912bb76777469295bb2c059faee907e7f3258/8-Table4-1.png": "Table 4. Keypoint detection AP on COCO test-dev. Ours (ResNet-50-FPN) is a single model that runs at 5 fps. CMUPose+++ [6] is the 2016 competition winner that uses multi-scale testing, post-processing with CPM [39], and filtering with an object detector, adding a cumulative \u223c5 points (clarified in personal communication). \u2020: G-RMI was trained on COCO plus MPII [1] (25k images), using two models (Inception-ResNet-v2 + ResNet101). As they use more data, this is not a direct comparison with Mask R-CNN.", "1a0912bb76777469295bb2c059faee907e7f3258/8-Table5-1.png": "Table 5. Multi-task learning of box, mask, and keypoint about the person category, evaluated on minival. All entries are trained on the same data for fair comparisons. The backbone is ResNet-50-FPN. The entry with 64.2 AP on minival has 62.7 AP on test-dev. The entry with 64.7 AP on minival has 63.1 AP on test-dev (see Table 4).", "1a0912bb76777469295bb2c059faee907e7f3258/8-Table6-1.png": "Table 6. RoIAlign vs. RoIPool for keypoint detection on minival.", "1a0912bb76777469295bb2c059faee907e7f3258/9-Figure7-1.png": "Figure 7. Mask R-CNN results on Cityscapes test (32.0 AP). The bottom-right image shows a failure prediction.", "1a0912bb76777469295bb2c059faee907e7f3258/9-Table7-1.png": "Table 7. Results on Cityscapes val (\u2018AP [val]\u2019 column) and test (remaining columns) sets. Our method uses ResNet-50-FPN."}, "referred_figures_tables": [["1a0912bb76777469295bb2c059faee907e7f3258/5-Table1-1.png"], ["1a0912bb76777469295bb2c059faee907e7f3258/6-Table2-1.png"], ["1a0912bb76777469295bb2c059faee907e7f3258/8-Figure6-1.png", "1a0912bb76777469295bb2c059faee907e7f3258/2-Figure2-1.png", "1a0912bb76777469295bb2c059faee907e7f3258/8-Figure6-1.png"]], "question_id": [9, 12, 19], "question": ["What metrics should be used for comparison of Mask R-CNN to the state of the art on the COCO dataset ?", "Why it is sufficient to predict a binary mask without concern for the categories once the instance has been classified as a whole ?", "How can we solve the chllenges of image segmentation ?"], "question_section": ["4. Experiments: Instance Segmentation", "4. Experiments: Instance Segmentation", "1. Introduction"], "question_trigger_sentence": ["We perform a thorough comparison of Mask R-CNN to the state of the art along with comprehensive ablations on the COCO dataset [28]. We report the standard COCO metrics including AP (averaged over IoU thresholds), AP50, AP75, and APS, APM, APL (AP at different scales). Unless noted, AP is evaluating using mask IoU.", "This alternative couples the tasks of mask and class prediction, and results in a severe loss in mask AP (5.5 points). This suggests that once the instance has been classified as a whole (by the box branch), it is sufficient to predict a binary mask without concern for the categories, which makes the model easier to train.", "Instance segmentation is challenging because it requires the correct detection of all objects in an image while also precisely segmenting each instance."], "question_type": ["Testing question", "Deep/complex question", "Deep question"], "evidential_info": [[{"context": "We perform a thorough comparison of Mask R-CNN to the state of the art along with comprehensive ablations on the COCO dataset [28]. We report the standard COCO metrics including AP (averaged over IoU thresholds), AP{}_{50}, AP{}_{75}, and AP{}_{S}, AP{}_{M}, AP{}_{L} (AP at different scales). Unless noted, AP is evaluating using mask IoU. As in previous work [5, 27], we train using the union of 80k train images and a 35k subset of val images (trainval35k), and report ablations on the remaining 5k val images (minival). We also report results on test-dev [28].", "rationale": "We report the standard COCO metrics including AP (averaged over IoU thresholds), AP{}_{50}, AP{}_{75}, and AP{}_{S}, AP{}_{M}, AP{}_{L} (AP at different scales). Unless noted, AP is evaluating using mask IoU."}, {"context": "We compare Mask R-CNN to the state-of-the-art methods in instance segmentation in Table 1. All instantiations of our model outperform baseline variants of previous state-of-the-art models. This includes MNC [10] and FCIS [26], the winners of the COCO 2015 and 2016 segmentation challenges, respectively. Without bells and whistles, Mask R-CNN with ResNet-101-FPN backbone outperforms FCIS+++ [26], which includes multi-scale train/test, horizontal flip test, and online hard example mining (OHEM) [38]. While outside the scope of this work, we expect many such improvements to be applicable to ours.", "rationale": "This includes MNC [10] and FCIS [26], the winners of the COCO 2015 and 2016 segmentation challenges, respectively. Without bells and whistles, Mask R-CNN with ResNet-101-FPN backbone outperforms FCIS+++ [26], which includes multi-scale train/test, horizontal flip test, and online hard example mining (OHEM) [38]. While outside the scope of this work, we expect many such improvements to be applicable to ours."}], [{"context": "Mask R-CNN decouples mask and class prediction: as the existing box branch predicts the class label, we generate a mask for each class without competition among classes (by a per-pixel sigmoid and a binary loss). In Table 2b, we compare this to using a per-pixel softmax and a multinomial loss (as commonly used in FCN [30]). This alternative couples the tasks of mask and class prediction, and results in a severe loss in mask AP (5.5 points). This suggests that once the instance has been classified as a whole (by the box branch), it is sufficient to predict a binary mask without concern for the categories, which makes the model easier to train.", "rationale": "Mask R-CNN decouples mask and class prediction: as the existing box branch predicts the class label, we generate a mask for each class without competition among classes (by a per-pixel sigmoid and a binary loss). In Table 2b, we compare this to using a per-pixel softmax and a multinomial loss (as commonly used in FCN [30]). This alternative couples the tasks of mask and class prediction, and results in a severe loss in mask AP (5.5 points). This suggests that once the instance has been classified as a whole (by the box branch), it is sufficient to predict a binary mask without concern for the categories, which makes the model easier to train."}], [{"context": "Instance segmentation is challenging because it requires the correct detection of all objects in an image while also precisely segmenting each instance. It therefore combines elements from the classical computer vision tasks of object detection, where the goal is to classify individual objects and localize each using a bounding box, and semantic segmentation, where the goal is to classify each pixel into a fixed set of categories without differentiating object instances.111Following common terminology, we use object detection to denote detection via bounding boxes, not masks, and semantic segmentation to denote per-pixel classification without differentiating instances. Yet we note that instance segmentation is both semantic and a form of detection.\u00a0Given this, one might expect a complex method is required to achieve good results. However, we show that a surprisingly simple, flexible, and fast system can surpass prior state-of-the-art instance segmentation results.", "rationale": "Following common terminology, we use object detection to denote detection via bounding boxes, not masks, and semantic segmentation to denote per-pixel classification without differentiating instances. Yet we note that instance segmentation is both semantic and a form of detection. Given this, one might expect a complex method is required to achieve good results. However, we show that a surprisingly simple, flexible, and fast system can surpass prior state-of-the-art instance segmentation results."}, {"context": "Most recently, Li et al. [26] combined the segment proposal system in [8] and object detection system in [11] for \u201cfully convolutional instance segmentation\u201d (FCIS). The common idea in [8, 11, 26] is to predict a set of position-sensitive output channels fully convolutionally. These channels simultaneously address object classes, boxes, and masks, making the system fast. But FCIS exhibits systematic errors on overlapping instances and creates spurious edges (Figure\u00a06), showing that it is challenged by the fundamental difficulties of segmenting instances.", "rationale": "These channels simultaneously address object classes, boxes, and masks, making the system fast. But FCIS exhibits systematic errors on overlapping instances and creates spurious edges (Figure 6), showing that it is challenged by the fundamental difficulties of segmenting instances."}, {"context": "Mask R-CNN outputs are visualized in Figures 2 and 5. Mask R-CNN achieves good results even under challenging conditions. In Figure 6 we compare our Mask R-CNN baseline and FCIS+++ [26]. FCIS+++ exhibits systematic artifacts on overlapping instances, suggesting that it is challenged by the fundamental difficulty of instance segmentation. Mask R-CNN shows no such artifacts.", "rationale": "it is challenged by the fundamental difficulty of instance segmentation. Mask R-CNN shows no such artifacts."}]], "composition": ["Metrics used for comparison are AP , multi-scale train/test, horizontal flip test, and online hard example mining (OHEM).", "it is sufficient to predict a binary mask without concern for the categories once the instance has been classified as a whole because Mask R-CNN decouples mask and class prediction: as the existing box branch predicts the class label,a mask is generated  for each class without competition among classes (by a per-pixel sigmoid and a binary loss)", "In this paper, It is shown that a surprisingly simple, flexible, and fast system can surpass prior state-of-the-art instance segmentation results. We use object detection to denote detection via bounding boxes, not masks, and semantic segmentation to denote per-pixel classification without differentiating instances. Given this, one might expect a complex method to be required to achieve good results."], "Is_figure_in_evidence": [false, false, true], "Is_table_in_evidence": [true, true, false], "question_key": ["738", "740", "747"], "passages": ["The vision community has rapidly improved object detection and semantic segmentation results over a short period of time. In large part, these advances have been driven by powerful baseline systems, such as the Fast/Faster R-CNN [12, 36] and Fully Convolutional Network (FCN) [30] frameworks for object detection and semantic segmentation, respectively. These methods are conceptually intuitive and offer flexibility and robustness, together with fast training and inference time. Our goal in this work is to develop a comparably enabling framework for instance segmentation.", "Instance segmentation is challenging because it requires the correct detection of all objects in an image while also precisely segmenting each instance. It therefore combines elements from the classical computer vision tasks of object detection, where the goal is to classify individual objects and localize each using a bounding box, and semantic segmentation, where the goal is to classify each pixel into a fixed set of categories without differentiating object instances.111Following common terminology, we use object detection to denote detection via bounding boxes, not masks, and semantic segmentation to denote per-pixel classification without differentiating instances. Yet we note that instance segmentation is both semantic and a form of detection.\u00a0Given this, one might expect a complex method is required to achieve good results. However, we show that a surprisingly simple, flexible, and fast system can surpass prior state-of-the-art instance segmentation results.", "Our method, called Mask R-CNN, extends Faster R-CNN [36] by adding a branch for predicting segmentation masks on each Region of Interest (RoI), in parallel with the existing branch for classification and bounding box regression (Figure\u00a01). The mask branch is a small FCN applied to each RoI, predicting a segmentation mask in a pixel-to-pixel manner. Mask R-CNN is simple to implement and train given the Faster R-CNN framework, which facilitates a wide range of flexible architecture designs. Additionally, the mask branch only adds a small computational overhead, enabling a fast system and rapid experimentation.", "In principle Mask R-CNN is an intuitive extension of Faster R-CNN, yet constructing the mask branch properly is critical for good results. Most importantly, Faster R-CNN was not designed for pixel-to-pixel alignment between network inputs and outputs. This is most evident in how RoIPool [18, 12], the de facto core operation for attending to instances, performs coarse spatial quantization for feature extraction. To fix the misalignment, we propose a simple, quantization-free layer, called RoIAlign, that faithfully preserves exact spatial locations. Despite being a seemingly minor change, RoIAlign has a large impact: it improves mask accuracy by relative 10% to 50%, showing bigger gains under stricter localization metrics. Second, we found it essential to decouple mask and class prediction: we predict a binary mask for each class independently, without competition among classes, and rely on the network\u2019s RoI classification branch to predict the category. In contrast, FCNs usually perform per-pixel multi-class categorization, which couples segmentation and classification, and based on our experiments works poorly for instance segmentation.", "Without bells and whistles, Mask R-CNN surpasses all previous state-of-the-art single-model results on the COCO instance segmentation task [28], including the heavily-engineered entries from the 2016 competition winner. As a by-product, our method also excels on the COCO object detection task. In ablation experiments, we evaluate multiple basic instantiations, which allows us to demonstrate its robustness and analyze the effects of core factors.", "Our models can run at about 200ms per frame on a GPU, and training on COCO takes one to two days on a single 8-GPU machine. We believe the fast train and test speeds, together with the framework\u2019s flexibility and accuracy, will benefit and ease future research on instance segmentation.", "Finally, we showcase the generality of our framework via the task of human pose estimation on the COCO keypoint dataset [28]. By viewing each keypoint as a one-hot binary mask, with minimal modification Mask R-CNN can be applied to detect instance-specific poses. Mask R-CNN surpasses the winner of the 2016 COCO keypoint competition, and at the same time runs at 5 fps. Mask R-CNN, therefore, can be seen more broadly as a flexible framework for instance-level recognition and can be readily extended to more complex tasks.", "We have released code to facilitate future research.", "The Region-based CNN (R-CNN) approach [13] to bounding-box object detection is to attend to a manageable number of candidate object regions [42, 20] and evaluate convolutional networks [25, 24] independently on each RoI. R-CNN was extended [18, 12] to allow attending to RoIs on feature maps using RoIPool, leading to fast speed and better accuracy. Faster R-CNN [36] advanced this stream by learning the attention mechanism with a Region Proposal Network (RPN). Faster R-CNN is flexible and robust to many follow-up improvements (e.g., [38, 27, 21]), and is the current leading framework in several benchmarks.", "Driven by the effectiveness of R-CNN, many approaches to instance segmentation are based on segment proposals. Earlier methods [13, 15, 16, 9] resorted to bottom-up segments [42, 2]. DeepMask [33] and following works [34, 8] learn to propose segment candidates, which are then classified by Fast R-CNN. In these methods, segmentation precedes recognition, which is slow and less accurate. Likewise, Dai et al. [10] proposed a complex multiple-stage cascade that predicts segment proposals from bounding-box proposals, followed by classification. Instead, our method is based on parallel prediction of masks and class labels, which is simpler and more flexible.", "Most recently, Li et al. [26] combined the segment proposal system in [8] and object detection system in [11] for \u201cfully convolutional instance segmentation\u201d (FCIS). The common idea in [8, 11, 26] is to predict a set of position-sensitive output channels fully convolutionally. These channels simultaneously address object classes, boxes, and masks, making the system fast. But FCIS exhibits systematic errors on overlapping instances and creates spurious edges (Figure\u00a06), showing that it is challenged by the fundamental difficulties of segmenting instances.", "Another family of solutions [23, 4, 3, 29] to instance segmentation are driven by the success of semantic segmentation. Starting from per-pixel classification results (e.g., FCN outputs), these methods attempt to cut the pixels of the same category into different instances. In contrast to the segmentation-first strategy of these methods, Mask R-CNN is based on an instance-first strategy. We expect a deeper incorporation of both strategies will be studied in the future.", "Mask R-CNN is conceptually simple: Faster R-CNN has two outputs for each candidate object, a class label and a bounding-box offset; to this we add a third branch that outputs the object mask. Mask R-CNN is thus a natural and intuitive idea. But the additional mask output is distinct from the class and box outputs, requiring extraction of much finer spatial layout of an object. Next, we introduce the key elements of Mask R-CNN, including pixel-to-pixel alignment, which is the main missing piece of Fast/Faster R-CNN.", "We begin by briefly reviewing the Faster R-CNN detector [36]. Faster R-CNN consists of two stages. The first stage, called a Region Proposal Network (RPN), proposes candidate object bounding boxes. The second stage, which is in essence Fast R-CNN [12], extracts features using RoIPool from each candidate box and performs classification and bounding-box regression. The features used by both stages can be shared for faster inference. We refer readers to [21] for latest, comprehensive comparisons between Faster R-CNN and other frameworks.", "Mask R-CNN adopts the same two-stage procedure, with an identical first stage (which is RPN). In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask for each RoI. This is in contrast to most recent systems, where classification depends on mask predictions (e.g. [33, 10, 26]). Our approach follows the spirit of Fast R-CNN [12] that applies bounding-box classification and regression in parallel (which turned out to largely simplify the multi-stage pipeline of original R-CNN [13]).", "Formally, during training, we define a multi-task loss on each sampled RoI as L=Lcls+Lbox+Lmask\ud835\udc3fsubscript\ud835\udc3f\ud835\udc50\ud835\udc59\ud835\udc60subscript\ud835\udc3f\ud835\udc4f\ud835\udc5c\ud835\udc65subscript\ud835\udc3f\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58L=L_{cls}+L_{box}+L_{mask}italic_L = italic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT + italic_L start_POSTSUBSCRIPT italic_b italic_o italic_x end_POSTSUBSCRIPT + italic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT. The classification loss Lclssubscript\ud835\udc3f\ud835\udc50\ud835\udc59\ud835\udc60L_{cls}italic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT and bounding-box loss Lboxsubscript\ud835\udc3f\ud835\udc4f\ud835\udc5c\ud835\udc65L_{box}italic_L start_POSTSUBSCRIPT italic_b italic_o italic_x end_POSTSUBSCRIPT are identical as those defined in [12]. The mask branch has a Km2\ud835\udc3esuperscript\ud835\udc5a2Km^{2}italic_K italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT-dimensional output for each RoI, which encodes K\ud835\udc3eKitalic_K binary masks of resolution m\u00d7m\ud835\udc5a\ud835\udc5am\\times mitalic_m \u00d7 italic_m, one for each of the K\ud835\udc3eKitalic_K classes. To this we apply a per-pixel sigmoid, and define Lmasksubscript\ud835\udc3f\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58L_{mask}italic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT as the average binary cross-entropy loss. For an RoI associated with ground-truth class k\ud835\udc58kitalic_k, Lmasksubscript\ud835\udc3f\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58L_{mask}italic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT is only defined on the k\ud835\udc58kitalic_k-th mask (other mask outputs do not contribute to the loss).", "Our definition of Lmasksubscript\ud835\udc3f\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58L_{mask}italic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT allows the network to generate masks for every class without competition among classes; we rely on the dedicated classification branch to predict the class label used to select the output mask. This decouples mask and class prediction. This is different from common practice when applying FCNs [30] to semantic segmentation, which typically uses a per-pixel softmax and a multinomial cross-entropy loss. In that case, masks across classes compete; in our case, with a per-pixel sigmoid and a binary loss, they do not. We show by experiments that this formulation is key for good instance segmentation results.", "A mask encodes an input object\u2019s spatial layout. Thus, unlike class labels or box offsets that are inevitably collapsed into short output vectors by fully-connected (fc) layers, extracting the spatial structure of masks can be addressed naturally by the pixel-to-pixel correspondence provided by convolutions.", "Specifically, we predict an m\u00d7m\ud835\udc5a\ud835\udc5am\\times mitalic_m \u00d7 italic_m mask from each RoI using an FCN [30]. This allows each layer in the mask branch to maintain the explicit m\u00d7m\ud835\udc5a\ud835\udc5am\\times mitalic_m \u00d7 italic_m object spatial layout without collapsing it into a vector representation that lacks spatial dimensions. Unlike previous methods that resort to fc layers for mask prediction [33, 34, 10], our fully convolutional representation requires fewer parameters, and is more accurate as demonstrated by experiments.", "This pixel-to-pixel behavior requires our RoI features, which themselves are small feature maps, to be well aligned to faithfully preserve the explicit per-pixel spatial correspondence. This motivated us to develop the following RoIAlign layer that plays a key role in mask prediction.", "RoIPool [12] is a standard operation for extracting a small feature map (e.g., 7\u00d7\\times\u00d77) from each RoI. RoIPool first quantizes a floating-number RoI to the discrete granularity of the feature map, this quantized RoI is then subdivided into spatial bins which are themselves quantized, and finally feature values covered by each bin are aggregated (usually by max pooling). Quantization is performed, e.g., on a continuous coordinate x\ud835\udc65xitalic_x by computing [x/16]delimited-[]\ud835\udc6516[x/16][ italic_x / 16 ], where 16 is a feature map stride and [\u22c5]delimited-[]\u22c5[\\cdot][ \u22c5 ] is rounding; likewise, quantization is performed when dividing into bins (e.g., 7\u00d7\\times\u00d77). These quantizations introduce misalignments between the RoI and the extracted features. While this may not impact classification, which is robust to small translations, it has a large negative effect on predicting pixel-accurate masks.", "To address this, we propose an RoIAlign layer that removes the harsh quantization of RoIPool, properly aligning the extracted features with the input. Our proposed change is simple: we avoid any quantization of the RoI boundaries or bins (i.e., we use x/16\ud835\udc6516x/16italic_x / 16 instead of [x/16]delimited-[]\ud835\udc6516[x/16][ italic_x / 16 ]). We use bilinear interpolation [22] to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and aggregate the result (using max or average), see Figure\u00a03 for details. We note that the results are not sensitive to the exact sampling locations, or how many points are sampled, as long as no quantization is performed.", "RoIAlign leads to large improvements as we show in \u00a74.2. We also compare to the RoIWarp operation proposed in [10].Unlike RoIAlign, RoIWarp overlooked the alignment issue and was implemented in [10] as quantizing RoI just like RoIPool. So even though RoIWarp also adopts bilinear resampling motivated by [22], it performs on par with RoIPool as shown by experiments (more details in Table\u00a02c), demonstrating the crucial role of alignment.", "To demonstrate the generality of our approach, we instantiate Mask R-CNN with multiple architectures. For clarity, we differentiate between: (i) the convolutional backbone architecture used for feature extraction over an entire image, and (ii) the network head for bounding-box recognition (classification and regression) and mask prediction that is applied separately to each RoI.", "We denote the backbone architecture using the nomenclature network-depth-features. We evaluate ResNet [19] and ResNeXt [45] networks of depth 50 or 101 layers. The original implementation of Faster R-CNN with ResNets [19] extracted features from the final convolutional layer of the 4-th stage, which we call C4. This backbone with ResNet-50, for example, is denoted by ResNet-50-C4. This is a common choice used in [19, 10, 21, 39].", "We also explore another more effective backbone recently proposed by Lin et al. [27], called a Feature Pyramid Network (FPN). FPN uses a top-down architecture with lateral connections to build an in-network feature pyramid from a single-scale input. Faster R-CNN with an FPN backbone extracts RoI features from different levels of the feature pyramid according to their scale, but otherwise the rest of the approach is similar to vanilla ResNet. Using a ResNet-FPN backbone for feature extraction with Mask R-CNN gives excellent gains in both accuracy and speed. For further details on FPN, we refer readers to [27].", "For the network head we closely follow architectures presented in previous work to which we add a fully convolutional mask prediction branch. Specifically, we extend the Faster R-CNN box heads from the ResNet [19] and FPN [27] papers. Details are shown in Figure\u00a04. The head on the ResNet-C4 backbone includes the 5-th stage of ResNet (namely, the 9-layer \u2018res5\u2019 [19]), which is compute-intensive. For FPN, the backbone already includes res5 and thus allows for a more efficient head that uses fewer filters.", "We note that our mask branches have a straightforward structure. More complex designs have the potential to improve performance but are not the focus of this work.", "We set hyper-parameters following existing Fast/Faster R-CNN work [12, 36, 27]. Although these decisions were made for object detection in original papers [12, 36, 27], we found our instance segmentation system is robust to them.", "As in Fast R-CNN, an RoI is considered positive if it has IoU with a ground-truth box of at least 0.5 and negative otherwise. The mask loss Lmasksubscript\ud835\udc3f\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58L_{mask}italic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT is defined only on positive RoIs. The mask target is the intersection between an RoI and its associated ground-truth mask.", "We adopt image-centric training [12]. Images are resized such that their scale (shorter edge) is 800 pixels [27]. Each mini-batch has 2 images per GPU and each image has N\ud835\udc41Nitalic_N sampled RoIs, with a ratio of 1:3 of positive to negatives [12]. N\ud835\udc41Nitalic_N is 64 for the C4 backbone (as in [12, 36]) and 512 for FPN (as in [27]). We train on 8 GPUs (so effective mini-batch size is 16) for 160k iterations, with a learning rate of 0.02 which is decreased by 10 at the 120k iteration. We use a weight decay of 0.0001 and momentum of 0.9. With ResNeXt [45], we train with 1 image per GPU and the same number of iterations, with a starting learning rate of 0.01.", "The RPN anchors span 5 scales and 3 aspect ratios, following [27]. For convenient ablation, RPN is trained separately and does not share features with Mask R-CNN, unless specified. For every entry in this paper, RPN and Mask R-CNN have the same backbones and so they are shareable.", "At test time, the proposal number is 300 for the C4 backbone (as in [36]) and 1000 for FPN (as in [27]). We run the box prediction branch on these proposals, followed by non-maximum suppression [14]. The mask branch is then applied to the highest scoring 100 detection boxes. Although this differs from the parallel computation used in training, it speeds up inference and improves accuracy (due to the use of fewer, more accurate RoIs). The mask branch can predict K\ud835\udc3eKitalic_K masks per RoI, but we only use the k\ud835\udc58kitalic_k-th mask, where k\ud835\udc58kitalic_k is the predicted class by the classification branch. The m\ud835\udc5amitalic_m\u00d7\\times\u00d7m\ud835\udc5amitalic_m floating-number mask output is then resized to the RoI size, and binarized at a threshold of 0.5.", "Note that since we only compute masks on the top 100 detection boxes, Mask R-CNN adds a small overhead to its Faster R-CNN counterpart (e.g., \u223csimilar-to\\scriptstyle\\sim\u223c20% on typical models).", "We perform a thorough comparison of Mask R-CNN to the state of the art along with comprehensive ablations on the COCO dataset [28]. We report the standard COCO metrics including AP (averaged over IoU thresholds), AP5050{}_{50}start_FLOATSUBSCRIPT 50 end_FLOATSUBSCRIPT, AP7575{}_{75}start_FLOATSUBSCRIPT 75 end_FLOATSUBSCRIPT, and APS\ud835\udc46{}_{S}start_FLOATSUBSCRIPT italic_S end_FLOATSUBSCRIPT, APM\ud835\udc40{}_{M}start_FLOATSUBSCRIPT italic_M end_FLOATSUBSCRIPT, APL\ud835\udc3f{}_{L}start_FLOATSUBSCRIPT italic_L end_FLOATSUBSCRIPT (AP at different scales). Unless noted, AP is evaluating using mask IoU. As in previous work [5, 27], we train using the union of 80k train images and a 35k subset of val images (trainval35k), and report ablations on the remaining 5k val images (minival). We also report results on test-dev [28].", "We compare Mask R-CNN to the state-of-the-art methods in instance segmentation in Table 1. All instantiations of our model outperform baseline variants of previous state-of-the-art models. This includes MNC [10] and FCIS [26], the winners of the COCO 2015 and 2016 segmentation challenges, respectively. Without bells and whistles, Mask R-CNN with ResNet-101-FPN backbone outperforms FCIS+++ [26], which includes multi-scale train/test, horizontal flip test, and online hard example mining (OHEM) [38]. While outside the scope of this work, we expect many such improvements to be applicable to ours.", "Mask R-CNN outputs are visualized in Figures 2 and 5. Mask R-CNN achieves good results even under challenging conditions. In Figure 6 we compare our Mask R-CNN baseline and FCIS+++ [26]. FCIS+++ exhibits systematic artifacts on overlapping instances, suggesting that it is challenged by the fundamental difficulty of instance segmentation. Mask R-CNN shows no such artifacts.", "We run a number of ablations to analyze Mask R-CNN. Results are shown in Table 2 and discussed in detail next.", "Table 2a shows Mask R-CNN with various backbones. It benefits from deeper networks (50 vs.\u00a0101) and advanced designs including FPN and ResNeXt. We note that not all frameworks automatically benefit from deeper or advanced networks (see benchmarking in [21]).", "Mask R-CNN decouples mask and class prediction: as the existing box branch predicts the class label, we generate a mask for each class without competition among classes (by a per-pixel sigmoid and a binary loss). In Table 2b, we compare this to using a per-pixel softmax and a multinomial loss (as commonly used in FCN [30]). This alternative couples the tasks of mask and class prediction, and results in a severe loss in mask AP (5.5 points). This suggests that once the instance has been classified as a whole (by the box branch), it is sufficient to predict a binary mask without concern for the categories, which makes the model easier to train.", "Our default instantiation predicts class-specific masks, i.e., one m\ud835\udc5amitalic_m\u00d7\\times\u00d7m\ud835\udc5amitalic_m mask per class. Interestingly, Mask R-CNN with class-agnostic masks (i.e., predicting a single m\ud835\udc5amitalic_m\u00d7\\times\u00d7m\ud835\udc5amitalic_m output regardless of class) is nearly as effective: it has 29.7 mask AP vs. 30.3 for the class-specific counterpart on ResNet-50-C4. This further highlights the division of labor in our approach which largely decouples classification and segmentation.", "An evaluation of our proposed RoIAlign layer is shown in Table\u00a02c. For this experiment we use the ResNet-50-C4 backbone, which has stride 16. RoIAlign improves AP by about 3 points over RoIPool, with much of the gain coming at high IoU (AP7575{}_{75}start_FLOATSUBSCRIPT 75 end_FLOATSUBSCRIPT). RoIAlign is insensitive to max/average pool; we use average in the rest of the paper.", "Additionally, we compare with RoIWarp proposed in MNC [10] that also adopt bilinear sampling. As discussed in \u00a73, RoIWarp still quantizes the RoI, losing alignment with the input. As can be seen in Table 2c, RoIWarp performs on par with RoIPool and much worse than RoIAlign. This highlights that proper alignment is key.", "We also evaluate RoIAlign with a ResNet-50-C5 backbone, which has an even larger stride of 32 pixels. We use the same head as in Figure 4 (right), as the res5 head is not applicable. Table 2d shows that RoIAlign improves mask AP by a massive 7.3 points, and mask AP7575{}_{75}start_FLOATSUBSCRIPT 75 end_FLOATSUBSCRIPT by 10.5 points (50% relative improvement). Moreover, we note that with RoIAlign, using stride-32 C5 features (30.9 AP) is more accurate than using stride-16 C4 features (30.3 AP, Table\u00a02c). RoIAlign largely resolves the long-standing challenge of using large-stride features for detection and segmentation.", "Finally, RoIAlign shows a gain of 1.5 mask AP and 0.5 box AP when used with FPN, which has finer multi-level strides. For keypoint detection that requires finer alignment, RoIAlign shows large gains even with FPN (Table\u00a06).", "Segmentation is a pixel-to-pixel task and we exploit the spatial layout of masks by using an FCN. In Table 2e, we compare multi-layer perceptrons (MLP) and FCNs, using a ResNet-50-FPN backbone. Using FCNs gives a 2.1 mask AP gain over MLPs. We note that we choose this backbone so that the conv layers of the FCN head are not pre-trained, for a fair comparison with MLP.", "We compare Mask R-CNN to the state-of-the-art COCO bounding-box object detection in Table\u00a03. For this result, even though the full Mask R-CNN model is trained, only the classification and box outputs are used at inference (the mask output is ignored). Mask R-CNN using ResNet-101-FPN outperforms the base variants of all previous state-of-the-art models, including the single-model variant of G-RMI [21], the winner of the COCO 2016 Detection Challenge. Using ResNeXt-101-FPN, Mask R-CNN further improves results, with a margin of 3.0 points box AP over the best previous single model entry from [39] (which used Inception-ResNet-v2-TDM).", "As a further comparison, we trained a version of Mask R-CNN but without the mask branch, denoted by \u201cFaster R-CNN, RoIAlign\u201d in Table 3. This model performs better than the model presented in [27] due to RoIAlign. On the other hand, it is 0.9 points box AP lower than Mask R-CNN. This gap of Mask R-CNN on box detection is therefore due solely to the benefits of multi-task training.", "Lastly, we note that Mask R-CNN attains a small gap between its mask and box AP: e.g., 2.7 points between 37.1 (mask, Table\u00a01) and 39.8 (box, Table\u00a03). This indicates that our approach largely closes the gap between object detection and the more challenging instance segmentation task.", "We train a ResNet-101-FPN model that shares features between the RPN and Mask R-CNN stages, following the 4-step training of Faster R-CNN [36]. This model runs at 195ms per image on an Nvidia Tesla M40 GPU (plus 15ms CPU time resizing the outputs to the original resolution), and achieves statistically the same mask AP as the unshared one. We also report that the ResNet-101-C4 variant takes \u223csimilar-to\\scriptstyle\\sim\u223c400ms as it has a heavier box head (Figure 4), so we do not recommend using the C4 variant in practice.", "Although Mask R-CNN is fast, we note that our design is not optimized for speed, and better speed/accuracy trade-offs could be achieved [21], e.g., by varying image sizes and proposal numbers, which is beyond the scope of this paper.", "Mask R-CNN is also fast to train. Training with ResNet-50-FPN on COCO trainval35k takes 32 hours in our synchronized 8-GPU implementation (0.72s per 16-image mini-batch), and 44 hours with ResNet-101-FPN. In fact, fast prototyping can be completed in less than one day when training on the train set. We hope such rapid training will remove a major hurdle in this area and encourage more people to perform research on this challenging topic.", "Our framework can easily be extended to human pose estimation. We model a keypoint\u2019s location as a one-hot mask, and adopt Mask R-CNN to predict K\ud835\udc3eKitalic_K masks, one for each of K\ud835\udc3eKitalic_K keypoint types (e.g., left shoulder, right elbow). This task helps demonstrate the flexibility of Mask R-CNN.", "We note that minimal domain knowledge for human pose is exploited by our system, as the experiments are mainly to demonstrate the generality of the Mask R-CNN framework. We expect that domain knowledge (e.g., modeling structures [6]) will be complementary to our simple approach.", "We make minor modifications to the segmentation system when adapting it for keypoints. For each of the K\ud835\udc3eKitalic_K keypoints of an instance, the training target is a one-hot m\u00d7m\ud835\udc5a\ud835\udc5am\\times mitalic_m \u00d7 italic_m binary mask where only a single pixel is labeled as foreground. During training, for each visible ground-truth keypoint, we minimize the cross-entropy loss over an m2superscript\ud835\udc5a2m^{2}italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT-way softmax output (which encourages a single point to be detected). We note that as in instance segmentation, the K\ud835\udc3eKitalic_K keypoints are still treated independently.", "We adopt the ResNet-FPN variant, and the keypoint head architecture is similar to that in Figure 4 (right). The keypoint head consists of a stack of eight 3\u00d7\\times\u00d73 512-d conv layers, followed by a deconv layer and 2\u00d7\\times\u00d7 bilinear upscaling, producing an output resolution of 56\u00d7\\times\u00d756. We found that a relatively high resolution output (compared to masks) is required for keypoint-level localization accuracy.", "Models are trained on all COCO trainval35k images that contain annotated keypoints. To reduce overfitting, as this training set is smaller, we train using image scales randomly sampled from [640, 800] pixels; inference is on a single scale of 800 pixels. We train for 90k iterations, starting from a learning rate of 0.02 and reducing it by 10 at 60k and 80k iterations. We use bounding-box NMS with a threshold of 0.5. Other details are identical as in \u00a73.1.", "We evaluate the person keypoint AP (APkpkp{}^{\\text{kp}}start_FLOATSUPERSCRIPT kp end_FLOATSUPERSCRIPT) and experiment with a ResNet-50-FPN backbone; more backbones will be studied in the appendix. Table\u00a04 shows that our result (62.7 APkpkp{}^{\\text{kp}}start_FLOATSUPERSCRIPT kp end_FLOATSUPERSCRIPT) is 0.9 points higher than the COCO 2016 keypoint detection winner [6] that uses a multi-stage processing pipeline (see caption of Table\u00a04). Our method is considerably simpler and faster.", "More importantly, we have a unified model that can simultaneously predict boxes, segments, and keypoints while running at 5 fps. Adding a segment branch (for the person category) improves the APkpkp{}^{\\text{kp}}start_FLOATSUPERSCRIPT kp end_FLOATSUPERSCRIPT to 63.1 (Table\u00a04) on test-dev. More ablations of multi-task learning on minival are in Table\u00a05. Adding the mask branch to the box-only (i.e., Faster R-CNN) or keypoint-only versions consistently improves these tasks. However, adding the keypoint branch reduces the box/mask AP slightly, suggesting that while keypoint detection benefits from multitask training, it does not in turn help the other tasks. Nevertheless, learning all three tasks jointly enables a unified system to efficiently predict all outputs simultaneously (Figure\u00a07).", "We also investigate the effect of RoIAlign on keypoint detection (Table\u00a06). Though this ResNet-50-FPN backbone has finer strides (e.g., 4 pixels on the finest level), RoIAlign still shows significant improvement over RoIPool and increases APkpkp{}^{\\text{kp}}start_FLOATSUPERSCRIPT kp end_FLOATSUPERSCRIPT by 4.4 points. This is because keypoint detections are more sensitive to localization accuracy. This again indicates that alignment is essential for pixel-level localization, including masks and keypoints.", "Given the effectiveness of Mask R-CNN for extracting object bounding boxes, masks, and keypoints, we expect it be an effective framework for other instance-level tasks."], "figure_types": {"1a0912bb76777469295bb2c059faee907e7f3258/1-Figure1-1.png": "schematic", "1a0912bb76777469295bb2c059faee907e7f3258/2-Figure2-1.png": "photograph(s)", "1a0912bb76777469295bb2c059faee907e7f3258/4-Figure3-1.png": "schematic", "1a0912bb76777469295bb2c059faee907e7f3258/5-Figure4-1.png": "photograph(s)", "1a0912bb76777469295bb2c059faee907e7f3258/5-Table1-1.png": "table", "1a0912bb76777469295bb2c059faee907e7f3258/6-Figure5-1.png": "photograph(s)", "1a0912bb76777469295bb2c059faee907e7f3258/6-Table2-1.png": "table", "1a0912bb76777469295bb2c059faee907e7f3258/7-Table3-1.png": "table", "1a0912bb76777469295bb2c059faee907e7f3258/8-Figure6-1.png": "photograph(s)", "1a0912bb76777469295bb2c059faee907e7f3258/8-Table4-1.png": "table", "1a0912bb76777469295bb2c059faee907e7f3258/8-Table5-1.png": "table", "1a0912bb76777469295bb2c059faee907e7f3258/8-Table6-1.png": "table", "1a0912bb76777469295bb2c059faee907e7f3258/9-Figure7-1.png": "photograph(s)", "1a0912bb76777469295bb2c059faee907e7f3258/9-Table7-1.png": "table"}}, "2212.10560": {"paper_id": "paper_3", "title": "Self-Instruct: Aligning Language Model with Self Generated Instructions", "arxiv_url": "https://arxiv.org/abs/2212.10560", "s2orc_url": "https://www.semanticscholar.org/paper/e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "all_figures_tables": {"e65b346d442e9962a4276dc1c1af2956d9d5f1eb/1-Figure1-1.png": "Figure 1: Selected tasks from the generated instruction data using vanilla GPT3. Some texts are reformatted for presentation. See Table 10 for more examples.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/14-Figure8-1.png": "Figure 8: Distribution of the ROUGE-L scores between seed instructions and their most similar instructions in SUPERNI (left) and the 252 user-oriented instructions (right).", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/15-Table4-1.png": "Table 4: Hyper-parameters for querying OpenAI API in different experiments.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/15-Table5-1.png": "Table 5: Prompt used for generating new instructions. 8 existing instructions are randomly sampled from the task pool for in-context demonstration. The model is allowed to generate instructions for new tasks, until it stops its generation, reaches its length limit or generates \u201cTask 16\u201d tokens.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/16-Table6-1.png": "Table 6: Prompt used for classifying whether a task instruction is a classification task or not.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/19-Figure9-1.png": "Figure 9: Human evaluation in done using a Google sheet, with predictions from different models present in random order and the model information being anonymized. Our expert evaluators are required to read the instruction and input, refer to the target, and then select the rating for the model\u2019s response from A/B/C/D, corresponding to the 4 levels described in \u00a74.4.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/2-Figure2-1.png": "Figure 2: A high-level overview of SELF-INSTRUCT. The process starts with a small seed set of tasks as the task pool. Random tasks are sampled from the task pool, and used to prompt an off-the-shelf LM to generate both new instructions and corresponding instances, followed by filtering low-quality or similar generations, and then added back to the initial repository of tasks. The resulting data can be used for the instruction tuning of the language model itself later to follow instructions better. Tasks shown in the figure are generated by GPT3.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/21-Table9-1.png": "Table 9: Examples in the user-oriented instructions dataset (\u00a74.4) and predictions from GPT3SELF-INST. The rightcolumn indicates one of the four quality ratings assigned to the model\u2019s response, with \u201cA\u201d indicating \u201cvalid and satisfying\u201d responses (highest) and \u201cD\u201d indicating \u201cirrelevant or invalid response\u201d (lowest).", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/22-Table10-1.png": "Table 10: Representative valid tasks generated by GPT3. As is discussed in \u00a73, these generated tasks cover a broad range of formats, text types, and underlying expertise, while being correct on more than half of all the generated tasks.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/23-Table11-1.png": "Table 11: Representative invalid tasks generated by GPT3. The problematic fields are indicated in the validity column. As discussed in \u00a73.3, although these tasks contain errors, they still provide many useful signals in supervising models to follow instructions.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/4-Table1-1.png": "Table 1: Statistics of the generated data by applying SELF-INSTRUCT to GPT3.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/5-Figure3-1.png": "Figure 3: The top 20 most common root verbs (inner circle) and their top 4 direct noun objects (outer circle) in the generated instructions. Despite their diversity, the instructions shown here only account for 14% of all the generated instructions because many instructions (e.g., \u201cClassify whether the user is satisfied with the service.\u201d) do not contain such a verb-noun structure.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/5-Figure4-1.png": "Figure 4: Distribution of the ROUGE-L scores between generated instructions and their most similar seed instructions.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/5-Table2-1.png": "Table 2: Data quality review for the instruction, input, and output of the generated data. See Table 10 and Table 11 for representative valid and invalid examples.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/6-Table3-1.png": "Table 3: Evaluation results on unseen tasks from SUPERNI (\u00a74.3). From the results, we see that 1\u20dd SELFINSTRUCT can boost GPT3 performance by a large margin (+33.1%) and 2\u20dd nearly matches the performance of InstructGPT001. Additionally, 3\u20dd it can further improvethe performance even when a large amount of labeled instruction data is present.", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/7-Figure6-1.png": "Figure 6: Performance of GPT3 model and its instruction-tuned variants, evaluated by human experts on our 252 user-oriented instructions (\u00a74.4). Human evaluators are instructed to rate the models\u2019 responses into four levels. The results indicate that GPT3SELF-INST outperforms all the other GPT3 variants trained on publicly available instructiondatasets. Additionally, GPT3SELF-INST scores nearly as good as InstructGPT001 (cf. footnote 1).", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/8-Figure7-1.png": "Figure 7: Human evaluation performance of GPT3SELF-INST models tuned with different sizes ofinstructions. \ud835\udc65-axis is in log scale. The smallest size is 175, where only the seed tasks are used for instruction tuning. We also evaluate whether improving the data quality will further improve the performance by distilling the outputs from InstructGPT003. We seeconsistent improvement from using larger data with better quality."}, "referred_figures_tables": [["e65b346d442e9962a4276dc1c1af2956d9d5f1eb/21-Table9-1.png"], ["e65b346d442e9962a4276dc1c1af2956d9d5f1eb/5-Table2-1.png", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/22-Table10-1.png", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/23-Table11-1.png"], ["e65b346d442e9962a4276dc1c1af2956d9d5f1eb/6-Table3-1.png"], ["e65b346d442e9962a4276dc1c1af2956d9d5f1eb/21-Table9-1.png"], ["e65b346d442e9962a4276dc1c1af2956d9d5f1eb/6-Table3-1.png"]], "question_id": [4, 7, 9, 15, 12], "question": ["Why is it crucial for the pipeline to identify whether the instruction represents a classification task? How are classification tasks particularly distinct or special?", "Beyond correctness, why did the authors not evaluate the actual quality, meaning, or usefulness of the generated instructions?", "Was the performance difference between Self-Instruct training and SuperNI training significant?", "How are the results of the input-first and output-first approach different?", "Why do the authors claim that human feedback may be less important when their experiments showed that InstructGPT, which had human-generated data, outperformed their model without human-generated data?"], "question_section": ["Automatic Instruction Data Generation", "4 SELF-INSTRUCT Data from GPT3", "5 Experimental Results", "3 Method", "6 Discussion and Limitation"], "question_trigger_sentence": ["Our pipeline for generating the instruction data consists of four steps: 1) instruction generation, 2) identifying whether the instruction represents a classification task or not, 3) instance generation with the input-first or the output-first approach, and 4) filtering low-quality data.", "We asked an expert annotator (co-author of this work) to label whether each instance is correct or not, in terms of the instruction, the instance input, and the instance output.", "Models trained on SUPERNI training set still achieve better performance on its evaluation set, which we attribute to the similar instruction style and formatting.", "Our pipeline for generating the instruction data consists of four steps: 1) instruction generation, 2) identifying whether the instruction represents a classification task or not, 3) instance generation with the input-first or the output-first approach, and 4) filtering low-quality data", "While the reality probably lies somewhere in between these two extremes, we conjecture that it is closer to H2, particularly for larger models."], "question_type": ["Deep/complex question", "Deep/complex question", "Testing question", "Testing question", "Deep/complex question"], "evidential_info": [[{"context": "Our pipeline for generating the instruction data consists of four steps: 1) instruction generation, 2) identifying whether the instruction represents a classification task or not, 3) instance generation with the input-first or the output-first approach, and 4) filtering low-quality data.", "rationale": "Explains that the authos have a different approach for classification and non-classification task. They also define what a classification task means - a task for which the outputs have only a small number of possible outputs. The authors identify whether a task is a classification one or not by prompting GPT3 in a custom format."}, {"context": "Because we need two different approaches for classification and non-classification tasks, we next identify whether the generated instruction represents a classification task or not.333More concretely, we regard tasks that have a limited and small output label space as classification tasks.We prompt vanilla GPT3 few-shot to determine this, using 12 classification instructions and 19 non-classification instructions from the seed tasks. The prompting template is shown in Table\u00a07.", "rationale": "While discussing the four broad phases of their proposed approach\u2019s pipeline, the authors list identifying whether the instruction represents a classification task as one step - an indication that this is, indeed, a crucial step."}, {"context": "Given the instructions and their task type, we generate instances for each instruction independently. This is challenging because it requires the model to understand what the target task is, based on the instruction, figure out what additional input fields are needed and generate them, and finally complete the task by producing the output. We found that pretrained language models can achieve this to a large extent when prompted with instruction-input-output in-context examples from other tasks.A natural way to do this is the Input-first Approach, where we can ask a language model to come up with the input fields first based on the instruction, and then produce the corresponding output. This generation order is similar to how models are used to respond to instruction and input, but here with in-context examples from other tasks. The prompting template is shown in Table\u00a08.", "rationale": "Explains one possible way, called the Input-First approach, to prompt language models to generate instructions. In this approach, you first prompt a language model to generate input fields based on an instruction, and then provide inputs to generate the corresponding output."}, {"context": "However, we found that this approach can generate inputs biased toward one label, especially for classification tasks (e.g., for grammar error detection, it usually generates grammatical input). Therefore, we additionally propose an Output-first Approach for classification tasks, where we first gener-ate the possible class labels, and then condition the input generation on each class label. The prompting template is shown in Table 9.4 We apply the output-first approach to the classification tasks identified in the former step, and the input-first approach to the remaining non-classification tasks.", "rationale": "This paragraph explains that the Input-First approach for generating examples does not work well for imbalanced classification tasks."}], [{"context": "So far, we have shown the quantity and diversity of the generated data, but its quality remains uncertain. To investigate this, we randomly sample 200 instructions and randomly select 1 instance per instruction. We asked an expert annotator (co-author of this work) to label whether each instance is correct or not, in terms of the instruction, the instance input, and the instance output.Evaluation results in Table\u00a02 show that most of the generated instructions are meaningful, while the generated instances may contain more noise (to a reasonable extent). However, we found that even though the generations may contain errors, most of them are still in the correct format or even partially correct, which can provide useful guidance for training models to follow instructions. We listed a number of good generations and bad generations in Table\u00a010 and Table\u00a011 respectively.", "rationale": "Explains how the authors evaluated the quality of the generated sentences through human evaluation. They randomly sampled 200 generated instructions out of the entire set, and for each of these selected a random instance. Then, they asked an expert to determine the quality of the the randomly sampled instance."}, {"context": "Evaluating models\u2019 performance on this evaluation set of diverse tasks is extremely challenging because different tasks require different expertise. Indeed, many of these tasks cannot be measured by automatic metrics or even be judged by normal crowdworkers (e.g., writing a program, or converting first-order logic into natural language). To get a more faithful evaluation, we asked the authors of the instructions to judge model predictions. The evaluators were asked to rate the output based on whether it accurately and effectively completes the task. We implemented a four-level rating system for categorizing the quality of the models\u2019 outputs, defined as follows:", "rationale": "Explains the challenges involved with using humans to evaluate their model outputs, and why they need expert annotators for some tasks (eg. solving coding questions) that laymen may be unable to verify. They define a rating scale for this human evaluation phase of their model, with four grades, A to D, with A being the best. The human evaluators were asked to determine if the output was acuurately and completely covers a task, and rate them accordingly."}, {"context": "Figure\u00a05 provides the performance of GPT3 model and its instruction-tuned counterparts on this newly written instruction set. As anticipated, the vanilla GPT3 language model is largely unable to respond to instructions, and all instruction-tuned models demonstrate comparatively higher performance,Nonetheless, GPT3{}_{\\textsc{Self-Inst}} (i.e., GPT3 model fine-tuned with Self-Instruct) outperforms those counterparts trained on T0 or SuperNI by a large margin, demonstrating the value of the generated data despite the noise.Compared with \\text{InstructGPT}_{\\text{001}} (c.f. footnote\u00a01), GPT3{}_{\\textsc{Self-Inst}} is quite close in the performance\u2014if we count acceptable response with minor imperfections (Rating-3) as valid, GPT3{}_{\\textsc{Self-Inst}} is only 5% behind \\text{InstructGPT}_{\\text{001}}. Lastly, our evaluation confirms the impressive instruction-following ability of \\text{InstructGPT}_{\\text{002}} & \\text{InstructGPT}_{\\text{003}} models. Although there are many factors behind this success, we conjecture that future work can largely benefit from improving the quality of our generated data by using human annotators or training a reward model to select better generations, similar to the algorithm used in Ouyang et\u00a0al. (2022).", "rationale": "Presents the results of the human evaluation of Self Instruct (the proposed model) and compares it with baselines. They find that vanilla GPT3 model is largely unable to respond to specialized instructions as requested, while their proposed approach is able to effectively do so."}], [{"context": "To evaluate Self-Instruct empirically, we run this framework on GPT3\u00a0Brown et\u00a0al. (2020), which is a vanilla LM (\u00a74).The iterative Self-Instruct process on this model leads to about 52k instructions, paired with about 82K instance inputs and target outputs.We observe that the resulting data provides a diverse range of creative tasks and over 50% of them have less than 0.3 ROUGE-L overlaps with the seed instructions (\u00a74.2).On this resulting data, we build GPT3{}_{\\textsc{Self-Inst}} by fine-tuning GPT3 (i.e., the same model used for generating the instructional data).We evaluate GPT3{}_{\\textsc{Self-Inst}} in comparison to various other models on both typical NLP tasks included in Super-NaturalInstructions\u00a0Wang et\u00a0al. (2022), and a set of new instructions that are created for novel usage of instruction-following models (\u00a75).The SuperNI results indicate thatGPT3{}_{\\textsc{Self-Inst}} outperforms GPT3 (the original model) by a large margin (+33.1%) and nearly matches the performance of \\text{InstructGPT}_{\\text{001}}. Moreover, our human evaluation on the newly-created instruction set shows that GPT3{}_{\\textsc{Self-Inst}} demonstrates a broad range of instruction following ability, outperforming models trained on other publicly available instruction datasets and leaving only a 5% gap behind \\text{InstructGPT}_{\\text{001}}.", "rationale": "Explains that Self-Instruct training outperforms SuperNI, but does not explain the magnitude of the outperformance."}, {"context": "Additionally, to compare Self-Instruct training with other publicly available instruction tuning data, we further finetune GPT3 model with data from PromptSource and SuperNI, which are used to train the T0 and Tk-Instruct models. We call them T0 training and SuperNI training for short, respectively.To save the training budget, we sampled 50K instances (but covering all their instructions) for each dataset, which has a comparable size to the instruction data we generated. Based on the findings from Wang et\u00a0al. (2022) and our early experiments, reducing the number of instances per task does not degrade the model\u2019s generalization performance to unseen tasks.", "rationale": "Explains how they compared the proposed model (Self-Instruct) with existing baselines of T0 and Tk-Instruct. They finetune GPT3 using 50,000 random samples from the dataset in Tk-Instruct, and they call this baseline \"SuperNI training\". They mention that the 50,000 samples are randomly generated by first sampling 50,000 random instructions and then sampling one instance for each sample. They justify this random sampling strategy to be fair by citing Wang et. al. (2022) and their early experiments that indicate that increasing the number of instances per instruction does not lead to significantly better performance."}, {"context": "We make the following observations from the results in Table\u00a03.Self-Instructboosts the instruction-following ability of GPT3 by a large margin. The vanilla GPT3 model basically cannot follow human instructions at all. Upon manual analysis, we find that it usually generates irrelevant and repetitive text, and does not know when to stop generation.Compared with other models that are not specifically trained for SuperNI, GPT3{}_{\\textsc{Self-Inst}} achieves better performance than T0 or the GPT3 finetuned on the T0 training set, which takes tremendous human labeling efforts. Notably, GPT3{}_{\\textsc{Self-Inst}} also nearly matches the performance of \\text{InstructGPT}_{\\text{001}}, which is trained with private user data and human-annotated labels.", "rationale": "This paragraph explains that vanilla GPT3 models are terrible at following human instruction and that the proposed model (Self-Instruct) vastly outperforms vanilla approaches. Additionally, they refer to Table 3 in this paragraph, which summarizes the performance of their model and the baselines. From that Table, we see that the proposed approach has a ROGUE-L score of 51.6 while SuperNI training has a score of 49.5."}, {"context": "Models trained on SuperNI training set still achieve better performance on its evaluation set, which we attribute to the similar instruction style and formatting. However, we show that Self-Instruct still brings in additional gains when combined with the SuperNI training set, proving its value as complementary data.", "rationale": "This explains the method they use to perform experiments (essentially involves generating 52,000 instructions and 82,000 instances) and the evaluation metric they used (ROGUE-L). They find that their model (Self-Instruct) outperforms GPT3 by 33.1% and leaves only a 5% gap between their performance and InstructGPT's performance."}, {"context": "Figure\u00a05 provides the performance of GPT3 model and its instruction-tuned counterparts on this newly written instruction set. As anticipated, the vanilla GPT3 language model is largely unable to respond to instructions, and all instruction-tuned models demonstrate comparatively higher performance,Nonetheless, GPT3{}_{\\textsc{Self-Inst}} (i.e., GPT3 model fine-tuned with Self-Instruct) outperforms those counterparts trained on T0 or SuperNI by a large margin, demonstrating the value of the generated data despite the noise.Compared with \\text{InstructGPT}_{\\text{001}} (c.f. footnote\u00a01), GPT3{}_{\\textsc{Self-Inst}} is quite close in the performance\u2014if we count acceptable response with minor imperfections (Rating-3) as valid, GPT3{}_{\\textsc{Self-Inst}} is only 5% behind \\text{InstructGPT}_{\\text{001}}. Lastly, our evaluation confirms the impressive instruction-following ability of \\text{InstructGPT}_{\\text{002}} & \\text{InstructGPT}_{\\text{003}} models. Although there are many factors behind this success, we conjecture that future work can largely benefit from improving the quality of our generated data by using human annotators or training a reward model to select better generations, similar to the algorithm used in Ouyang et\u00a0al. (2022).", "rationale": "Authors explain that InstructGPT, which is one of the best existing models in 2022 for tasks such as these, was trained with a large private data-corpus, combined with expensive human annotation at scale. Their approach, which requires an order of magnitude lesser human involvement lags behind this state-of-the-art baseline by only 5%"}, {"context": "We introduce Self-Instruct, a task-agnostic method to improve the instruction-following capabilities of language models via its own generation of instruction data (instruction, input, and output samples) and bootstrapping with it. Our method conducts instruction-tuning of the original model on the pruned subset of generated samples. On experimenting with vanilla GPT3, we observe a 33% absolute improvement over the original model on Super-NaturalInstructions. This performance is on par with \\text{InstructGPT}_{\\text{001}}, which is trained with private user data and expensive human annotations. Furthermore, we curate a set of expert-written instructions for novel tasks. Human evaluation on this set shows that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind \\text{InstructGPT}_{\\text{001}}. We hope Self-Instruct can serve as the first step to align pretrained language models to follow human instructions, and future work can build on top of this data to improve instruction-following models.", "rationale": "This paragraph explains that self-instruct (the proposed model) outperforms models trained on T0 or SuperNI by a large margin, in their human evaluation process."}], [{"context": "Because we need two different approaches for classification and non-classification tasks, we next identify whether the generated instruction represents a classification task or not.333More concretely, we regard tasks that have a limited and small output label space as classification tasks.We prompt vanilla GPT3 few-shot to determine this, using 12 classification instructions and 19 non-classification instructions from the seed tasks. The prompting template is shown in Table\u00a07.", "rationale": "Explains how the \"output-first\" algorithm works, and that they use this approach for generating inputs when encountering classification problems that have imbalanced classes (i.e. one large class and other small classes). In this approach, the output labels are generated first and the language model is then asked to generate an input instance (sample) for each of these class labels."}, {"context": "Given the instructions and their task type, we generate instances for each instruction independently. This is challenging because it requires the model to understand what the target task is, based on the instruction, figure out what additional input fields are needed and generate them, and finally complete the task by producing the output. We found that pretrained language models can achieve this to a large extent when prompted with instruction-input-output in-context examples from other tasks.A natural way to do this is the Input-first Approach, where we can ask a language model to come up with the input fields first based on the instruction, and then produce the corresponding output. This generation order is similar to how models are used to respond to instruction and input, but here with in-context examples from other tasks. The prompting template is shown in Table\u00a08.", "rationale": "Explains how the \"input-first\" approach works by prompting a LM to generate the input fields needed, and then using that information to prompt and get an output. They use this method for all tasks that are not classification based."}, {"context": "However, we found that this approach can generate inputs biased toward one label, especially for classification tasks (e.g., for grammar error detection, it usually generates grammatical input). Therefore, we additionally propose anOutput-first Approach for classification tasks, where we first generate the possible class labels, and then condition the input generation on each class label. The prompting template is shown in Table\u00a09.444In this work, we use a fixed set of seed tasks for prompting the instance generation, and thus only generate a small number of instances per task in one round. Future work can use randomly sampled tasks to prompt the model to generate a larger number of instances in multiple rounds. We apply the output-first approach to the classification tasks identified in the former step, and the input-first approach to the remaining non-classification tasks.", "rationale": "They define what a classification task means (any task that has a small, finite number of possible outputs) and explain how they identify classification vs. non-classification items."}], [{"context": "To evaluate Self-Instruct empirically, we run this framework on GPT3\u00a0Brown et\u00a0al. (2020), which is a vanilla LM (\u00a74).The iterative Self-Instruct process on this model leads to about 52k instructions, paired with about 82K instance inputs and target outputs.We observe that the resulting data provides a diverse range of creative tasks and over 50% of them have less than 0.3 ROUGE-L overlaps with the seed instructions (\u00a74.2).On this resulting data, we build GPT3{}_{\\textsc{Self-Inst}} by fine-tuning GPT3 (i.e., the same model used for generating the instructional data).We evaluate GPT3{}_{\\textsc{Self-Inst}} in comparison to various other models on both typical NLP tasks included in Super-NaturalInstructions\u00a0Wang et\u00a0al. (2022), and a set of new instructions that are created for novel usage of instruction-following models (\u00a75).The SuperNI results indicate thatGPT3{}_{\\textsc{Self-Inst}} outperforms GPT3 (the original model) by a large margin (+33.1%) and nearly matches the performance of \\text{InstructGPT}_{\\text{001}}. Moreover, our human evaluation on the newly-created instruction set shows that GPT3{}_{\\textsc{Self-Inst}} demonstrates a broad range of instruction following ability, outperforming models trained on other publicly available instruction datasets and leaving only a 5% gap behind \\text{InstructGPT}_{\\text{001}}.", "rationale": "The authors mention that self instruct nearly matches the performance of InstructGPT, despite not hacing access to huamn created labels or large private datasets."}, {"context": "We evaluate \\text{InstructGPT}_{\\text{}}\u00a0Ouyang et\u00a0al. (2022),which is developed by OpenAI based on GPT3 to follow human instructions better and has been found by the community to have impressive zero-shot abilities.There are various generations of these models,where newer ones use more expansive data or algorithmic novelties101010https://beta.openai.com/docs/model-index-for-researchers.For our SuperNI experiments in \u00a75.3, we only compare with their text-davinci-001 engine, because their newer engines are trained with the latest user data and are likely to already see the SuperNI evaluation set. For our human evaluation of these models on newly written instructions, we include their 001, 002 and 003 engines for completeness.", "rationale": "Explains how they evaluated the performance of InstructGPT. They used the OpenAI API to generate samples, and claim that InstructGPT was found to have had \"impressive\" abilities at doing tasks zero-shot (i.e. with no examples)."}, {"context": "We make the following observations from the results in Table\u00a03.Self-Instructboosts the instruction-following ability of GPT3 by a large margin. The vanilla GPT3 model basically cannot follow human instructions at all. Upon manual analysis, we find that it usually generates irrelevant and repetitive text, and does not know when to stop generation.Compared with other models that are not specifically trained for SuperNI, GPT3{}_{\\textsc{Self-Inst}} achieves better performance than T0 or the GPT3 finetuned on the T0 training set, which takes tremendous human labeling efforts. Notably, GPT3{}_{\\textsc{Self-Inst}} also nearly matches the performance of \\text{InstructGPT}_{\\text{001}}, which is trained with private user data and human-annotated labels.", "rationale": "Explains how their approach, Self-Instruct, is only 5% behind InstructGPT."}, {"context": "Figure\u00a05 provides the performance of GPT3 model and its instruction-tuned counterparts on this newly written instruction set. As anticipated, the vanilla GPT3 language model is largely unable to respond to instructions, and all instruction-tuned models demonstrate comparatively higher performance,Nonetheless, GPT3{}_{\\textsc{Self-Inst}} (i.e., GPT3 model fine-tuned with Self-Instruct) outperforms those counterparts trained on T0 or SuperNI by a large margin, demonstrating the value of the generated data despite the noise.Compared with \\text{InstructGPT}_{\\text{001}} (c.f. footnote\u00a01), GPT3{}_{\\textsc{Self-Inst}} is quite close in the performance\u2014if we count acceptable response with minor imperfections (Rating-3) as valid, GPT3{}_{\\textsc{Self-Inst}} is only 5% behind \\text{InstructGPT}_{\\text{001}}. Lastly, our evaluation confirms the impressive instruction-following ability of \\text{InstructGPT}_{\\text{002}} & \\text{InstructGPT}_{\\text{003}} models. Although there are many factors behind this success, we conjecture that future work can largely benefit from improving the quality of our generated data by using human annotators or training a reward model to select better generations, similar to the algorithm used in Ouyang et\u00a0al. (2022).", "rationale": "Explains that vanilla GPT3 is largely unsuitable at tasks that require following instructions, that Self-Instruct, Instruct GPT (001, 002 and 003) are vastly better, and that further improvement in their proposed model output is possible by augmenting it with human annotators."}, {"context": "A series of works have found evidence that vanilla language models can be effective at following general language instructions if tuned with annotated \u201cinstructional\u201d data \u2013 datasets containing language instructional commands and their desired outcome based on human judgement\u00a0Weller et\u00a0al. (2020); Mishra et\u00a0al. (2022); Wang et\u00a0al. (2022); Wei et\u00a0al. (2022); Sanh et\u00a0al. (2022); Ouyang et\u00a0al. (2022); Parmar et\u00a0al. (2022); Scialom et\u00a0al. (2022); Chung et\u00a0al. (2022); Luo et\u00a0al. (2022); Puri et\u00a0al. (2022); Yin et\u00a0al. (2022); Chakrabarty et\u00a0al. (2022); Lin et\u00a0al. (2022); Gupta et\u00a0al. (2022); Muennighoff et\u00a0al. (2022).Additionally, they show a direct correlation between the size and diversity of the \u201cinstructional\u201d data and the generalizability of resulting models to unseen tasks.Since these developments depend on human annotated \u201cinstructional\u201d data, this poses a bottleneck for progress toward more generalizable models(for example see Fig.\u00a05a in Wang et\u00a0al., 2022).Our work aims to tackle this bottleneck by reducing the dependence on human annotators.", "rationale": "The authors attempt to explain the range of opinions that are possible on the issue of whether human feedback is an essential ingredient for language models (LMs). One extreme is that huamn feedback is unavoidable and is the only way LMs can learn about new topics that did not come up during their pretraining phase. The alternate hypothesis is that human feedback, while helpful, is merely optional since language models are familiar with how instructions work thanks to the data in their pretraining phase. The authors posit that their experiments with Self-Instruct might serve as evidence to show that the latter is probably the case (i.e. human feedback is not essential)."}, {"context": "(H_{1})Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs need to learn about issues that were not quite learned during pre-training.(H_{2})Human feedback is an optional aspect of instruction-tuning as LMs are already quite familiar with instructions from their pre-training. Observing the human feedback is merely a lightweight process for aligning their pre-training distribution/objective which might be replaceable with a different process.While the reality probably lies somewhere in between these two extremes, we conjecturethat it is closer to H_{2}, particularly for larger models.This intuition, that LMs already know much about language instructions, is a key motivation for Self-Instruct and is also supported by its empirical success.", "rationale": "Reaffirms the 5% gap between their model and InstructGPT3."}, {"context": "We introduce Self-Instruct, a task-agnostic method to improve the instruction-following capabilities of language models via its own generation of instruction data (instruction, input, and output samples) and bootstrapping with it. Our method conducts instruction-tuning of the original model on the pruned subset of generated samples. On experimenting with vanilla GPT3, we observe a 33% absolute improvement over the original model on Super-NaturalInstructions. This performance is on par with \\text{InstructGPT}_{\\text{001}}, which is trained with private user data and expensive human annotations. Furthermore, we curate a set of expert-written instructions for novel tasks. Human evaluation on this set shows that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind \\text{InstructGPT}_{\\text{001}}. We hope Self-Instruct can serve as the first step to align pretrained language models to follow human instructions, and future work can build on top of this data to improve instruction-following models.", "rationale": "Explains how InstructGPT's (Ouyang et al. 2022) structure and methods are opaque and not publicly known."}, {"context": "Additionally, despite the remarkable performance of models like \\text{InstructGPT}_{\\text{}}\u00a0Ouyang et\u00a0al. (2022), their construction process remains quite opaque.In particular, the role of data has remained understudied due to limited transparency and data released by major corporate entities behind these key models.Addressing such challenges necessitates the creation of a large-scale, public dataset covering a broad range of tasks.", "rationale": "Lists out some literature in the field that successfully used manually-annotated data to improve performance on instruction/task based benchmarks."}]], "composition": ["The main reason why this is a crucial step is because the authors\u2019 pipeline uses a different approach for classification tasks. For non-classification tasks, the authors first prompt a language model to come up with the input fields require, then provide sample inputs, for which the language model generates outputs. However, for classification tasks, the authors first generate the list of classes, and then require the model to provide an example for that instruction for each class. They do this because the first approach, used for non-classification instructions, does not work well for unbalanced classes. This step, of identifying classification tasks, is important since it is not possible to use the same generation technique for both classification and non-classification tasks effectively with the same generation method.", "One reason to explain why the authors did not perform more comprehensive quality evaluation of the generated outputs is the difficulty in judging the output of the model. Some tasks cannot be quickly verified by the average human (one example the authors provide for this is converting first-order logic into natural language - a task that only experts with the appropriate domain knowledge can perform). However, despite this challenge, the authors do perform some analysis to gauge the overall quality of the generated samples. They randomly select 200 instructions, and for each sample they examine the quality of one instance within each instruction. Doing this evaluation reveals that their approach performs much better than a vanilla GPT3 (i.e. a bare-bones GPT3 with nothing else).", "While it does appear as though there is a measurable performance improvement from SuperNI to Self-Instruct, quantifying the impact and magnitude of that improvement is not straightforward. Evaluations with ROGUE-L scores find that the absolute difference between both these methods is not very high, though additional information and context may be needed to judge the meaning of the absolute difference between these numbers. The authors do claim that they outperform T0 or SuperNI by a large margin, which is strong evidence to suggest that the difference was indeed significant, but such claims must be taken with some grains of salt since authors are usually incentivized to show their models are the best.", "If by results, you are referring to the outputs of these approaches, then the final output will look very similar - the output for each instance will consist of a tuple of an (input, output) where input and output follow the instructions for a certain task. However, the order in which this output is generated will differ -- for example, in \"input first\" approach, the input is generated first, while in the output first case, the language model is conditioned to provide the required output. On the other hand, if, by results, you are referring to \"performance\" of both of these approaches, the authors mention that the input first approach performs very poorly on classification instances, which is why they proposed the alternative approach of output-first generation for classification tasks.", "The authors claim that human feedback might not be essential since their model is able to almost meet the performance of InstructGPT despite not having access to private human-generated training data or manual annotations. They claim that their model's success, of almost reaching InstructGPT performance with only a 5% gap is a strong indication that human data, while useful is not necessarily essential for teaching models how to follow instructions. Additionally, they point out that their work is merely a beginning step in research in this field - while numerous studies have successfully used human annotations to improve performance, studies that attempt to remove the human requirement have not been as explored. Also, the authors do acknowledge that the truth is somewhere in between the two extremes of (1) human instructional data is essential, or (2) such data is largely optional, and similar results can be achieved without it."], "Is_figure_in_evidence": [true, false, false, true, false], "Is_table_in_evidence": [true, true, true, false, true], "question_key": ["749", "754", "756", "759", "765"], "passages": ["The recent NLP literature has witnessed a tremendous amount of activity in building models that can follow natural language instructions\u00a0(Mishra et\u00a0al., 2022; Wei et\u00a0al., 2022; Sanh et\u00a0al., 2022; Wang et\u00a0al., 2022; Ouyang et\u00a0al., 2022; Chung et\u00a0al., 2022, i.a.).These developments are powered by two key components: large pre-trained language models (LM) and human-written instruction data.PromptSource\u00a0(Bach et\u00a0al., 2022) and Super-NaturalInstructions\u00a0(Wang et\u00a0al., 2022) are two notable recent datasets that use extensive manual annotation for collecting instructions to construct T00\u00a0Bach et\u00a0al. (2022); Sanh et\u00a0al. (2022) and Tk\ud835\udc58kitalic_k-Instruct\u00a0Wang et\u00a0al. (2022).However, this process is costly and often suffers limited diversity given that most human generations tend to be popular NLP tasks, falling short of covering a true variety of tasks and different ways to describe them.Given these limitations, continuing to improve the quality of instruction-tuned modelsnecessitates the development of alternative approaches for supervising instruction-tuned models.", "In this work, we introduce Self-Instruct, a semi-automated process for instruction-tuning a pretrained LM using instructional signals from the model itself.The overall process is an iterative bootstrapping algorithm (see Figure\u00a01), which starts off with a limited (e.g., 175 in our study) seed set of manually-written instructions that are used to guide the overall generation.In the first phase, the model is prompted to generate instructions for new tasks.This step leverages the existing collection of instructions to createmore broad-coverage instructions that define (often new) tasks.Given the newly-generated set of instructions, the framework also creates input-output instances for them, which can be later used for supervising the instruction tuning.Finally, various measures are used to prune low-quality and repeated instructions, before adding them to the task pool.This process can be repeated for many interactions until reaching a large number of tasks.", "To evaluate Self-Instruct empirically, we run this framework on GPT3\u00a0Brown et\u00a0al. (2020), which is a vanilla LM (\u00a74).The iterative Self-Instruct process on this model leads to about 52k instructions, paired with about 82K instance inputs and target outputs.We observe that the resulting data provides a diverse range of creative tasks and over 50% of them have less than 0.3 ROUGE-L overlaps with the seed instructions (\u00a74.2).On this resulting data, we build GPT3Self-InstSelf-Inst{}_{\\textsc{Self-Inst}}start_FLOATSUBSCRIPT Self-Inst end_FLOATSUBSCRIPT by fine-tuning GPT3 (i.e., the same model used for generating the instructional data).We evaluate GPT3Self-InstSelf-Inst{}_{\\textsc{Self-Inst}}start_FLOATSUBSCRIPT Self-Inst end_FLOATSUBSCRIPT in comparison to various other models on both typical NLP tasks included in Super-NaturalInstructions\u00a0Wang et\u00a0al. (2022), and a set of new instructions that are created for novel usage of instruction-following models (\u00a75).The SuperNI results indicate thatGPT3Self-InstSelf-Inst{}_{\\textsc{Self-Inst}}start_FLOATSUBSCRIPT Self-Inst end_FLOATSUBSCRIPT outperforms GPT3 (the original model) by a large margin (+33.1%) and nearly matches the performance of InstructGPT001subscriptInstructGPT001\\text{InstructGPT}_{\\text{001}}InstructGPT start_POSTSUBSCRIPT 001 end_POSTSUBSCRIPT. Moreover, our human evaluation on the newly-created instruction set shows that GPT3Self-InstSelf-Inst{}_{\\textsc{Self-Inst}}start_FLOATSUBSCRIPT Self-Inst end_FLOATSUBSCRIPT demonstrates a broad range of instruction following ability, outperforming models trained on other publicly available instruction datasets and leaving only a 5% gap behind InstructGPT001subscriptInstructGPT001\\text{InstructGPT}_{\\text{001}}InstructGPT start_POSTSUBSCRIPT 001 end_POSTSUBSCRIPT.", "In summary, our contributions are: (1) Self-Instruct, a method for inducing instruction-following capabilitywith minimal human-labeled data;(2) We demonstrate its effectiveness via extensive instruction-tuning experiments;(3) We release a large synthetic dataset of 52K instructions and a set of manually-written novel tasks for building and evaluating future instruction-following models.", "A series of works have found evidence that vanilla language models can be effective at following general language instructions if tuned with annotated \u201cinstructional\u201d data \u2013 datasets containing language instructional commands and their desired outcome based on human judgement\u00a0Weller et\u00a0al. (2020); Mishra et\u00a0al. (2022); Wang et\u00a0al. (2022); Wei et\u00a0al. (2022); Sanh et\u00a0al. (2022); Ouyang et\u00a0al. (2022); Parmar et\u00a0al. (2022); Scialom et\u00a0al. (2022); Chung et\u00a0al. (2022); Luo et\u00a0al. (2022); Puri et\u00a0al. (2022); Yin et\u00a0al. (2022); Chakrabarty et\u00a0al. (2022); Lin et\u00a0al. (2022); Gupta et\u00a0al. (2022); Muennighoff et\u00a0al. (2022).Additionally, they show a direct correlation between the size and diversity of the \u201cinstructional\u201d data and the generalizability of resulting models to unseen tasks.Since these developments depend on human annotated \u201cinstructional\u201d data, this poses a bottleneck for progress toward more generalizable models(for example see Fig.\u00a05a in Wang et\u00a0al., 2022).Our work aims to tackle this bottleneck by reducing the dependence on human annotators.", "Additionally, despite the remarkable performance of models like InstructGPTsubscriptInstructGPT\\text{InstructGPT}_{\\text{}}InstructGPT start_POSTSUBSCRIPT end_POSTSUBSCRIPT\u00a0Ouyang et\u00a0al. (2022), their construction process remains quite opaque.In particular, the role of data has remained understudied due to limited transparency and data released by major corporate entities behind these key models.Addressing such challenges necessitates the creation of a large-scale, public dataset covering a broad range of tasks.", "Instruction-following models have also been of interest in the multi-modal learning literature Fried et\u00a0al. (2018); Shridhar et\u00a0al. (2020); Min et\u00a0al. (2022); Weir et\u00a0al. (2022).Self-Instruct, as a general approach to expanding data, can potentially also be helpful in those settings; however, this is out of the scope of this work.", "A variety of works have relied on generative LMs for data generation\u00a0Schick and Sch\u00fctze (2021); Wang et\u00a0al. (2021); Liu et\u00a0al. (2022); Meng et\u00a0al. (2022) or augmentation\u00a0Feng et\u00a0al. (2021); Yang et\u00a0al. (2020); Mekala et\u00a0al. (2022). For example, Schick and Sch\u00fctze (2021) propose to replace human annotations of a given task with prompting large LMs and use the resulting data for fine-tuning (often smaller) models in the context of SuperGLUE tasks\u00a0Wang et\u00a0al. (2019).While our work can be viewed as a form of \u201caugmentation,\u201d our work differs from this line in that it is not specific to a particular task (say, QA or NLI). In contrast, a distinct motivation for Self-Instruct is to bootstrap new task definitions that may not have been defined before by any NLP practitioner (though potentially still important for downstream users).", "A typical self-training framework \u00a0He et\u00a0al. (2019); Xie et\u00a0al. (2020); Du et\u00a0al. (2021); Amini et\u00a0al. (2022); Huang et\u00a0al. (2022) uses trained models to assign labels to unlabeled data and then leverages the newly labeled data to improve the model. In a similar line, Zhou et\u00a0al. (2022a) use multiple prompts to specify a single task and proposeto regularize via prompt consistency, encouragingconsistent predictions over theprompts. This allows eitherfinetuning the model with extra unlabeledtraining data, or direct application at inference time.While Self-Instruct has some similarities with the self-training literature, most self-training methods assume a specific target task as well as unlabeled examples under it; in contrast, Self-Instruct produces a variety of tasks from scratch.", "Knowledge distillation\u00a0Hinton et\u00a0al. (2015); Sanh et\u00a0al. (2019); West et\u00a0al. (2021); Magister et\u00a0al. (2022) often involves the transfer of knowledge from larger models to smaller ones. Self-Instruct can also be viewed as a form of \u201cknowledge distillation\", however, it differs from this line in the following ways: (1) the source and target of distillation are the same, i.e., a model\u2019s knowledge is distilled to itself; (2) the content of distillation is in the form of an instruction task (i.e., instructions that define a task, and a set of examples that instantiate it).", "A series of recent works use language models to bootstrap some inferences using specialized methods.NPPrompt\u00a0Zhao et\u00a0al. (2022) provides a method to generate predictions for semantic labels without any fine-tuning. It uses a model\u2019s own embeddings to automatically find words relevant to the label of the data sample and hence reduces the dependency on manual mapping from model prediction to label (verbalizers).STAR\u00a0Zelikman et\u00a0al. (2022) iteratively leverages a small number of rationale examples and a large dataset without rationales, to bootstrap a model\u2019s ability to perform reasoning. Self-Correction\u00a0Welleck et\u00a0al. (2022) decouples an imperfect base generator (model) from a separate corrector that learns to iteratively correct imperfect generations and demonstrates improvement over the base generator. Our work instead focuses on bootstrapping new tasks in the instruction paradigm.", "A series of recent works\u00a0Zhou et\u00a0al. (2022b); Ye et\u00a0al. (2022); Singh et\u00a0al. (2022); Honovich et\u00a0al. (2022) generate instructions of a task given a few examples. While Self-Instruct also involves instruction generation, a major difference in our case is it is task-agnostic; we generate new tasks (instructions along with instances) from scratch.", "Annotating large-scale instruction data can be challenging for humans because it requires 1) creativity to come up with novel tasks and 2) expertise for writing the labeled instances for each task.In this section, we detail our process for Self-Instruct, which refers to the pipeline of generating tasks with a vanilla pretrained language model itself and then conducting instruction tuning with this generated data in order to align the language model to follow instructions better. This pipeline is depicted in Figure\u00a01.", "The instruction data we want to generate contains a set of instructions {It}subscript\ud835\udc3c\ud835\udc61\\{I_{t}\\}{ italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }, each of which defines a task t\ud835\udc61titalic_t in natural language. Each task has one or more input-output instances (Xt,Yt)subscript\ud835\udc4b\ud835\udc61subscript\ud835\udc4c\ud835\udc61(X_{t},Y_{t})( italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ).A model M\ud835\udc40Mitalic_M is expected to produce the output y\ud835\udc66yitalic_y, given the task instruction Itsubscript\ud835\udc3c\ud835\udc61I_{t}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and the instance input x\ud835\udc65xitalic_x: M(It,x)=y,for(x,y)\u2208(Xt,Yt)formulae-sequence\ud835\udc40subscript\ud835\udc3c\ud835\udc61\ud835\udc65\ud835\udc66for\ud835\udc65\ud835\udc66subscript\ud835\udc4b\ud835\udc61subscript\ud835\udc4c\ud835\udc61M(I_{t},x)=y,\\;\\mbox{for}\\ (x,y)\\in(X_{t},Y_{t})italic_M ( italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_x ) = italic_y , for ( italic_x , italic_y ) \u2208 ( italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ).Note that the instruction and instance input does not have a strict boundary in many cases. For example, \u201cwrite an essay about school safety\u201d can be a valid instruction that we expect models to respond to directly, while it can also be formulated as \u201cwrite an essay about the following topic\u201d as the instruction, and \u201cschool safety\u201d as an instance input. To encourage the diversity of the data format, we allow such instructions that do not require additional input (i.e., x\ud835\udc65xitalic_x is empty).", "Our pipeline for generating the instruction data consists of four steps: 1) instruction generation, 2) identifying whether the instruction represents a classification task or not, 3) instance generation with the input-first or the output-first approach, and 4) filtering low-quality data.", "Self-Instruct is based on a finding that large pretrained language models can be prompted to generate new and novel instructions when presented with some existing instructions in the context. This provides us with a way to grow the instruction data from a small set of seed human-written instructions.We propose to generate a diverse set of instructions in a bootstrapping fashion. We initiate the task pool with 175 tasks (1 instruction and 1 instance for each task) written by our authors. For every step, we sample 8 task instructions from this pool as in-context examples.Of the 8 instructions, 6 are from the human-written tasks,and 2 are from the model-generated tasks in previous steps to promote diversity.The prompting template is shown in Table\u00a06.", "Because we need two different approaches for classification and non-classification tasks, we next identify whether the generated instruction represents a classification task or not.333More concretely, we regard tasks that have a limited and small output label space as classification tasks.We prompt vanilla GPT3 few-shot to determine this, using 12 classification instructions and 19 non-classification instructions from the seed tasks. The prompting template is shown in Table\u00a07.", "Given the instructions and their task type, we generate instances for each instruction independently. This is challenging because it requires the model to understand what the target task is, based on the instruction, figure out what additional input fields are needed and generate them, and finally complete the task by producing the output. We found that pretrained language models can achieve this to a large extent when prompted with instruction-input-output in-context examples from other tasks.A natural way to do this is the Input-first Approach, where we can ask a language model to come up with the input fields first based on the instruction, and then produce the corresponding output. This generation order is similar to how models are used to respond to instruction and input, but here with in-context examples from other tasks. The prompting template is shown in Table\u00a08.", "However, we found that this approach can generate inputs biased toward one label, especially for classification tasks (e.g., for grammar error detection, it usually generates grammatical input). Therefore, we additionally propose anOutput-first Approach for classification tasks, where we first generate the possible class labels, and then condition the input generation on each class label. The prompting template is shown in Table\u00a09.444In this work, we use a fixed set of seed tasks for prompting the instance generation, and thus only generate a small number of instances per task in one round. Future work can use randomly sampled tasks to prompt the model to generate a larger number of instances in multiple rounds. We apply the output-first approach to the classification tasks identified in the former step, and the input-first approach to the remaining non-classification tasks.", "To encourage diversity, a new instruction is added to the task pool only when its ROUGE-L overlap with any existing instruction is less than 0.7.We also exclude instructions that contain some specific keywords (e.g., images, pictures, graphs) that usually can not be processed by language models. When generating new instances for each instruction, we filter out instances that are exactly the same or those with the same input but different outputs.", "After the creation of the large-scale instruction data, we use this data to finetune the original language model (i.e., Self-Instruct). To do this, we concatenate the instruction and instance input as a prompt and train the model to generate the instance output in a standard supervised way. To make the model robust to different formats, we use multiple templates to encode the instruction and instance input together. For example, the instruction can be prefixed with \u201cTask:\u201d or not, the input can be prefixed with \u201cInput:\u201d or not, \u201cOutput:\u201d can be appended at the end of the prompt, and different numbers of break lines can be put in the middle, etc.", "In this section, we apply our method for inducing instruction data to GPT3 as a case study. We use the largest GPT3 language model (\u201cdavinci\u201d engine) accessed through the OpenAI API555https://openai.com/api/. The parameters for making queries are described in Appendix A.1. Here we present an overview of the generated data.", "Table\u00a01 describes the basic statistics of the generated data. We generate a total of over 52K instructions, and more than 82K instances corresponding to these instructions after filtering.", "To study what types of instructions are generated and how diverse they are, we identify the verb-noun structure in the generated instructions. We use the Berkeley Neural Parser666https://parser.kitaev.io/ (Kitaev and Klein, 2018; Kitaev et\u00a0al., 2019) to parse the instructions, and then extract the verb that is closest to the root of the parse tree as well as its first direct noun object. 26,559 out of the 52,445 instructions contain such structure; other instructions usually contain more complex clauses (e.g., \u201cClassify whether this tweet contains political content or not.\u201d) or are framed as questions (e.g., \u201cWhich of these statements are true?\u201d).We plot the top 20 most common root verbs and their top 4direct noun objects in Figure\u00a04, which accounts for 14% of the entire set. Overall, we see quite diverse intents and textual formats in these instructions.", "We further study how the generated instructions differ from the seed instructions that are used to prompt the generation. For each generated instruction, we compute its highest ROUGE-L overlap with the 175 seed instructions. We plot the distribution of these ROUGE-L scores in Figure\u00a04, indicating a decent number of new instructions that do not have much overlap with the seeds.We also demonstrate diversity in length of the instructions, instance inputs, and instance outputs in Figure\u00a04.", "So far, we have shown the quantity and diversity of the generated data, but its quality remains uncertain. To investigate this, we randomly sample 200 instructions and randomly select 1 instance per instruction. We asked an expert annotator (co-author of this work) to label whether each instance is correct or not, in terms of the instruction, the instance input, and the instance output.Evaluation results in Table\u00a02 show that most of the generated instructions are meaningful, while the generated instances may contain more noise (to a reasonable extent). However, we found that even though the generations may contain errors, most of them are still in the correct format or even partially correct, which can provide useful guidance for training models to follow instructions. We listed a number of good generations and bad generations in Table\u00a010 and Table\u00a011 respectively.", "We conduct experiments to measure and compare the quality of models under various instruction tuning setups.We first describe our models and other baselines, followed by our experiments.", "With the instruction-generated instruction data, we conduct instruction tuning for the GPT3 model itself (the \u201cdavinci\u201d engine). As we described in \u00a73.3, we use various templates to concatenate the instruction and input, and train the model to generate the output. This finetuning is done through the OpenAI finetuning API777https://beta.openai.com/docs/guides/fine-tuning. We use the default hyper-parameters, except that we set the prompt loss weight to 0, and we train the model for 2 epochs. We refer the readers to Appendix\u00a0A.2 for additional finetuning details. The resulting model is denoted as GPT3Self-InstSelf-Inst{}_{\\textsc{Self-Inst}}start_FLOATSUBSCRIPT Self-Inst end_FLOATSUBSCRIPT.", "We evaluate T5-LM Lester et\u00a0al. (2021); Raffel et\u00a0al. (2020) and GPT3\u00a0Brown et\u00a0al. (2020) as the vanilla LM baselines (only pre-training, no additional fine-tuning).These baselines will indicate the extent to which off-the-shelf LMs are capable of following instructions naturally immediately after pretraining.", "T00 and Tk\ud835\udc58kitalic_k-Instruct are two instruction-tuned models proposed in Sanh et\u00a0al. (2022) and Wang et\u00a0al. (2022) respectively, and are demonstrated to be able to follow instructions for many NLP tasks.Both of these models are finetuned from the T5\u00a0Raffel et\u00a0al. (2020) checkpoints and are publicly available888https://huggingface.co/bigscience/T0999https://huggingface.co/allenai/tk-instruct-11b-def. For both of these models, we use their largest version with 11B parameters.", "We evaluate InstructGPTsubscriptInstructGPT\\text{InstructGPT}_{\\text{}}InstructGPT start_POSTSUBSCRIPT end_POSTSUBSCRIPT\u00a0Ouyang et\u00a0al. (2022),which is developed by OpenAI based on GPT3 to follow human instructions better and has been found by the community to have impressive zero-shot abilities.There are various generations of these models,where newer ones use more expansive data or algorithmic novelties101010https://beta.openai.com/docs/model-index-for-researchers.For our SuperNI experiments in \u00a75.3, we only compare with their text-davinci-001 engine, because their newer engines are trained with the latest user data and are likely to already see the SuperNI evaluation set. For our human evaluation of these models on newly written instructions, we include their 001, 002 and 003 engines for completeness.", "Additionally, to compare Self-Instruct training with other publicly available instruction tuning data, we further finetune GPT3 model with data from PromptSource and SuperNI, which are used to train the T00 and Tk\ud835\udc58kitalic_k-Instruct models. We call them T00 training and SuperNI training for short, respectively.To save the training budget, we sampled 50K instances (but covering all their instructions) for each dataset, which has a comparable size to the instruction data we generated. Based on the findings from Wang et\u00a0al. (2022) and our early experiments, reducing the number of instances per task does not degrade the model\u2019s generalization performance to unseen tasks.", "We first evaluate the models\u2019 ability to follow instructions on typical NLP tasks in a zero-shot fashion.We use the evaluation set of SuperNI \u00a0Wang et\u00a0al. (2022), which consists of 119 tasks with 100 instances in each task.In this work, we mainly focus on the zero-shot setup, i.e., the model is prompted with the definition of the tasks only, without in-context demonstration examples.For all our requests to the GPT3 variants, we use the deterministic generation mode (temperature as 0 and no nucleus sampling) without specific stop sequences.", "We make the following observations from the results in Table\u00a03.Self-Instructboosts the instruction-following ability of GPT3 by a large margin. The vanilla GPT3 model basically cannot follow human instructions at all. Upon manual analysis, we find that it usually generates irrelevant and repetitive text, and does not know when to stop generation.Compared with other models that are not specifically trained for SuperNI, GPT3Self-InstSelf-Inst{}_{\\textsc{Self-Inst}}start_FLOATSUBSCRIPT Self-Inst end_FLOATSUBSCRIPT achieves better performance than T00 or the GPT3 finetuned on the T00 training set, which takes tremendous human labeling efforts. Notably, GPT3Self-InstSelf-Inst{}_{\\textsc{Self-Inst}}start_FLOATSUBSCRIPT Self-Inst end_FLOATSUBSCRIPT also nearly matches the performance of InstructGPT001subscriptInstructGPT001\\text{InstructGPT}_{\\text{001}}InstructGPT start_POSTSUBSCRIPT 001 end_POSTSUBSCRIPT, which is trained with private user data and human-annotated labels.", "Models trained on SuperNI training set still achieve better performance on its evaluation set, which we attribute to the similar instruction style and formatting. However, we show that Self-Instruct still brings in additional gains when combined with the SuperNI training set, proving its value as complementary data.", "Despite the comprehensiveness of SuperNI in collecting existing NLP tasks, most of these NLP tasks were proposed for research purposes and skewed toward classification. To better access the practical value of instruction-following models, a subset of the authors curate a new set of instructions motivated by user-oriented applications.We first brainstorm different domains where large LMs may be useful (e.g., email writing, social media, productivity tools, entertainment, programming), then craft instructions related to each domain along with an input-output instance (again, input is optional). We aim to diversify the styles and formats of these tasks (e.g., instructions may be long or short; input/output may take the form of bullet points, tables, codes, equations, etc.). In total, we create 252 instructions with 1 instance per instruction. We believe it can serve as a testbed for evaluating how instruction-based models handle diverse and unfamiliar instructions. Table\u00a04 presents a small portion of the 252 tasks. The whole test set will be available upon request.", "Evaluating models\u2019 performance on this evaluation set of diverse tasks is extremely challenging because different tasks require different expertise. Indeed, many of these tasks cannot be measured by automatic metrics or even be judged by normal crowdworkers (e.g., writing a program, or converting first-order logic into natural language). To get a more faithful evaluation, we asked the authors of the instructions to judge model predictions. The evaluators were asked to rate the output based on whether it accurately and effectively completes the task. We implemented a four-level rating system for categorizing the quality of the models\u2019 outputs, defined as follows:", "\u2022Rating-A: The response is valid and satisfying.\u2022Rating-B: The response is acceptable but has minor errors or imperfections that can be improved.\u2022Rating-C: The response is relevant and responds to the instruction, but it has significant errors in the content. For example, GPT3 might generate a valid output first, but continue to generate other irrelevant things.\u2022Rating-D: The response is irrelevant or invalid, including repetition of the input, totally irrelevant output, etc.", "Figure\u00a05 provides the performance of GPT3 model and its instruction-tuned counterparts on this newly written instruction set. As anticipated, the vanilla GPT3 language model is largely unable to respond to instructions, and all instruction-tuned models demonstrate comparatively higher performance,Nonetheless, GPT3Self-InstSelf-Inst{}_{\\textsc{Self-Inst}}start_FLOATSUBSCRIPT Self-Inst end_FLOATSUBSCRIPT (i.e., GPT3 model fine-tuned with Self-Instruct) outperforms those counterparts trained on T00 or SuperNI by a large margin, demonstrating the value of the generated data despite the noise.Compared with InstructGPT001subscriptInstructGPT001\\text{InstructGPT}_{\\text{001}}InstructGPT start_POSTSUBSCRIPT 001 end_POSTSUBSCRIPT (c.f. footnote\u00a01), GPT3Self-InstSelf-Inst{}_{\\textsc{Self-Inst}}start_FLOATSUBSCRIPT Self-Inst end_FLOATSUBSCRIPT is quite close in the performance\u2014if we count acceptable response with minor imperfections (Rating-3) as valid, GPT3Self-InstSelf-Inst{}_{\\textsc{Self-Inst}}start_FLOATSUBSCRIPT Self-Inst end_FLOATSUBSCRIPT is only 5% behind InstructGPT001subscriptInstructGPT001\\text{InstructGPT}_{\\text{001}}InstructGPT start_POSTSUBSCRIPT 001 end_POSTSUBSCRIPT. Lastly, our evaluation confirms the impressive instruction-following ability of InstructGPT002subscriptInstructGPT002\\text{InstructGPT}_{\\text{002}}InstructGPT start_POSTSUBSCRIPT 002 end_POSTSUBSCRIPT & InstructGPT003subscriptInstructGPT003\\text{InstructGPT}_{\\text{003}}InstructGPT start_POSTSUBSCRIPT 003 end_POSTSUBSCRIPT models. Although there are many factors behind this success, we conjecture that future work can largely benefit from improving the quality of our generated data by using human annotators or training a reward model to select better generations, similar to the algorithm used in Ouyang et\u00a0al. (2022).", "We present a selection of user-oriented tasks, the corresponding GPT3Self-InstSelf-Inst{}_{\\textsc{Self-Inst}}start_FLOATSUBSCRIPT Self-Inst end_FLOATSUBSCRIPT-produced responses and annotator ratings in Table\u00a04. We see that even for responses rated as level 2, the model demonstrates extensive steps in solving the task, even though its final output is incorrect.   ", "It is worthwhile to reflect on the role that high-quality human feedback playsin enabling the recent successes on instruction-tuning LMs\u00a0Mishra et\u00a0al. (2022); Wang et\u00a0al. (2022); Wei et\u00a0al. (2022); Sanh et\u00a0al. (2022); Ouyang et\u00a0al. (2022).Here are two extreme hypotheses:", "(H1subscript\ud835\udc3b1H_{1}italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT)Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs need to learn about issues that were not quite learned during pre-training.(H2subscript\ud835\udc3b2H_{2}italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT)Human feedback is an optional aspect of instruction-tuning as LMs are already quite familiar with instructions from their pre-training. Observing the human feedback is merely a lightweight process for aligning their pre-training distribution/objective which might be replaceable with a different process.While the reality probably lies somewhere in between these two extremes, we conjecturethat it is closer to H2subscript\ud835\udc3b2H_{2}italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, particularly for larger models.This intuition, that LMs already know much about language instructions, is a key motivation for Self-Instruct and is also supported by its empirical success.", "Beyond the immediate focus of this paper, we believe that Self-Instruct may help bring more transparency to what happens \u201cbehind the scenes\u201d of widely-used instruction-tuned models like InstructGPTsubscriptInstructGPT\\text{InstructGPT}_{\\text{}}InstructGPT start_POSTSUBSCRIPT end_POSTSUBSCRIPT.Unfortunately, such industrial models remain behind the API walls as their datasets are not released, and hence there is little understanding of their constructions and why they demonstrate impressive capabilities.The burden now falls on academia to better understand the source of success in these models and strive for better \u2013 yet open \u2013 models. We believe our findings in this paper demonstrate the importance of diverse instruction data, and our large synthetic dataset can be the first step toward higher-quality data for building better instruction-following models.", "Here, we discuss some limitations of this work to inspire future research in this direction.", "Self-Instruct depends on LMs, and it will inherit all the limitations that carry over with LMs.As recent studies have shown\u00a0Razeghi et\u00a0al. (2022); Kandpal et\u00a0al. (2022), tail phenomena pose a serious challenge to the success of LMs. In other words, LMs\u2019 largest gains correspond to the frequent uses of languages (head of the language use distribution), and there are minimal gains in the low-frequency contexts.Similarly, in the context of this work, it would not be surprising if the majority of the gains by Self-Instruct are skewed towardtasks or instructions that present more frequently in the pre-training corpus.As a consequence, the approach might show brittleness with respect to uncommon and creative instructions.", "Because of Self-Instruct\u2019s dependence on the inductive biases extracted from LMs, it might work best for larger models.If true, this may create barriers to access for those who may not have large computing resources.We hope future studies will carefully study the gains as a function of model size or various other parameters.It is worthwhile to note that instruction-tuning with human annotation also suffers from a similar limitation: gains of instruction-tuning are higher for larger model\u00a0Wei et\u00a0al. (2022).", "A point of concern for the authors is the unintended consequences of this iterative algorithm, such as the amplification of problematic social biases (stereotypes or slurs about genders, races, etc.).Relatedly, one observed challenge in this process is the algorithm\u2019s difficulty in producing balanced labels, which reflected models\u2019 prior biases.We hope future work will hash out such details to better understand the pros and cons of the approach.", "We introduce Self-Instruct, a task-agnostic method to improve the instruction-following capabilities of language models via its own generation of instruction data (instruction, input, and output samples) and bootstrapping with it. Our method conducts instruction-tuning of the original model on the pruned subset of generated samples. On experimenting with vanilla GPT3, we observe a 33% absolute improvement over the original model on Super-NaturalInstructions. This performance is on par with InstructGPT001subscriptInstructGPT001\\text{InstructGPT}_{\\text{001}}InstructGPT start_POSTSUBSCRIPT 001 end_POSTSUBSCRIPT, which is trained with private user data and expensive human annotations. Furthermore, we curate a set of expert-written instructions for novel tasks. Human evaluation on this set shows that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT001subscriptInstructGPT001\\text{InstructGPT}_{\\text{001}}InstructGPT start_POSTSUBSCRIPT 001 end_POSTSUBSCRIPT. We hope Self-Instruct can serve as the first step to align pretrained language models to follow human instructions, and future work can build on top of this data to improve instruction-following models."], "figure_types": {"e65b346d442e9962a4276dc1c1af2956d9d5f1eb/1-Figure1-1.png": "other", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/14-Figure8-1.png": "plot", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/15-Table4-1.png": "table", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/15-Table5-1.png": "other", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/16-Table6-1.png": "table", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/19-Figure9-1.png": "table", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/2-Figure2-1.png": "schematic", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/21-Table9-1.png": "table", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/22-Table10-1.png": "table", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/23-Table11-1.png": "table", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/4-Table1-1.png": "table", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/5-Figure3-1.png": "plot", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/5-Figure4-1.png": "plot", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/5-Table2-1.png": "table", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/6-Table3-1.png": "table", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/7-Figure6-1.png": "plot", "e65b346d442e9962a4276dc1c1af2956d9d5f1eb/8-Figure7-1.png": "plot"}}, "1812.02833": {"paper_id": "paper_30", "title": "Disentangling Disentanglement in Variational Autoencoders", "arxiv_url": "https://arxiv.org/abs/1812.02833", "s2orc_url": "https://www.semanticscholar.org/paper/3f51216e1834c4fe06b08df87901ae0d77de2567", "all_figures_tables": {"3f51216e1834c4fe06b08df87901ae0d77de2567/10-Figure3-1.png": "Figure 3: PDF of Gaussian mixture model prior, i.e. p(z) as per (8).", "3f51216e1834c4fe06b08df87901ae0d77de2567/3-Figure1-1.png": "Figure 1: Reconstruction loss vs disentanglement metric [16] for \u03b2-VAE (i.e. (3) with \u03b1 = 0) trained on the 2D Shapes dataset [21]. Shaded areas represent 95% confidence intervals for disentanglement metric estimate, calculated using 100 separately trained networks. See Appendix B for details. [Left] Using an anisotropic Gaussian with diagonal covariance either fixed to the principal component values or learned during training. Point labels represent different values of \u03b2. [Right] Using p\u03bd\u03b8 (z) =\u220f i STUDENT-T(zi; \u03bd) for different degrees of freedom \u03bd with \u03b2 = 1. Note that p \u03bd \u03b8 (z) \u2192 N (z;0, I) as \u03bd \u2192 \u221e, and reducing \u03bd only incurs a minor increase in reconstruction loss.", "3f51216e1834c4fe06b08df87901ae0d77de2567/4-Figure2-1.png": "Figure 2: Density of aggregate posterior q\u03d5(z) for different values of \u03b1 and \u03b2. [Top] \u03b1 = 0, \u03b2 \u2208 {0.01, 0.3, 0.5, 1.0, 1.2}. [Bottom] \u03b2 = 0, \u03b1 \u2208 {1, 2, 3, 5, 8}. We see that increasing \u03b2 increases the level of overlap in q\u03d5(z), as a consequence of increasing the encoder variance for individual datapoints. When \u03b2 is too large, the encoding of a datapoint loses meaning. Also, as a single datapoint encodes to a Gaussian distribution, q\u03d5(z|x) is unable to match p\u03b8(z) exactly. Because q\u03d5(z|x) \u2192 q\u03d5(z) when \u03b2 \u2192 \u221e, this in turn means that overly large values of \u03b2 actually cause a mismatch between q\u03d5(z) and p\u03b8(z) (see top right). Increasing \u03b1, instead always improves the match between q\u03d5(z) and p\u03b8(z). Here, the finiteness of the dataset and the choice of divergence results in an increase in the overlap with increasing \u03b1, but only up to the level required for a non-negligible overlap between the nearby datapoints, such that large values of \u03b1 do not cause the encodings to lose significance.", "3f51216e1834c4fe06b08df87901ae0d77de2567/9-Table1-1.png": "Table 1: Encoder and decoder architectures."}, "referred_figures_tables": [["3f51216e1834c4fe06b08df87901ae0d77de2567/4-Figure2-1.png"]], "question_id": [9], "question": ["What is the purpose of using a non-isotropic Gaussian prior in the VAE model?"], "question_section": ["6. Experiments"], "question_trigger_sentence": ["We first show how subtle changes to the prior distribution can yield improvements in disentanglement. The standard choice of an isotropic Gaussian has previously been justified by the correct assertion that the latents are independent under the prior (Higgins et al., 2016). However, as explained in \u00a7 4.1, the rotational invariance of this prior means that it does not directly encourage axis-aligned representations. Priors that break this rotational invariance should be better suited for learning disentangled representations. We assess this hypothesis by training a \u03b2-VAE (i.e. (7) with \u03b1 = 0) on the 2D Shapes dataset (Matthey et al., 2017) and evaluating disentanglement using the metric of Kim and Mnih (2018)."], "question_type": ["Deep/complex question"], "evidential_info": [[{"context": "Figure\u00a02 demonstrates that notable improvements in disentanglement can be achieved by using non-isotropic priors:for a given reconstruction loss, implicitly fixed by \\beta, non-isotropic Gaussian priors got better disentanglement scores, with further improvement achieved when the prior variance is learnt.With a product of Student-t priors p_{\\nu}(\\bm{z}) (noting p_{\\nu}(\\bm{z})\\rightarrow\\mathcal{N}(\\bm{z};\\mathbf{0},\\mathbf{I}) as \\nu\\rightarrow\\infty), reducing\u00a0\\nu only incurred a minor reconstruction penalty, for improved disentanglement.Interestingly,very low values of\u00a0\\nu caused the disentanglement score to drop again (though still giving higher values than the Gaussian).We speculate that this may be related to the effect of heavy tails on the disentanglement metric itself, rather than being an objectively worse disentanglement.Another interesting result was that for an isotropic Gaussian prior, as per the original \\beta-vae setup, no gains at all were achieved in disentanglement by increasing \\beta.", "rationale": "Figure 2 demonstrates that notable improvements in disentanglement can be achieved by using non-isotropic priors: for a given reconstruction loss, implicitly fixed by \\beta, non-isotropic Gaussian priors got better disentanglement scores, with further improvement achieved when the prior variance is learnt"}]], "composition": ["the purpose of using a non-isotropic Gaussian prior in the VAE model is to get better disentanglement scores, with further improvement achieved when the prior variance is learnt"], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["775"], "passages": ["An oft-stated motivation for learning disentangled representations of data with deep generative models is a desire to achieve interpretability\u00a0[5, 10]\u2014particularly the decomposability\u00a0[see \u00a73.2.1 in 33] of latent representations to admit intuitive explanations.Most work has focused on capturing purely independent factors of variation\u00a0[10, 7, 16, 25, 4, 57, 3, 8, 17, 15, 59], typically evaluating this using purpose-built, synthetic data\u00a0[15, 17, 25], whose generative factors are independent by construction.", "This conventional view of disentanglement, as recovering independence, has subsequently motivated the development of formal evaluation metrics for independence\u00a0[15, 25], which in turn has driven the development of objectives that target these metrics, often by employing regularisers explicitly encouraging independence in the representations\u00a0[15, 25, 16].", "We argue that such an approach is not generalisable, and potentially even harmful, to learning interpretable representations for more complicated problems, where such simplistic representations cannot accurately mimic the generation of high dimensional data from low dimensional latent spaces, and more richly structured dependencies are required.", "We posit a generalisation of disentanglement in vaes\u2014decomposing their latent representations\u2014that can help avoid such pitfalls.We characterise decomposition in vaes as the fulfilment of two factors:a) the latent encodings of data having an appropriate level of overlap, andb) the aggregate encoding of data conforming to a desired structure, represented through the prior.We emphasize that neither of these factors is sufficient in isolation: withoutan appropriate level of overlap, encodings can degrade to a lookup tablewhere the latents convey little information about data, and without the aggregate encoding of data following a desired structure, the encodings do not decompose as desired.", "Disentanglement implicitly makes a choice of decomposition: that the latent features are independent of one another.We make this explicit and exploit it to both provide improvement to disentanglement through judicious choices of structure in the prior, and to introduce a more general framework flexible enough to capture alternate, more complex, notions of decomposition such as sparsity, clustering, hierarchical structuring, or independent subspaces.", "To connect our framework with existing approaches for encouraging disentanglement, we provide a theoretical analysis of the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae\u00a0[17, 3, 2], and show that it typically only allows control of latent overlap, the first decomposition factor.We show that it can be interpreted, up to a constant offset, as the standard vae objective with its prior annealed as p\u03b8(\ud835\udc9b)\u03b2subscript\ud835\udc5d\ud835\udf03superscript\ud835\udc9b\ud835\udefdp_{{\\theta}}\\left(\\bm{z}\\right)^{\\beta}italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_z ) start_POSTSUPERSCRIPT italic_\u03b2 end_POSTSUPERSCRIPT and an additional maximum entropy regularization of the encoder that increases the stochasticity of the encodings.Specialising this result for the typical choice of a Gaussian encoder and isotropic Gaussian prior indicates that the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae, up to a scaling of the latent space, is equivalent to the vae plus a regulariser encouraging higher encoder variance.Moreover, this objective is invariant to rotations of the learned latent representation, meaning that it does not, on its own, encourage the latent variables to take on meaningful representations any more than an arbitrary rotation of them.", "We confirm these results empirically, while further using our decomposition framework to show that simple manipulations to the prior can improve disentanglement, and other decompositions, with little or no detriment to the reconstruction accuracy.Further, motivated by our analysis, we propose an alternative objective that takes into account the distinct needs of the two factors of decomposition, and use it to learn clustered and sparse representations as demonstrations of alternative forms of decomposition.An implementation of our experiments and suggested methods is provided at\u00a0http://github.com/iffsid/disentangling-disentanglement.", "Let \ud835\udc99\ud835\udc99\\bm{x}bold_italic_x be an \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X-valued random variable distributed according to an unknown generative process with density p\ud835\udc9f(\ud835\udc99)subscript\ud835\udc5d\ud835\udc9f\ud835\udc99p_{\\mathcal{D}}(\\bm{x})italic_p start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( bold_italic_x ) and from which we have observations, X={\ud835\udc991,\u2026,\ud835\udc99n}\ud835\udc4bsubscript\ud835\udc991\u2026subscript\ud835\udc99\ud835\udc5bX=\\{\\bm{x}_{1},\\dots,\\bm{x}_{n}\\}italic_X = { bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }.The aim is to learn a latent-variable model p\u03b8(\ud835\udc99,\ud835\udc9b)subscript\ud835\udc5d\ud835\udf03\ud835\udc99\ud835\udc9bp_{{\\theta}}\\left(\\bm{x},\\bm{z}\\right)italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x , bold_italic_z ) that captures this generative process, comprising of a fixed111Learning the prior is possible, but omitted for simplicity. prior over latents p(\ud835\udc9b)\ud835\udc5d\ud835\udc9bp(\\bm{z})italic_p ( bold_italic_z ) and a parametric likelihood p\u03b8(\ud835\udc99|\ud835\udc9b)subscript\ud835\udc5d\ud835\udf03conditional\ud835\udc99\ud835\udc9bp_{{\\theta}}\\left(\\bm{x}|\\bm{z}\\right)italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_z ).Learning proceeds by minimising a divergence between the true data generating distribution and the model w.r.t \u03b8\ud835\udf03\\thetaitalic_\u03b8, typicallyargmin\ud835\udf3d\u2061KL\u2061(p\ud835\udc9f(\ud835\udc99)\u2225p\u03b8(\ud835\udc99))=argmax\ud835\udf3d\u2061\ud835\udd3cp\ud835\udc9f(\ud835\udc99)\u2061[log\u2061p\u03b8(\ud835\udc99)]subscriptargmin\ud835\udf3dKLconditionalsubscript\ud835\udc5d\ud835\udc9f\ud835\udc99subscript\ud835\udc5d\ud835\udf03\ud835\udc99subscriptargmax\ud835\udf3dsubscript\ud835\udd3csubscript\ud835\udc5d\ud835\udc9f\ud835\udc99subscript\ud835\udc5d\ud835\udf03\ud835\udc99\\displaystyle\\operatorname*{arg\\,min}_{\\bm{\\theta}}\\operatorname{\\scalebox{0.95}{\\text{KL}}}\\left(p_{\\mathcal{D}}(\\bm{x})\\,\\|\\;p_{{\\theta}}\\left(\\bm{x}\\right)\\right)=\\operatorname*{arg\\,max}_{\\bm{\\theta}}\\operatorname{{}\\mathbb{E}}_{p_{\\mathcal{D}}(\\bm{x})}\\left[\\log p_{{\\theta}}\\left(\\bm{x}\\right)\\right]start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT DKL ( italic_p start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( bold_italic_x ) \u2225 italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x ) ) = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( bold_italic_x ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x ) ]where p\u03b8(\ud835\udc99)=\u222b\ud835\udcb5p\u03b8(\ud835\udc99|\ud835\udc9b)p(\ud835\udc9b)\ud835\udc51\ud835\udc9bsubscript\ud835\udc5d\ud835\udf03\ud835\udc99subscript\ud835\udcb5subscript\ud835\udc5d\ud835\udf03conditional\ud835\udc99\ud835\udc9b\ud835\udc5d\ud835\udc9bdifferential-d\ud835\udc9bp_{{\\theta}}\\left(\\bm{x}\\right)=\\int_{\\mathcal{Z}}p_{{\\theta}}\\left(\\bm{x}|\\bm{z}\\right)p(\\bm{z})d\\bm{z}italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x ) = \u222b start_POSTSUBSCRIPT caligraphic_Z end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_z ) italic_p ( bold_italic_z ) italic_d bold_italic_z is the marginal likelihood, or evidence, of datapoint\u00a0\ud835\udc99\ud835\udc99\\bm{x}bold_italic_x under the model, approximated by averaging over the observations.", "However, estimating\u00a0p\u03b8(\ud835\udc99)subscript\ud835\udc5d\ud835\udf03\ud835\udc99p_{{\\theta}}\\left(\\bm{x}\\right)italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x ) (or its gradients) to any sufficient degree of accuracy is typically infeasible.A common strategy to ameliorate this issue involves the introduction of a parametric inference model q\u03d5(\ud835\udc9b|\ud835\udc99)subscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99q_{{\\phi}}\\left(\\bm{z}|\\bm{x}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) to construct a variational evidence lower bound (elbo) on log\u2061p\u03b8(\ud835\udc99)subscript\ud835\udc5d\ud835\udf03\ud835\udc99\\log p_{{\\theta}}\\left(\\bm{x}\\right)roman_log italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x ) as follows\u2112(\ud835\udc99;\u03b8,\u03d5)\u225clogp\u03b8(\ud835\udc99)\u2212KL(q\u03d5(\ud835\udc9b|\ud835\udc99)\u2225p\u03b8(\ud835\udc9b|\ud835\udc99))=\ud835\udd3cq\u03d5(\ud835\udc9b|\ud835\udc99)[log\u2061p\u03b8(\ud835\udc99|\ud835\udc9b)]\u2212KL\u2061(q\u03d5(\ud835\udc9b|\ud835\udc99)\u2225p(\ud835\udc9b)).\\displaystyle\\begin{split}\\mathcal{L}(\\bm{x};\\!\\theta,\\!\\phi)\\!&\\triangleq\\!\\log p_{{\\theta}}\\left(\\bm{x}\\right)-\\operatorname{\\scalebox{0.95}{\\text{KL}}}\\left(q_{{\\phi}}\\left(\\bm{z}|\\bm{x}\\right)\\,\\|\\;p_{{\\theta}}\\left(\\bm{z}|\\bm{x}\\right)\\right)\\\\\\!&=\\!\\mathbb{E}_{q_{{\\phi}}\\left(\\bm{z}|\\bm{x}\\right)\\!}[\\log p_{{\\theta}}(\\bm{x}|\\bm{z})]\\!-\\!\\operatorname{\\scalebox{0.95}{\\text{KL}}}\\left(q_{{\\phi}}(\\bm{z}|\\bm{x})\\!\\,\\|\\;\\!p(\\bm{z})\\!\\right).\\!\\!\\!\\end{split}start_ROW start_CELL caligraphic_L ( bold_italic_x ; italic_\u03b8 , italic_\u03d5 ) end_CELL start_CELL \u225c roman_log italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x ) - DKL ( italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) \u2225 italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) ) end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL = blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_z ) ] - DKL ( italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) \u2225 italic_p ( bold_italic_z ) ) . end_CELL end_ROW(1)A variational autoencoder  (vae)\u00a0[27, 48] views this objective from the perspective of a deep stochastic autoencoder, taking the inference model q\u03d5(\ud835\udc9b|\ud835\udc99)subscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99q_{{\\phi}}\\left(\\bm{z}|\\bm{x}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) to be an encoder and the likelihood model p\u03b8(\ud835\udc99|\ud835\udc9b)subscript\ud835\udc5d\ud835\udf03conditional\ud835\udc99\ud835\udc9bp_{{\\theta}}\\left(\\bm{x}|\\bm{z}\\right)italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_z ) to be a decoder.Here\u00a0\u03b8\ud835\udf03\\thetaitalic_\u03b8 and\u00a0\u03d5italic-\u03d5\\phiitalic_\u03d5 are neural network parameters, and learning happens via stochastic gradient ascent (sga) using unbiased estimates of \u2207\u03b8,\u03d51n\u2211i=1n\u2112(\ud835\udc99i;\u03b8,\u03d5)subscript\u2207\ud835\udf03italic-\u03d51\ud835\udc5bsuperscriptsubscript\ud835\udc561\ud835\udc5b\u2112subscript\ud835\udc99\ud835\udc56\ud835\udf03italic-\u03d5\\nabla_{\\theta,\\phi}\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}(\\bm{x}_{i};{\\theta},{\\phi})\u2207 start_POSTSUBSCRIPT italic_\u03b8 , italic_\u03d5 end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT caligraphic_L ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; italic_\u03b8 , italic_\u03d5 ).Note that when clear from the context, we denote \u2112(\ud835\udc99;\u03b8,\u03d5)\u2112\ud835\udc99\ud835\udf03italic-\u03d5\\mathcal{L}(\\bm{x};\\theta,\\phi)caligraphic_L ( bold_italic_x ; italic_\u03b8 , italic_\u03d5 ) as simply \u2112(\ud835\udc99)\u2112\ud835\udc99\\mathcal{L}(\\bm{x})caligraphic_L ( bold_italic_x ).", "Disentanglement, as typically employed in literature, refers to independence among features in a representation\u00a0[5, 15, 18].Conceptually, however, it has a long history, far longer than we could reasonably do justice here, and is far from specific to vaes.The idea stems back to traditional methods such as ICA\u00a0[58, 23] and conventional autoencoders\u00a0[50], through to a range of modern approaches employing deep learning\u00a0[47, 36, 9, 37, 1, 19, 11].", "Of particular relevance to this work are approaches that explore disentanglement in the context of vaes\u00a0[17, 3, 51, 25, 8, 16].Here one aims to achieve independence between the dimensions of the aggregate encoding, typically defined asq\u03d5(\ud835\udc9b)\u225c\ud835\udd3cp\ud835\udc9f(\ud835\udc99)\u2061[q(\ud835\udc9b|\ud835\udc99)]\u22481n\u2211inq(\ud835\udc9b|\ud835\udc99i)\u225csubscript\ud835\udc5eitalic-\u03d5\ud835\udc9bsubscript\ud835\udd3csubscript\ud835\udc5d\ud835\udc9f\ud835\udc99\ud835\udc5econditional\ud835\udc9b\ud835\udc991\ud835\udc5bsuperscriptsubscript\ud835\udc56\ud835\udc5b\ud835\udc5econditional\ud835\udc9bsubscript\ud835\udc99\ud835\udc56q_{\\phi}(\\bm{z})\\triangleq\\operatorname{{}\\mathbb{E}}_{p_{\\mathcal{D}}(\\bm{x})}\\left[q(\\bm{z}|\\bm{x})\\right]\\approx\\frac{1}{n}\\sum_{i}^{n}q(\\bm{z}|\\bm{x}_{i})italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) \u225c blackboard_E start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( bold_italic_x ) end_POSTSUBSCRIPT [ italic_q ( bold_italic_z | bold_italic_x ) ] \u2248 divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_q ( bold_italic_z | bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).The significance of q\u03d5(\ud835\udc9b)subscript\ud835\udc5eitalic-\u03d5\ud835\udc9bq_{\\phi}(\\bm{z})italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) is that it is the marginal distribution induced on the latents by sampling a datapoint and then using the encoder to sample an encoding given that datapoint. It can thus informally be thought of as the pushforward distribution for \u201csampling\u201d representations in the latent space.", "Within the disentangled vaes literature, there is also a distinction between unsupervised approaches, and semi-supervised approaches wherein one has access to the true generative factor values for some subset of data\u00a0[28, 51, 6].Our focus, however, is on the unsupervised setting.", "Much of the prior work in the field has either implicitly or explicitly presumed a slightly more ambitious definition of disentanglement than considered above: that it is a measure of how well one captures true factors of variation (which happen to be independent by construction for synthetic data), rather than just independent factors.After all, if we wish for our learned representations to be interpretable, it is necessary for the latent variables to take on clear-cut meaning.", "One such definition is given by Eastwood and Williams [15], who define it as the extent to which a latent dimension\u00a0d\u2208D\ud835\udc51\ud835\udc37d\\in Ditalic_d \u2208 italic_D in a representation predicts a true generative factor\u00a0k\u2208K\ud835\udc58\ud835\udc3ek\\in Kitalic_k \u2208 italic_K, with each latent capturing at most one generative factor.This implicitly assumes D\u2265K\ud835\udc37\ud835\udc3eD\\geq Kitalic_D \u2265 italic_K, as otherwise the latents are unable to explain all the true generative factors.However, for real data, the association is more likely D\u226aKmuch-less-than\ud835\udc37\ud835\udc3eD\\ll Kitalic_D \u226a italic_K, with one learning a low-dimensional abstraction of a complex process involving many factors.Consequently, such simplistic representations cannot, by definition, be found for more complex datasets that require more richly structured dependencies to be able to encode the information required to generate higher dimensional data.Moreover, for complex datasets involving a finite set of datapoints, it might not be reasonable to presume that one could capture the elements of the true generative process\u2014the data itself might not contain sufficient information to recover these and even if it does, the computation required to achieve this through model learning is unlikely to be tractable.", "The subsequent need for richly structured dependencies between latent dimensions has been reflected in the motivation for a handful of approaches\u00a0[51, 6, 24, 16] that explore this through graphical models, although employing mutually-inconsistent, and not generalisable, interpretations of disentanglement.This motivates our development of a decomposition framework as a means of extending beyond the limitations of disentanglement.", "The commonly assumed notion of disentanglement is quite restrictive for complex models where the true generative factors are not independent, very large in number, or where it cannot be reasonably assumed that there is a well-defined set of \u201ctrue\u201d generative factors (as will be the case for many, if not most, real datasets).To this end, we introduce a generalization of disentanglement, decomposition, which at a high-level can be thought of as imposing a desired structure on the learned representations.This permits disentanglement as a special case, for which the desired structure is that q\u03d5(\ud835\udc9b)subscript\ud835\udc5eitalic-\u03d5\ud835\udc9bq_{{\\phi}}\\left(\\bm{z}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) factors along its dimensions.", "We characterise the decomposition of latent spaces in vaes to be the fulfilment of two factors (as shown in Figure\u00a01):a.An \u201cappropriate\u201d level of overlap in the latent space\u2014ensuring that the range of latent values capable of encoding a particular datapoint is neither too small, nor too large.This is, in general, dictated by the level of stochasticity in the encoder: the noisier the encoding process is, the higher the number of datapoints which can plausibly give rise to a particular encoding.b.The aggregate encoding q\u03d5(\ud835\udc9b)subscript\ud835\udc5eitalic-\u03d5\ud835\udc9bq_{{\\phi}}\\left(\\bm{z}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) matching the prior p(\ud835\udc9b)\ud835\udc5d\ud835\udc9bp\\left(\\bm{z}\\right)italic_p ( bold_italic_z ), where the latter expresses the desired dependency structure between latents.", "The overlap factor\u00a0Item\u00a0a is perhaps best understood by considering extremes\u2014too little, and the latents effectively become a lookup table; too much, and the data and latents do not convey information about each other.In either case, meaningfulness of the latent encodings is lost.Thus, without the appropriate level of overlap\u2014dictated both by noise in the true generative process and dataset size\u2014it is not possible to enforce meaningful structure on the latent space.Though quantitatively formalising overlap in general scenarios is surprisingly challenging (c.f.\u00a0\u00a0\u00a7\u00a7\u00a07 and\u00a0D), we note for now that when the encoder distribution is unimodal, it is typically well-characterized by the mutual information between the data and the latents\u00a0I(\ud835\udc99;\ud835\udc9b)\ud835\udc3c\ud835\udc99\ud835\udc9bI(\\bm{x};\\bm{z})italic_I ( bold_italic_x ; bold_italic_z ).", "The regularisation factor\u00a0Item\u00a0b enforces a congruence between the (aggregate) latent embeddings of data and the dependency structures expressed in the prior.We posit that such structure is best expressed in the prior, as opposed to explicit independence regularisation of the marginal posterior\u00a0[25, 8], to enable the generative model to express the desired decomposition, and to avoid potentially violating self-consistency between the encoder, decoder, and true datagenerating distributions.The prior also provides a rich and flexible means of expressing desired structure by defining a generative process that encapsulates dependencies between variables, as with a graphical model.", "Critically, neither factor is sufficient in isolation.An inappropriate level of overlap in the latent space will impede interpretability, irrespective of quality of regularisation, as the latent space need not be meaningful.Conversely, without the pressure to regularise to the prior, the latent space is under no constraint to exhibit the desired structure.", "Decomposition is inherently subjective as we must choose the structure of the prior we regularise to depending on how we intend to use our learned model or what kind of features we would like to uncover from the data.This may at first seem unsatisfactory compared to the seemingly objective adjustments often made to the elbo by disentanglement methods.However, disentanglement is itself a subjective choice for the decomposition.We can embrace this subjective nature through judicious choices of the prior distribution; ignoring this imposes unintended assumptions which can have unwanted effects.For example, as we will later show, the rotational invariance of the standard prior p(\ud835\udc9b)=\ud835\udca9(\ud835\udc9b;0,I)\ud835\udc5d\ud835\udc9b\ud835\udca9\ud835\udc9b0\ud835\udc3cp(\\bm{z})=\\mathcal{N}(\\bm{z};0,I)italic_p ( bold_italic_z ) = caligraphic_N ( bold_italic_z ; 0 , italic_I ) can actually hinder disentanglement.", "To connect existing approaches to our proposed framework, we now consider, as a case study, the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae\u00a0[17]\u2014an adaptation of the vae objective (elbo) to learn better-disentangled representations.Specifically, it scales the KL term in the standard ELBO by a factor \u03b2>0\ud835\udefd0\\beta>0italic_\u03b2 > 0 as\u2112\u03b2(\ud835\udc99)=\ud835\udd3cq\u03d5(\ud835\udc9b|\ud835\udc99)\u2061[log\u2061p\u03b8(\ud835\udc99|\ud835\udc9b)]\u2212\u03b2KL\u2061(q\u03d5(\ud835\udc9b|\ud835\udc99)\u2225p(\ud835\udc9b)).subscript\u2112\ud835\udefd\ud835\udc99subscript\ud835\udd3csubscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99subscript\ud835\udc5d\ud835\udf03conditional\ud835\udc99\ud835\udc9b\ud835\udefdKLconditionalsubscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99\ud835\udc5d\ud835\udc9b\\displaystyle\\mathcal{L}_{\\beta}(\\bm{x})\\!=\\!\\operatorname{{}\\mathbb{E}}_{q_{{\\phi}}\\left(\\bm{z}|\\bm{x}\\right)\\!}\\left[\\log p_{{\\theta}}\\left(\\bm{x}|\\bm{z}\\right)\\right]\\!-\\!\\beta\\operatorname{\\scalebox{0.95}{\\text{KL}}}\\left(q_{{\\phi}}\\left(\\bm{z}|\\bm{x}\\right)\\!\\,\\|\\;\\!p\\left(\\bm{z}\\right)\\!\\right).\\!\\!caligraphic_L start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT ( bold_italic_x ) = blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_z ) ] - italic_\u03b2 DKL ( italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) \u2225 italic_p ( bold_italic_z ) ) .(2)Hoffman et\u00a0al. [21] showed that the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae target can be viewed as a standard elbo with the alternative prior r(\ud835\udc9b)\u221dq\u03d5(\ud835\udc9b)(1\u2212\u03b2)p(\ud835\udc9b)\u03b2proportional-to\ud835\udc5f\ud835\udc9bsubscript\ud835\udc5eitalic-\u03d5superscript\ud835\udc9b1\ud835\udefd\ud835\udc5dsuperscript\ud835\udc9b\ud835\udefdr(\\bm{z})\\propto q_{{\\phi}}\\left(\\bm{z}\\right)^{(1-\\beta)}p(\\bm{z})^{\\beta}italic_r ( bold_italic_z ) \u221d italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) start_POSTSUPERSCRIPT ( 1 - italic_\u03b2 ) end_POSTSUPERSCRIPT italic_p ( bold_italic_z ) start_POSTSUPERSCRIPT italic_\u03b2 end_POSTSUPERSCRIPT, along with terms involving the mutual information and the prior\u2019s normalising constant.", "We now introduce an alternate deconstruction as follows", "Clearly, the second term in\u00a0Eq.\u00a03, enforcing a maximum entropy regulariser on the posterior\u00a0q\u03d5(\ud835\udc9b\u2223\ud835\udc99)subscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99q_{{\\phi}}\\left(\\bm{z}\\mid\\bm{x}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z \u2223 bold_italic_x ), allows the value of\u00a0\u03b2\ud835\udefd\\betaitalic_\u03b2 to affect the overlap of encodings in the latent space.We thus see that it provides a means of controlling decomposition factor (a).However, it is itself not sufficient to enforce disentanglement.For example, the entropy of q\u03d5(\ud835\udc9b\u2223\ud835\udc99)subscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99q_{{\\phi}}\\left(\\bm{z}\\mid\\bm{x}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z \u2223 bold_italic_x ) is independent of its mean \u03bc\u03b8(\ud835\udc99)subscript\ud835\udf07\ud835\udf03\ud835\udc99\\mu_{\\theta}(\\bm{x})italic_\u03bc start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x ) and is independent to rotations of \ud835\udc9b\ud835\udc9b\\bm{z}bold_italic_z, so it is clearly incapable of discouraging certain representations with poor disentanglement.All the same, having the wrong level of regularization can, in turn, lead to an inappropriate level of overlap and undermine the ability to disentangle. Consequently, this term is still important.", "Although the precise impact of prior annealing depends on the original form of the prior, the high-level effect is the same\u2014larger values of \u03b2\ud835\udefd\\betaitalic_\u03b2 cause the effective latent space to collapse towards the modes of the prior.For uni-modal priors, the main effect of annealing is to reduce the scaling of \ud835\udc9b\ud835\udc9b\\bm{z}bold_italic_z; indeed this is the only effect for generalized Gaussian distributions.While this would appear not to have any tangible effects, closer inspection suggests otherwise\u2014it ensures that the scaling of the encodings matches that of the prior.Only incorporating the maximum-entropy regularisation will simply cause the scaling of the latent space to increase.The rescaling of the prior now cancels this effect, ensuring the scaling of q\u03d5(\ud835\udc9b)subscript\ud835\udc5eitalic-\u03d5\ud835\udc9bq_{{\\phi}}\\left(\\bm{z}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) matches that of p(\ud835\udc9b)\ud835\udc5d\ud835\udc9bp(\\bm{z})italic_p ( bold_italic_z ).", "Taken together, this implies that the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae\u2019s ability to encourage disentanglement is predominantly through direct control over the level of overlap.It places no other direct constraint on the latents to disentangle (although in some cases, the annealed prior may inadvertently encourage better disentanglement), but instead helps avoid the pitfalls of inappropriate overlap.Amongst other things, this explains why large \u03b2\ud835\udefd\\betaitalic_\u03b2 is not universally beneficial for disentanglement, as the level of overlap can be increased too far.", "We can gain further insights into the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae in the common use case\u2014assuming a Gaussian prior, p(\ud835\udc9b)=\ud835\udca9(\ud835\udc9b;0,\u03a3)\ud835\udc5d\ud835\udc9b\ud835\udca9\ud835\udc9b0\u03a3p(\\bm{z})=\\mathcal{N}(\\bm{z};0,\\Sigma)italic_p ( bold_italic_z ) = caligraphic_N ( bold_italic_z ; 0 , roman_\u03a3 ), and Gaussian encoder, q\u03d5(\ud835\udc9b\u2223\ud835\udc99)=\ud835\udca9(\ud835\udc9b;\u03bc\u03d5(\ud835\udc99),S\u03d5(\ud835\udc99))subscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99\ud835\udca9\ud835\udc9bsubscript\ud835\udf07italic-\u03d5\ud835\udc99subscript\ud835\udc46italic-\u03d5\ud835\udc99q_{{\\phi}}\\left(\\bm{z}\\mid\\bm{x}\\right)=\\mathcal{N}\\left(\\bm{z};\\mu_{\\phi}(\\bm{x}),S_{\\phi}(\\bm{x})\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z \u2223 bold_italic_x ) = caligraphic_N ( bold_italic_z ; italic_\u03bc start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_x ) , italic_S start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_x ) ).Here it is straightforward to see that annealing simply scales the latent space by 1/\u03b21\ud835\udefd1/\\sqrt{\\beta}1 / square-root start_ARG italic_\u03b2 end_ARG, i.e. f\u03b2(\ud835\udc9b)=\ud835\udca9(\ud835\udc9b;0,\u03a3/\u03b2)subscript\ud835\udc53\ud835\udefd\ud835\udc9b\ud835\udca9\ud835\udc9b0\u03a3\ud835\udefdf_{\\beta}(\\bm{z})=\\mathcal{N}(\\bm{z};0,\\Sigma/\\beta)italic_f start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT ( bold_italic_z ) = caligraphic_N ( bold_italic_z ; 0 , roman_\u03a3 / italic_\u03b2 ).Given this, it is easy to see that a vae trained with the adjusted target \u2112(\ud835\udc99;\u03c0\u03b8,\u03b2,q\u03d5)\u2112\ud835\udc99subscript\ud835\udf0b\ud835\udf03\ud835\udefdsubscript\ud835\udc5eitalic-\u03d5\\mathcal{L}\\left(\\bm{x};\\pi_{\\theta,\\beta},q_{\\phi}\\right)caligraphic_L ( bold_italic_x ; italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 , italic_\u03b2 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ), but appropriately scaling the latent space, will behave identically to one trained with the original target \u2112(\ud835\udc99)\u2112\ud835\udc99\\mathcal{L}(\\bm{x})caligraphic_L ( bold_italic_x ).It will also have an identical elbo as the expected reconstruction is trivially the same, while the kl between Gaussians is invariant to scaling both equally.More precisely, we have the following result.", "Noting that as c\ud835\udc50citalic_c is irrelevant to the training process, this indicates an equivalence, up to scaling of the latent space, between training with the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae objective and a maximum-entropy regularised version of the standard ELBO\u2112H,\u03b2(\ud835\udc99)\u225c\u2112(\ud835\udc99)+(\u03b2\u22121)2log\u2061|S\u03d5(\ud835\udc99)|,\u225csubscript\u2112\ud835\udc3b\ud835\udefd\ud835\udc99\u2112\ud835\udc99\ud835\udefd12subscript\ud835\udc46italic-\u03d5\ud835\udc99\\displaystyle\\mathcal{L}_{H,\\beta}(\\bm{x})\\triangleq\\mathcal{L}(\\bm{x})+\\frac{(\\beta-1)}{2}\\log\\lvert S_{\\phi}(\\bm{x})\\rvert,caligraphic_L start_POSTSUBSCRIPT italic_H , italic_\u03b2 end_POSTSUBSCRIPT ( bold_italic_x ) \u225c caligraphic_L ( bold_italic_x ) + divide start_ARG ( italic_\u03b2 - 1 ) end_ARG start_ARG 2 end_ARG roman_log | italic_S start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_x ) | ,(5)whenever p(\ud835\udc9b)\ud835\udc5d\ud835\udc9bp\\left(\\bm{z}\\right)italic_p ( bold_italic_z ) and q\u03d5(\ud835\udc9b\u2223\ud835\udc99)subscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99q_{{\\phi}}\\left(\\bm{z}\\mid\\bm{x}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z \u2223 bold_italic_x ) are Gaussian.Note that we implicitly presume suitable adjustment of neural-network hyper-parameters and the stochastic gradient scheme to account for the change of scaling in the optimal networks.", "Moreover, the stationary points for the two objectives \u2112\u03b2(\ud835\udc99;\u03b8,\u03d5)subscript\u2112\ud835\udefd\ud835\udc99\ud835\udf03italic-\u03d5\\mathcal{L}_{\\beta}(\\bm{x};\\theta,\\phi)caligraphic_L start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT ( bold_italic_x ; italic_\u03b8 , italic_\u03d5 ) and \u2112H,\u03b2(\ud835\udc99;\u03b8\u2032,\u03d5\u2032)subscript\u2112\ud835\udc3b\ud835\udefd\ud835\udc99superscript\ud835\udf03\u2032superscriptitalic-\u03d5\u2032\\mathcal{L}_{H,\\beta}\\left(\\bm{x};\\theta^{\\prime},\\phi^{\\prime}\\right)caligraphic_L start_POSTSUBSCRIPT italic_H , italic_\u03b2 end_POSTSUBSCRIPT ( bold_italic_x ; italic_\u03b8 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_\u03d5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) are equivalent (c.f.\u00a0Corollary 2 in Appendix\u00a0A), indicating that optimising for\u00a0(5) leads to networks equivalentto those from optimising the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae objective\u00a0Eq.\u00a02, up to scaling the encodings by a factor of \u03b2\ud835\udefd\\sqrt{\\beta}square-root start_ARG italic_\u03b2 end_ARG.Under the isotropic Gaussian prior setting, we further have the following result showing that the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae objective is invariant to rotations of the latent space.", "This shows that the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae objective does not directly encourage latent variables to take on meaningful representations when using the standard choice of an isotropic Gaussian prior.In fact, on its own, it encourages latent representations which match the true generative factors no more than it encourages any arbitrary rotation of these factors, with such rotations capable of exhibiting strong correlations between latents.This view is further supported by our empirical results (see\u00a0Figure\u00a02), where we did not observe any gains in disentanglement (using the metric from Kim and Mnih [25]) from increasing \u03b2>0\ud835\udefd0\\beta>0italic_\u03b2 > 0 with an isotropic Gaussian prior trained on the 2D Shapes dataset\u00a0[38].It may also go some way to explaining the extremely high levels of variation we found in the disentanglement-metric scores between different random seeds at train time.", "It should be noted, however, that the value of \u03b2\ud835\udefd\\betaitalic_\u03b2 can indirectly influence the level of disentanglement when using a mean-field assumption for the encoder distribution (i.e. restricting S\u03d5(x)subscript\ud835\udc46italic-\u03d5\ud835\udc65S_{\\phi}(x)italic_S start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_x ) to be diagonal).As noted by\u00a0St\u00fchmer et\u00a0al. [52], Rolinek et\u00a0al. [49], increasing \u03b2\ud835\udefd\\betaitalic_\u03b2 can reinforce existing inductive biases, wherein mean-field assumptions encourage representations which reduce dependence between the latent dimensions\u00a0[55].", "Given the characterisation set out above, we now develop an objective that incorporates the effect of both factors\u00a0(a) and\u00a0(b).Our analysis of the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae tells us that its objective allows direct control over the level of overlap, i.e.\u00a0factor\u00a0Item\u00a0a.To incorporate direct control over the regularisation\u00a0Item\u00a0b between the marginal posterior and the prior, we add a divergence term\u00a0\ud835\udd3b(q\u03d5(z),p(\ud835\udc9b))\ud835\udd3bsubscript\ud835\udc5eitalic-\u03d5\ud835\udc67\ud835\udc5d\ud835\udc9b\\mathbb{D}(q_{{\\phi}}\\left(z\\right),p(\\bm{z}))blackboard_D ( italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_z ) , italic_p ( bold_italic_z ) ), yielding\u2112\u03b1,\u03b2(\ud835\udc99)=\ud835\udd3cq\u03d5(\ud835\udc9b\u2223\ud835\udc99)\u2061[log\u2061p\u03b8(\ud835\udc99\u2223\ud835\udc9b)]\u2212\u03b2KL\u2061(q\u03d5(\ud835\udc9b\u2223\ud835\udc99)\u2225p(\ud835\udc9b))\u2212\u03b1\ud835\udd3b(q\u03d5(\ud835\udc9b),p(\ud835\udc9b))subscript\u2112\ud835\udefc\ud835\udefd\ud835\udc99subscript\ud835\udd3csubscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99subscript\ud835\udc5d\ud835\udf03conditional\ud835\udc99\ud835\udc9b\ud835\udefdKLconditionalsubscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99\ud835\udc5d\ud835\udc9b\ud835\udefc\ud835\udd3bsubscript\ud835\udc5eitalic-\u03d5\ud835\udc9b\ud835\udc5d\ud835\udc9b\\displaystyle\\begin{split}\\mathcal{L}_{\\alpha,\\beta}&(\\bm{x})=\\operatorname{{}\\mathbb{E}}_{q_{{\\phi}}\\left(\\bm{z}\\mid\\bm{x}\\right)}\\left[\\log p_{{\\theta}}\\left(\\bm{x}\\mid\\bm{z}\\right)\\right]\\\\&-\\beta~{}\\operatorname{\\scalebox{0.95}{\\text{KL}}}\\left(q_{{\\phi}}\\left(\\bm{z}\\mid\\bm{x}\\right)\\,\\|\\;p(\\bm{z})\\right)-\\alpha~{}\\mathbb{D}(q_{{\\phi}}\\left(\\bm{z}\\right),p(\\bm{z}))\\end{split}start_ROW start_CELL caligraphic_L start_POSTSUBSCRIPT italic_\u03b1 , italic_\u03b2 end_POSTSUBSCRIPT end_CELL start_CELL ( bold_italic_x ) = blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z \u2223 bold_italic_x ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x \u2223 bold_italic_z ) ] end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL - italic_\u03b2 DKL ( italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z \u2223 bold_italic_x ) \u2225 italic_p ( bold_italic_z ) ) - italic_\u03b1 blackboard_D ( italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) , italic_p ( bold_italic_z ) ) end_CELL end_ROW(7)allowing control over how much factors (a) and\u00a0(b) are enforced, through appropriate setting of \u03b2\ud835\udefd\\betaitalic_\u03b2 and \u03b1\ud835\udefc\\alphaitalic_\u03b1 respectively.", "Note that such an additional term has been previously considered by Kumar et\u00a0al. [30], with \ud835\udd3b(q\u03d5(\ud835\udc9b),p(\ud835\udc9b))=KL\u2061(q\u03d5(\ud835\udc9b)\u2225p(\ud835\udc9b))\ud835\udd3bsubscript\ud835\udc5eitalic-\u03d5\ud835\udc9b\ud835\udc5d\ud835\udc9bKLconditionalsubscript\ud835\udc5eitalic-\u03d5\ud835\udc9b\ud835\udc5d\ud835\udc9b\\mathbb{D}(q_{{\\phi}}\\left(\\bm{z}\\right),p(\\bm{z}))=\\operatorname{\\scalebox{0.95}{\\text{KL}}}\\left(q_{{\\phi}}\\left(\\bm{z}\\right)\\,\\|\\;p(\\bm{z})\\right)blackboard_D ( italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) , italic_p ( bold_italic_z ) ) = DKL ( italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) \u2225 italic_p ( bold_italic_z ) ), although for the sake of tractability they rely instead on moment matching using covariances.There have also been a number of approaches that decompose the standard vae objective in different ways [e.g.\u00a0 20, 16, 13] to expose KL\u2061(q\u03d5(\ud835\udc9b)\u2225p(\ud835\udc9b))KLconditionalsubscript\ud835\udc5eitalic-\u03d5\ud835\udc9b\ud835\udc5d\ud835\udc9b\\operatorname{\\scalebox{0.95}{\\text{KL}}}\\left(q_{{\\phi}}\\left(\\bm{z}\\right)\\,\\|\\;p(\\bm{z})\\right)DKL ( italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) \u2225 italic_p ( bold_italic_z ) ) as a component, but, as we discuss in Appendix\u00a0C, this can be difficult to compute correctly in practice, with common approaches leading to highly biased estimates whose practical behaviour is very different than the divergence they are estimating, unless very large batch sizes are used.", "Wasserstein Auto-Encoders\u00a0[53] formulate an objective that includes a general divergence term between the prior and marginal posterior, computed using either maximum mean discrepancy (mmd) or a variational formulation of the Jensen-Shannon divergence (a.k.a gan loss).However, we find that empirically, choosing the mmd\u2019s kernel and numerically stabilising its U-statistics estimator to be tricky, and designing and learning a gan to be cumbersome and unstable.Consequently, the problems of choosing an appropriate \ud835\udd3b(q\u03d5(\ud835\udc9b),p(\ud835\udc9b))\ud835\udd3bsubscript\ud835\udc5eitalic-\u03d5\ud835\udc9b\ud835\udc5d\ud835\udc9b\\mathbb{D}(q_{{\\phi}}\\left(\\bm{z}\\right),p(\\bm{z}))blackboard_D ( italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) , italic_p ( bold_italic_z ) ) and generating reliable estimates for this choice are tightly coupled, with a general purpose solution remaining an important open problem; see further discussion in Appendix\u00a0C.", "We first show how subtle changes to the prior distribution can yield improvements in disentanglement.The standard choice of an isotropic Gaussian has previously been justified by the correct assertion that the latents are independent under the prior\u00a0[17].However, as explained in\u00a0\u00a7\u00a04.1, the rotational invariance of this prior means that it does not directly encourage axis-aligned representations.Priors that break this rotational invariance should be better suited for learning disentangled representations.We assess this hypothesis by training a \u03b2\ud835\udefd\\betaitalic_\u03b2-vae (i.e.\u00a0(7) with \u03b1=0\ud835\udefc0\\alpha=0italic_\u03b1 = 0) on the 2D Shapes dataset\u00a0[38] and evaluating disentanglement using the metric of\u00a0Kim and Mnih [25].", "Figure\u00a02 demonstrates that notable improvements in disentanglement can be achieved by using non-isotropic priors:for a given reconstruction loss, implicitly fixed by \u03b2\ud835\udefd\\betaitalic_\u03b2, non-isotropic Gaussian priors got better disentanglement scores, with further improvement achieved when the prior variance is learnt.With a product of Student-t priors p\u03bd(\ud835\udc9b)subscript\ud835\udc5d\ud835\udf08\ud835\udc9bp_{\\nu}(\\bm{z})italic_p start_POSTSUBSCRIPT italic_\u03bd end_POSTSUBSCRIPT ( bold_italic_z ) (noting p\u03bd(\ud835\udc9b)\u2192\ud835\udca9(\ud835\udc9b;\ud835\udfce,\ud835\udc08)\u2192subscript\ud835\udc5d\ud835\udf08\ud835\udc9b\ud835\udca9\ud835\udc9b0\ud835\udc08p_{\\nu}(\\bm{z})\\rightarrow\\mathcal{N}(\\bm{z};\\mathbf{0},\\mathbf{I})italic_p start_POSTSUBSCRIPT italic_\u03bd end_POSTSUBSCRIPT ( bold_italic_z ) \u2192 caligraphic_N ( bold_italic_z ; bold_0 , bold_I ) as \u03bd\u2192\u221e\u2192\ud835\udf08\\nu\\rightarrow\\inftyitalic_\u03bd \u2192 \u221e), reducing\u00a0\u03bd\ud835\udf08\\nuitalic_\u03bd only incurred a minor reconstruction penalty, for improved disentanglement.Interestingly,very low values of\u00a0\u03bd\ud835\udf08\\nuitalic_\u03bd caused the disentanglement score to drop again (though still giving higher values than the Gaussian).We speculate that this may be related to the effect of heavy tails on the disentanglement metric itself, rather than being an objectively worse disentanglement.Another interesting result was that for an isotropic Gaussian prior, as per the original \u03b2\ud835\udefd\\betaitalic_\u03b2-vae setup, no gains at all were achieved in disentanglement by increasing \u03b2\ud835\udefd\\betaitalic_\u03b2.", "We next consider an alternative decomposition one might wish to impose\u2014clustering of the latent space.For this, we use the \u201cpinwheels\u201d dataset from\u00a0[24] and a mixture of four equally-weighted Gaussians as our prior.We then conduct an ablation study to observe the effect of varying \u03b1\ud835\udefc\\alphaitalic_\u03b1 and \u03b2\ud835\udefd\\betaitalic_\u03b2 in \u2112\u03b1,\u03b2(\ud835\udc31)subscript\u2112\ud835\udefc\ud835\udefd\ud835\udc31\\mathcal{L}_{\\alpha,\\beta}(\\mathbf{x})caligraphic_L start_POSTSUBSCRIPT italic_\u03b1 , italic_\u03b2 end_POSTSUBSCRIPT ( bold_x ) (as per\u00a0(7)) on the learned representations, taking the divergence to be KL(p(\ud835\udc9b)||q\u03d5(\ud835\udc9b))\\text{KL}\\left(p(\\bm{z})||q_{{\\phi}}\\left(\\bm{z}\\right)\\right)KL ( italic_p ( bold_italic_z ) | | italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) ) (see Appendix\u00a0B for details).", "We see in Figure\u00a03 that increasing \u03b2\ud835\udefd\\betaitalic_\u03b2 increases the level of overlap in q\u03d5(\ud835\udc9b)subscript\ud835\udc5eitalic-\u03d5\ud835\udc9bq_{{\\phi}}\\left(\\bm{z}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ), as a consequence of increasing the encoder variance for individual datapoints.When \u03b2\ud835\udefd\\betaitalic_\u03b2 is too large, the encoding of a datapoint loses meaning.Also, as a single datapoint encodes to a Gaussian distribution, q\u03d5(\ud835\udc9b|\ud835\udc99)subscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99q_{{\\phi}}\\left(\\bm{z}|\\bm{x}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) is unable to match p(\ud835\udc9b)\ud835\udc5d\ud835\udc9bp\\left(\\bm{z}\\right)italic_p ( bold_italic_z ) exactly.Because q\u03d5(\ud835\udc9b|\ud835\udc99)\u2192q\u03d5(\ud835\udc9b)\u2192subscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99subscript\ud835\udc5eitalic-\u03d5\ud835\udc9bq_{{\\phi}}\\left(\\bm{z}|\\bm{x}\\right)\\rightarrow q_{{\\phi}}\\left(\\bm{z}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) \u2192 italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) when \u03b2\u2192\u221e\u2192\ud835\udefd\\beta\\rightarrow\\inftyitalic_\u03b2 \u2192 \u221e, this in turn means thatoverly large values of \u03b2\ud835\udefd\\betaitalic_\u03b2 actually cause a mismatch between q\u03d5(\ud835\udc9b)subscript\ud835\udc5eitalic-\u03d5\ud835\udc9bq_{{\\phi}}\\left(\\bm{z}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) and p(\ud835\udc9b)\ud835\udc5d\ud835\udc9bp\\left(\\bm{z}\\right)italic_p ( bold_italic_z ) (see top right of Figure\u00a03).Increasing \u03b1\ud835\udefc\\alphaitalic_\u03b1, instead always improved the match between q\u03d5(\ud835\udc9b)subscript\ud835\udc5eitalic-\u03d5\ud835\udc9bq_{{\\phi}}\\left(\\bm{z}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z ) and p(\ud835\udc9b)\ud835\udc5d\ud835\udc9bp\\left(\\bm{z}\\right)italic_p ( bold_italic_z ).Here, the finiteness of the dataset and the choice of divergence results in an increase in overlap with increasing \u03b1\ud835\udefc\\alphaitalic_\u03b1, but only up to the level required for a non-negligible overlap between the nearby datapoints: large values of \u03b1\ud835\udefc\\alphaitalic_\u03b1 did not cause the encodings to collapse to a mode.", "Finally, we consider a commonly desired decomposition\u2014sparsity, which stipulates that only a small fraction of available factors are employed.That is, a sparse representation [39] can be thought of as one where each embedding has a significant proportion of its dimensions off, i.e. close to 00.Sparsity has often been considered for feature-learning [31, 12] and employed in the probabilistic modelling literature [45, 32].", "Common ways to achieve sparsity are through a specific penalty (e.g. l1subscript\ud835\udc591l_{1}italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) or a careful choice of prior (peaked at 0).Concomitant with our overarching desire to encode requisite structure in the prior, we adopt the latter, constructing a sparse prior asp(\ud835\udc9b)=\u220fd(1\u2212\u03b3)\ud835\udca9(zd;0,1)+\u03b3\ud835\udca9(zd;0,\u03c302)\ud835\udc5d\ud835\udc9bsubscriptproduct\ud835\udc511\ud835\udefe\ud835\udca9subscript\ud835\udc67\ud835\udc5101\ud835\udefe\ud835\udca9subscript\ud835\udc67\ud835\udc510superscriptsubscript\ud835\udf0e02p(\\bm{z})=\\prod\\nolimits_{d}~{}(1-\\gamma)~{}\\mathcal{N}(z_{d};0,1)+\\gamma~{}\\mathcal{N}(z_{d};0,\\sigma_{0}^{2})italic_p ( bold_italic_z ) = \u220f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( 1 - italic_\u03b3 ) caligraphic_N ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ; 0 , 1 ) + italic_\u03b3 caligraphic_N ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ; 0 , italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )with \u03c302=0.05superscriptsubscript\ud835\udf0e020.05\\sigma_{0}^{2}=0.05italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.05.This mixture distribution can be interpreted as a mixture of samples being either off or on, whose proportion is set by the weight parameter \u03b3\ud835\udefe\\gammaitalic_\u03b3.We use this prior to learn a vae for the Fashion-MNIST dataset\u00a0[56] using the objective \u2112\u03b1,\u03b2(\ud835\udc31)subscript\u2112\ud835\udefc\ud835\udefd\ud835\udc31\\mathcal{L}_{\\alpha,\\beta}(\\mathbf{x})caligraphic_L start_POSTSUBSCRIPT italic_\u03b1 , italic_\u03b2 end_POSTSUBSCRIPT ( bold_x ) (as per\u00a0(7)), taking the divergence to be an mmd with a kernel that only considers difference between the marginal distributions (see Appendix\u00a0B for details).", "We measure a representation\u2019s sparsity using the Hoyer extrinsic metric\u00a0[22].For \ud835\udc9a\u2208\u211dd\ud835\udc9asuperscript\u211d\ud835\udc51\\bm{y}\\in\\mathbb{R}^{d}bold_italic_y \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT,Hoyer(\ud835\udc9a)=d\u2212\u2016\ud835\udc9a\u20161/\u2016\ud835\udc9a\u20162d\u22121\u2208[0,1],Hoyer\ud835\udc9a\ud835\udc51subscriptnorm\ud835\udc9a1subscriptnorm\ud835\udc9a2\ud835\udc51101\\displaystyle\\text{Hoyer}~{}(\\bm{y})=\\frac{\\sqrt{d}-\\|\\bm{y}\\|_{1}/\\|\\bm{y}\\|_{2}}{\\sqrt{d}-1}\\in[0,1],Hoyer ( bold_italic_y ) = divide start_ARG square-root start_ARG italic_d end_ARG - \u2225 bold_italic_y \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT / \u2225 bold_italic_y \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG square-root start_ARG italic_d end_ARG - 1 end_ARG \u2208 [ 0 , 1 ] ,yielding 00 for a fully dense vector and 1111 for a fully sparse vector.Rather than employing this metric directly to the mean encoding of each datapoint, we first normalise each dimension to have a standard deviation of 1111 under its aggregate distribution, i.e. we use z\u00afd=zd/\u03c3(zd)subscript\u00af\ud835\udc67\ud835\udc51subscript\ud835\udc67\ud835\udc51\ud835\udf0esubscript\ud835\udc67\ud835\udc51\\bar{z}_{d}=z_{d}/\\sigma(z_{d})over\u00af start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT / italic_\u03c3 ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) where \u03c3(zd)\ud835\udf0esubscript\ud835\udc67\ud835\udc51\\sigma(z_{d})italic_\u03c3 ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) is the standard deviation of dimension d\ud835\udc51ditalic_d of the latent encoding taken over the dataset.This normalisation is important as one could achieve a \u201csparse\u201d representation simply by having different dimensions vary along different length scales (something the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae encourages through its pruning of dimensions\u00a0[52]), whereas we desire a representation where different datapoints \u201cactivate\u201d different features.We then compute overall sparsity by averaging over the dataset as Sparsity=1n\u2211inHoyer(\ud835\udc9b\u00afi)Sparsity1\ud835\udc5bsuperscriptsubscript\ud835\udc56\ud835\udc5bHoyersubscript\u00af\ud835\udc9b\ud835\udc56\\text{Sparsity}=\\frac{1}{n}\\sum\\nolimits_{i}^{n}\\text{Hoyer}~{}(\\bar{\\bm{z}}_{i})Sparsity = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT Hoyer ( over\u00af start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).Figure\u00a04 (left) shows that substantial sparsity can be gained by replacing a Gaussian prior (\u03b3=0\ud835\udefe0\\gamma=0italic_\u03b3 = 0) by a sparse prior (\u03b3=0.8\ud835\udefe0.8\\gamma=0.8italic_\u03b3 = 0.8).It further shows substantial gains from the inclusion of the aggregate posterior regularization, with \u03b1=0\ud835\udefc0\\alpha=0italic_\u03b1 = 0 giving far low sparsity than \u03b1>0\ud835\udefc0\\alpha>0italic_\u03b1 > 0, when using our sparse prior.The use of our sparse prior did not generally harm the reconstruction compared.Large values of \u03b1\ud835\udefc\\alphaitalic_\u03b1 did slightly worsen the reconstruction, but this drop-off was much slower than increases in \u03b2\ud835\udefd\\betaitalic_\u03b2 (note that \u03b1\ud835\udefc\\alphaitalic_\u03b1 is increased to much higher levels than \u03b2\ud835\udefd\\betaitalic_\u03b2).Interestingly, we see that \u03b2\ud835\udefd\\betaitalic_\u03b2 being either too low or too high also harmed the sparsity.", "We explore the qualitative effects of sparsity in Figure\u00a05, using a network trained with \u03b1=1000,\u03b2=1,formulae-sequence\ud835\udefc1000\ud835\udefd1\\alpha=1000,\\beta=1,italic_\u03b1 = 1000 , italic_\u03b2 = 1 , and \u03b3=0.8\ud835\udefe0.8\\gamma=0.8italic_\u03b3 = 0.8, corresponding to one of the models in Figure\u00a04\u00a0(left).The top plot shows the average encoding magnitude for data corresponding to 3 of the 10 classes in the Fashion-MNIST dataset.It clearly shows that the different classes (trousers, dress, and shirt) predominantly encode information along different sets of dimensions, as expected for sparse representations (c.f.\u00a0Appendix\u00a0B for plots for all classes).For each of these classes, we explore the latent space along a particular \u2018active\u2019 dimension\u2014one with high average encoding magnitude\u2014to observe if they capture meaningful features in the image.We first identify a suitable \u2018active\u2019 dimension for a given instance (top row) from the dimension-wise magnitudes of its encoding, by choosing one, say\u00a0d\ud835\udc51ditalic_d, where the magnitude far exceeds \u03c302superscriptsubscript\ud835\udf0e02\\sigma_{0}^{2}italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT.Given encoding value\u00a0\ud835\udc9bdsubscript\ud835\udc9b\ud835\udc51\\bm{z}_{d}bold_italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, we then interpolate along this dimension (keeping all others fixed) in the range (\ud835\udc9bd,\ud835\udc9bd+sign(\ud835\udc9bd))subscript\ud835\udc9b\ud835\udc51subscript\ud835\udc9b\ud835\udc51signsubscript\ud835\udc9b\ud835\udc51(\\bm{z}_{d},\\bm{z}_{d}+\\mathrm{sign}(\\bm{z}_{d}))( bold_italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , bold_italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT + roman_sign ( bold_italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ); the sign of\u00a0\ud835\udc9bdsubscript\ud835\udc9b\ud835\udc51\\bm{z}_{d}bold_italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT indicating the direction of interpolation.Exploring the latent space in such a manner demonstrates a variety of consistent feature transformations in the image, both within class (a, b, c), and across classes (d), indicating that these sparse dimensions do capture meaningful features in the image.", "Concurrent to our work,\u00a0Tonolini et\u00a0al. [54] also considered imposing sparsity in vaes with a spike-slab prior (such that \u03c30\u21920\u2192subscript\ud835\udf0e00\\sigma_{0}\\rightarrow 0italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2192 0).In contrast to our work, they do not impose a constraint on the aggregateencoder, nor do they evaluate their results with a quantitative sparsity metric that accounts for the varying length scales of different latent dimensions", "Precisely formalising what constitutes the level of overlap in the latent space is surprisingly subtle.Prior work has typically instead considered controlling the level of compression through the mutual information between data and latents\u00a0I(\ud835\udc99;\ud835\udc9b)\ud835\udc3c\ud835\udc99\ud835\udc9bI(\\bm{x};\\bm{z})italic_I ( bold_italic_x ; bold_italic_z )\u00a0[3, 2, 20, 41], with, for example,\u00a0[41] going on to discuss how controlling the compression can \u201cexplicitly encourage useful representations.\u201dAlthough\u00a0I(\ud835\udc99;\ud835\udc9b)\ud835\udc3c\ud835\udc99\ud835\udc9bI(\\bm{x};\\bm{z})italic_I ( bold_italic_x ; bold_italic_z ) provides a perfectly serviceable characterisation of overlap in a number of cases, the two are not universally equivalent and we argue that it is the latter which is important in achieving useful representations.In particular, if the form of the encoding distribution is not fixed\u2014as when employing normalising flows, for example\u2014I(\ud835\udc99;\ud835\udc9b)\ud835\udc3c\ud835\udc99\ud835\udc9bI(\\bm{x};\\bm{z})italic_I ( bold_italic_x ; bold_italic_z ) does not necessarily characterise overlap well.We discuss this in greater detail in Appendix\u00a0D.", "However, when the encoder is unimodal with fixed form (in particularly the tail behaviour is fixed) and the prior is well-characterised by Euclidean distances, then these factors have a substantially reduced ability to vary for a given I(\ud835\udc99;\ud835\udc9b)\ud835\udc3c\ud835\udc99\ud835\udc9bI(\\bm{x};\\bm{z})italic_I ( bold_italic_x ; bold_italic_z ), which subsequently becomes a good characterisation of the level of overlap.When q\u03d5(\ud835\udc9b|\ud835\udc99)subscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99q_{{\\phi}}\\left(\\bm{z}|\\bm{x}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) is Gaussian, controlling the variance of q\u03d5(\ud835\udc9b|\ud835\udc99)subscript\ud835\udc5eitalic-\u03d5conditional\ud835\udc9b\ud835\udc99q_{{\\phi}}\\left(\\bm{z}|\\bm{x}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) (with a fixed q\u03d5(\ud835\udc9b)subscript\ud835\udc5eitalic-\u03d5\ud835\udc9bq_{{\\phi}}\\left(\\bm{z}\\right)italic_q start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z )) should similarly provide an effective means of achieving the desired overlap behaviour.As this is the most common use case, we leave the development of more a general definition of overlap to future work, simply noting that this is an important consideration when using flexible encoder distributions.", "In concurrently published work,\u00a0Locatello et\u00a0al. [34] question the plausibility of learning unsupervised disentangled representations with meaningful features, based on theoretical analyses showing an equivalence class of generative models, many members of which could be entangled.Though their analysis is sound, we posit a counterargument to their conclusions, based on the stochastic nature of the encodings used during training.Namely, that this stochasticity means that they need not give rise to the same\u00a0elbo scores (an important exception is the rotational invariance for isotropic Gaussian priors).Essentially, the encoding noise forces nearby encodings to relate to similar datapoints, while standard choices for the likelihood distribution (e.g.\u00a0assuming conditional independence) ensure that information is stored in the encodings, not just in the generative network.These restrictions mean that the\u00a0elbo prefers smooth representations and, provided the prior is not rotationally invariant, means that there no longer need be a class of different representations with the same\u00a0elbo; simpler representations are preferred to more complex ones.", "The exact form of the encoding distribution is also important here.For example, imagine we restrict the encoder variance to be isotropic and then use a two dimensional prior where one latent dimension has a much larger variance than the other.It will be possible to store more information in the prior dimension with higher variance (as we can spread points out more relative to the encoder variance).Consequently, that dimension is more likely to correspond to an important factor of the generative process than the other.Of course, this does not imply that this is a true factor of variation in the generative process, but neither is the meaning that can be attributed to each dimension completely arbitrary.", "All the same, we agree that an important area for future work is to assess when, and to what extent, one might expect learned representations to mimic the true generative process, and, critically, when it should not.For this reason, we actively avoid including any notion of a true generative process in our definition of decomposition, but note that, analogously to disentanglement, it permits such extension in scenarios where doing so can be shown to be appropriate.", "In this work, we explored and analysed the fundamental characteristics of learning disentangled representations, and showed how these can be generalised to a more general framework of decomposition\u00a0[33].We characterised the learning of decomposed latent representation with vaes in terms of the control of two factors:i) overlap in the latent space between encodings of different datapoints, andii) regularisation of the aggregate encoding distribution to the given prior, which encodes the structure one would wish for the latent space to have.", "Connecting prior work on disentanglement to this framework, we analysed the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae objective to show that its contribution to disentangling is primarily through direct control of the level of overlap between encodings of the data, expressed by maximising the entropy of the encoding distribution.In the commonly encountered case of assuming an isotropic Gaussian prior and an independent Gaussian posterior, we showed that control of overlap is the only effect of the \u03b2\ud835\udefd\\betaitalic_\u03b2-vae.Motivated by this observation, we developed an alternate objective for the elbo that allows control of the two factors of decomposability through an additional regularisation term.We then conducted empirical evaluations using this objective, targeting alternate forms of decompositions such as clustering and sparsity, and observed the effect of varying the extent of regularisation to the prior on the quality of the resulting clustering and sparseness of the learnt embeddings.The results indicate that we were successful in attaining those decompositions."], "figure_types": {"3f51216e1834c4fe06b08df87901ae0d77de2567/10-Figure3-1.png": "plot", "3f51216e1834c4fe06b08df87901ae0d77de2567/3-Figure1-1.png": "plot", "3f51216e1834c4fe06b08df87901ae0d77de2567/4-Figure2-1.png": "plot", "3f51216e1834c4fe06b08df87901ae0d77de2567/9-Table1-1.png": "table"}}, "1512.03385": {"paper_id": "paper_31", "title": "Deep Residual Learning for Image Recognition", "arxiv_url": "https://arxiv.org/abs/1512.03385", "s2orc_url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d", "all_figures_tables": {"2c03df8b48bf3fa39054345bafabfeff15bfd11d/1-Figure1-1.png": "Figure 1. Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer \u201cplain\u201d networks. The deeper network has higher training error, and thus test error. Similar phenomena on ImageNet is presented in Fig. 4.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/11-Table11-1.png": "Table 11. Detection results on the PASCAL VOC 2012 test set (http://host.robots.ox.ac.uk:8080/leaderboard/ displaylb.php?challengeid=11&compid=4). The baseline is the Faster R-CNN system. The system \u201cbaseline+++\u201d include box refinement, context, and multi-scale testing in Table 9.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/11-Table12-1.png": "Table 12. Our results (mAP, %) on the ImageNet detection dataset. Our detection system is Faster R-CNN [32] with the improvements in Table 9, using ResNet-101.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/11-Table9-1.png": "Table 9. Object detection improvements on MS COCO using Faster R-CNN and ResNet-101.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/12-Table13-1.png": "Table 13. Localization error (%) on the ImageNet validation. In the column of \u201cLOC error on GT class\u201d ([41]), the ground truth class is used. In the \u201ctesting\u201d column, \u201c1-crop\u201d denotes testing on a center crop of 224\u00d7224 pixels, \u201cdense\u201d denotes dense (fully convolutional) and multi-scale testing.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/12-Table14-1.png": "Table 14. Comparisons of localization error (%) on the ImageNet dataset with state-of-the-art methods.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/2-Figure2-1.png": "Figure 2. Residual learning: a building block.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/4-Figure3-1.png": "Figure 3. Example network architectures for ImageNet. Left: the VGG-19 model [41] (19.6 billion FLOPs) as a reference. Middle: a plain network with 34 parameter layers (3.6 billion FLOPs). Right: a residual network with 34 parameter layers (3.6 billion FLOPs). The dotted shortcuts increase dimensions. Table 1 shows more details and other variants.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/5-Figure4-1.png": "Figure 4. Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/5-Table1-1.png": "Table 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Downsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/5-Table2-1.png": "Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation. Here the ResNets have no extra parameter compared to their plain counterparts. Fig. 4 shows the training procedures.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/6-Figure5-1.png": "Figure 5. A deeper residual function F for ImageNet. Left: a building block (on 56\u00d756 feature maps) as in Fig. 3 for ResNet34. Right: a \u201cbottleneck\u201d building block for ResNet-50/101/152.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/6-Table3-1.png": "Table 3. Error rates (%, 10-crop testing) on ImageNet validation. VGG-16 is based on our test. ResNet-50/101/152 are of option B that only uses projections for increasing dimensions.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/6-Table4-1.png": "Table 4. Error rates (%) of single-model results on the ImageNet validation set (except \u2020 reported on the test set).", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/6-Table5-1.png": "Table 5. Error rates (%) of ensembles. The top-5 error is on the test set of ImageNet and reported by the test server.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/7-Table6-1.png": "Table 6. Classification error on the CIFAR-10 test set. All methods are with data augmentation. For ResNet-110, we run it 5 times and show \u201cbest (mean\u00b1std)\u201d as in [43].", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/8-Figure6-1.png": "Figure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error of plain-110 is higher than 60% and not displayed. Middle: ResNets. Right: ResNets with 110 and 1202 layers.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/8-Figure7-1.png": "Figure 7. Standard deviations (std) of layer responses on CIFAR10. The responses are the outputs of each 3\u00d73 layer, after BN and before nonlinearity. Top: the layers are shown in their original order. Bottom: the responses are ranked in descending order.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/8-Table7-1.png": "Table 7. Object detection mAP (%) on the PASCAL VOC 2007/2012 test sets using baseline Faster R-CNN. See also Table 10 and 11 for better results.", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/8-Table8-1.png": "Table 8. Object detection mAP (%) on the COCO validation set using baseline Faster R-CNN. See also Table 9 for better results."}, "referred_figures_tables": [["2c03df8b48bf3fa39054345bafabfeff15bfd11d/5-Figure4-1.png", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/5-Table2-1.png"]], "question_id": [19], "question": ["How does the depth of the residual networks affect their performance in the experiments?"], "question_section": ["4. Experiments"], "question_trigger_sentence": ["In Table 3, the ResNet-1001 has even lower error rate than the ResNet-152, even though it has a similar number of parameters."], "question_type": ["Deep/complex question"], "evidential_info": [[{"context": "[\" We have three major observations from Table 2 and Fig. 4. First, the situation is reversed with residual learning \u2026\u2026. from increased depth..\"]", "rationale": "[The 34-layer ResNet is better than the 18-layer ResNet (by 2.8%). The 34-layer ResNet exhibits"}]], "composition": ["[The increased depth of Residual network improves performance of this network, lower training error and make it generalizable to data. It also addresses degradation problem ]"], "Is_figure_in_evidence": [true], "Is_table_in_evidence": [false], "question_key": ["791"], "passages": ["Deep convolutional neural networks [22, 21] have led to a series of breakthroughs for image classification [21, 50, 40]. Deep networks naturally integrate low/mid/high-level features [50] and classifiers in an end-to-end multi-layer fashion, and the \u201clevels\u201d of features can be enriched by the number of stacked layers (depth).Recent evidence [41, 44] reveals that network depth is of crucial importance, and the leading results [41, 44, 13, 16] on the challenging ImageNet dataset [36] all exploit \u201cvery deep\u201d [41] models, with a depth of sixteen [41] to thirty [16]. Many other nontrivial visual recognition tasks [8, 12, 7, 32, 27] have also greatly benefited from very deep models.", "Driven by the significance of depth, a question arises: Is learning better networks as easyas stacking more layers?An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [1, 9], which hamper convergence from the beginning. This problem, however, has been largely addressed by normalized initialization [23, 9, 37, 13] and intermediate normalization layers [16], which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation [22].", "When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as reported in [11, 42] and thoroughly verified by our experiments. Fig.\u00a01 shows a typical example.", "The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize. Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).", "In this paper, we address the degradation problem by introducing a deep residual learning framework.Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual mapping. Formally, denoting the desired underlying mapping as \u210b(\ud835\udc31)\u210b\ud835\udc31\\mathcal{H}(\\mathbf{x})caligraphic_H ( bold_x ), we let the stacked nonlinear layers fit another mapping of \u2131(\ud835\udc31):=\u210b(\ud835\udc31)\u2212\ud835\udc31assign\u2131\ud835\udc31\u210b\ud835\udc31\ud835\udc31\\mathcal{F}(\\mathbf{x}):=\\mathcal{H}(\\mathbf{x})-\\mathbf{x}caligraphic_F ( bold_x ) := caligraphic_H ( bold_x ) - bold_x. The original mapping is recast into \u2131(\ud835\udc31)+\ud835\udc31\u2131\ud835\udc31\ud835\udc31\\mathcal{F}(\\mathbf{x})+\\mathbf{x}caligraphic_F ( bold_x ) + bold_x.We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.", "The formulation of \u2131(\ud835\udc31)+\ud835\udc31\u2131\ud835\udc31\ud835\udc31\\mathcal{F}(\\mathbf{x})+\\mathbf{x}caligraphic_F ( bold_x ) + bold_x can be realized by feedforward neural networks with \u201cshortcut connections\u201d (Fig.\u00a02). Shortcut connections [2, 34, 49] are those skipping one or more layers. In our case, the shortcut connections simply perform identity mapping, and their outputs are added to the outputs of the stacked layers (Fig.\u00a02). Identity shortcut connections add neither extra parameter nor computational complexity. The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries (e.g., Caffe [19]) without modifying the solvers.", "We present comprehensive experiments on ImageNet [36] to show the degradation problem and evaluate our method.We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart \u201cplain\u201d nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.", "Similar phenomena are also shown on the CIFAR-10 set [20], suggesting that the optimization difficulties and the effects of our method are not just akin to a particular dataset. We present successfully trained models on this dataset with over 100 layers, and explore models with over 1000 layers.", "On the ImageNet classification dataset [36], we obtain excellent results by extremely deep residual nets.Our 152-layer residual net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets [41]. Our ensemble has 3.57% top-5 error on the ImageNet test set, and won the 1st place in the ILSVRC 2015 classification competition. The extremely deep representations also have excellent generalization performance on other recognition tasks, and lead us to further win the 1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC & COCO 2015 competitions. This strong evidence shows that the residual learning principle is generic, and we expect that it is applicable in other vision and non-vision problems.", "Residual Representations.In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD.Both of them are powerful shallow representations for image retrieval and classification [4, 48].For vector quantization, encoding residual vectors [17] is shown to be more effective than encoding original vectors.", "In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale. An alternative to Multigrid is hierarchical basis preconditioning [45, 46], which relies on variables that represent residual vectors between two scales. It has been shown [3, 45, 46] that these solvers converge much faster than standard solvers that are unaware of the residual nature of the solutions. These methods suggest that a good reformulation or preconditioning can simplify the optimization.", "Shortcut Connections.Practices and theories that lead to shortcut connections [2, 34, 49] have been studied for a long time.An early practice of training multi-layer perceptrons (MLPs) is to add a linear layer connected from the network input to the output [34, 49]. In [44, 24], a few intermediate layers are directly connected to auxiliary classifiers for addressing vanishing/exploding gradients. The papers of [39, 38, 31, 47] propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections. In [44], an \u201cinception\u201d layer is composed of a shortcut branch and a few deeper branches.", "Concurrent with our work, \u201chighway networks\u201d [42, 43] present shortcut connections with gating functions [15]. These gates are data-dependent and have parameters, in contrast to our identity shortcuts that are parameter-free. When a gated shortcut is \u201cclosed\u201d (approaching zero), the layers in highway networks represent non-residual functions. On the contrary, our formulation always learns residual functions; our identity shortcuts are never closed, and all information is always passed through, with additional residual functions to be learned. In addition, highway networks have not demonstrated accuracy gains with extremely increased depth (e.g., over 100 layers).", "Let us consider \u210b(\ud835\udc31)\u210b\ud835\udc31\\mathcal{H}(\\mathbf{x})caligraphic_H ( bold_x ) as an underlying mapping to be fit by a few stacked layers (not necessarily the entire net), with \ud835\udc31\ud835\udc31\\mathbf{x}bold_x denoting the inputs to the first of these layers. If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions222This hypothesis, however, is still an open question. See [28]., then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e., \u210b(\ud835\udc31)\u2212\ud835\udc31\u210b\ud835\udc31\ud835\udc31\\mathcal{H}(\\mathbf{x})-\\mathbf{x}caligraphic_H ( bold_x ) - bold_x (assuming that the input and output are of the same dimensions).So rather than expect stacked layers to approximate \u210b(\ud835\udc31)\u210b\ud835\udc31\\mathcal{H}(\\mathbf{x})caligraphic_H ( bold_x ), we explicitly let these layers approximate a residual function \u2131(\ud835\udc31):=\u210b(\ud835\udc31)\u2212\ud835\udc31assign\u2131\ud835\udc31\u210b\ud835\udc31\ud835\udc31\\mathcal{F}(\\mathbf{x}):=\\mathcal{H}(\\mathbf{x})-\\mathbf{x}caligraphic_F ( bold_x ) := caligraphic_H ( bold_x ) - bold_x. The original function thus becomes \u2131(\ud835\udc31)+\ud835\udc31\u2131\ud835\udc31\ud835\udc31\\mathcal{F}(\\mathbf{x})+\\mathbf{x}caligraphic_F ( bold_x ) + bold_x. Although both forms should be able to asymptotically approximate the desired functions (as hypothesized), the ease of learning might be different.", "This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig.\u00a01, left). As we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counterpart. The degradation problem suggests that the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers. With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.", "In real cases, it is unlikely that identity mappings are optimal, but our reformulation may help to precondition the problem. If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one. We show by experiments (Fig.\u00a07) that the learned residual functions in general have small responses, suggesting that identity mappings provide reasonable preconditioning.", "We adopt residual learning to every few stacked layers.A building block is shown in Fig.\u00a02. Formally, in this paper we consider a building block defined as:\ud835\udc32=\u2131(\ud835\udc31,{Wi})+\ud835\udc31.\ud835\udc32\u2131\ud835\udc31subscript\ud835\udc4a\ud835\udc56\ud835\udc31\\mathbf{y}=\\mathcal{F}(\\mathbf{x},\\{W_{i}\\})+\\mathbf{x}.bold_y = caligraphic_F ( bold_x , { italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } ) + bold_x .(1)Here \ud835\udc31\ud835\udc31\\mathbf{x}bold_x and \ud835\udc32\ud835\udc32\\mathbf{y}bold_y are the input and output vectors of the layers considered. The function \u2131(\ud835\udc31,{Wi})\u2131\ud835\udc31subscript\ud835\udc4a\ud835\udc56\\mathcal{F}(\\mathbf{x},\\{W_{i}\\})caligraphic_F ( bold_x , { italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } ) represents the residual mapping to be learned. For the example in Fig.\u00a02 that has two layers, \u2131=W2\u03c3(W1\ud835\udc31)\u2131subscript\ud835\udc4a2\ud835\udf0esubscript\ud835\udc4a1\ud835\udc31\\mathcal{F}=W_{2}\\sigma(W_{1}\\mathbf{x})caligraphic_F = italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_\u03c3 ( italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_x ) in which \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 denotes ReLU [29] and the biases are omitted for simplifying notations. The operation \u2131+\ud835\udc31\u2131\ud835\udc31\\mathcal{F}+\\mathbf{x}caligraphic_F + bold_x is performed by a shortcut connection and element-wise addition. We adopt the second nonlinearity after the addition (i.e., \u03c3(\ud835\udc32)\ud835\udf0e\ud835\udc32\\sigma(\\mathbf{y})italic_\u03c3 ( bold_y ), see Fig.\u00a02).", "The shortcut connections in Eqn.(1) introduce neither extra parameter nor computation complexity. This is not only attractive in practice but also important in our comparisons between plain and residual networks. We can fairly compare plain/residual networks that simultaneously have the same number of parameters, depth, width, and computational cost (except for the negligible element-wise addition).", "The dimensions of \ud835\udc31\ud835\udc31\\mathbf{x}bold_x and \u2131\u2131\\mathcal{F}caligraphic_F must be equal in Eqn.(1). If this is not the case (e.g., when changing the input/output channels), we can perform a linear projection Wssubscript\ud835\udc4a\ud835\udc60W_{s}italic_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT by the shortcut connections to match the dimensions:\ud835\udc32=\u2131(\ud835\udc31,{Wi})+Ws\ud835\udc31.\ud835\udc32\u2131\ud835\udc31subscript\ud835\udc4a\ud835\udc56subscript\ud835\udc4a\ud835\udc60\ud835\udc31\\mathbf{y}=\\mathcal{F}(\\mathbf{x},\\{W_{i}\\})+W_{s}\\mathbf{x}.bold_y = caligraphic_F ( bold_x , { italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } ) + italic_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT bold_x .(2)We can also use a square matrix Wssubscript\ud835\udc4a\ud835\udc60W_{s}italic_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT in Eqn.(1). But we will show by experiments that the identity mapping is sufficient for addressing the degradation problem and is economical, and thus Wssubscript\ud835\udc4a\ud835\udc60W_{s}italic_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is only used when matching dimensions.", "The form of the residual function \u2131\u2131\\mathcal{F}caligraphic_F is flexible. Experiments in this paper involve a function \u2131\u2131\\mathcal{F}caligraphic_F that has two or three layers (Fig.\u00a05), while more layers are possible. But if \u2131\u2131\\mathcal{F}caligraphic_F has only a single layer, Eqn.(1) is similar to a linear layer: \ud835\udc32=W1\ud835\udc31+\ud835\udc31\ud835\udc32subscript\ud835\udc4a1\ud835\udc31\ud835\udc31\\mathbf{y}=W_{1}\\mathbf{x}+\\mathbf{x}bold_y = italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_x + bold_x, for which we have not observed advantages.", "We also note that although the above notations are about fully-connected layers for simplicity, they are applicable to convolutional layers. The function \u2131(\ud835\udc31,{Wi})\u2131\ud835\udc31subscript\ud835\udc4a\ud835\udc56\\mathcal{F}(\\mathbf{x},\\{W_{i}\\})caligraphic_F ( bold_x , { italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } ) can represent multiple convolutional layers. The element-wise addition is performed on two feature maps, channel by channel.", "We have tested various plain/residual nets, and have observed consistent phenomena. To provide instances for discussion, we describe two models for ImageNet as follows.", "Plain Network.Our plain baselines (Fig.\u00a03, middle) are mainly inspired by the philosophy of VGG nets [41] (Fig.\u00a03, left).The convolutional layers mostly have 3\u00d7\\times\u00d73 filters and follow two simple design rules: (i) for the same output feature map size, the layers have the same number of filters; and (ii) if the feature map size is halved, the number of filters is doubled so as to preserve the time complexity per layer. We perform downsampling directly by convolutional layers that have a stride of 2.The network ends with a global average pooling layer and a 1000-way fully-connected layer with softmax. The total number of weighted layers is 34 in Fig.\u00a03 (middle).", "It is worth noticing that our model has fewer filters and lower complexity than VGG nets [41] (Fig.\u00a03, left). Our 34-layer baseline has 3.6 billion FLOPs (multiply-adds), which is only 18% of VGG-19 (19.6 billion FLOPs).", "Residual Network.Based on the above plain network, we insert shortcut connections (Fig.\u00a03, right) which turn the network into its counterpart residual version.The identity shortcuts (Eqn.(1)) can be directly used when the input and output are of the same dimensions (solid line shortcuts in Fig.\u00a03).When the dimensions increase (dotted line shortcuts in Fig.\u00a03), we consider two options:(A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter;(B) The projection shortcut in Eqn.(2) is used to match dimensions (done by 1\u00d7\\times\u00d71 convolutions).For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2.", "Our implementation for ImageNet follows the practice in [21, 41]. The image is resized with its shorter side randomly sampled in [256,480]256480[256,480][ 256 , 480 ] for scale augmentation [41]. A 224\u00d7\\times\u00d7224 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted [21]. The standard color augmentation in [21] is used.We adopt batch normalization (BN) [16] right after each convolution and before activation, following [16].We initialize the weights as in [13] and train all plain/residual nets from scratch.We use SGD with a mini-batch size of 256. The learning rate starts from 0.1 and is divided by 10 when the error plateaus, and the models are trained for up to 60\u00d710460superscript10460\\times 10^{4}60 \u00d7 10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT iterations. We use a weight decay of 0.0001 and a momentum of 0.9. We do not use dropout [14], following the practice in [16].", "In testing, for comparison studies we adopt the standard 10-crop testing [21].For best results, we adopt the fully-convolutional form as in [41, 13], and average the scores at multiple scales (images are resized such that the shorter side is in {224,256,384,480,640}224256384480640\\{224,256,384,480,640\\}{ 224 , 256 , 384 , 480 , 640 }).", "We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images, and evaluated on the 50k validation images. We also obtain a final result on the 100k test images, reported by the test server. We evaluate both top-1 and top-5 error rates.", "Plain Networks.We first evaluate 18-layer and 34-layer plain nets. The 34-layer plain net is in Fig.\u00a03 (middle). The 18-layer plain net is of a similar form. See Table\u00a01 for detailed architectures.", "The results in Table\u00a02 show that the deeper 34-layer plain net has higher validation error than the shallower 18-layer plain net. To reveal the reasons, in Fig.\u00a04 (left) we compare their training/validation errors during the training procedure. We have observed the degradation problem - the 34-layer plain net has higher training error throughout the whole training procedure, even though the solution space of the 18-layer plain network is a subspace of that of the 34-layer one.", "We argue that this optimization difficulty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN [16], which ensures forward propagated signals to have non-zero variances. We also verify that the backward propagated gradients exhibit healthy norms with BN. So neither forward nor backward signals vanish.In fact, the 34-layer plain net is still able to achieve competitive accuracy (Table\u00a03), suggesting that the solver works to some extent. We conjecture that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the training error333We have experimented with more training iterations (3\u00d7\\times\u00d7) and still observed the degradation problem, suggesting that this problem cannot be feasibly addressed by simply using more iterations..The reason for such optimization difficulties will be studied in the future.", "Residual Networks.Next we evaluate 18-layer and 34-layer residual nets (ResNets). The baseline architectures are the same as the above plain nets, expect that a shortcut connection is added to each pair of 3\u00d7\\times\u00d73 filters as in Fig.\u00a03 (right). In the first comparison (Table\u00a02 and Fig.\u00a04 right), we use identity mapping for all shortcuts and zero-padding for increasing dimensions (option A). So they have no extra parameter compared to the plain counterparts.", "We have three major observations from Table\u00a02 and Fig.\u00a04. First, the situation is reversed with residual learning \u2013 the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%). More importantly, the 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data. This indicates that the degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth.", "Second, compared to its plain counterpart, the 34-layer ResNet reduces the top-1 error by 3.5% (Table\u00a02), resulting from the successfully reduced training error (Fig.\u00a04 right vs. left). This comparison verifies the effectiveness of residual learning on extremely deep systems.", "Last, we also note that the 18-layer plain/residual nets are comparably accurate (Table\u00a02), but the 18-layer ResNet converges faster (Fig.\u00a04 right vs. left).When the net is \u201cnot overly deep\u201d (18 layers here), the current SGD solver is still able to find good solutions to the plain net. In this case, the ResNet eases the optimization by providing faster convergence at the early stage.", "Identity vs. Projection Shortcuts.We have shown that parameter-free, identity shortcuts help with training. Next we investigate projection shortcuts (Eqn.(2)).In Table\u00a03 we compare three options: (A) zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameter-free (the same as Table\u00a02 and Fig.\u00a04 right); (B) projection shortcuts are used for increasing dimensions, and other shortcuts are identity; and (C) all shortcuts are projections.", "Table\u00a03 shows that all three options are considerably better than the plain counterpart.B is slightly better than A. We argue that this is because the zero-padded dimensions in A indeed have no residual learning. C is marginally better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts. But the small differences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem. So we do not use option C in the rest of this paper, to reduce memory/time complexity and model sizes. Identity shortcuts are particularly important for not increasing the complexity of the bottleneck architectures that are introduced below.", "Deeper Bottleneck Architectures. Next we describe our deeper nets for ImageNet. Because of concerns on the training time that we can afford, we modify the building block as a bottleneck design444Deeper non-bottleneck ResNets (e.g., Fig.\u00a05 left) also gain accuracy from increased depth (as shown on CIFAR-10), but are not as economical as the bottleneck ResNets. So the usage of bottleneck designs is mainly due to practical considerations. We further note that the degradation problem of plain nets is also witnessed for the bottleneck designs..For each residual function \u2131\u2131\\mathcal{F}caligraphic_F, we use a stack of 3 layers instead of 2 (Fig.\u00a05). The three layers are 1\u00d7\\times\u00d71, 3\u00d7\\times\u00d73, and 1\u00d7\\times\u00d71 convolutions, where the 1\u00d7\\times\u00d71 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3\u00d7\\times\u00d73 layer a bottleneck with smaller input/output dimensions.Fig.\u00a05 shows an example, where both designs have similar time complexity.", "The parameter-free identity shortcuts are particularly important for the bottleneck architectures. If the identity shortcut in Fig.\u00a05 (right) is replaced with projection, one can show that the time complexity and model size are doubled, as the shortcut is connected to the two high-dimensional ends. So identity shortcuts lead to more efficient models for the bottleneck designs.", "50-layer ResNet: We replace each 2-layer block in the 34-layer net with this 3-layer bottleneck block, resulting in a 50-layer ResNet (Table\u00a01). We use option B for increasing dimensions.This model has 3.8 billion FLOPs.", "101-layer and 152-layer ResNets: We construct 101-layer and 152-layer ResNets by using more 3-layer blocks (Table\u00a01).Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).", "The 50/101/152-layer ResNets are more accurate than the 34-layer ones by considerable margins (Table\u00a03 and\u00a05). We do not observe the degradation problem and thus enjoy significant accuracy gains from considerably increased depth. The benefits of depth are witnessed for all evaluation metrics (Table\u00a03 and\u00a05).", "Comparisons with State-of-the-art Methods.In Table\u00a05 we compare with the previous best single-model results.Our baseline 34-layer ResNets have achieved very competitive accuracy.Our 152-layer ResNet has a single-model top-5 validation error of 4.49%. This single-model result outperforms all previous ensemble results (Table\u00a05).We combine six models of different depth to form an ensemble (only with two 152-layer ones at the time of submitting). This leads to 3.57% top-5 error on the test set (Table\u00a05). This entry won the 1st place in ILSVRC 2015.", "We conducted more studies on the CIFAR-10 dataset [20], which consists of 50k training images and 10k testing images in 10 classes. We present experiments trained on the training set and evaluated on the test set.Our focus is on the behaviors of extremely deep networks, but not on pushing the state-of-the-art results, so we intentionally use simple architectures as follows.", "The plain/residual architectures follow the form in Fig.\u00a03 (middle/right).The network inputs are 32\u00d7\\times\u00d732 images, with the per-pixel mean subtracted. The first layer is 3\u00d7\\times\u00d73 convolutions. Then we use a stack of 6n6\ud835\udc5b6n6 italic_n layers with 3\u00d7\\times\u00d73 convolutions on the feature maps of sizes {32,16,8}32168\\{32,16,8\\}{ 32 , 16 , 8 } respectively, with 2n\ud835\udc5bnitalic_n layers for each feature map size. The numbers of filters are {16,32,64}163264\\{16,32,64\\}{ 16 , 32 , 64 } respectively. The subsampling is performed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. There are totally 6n\ud835\udc5bnitalic_n+2 stacked weighted layers. The following table summarizes the architecture:output map size32\u00d7\\times\u00d73216\u00d7\\times\u00d7168\u00d7\\times\u00d78# layers1+2n\ud835\udc5bnitalic_n2n\ud835\udc5bnitalic_n2n\ud835\udc5bnitalic_n# filters163264When shortcut connections are used, they are connected to the pairs of 3\u00d7\\times\u00d73 layers (totally 3n3\ud835\udc5b3n3 italic_n shortcuts). On this dataset we use identity shortcuts in all cases (i.e., option A), so our residual models have exactly the same depth, width, and number of parameters as the plain counterparts.", "We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in [13] and BN [16] but with no dropout. These models are trained with a mini-batch size of 128 on two GPUs. We start with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations, which is determined on a 45k/5k train/val split. We follow the simple data augmentation in [24] for training: 4 pixels are padded on each side, and a 32\u00d7\\times\u00d732 crop is randomly sampled from the padded image or its horizontal flip. For testing, we only evaluate the single view of the original 32\u00d7\\times\u00d732 image.", "We compare n={3,5,7,9}\ud835\udc5b3579n=\\{3,5,7,9\\}italic_n = { 3 , 5 , 7 , 9 }, leading to 20, 32, 44, and 56-layer networks.Fig.\u00a06 (left) shows the behaviors of the plain nets. The deep plain nets suffer from increased depth, and exhibit higher training error when going deeper. This phenomenon is similar to that on ImageNet (Fig.\u00a04, left) and on MNIST (see [42]), suggesting that such an optimization difficulty is a fundamental problem.", "Fig.\u00a06 (middle) shows the behaviors of ResNets. Also similar to the ImageNet cases (Fig.\u00a04, right), our ResNets manage to overcome the optimization difficulty and demonstrate accuracy gains when the depth increases.", "We further explore n=18\ud835\udc5b18n=18italic_n = 18 that leads to a 110-layer ResNet. In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging555With an initial learning rate of 0.1, it starts converging (<<<90% error) after several epochs, but still reaches similar accuracy.. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training. The rest of the learning schedule is as done previously. This 110-layer network converges well (Fig.\u00a06, middle). It has fewer parameters than other deep and thin networks such as FitNet [35] and Highway [42] (Table\u00a06), yet is among the state-of-the-art results (6.43%, Table\u00a06).", "Analysis of Layer Responses.Fig.\u00a07 shows the standard deviations (std) of the layer responses. The responses are the outputs of each 3\u00d7\\times\u00d73 layer, after BN and before other nonlinearity (ReLU/addition). For ResNets, this analysis reveals the response strength of the residual functions.Fig.\u00a07 shows that ResNets have generally smaller responses than their plain counterparts. These results support our basic motivation (Sec.3.1) that the residual functions might be generally closer to zero than the non-residual functions.We also notice that the deeper ResNet has smaller magnitudes of responses, as evidenced by the comparisons among ResNet-20, 56, and 110 in Fig.\u00a07. When there are more layers, an individual layer of ResNets tends to modify the signal less.", "Exploring Over 1000 layers.We explore an aggressively deep model of over 1000 layers. We set n=200\ud835\udc5b200n=200italic_n = 200 that leads to a 1202-layer network, which is trained as described above. Our method shows no optimization difficulty, and this 103superscript10310^{3}10 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT-layer network is able to achieve training error <<<0.1% (Fig.\u00a06, right). Its test error is still fairly good (7.93%, Table\u00a06).", "But there are still open problems on such aggressively deep models.The testing result of this 1202-layer network is worse than that of our 110-layer network, although both have similar training error. We argue that this is because of overfitting.The 1202-layer network may be unnecessarily large (19.4M) for this small dataset. Strong regularization such as maxout [10] or dropout [14] is applied to obtain the best results ([10, 25, 24, 35]) on this dataset.In this paper, we use no maxout/dropout and just simply impose regularization via deep and thin architectures by design, without distracting from the focus on the difficulties of optimization. But combining with stronger regularization may improve results, which we will study in the future.", "Our method has good generalization performance on other recognition tasks. Table\u00a08 and \u00a08 show the object detection baseline results on PASCAL VOC 2007 and 2012 [5] and COCO [26]. We adopt Faster R-CNN [32] as the detection method. Here we are interested in the improvements of replacing VGG-16 [41] with ResNet-101. The detection implementation (see appendix) of using both models is the same, so the gains can only be attributed to better networks. Most remarkably, on the challenging COCO dataset we obtain a 6.0% increase in COCO\u2019s standard metric (mAP@[.5, .95]), which is a 28% relative improvement. This gain is solely due to the learned representations.", "Based on deep residual nets, we won the 1st places in several tracks in ILSVRC & COCO 2015 competitions: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. The details are in the appendix."], "figure_types": {"2c03df8b48bf3fa39054345bafabfeff15bfd11d/1-Figure1-1.png": "plot", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/11-Table11-1.png": "table", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/11-Table12-1.png": "table", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/11-Table9-1.png": "table", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/12-Table13-1.png": "table", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/12-Table14-1.png": "table", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/2-Figure2-1.png": "schematic", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/4-Figure3-1.png": "schematic", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/5-Figure4-1.png": "plot", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/5-Table1-1.png": "table", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/5-Table2-1.png": "table", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/6-Figure5-1.png": "schematic", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/6-Table3-1.png": "table", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/6-Table4-1.png": "table", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/6-Table5-1.png": "table", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/7-Table6-1.png": "table", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/8-Figure6-1.png": "plot", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/8-Figure7-1.png": "plot", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/8-Table7-1.png": "table", "2c03df8b48bf3fa39054345bafabfeff15bfd11d/8-Table8-1.png": "table"}}, "1804.06655": {"paper_id": "paper_32", "title": "Deep Face Recognition", "arxiv_url": "https://arxiv.org/abs/1804.06655", "s2orc_url": "https://www.semanticscholar.org/paper/21117380118ddce47b3c515c5228372c513e61ba", "all_figures_tables": {"21117380118ddce47b3c515c5228372c513e61ba/10-Figure12-1.png": "Fig. 12. The development of different methods of face processing. Red, green, orange and blue rectangles represent CNN model, SAE model, 3D model and GAN model, respectively.", "21117380118ddce47b3c515c5228372c513e61ba/11-Figure13-1.png": "Fig. 13. Iterative CNN network for reconstructing a 3D face. [165]", "21117380118ddce47b3c515c5228372c513e61ba/12-Figure14-1.png": "Fig. 14. General framework of TP-GAN. [91]", "21117380118ddce47b3c515c5228372c513e61ba/13-Figure15-1.png": "Fig. 15. The evolution of FR datasets. Before 2007, early work in FR focused on controlled and small-scale datasets. In 2007, LFW [90] dataset was introduced which marks the beginning of FR under unconstrained conditions. Since then, more testing databases with different tasks and scenes are designed. And in 2014, CASIA-Webface [243] provided the first widely-used public training dataset, large-scale training datasets begun to be hot topic. Red rectangles represent training datasets, and other color rectangles represent testing datasets with different task and scenes.", "21117380118ddce47b3c515c5228372c513e61ba/13-Figure16-1.png": "Fig. 16. The distribution of three new large-scale databases suitable for training of deep model. They have larger scale than the widely-used CAISAWeb database. The vertical axis displays number of images per person, and the horizontal axis shows person IDs.", "21117380118ddce47b3c515c5228372c513e61ba/13-Figure17-1.png": "Fig. 17. A visualization of size and estimated noise percentage of datasets. [204]", "21117380118ddce47b3c515c5228372c513e61ba/13-TableVI-1.png": "TABLE VI THE COMMONLY USED FR DATASETS FOR TRAINING", "21117380118ddce47b3c515c5228372c513e61ba/14-TableVII-1.png": "TABLE VII RACIAL BIAS IN EXISTING COMMERCIAL RECOGNITION APIS AND FACE RECOGNITION ALGORITHMS. FACE VERIFICATION ACCURACIES (%) ON RFW DATABASE ARE GIVEN [210].", "21117380118ddce47b3c515c5228372c513e61ba/15-Figure18-1.png": "Fig. 18. The comparison of different training protocol and evaluation tasks. In terms of training protocol, FR model can be evaluated under subject-dependent or independent settings according to whether testing identities appear in training set. In terms of testing tasks, the performance of recognition model can be evaluated under face verification, close-set face identification, open-set face identification settings.", "21117380118ddce47b3c515c5228372c513e61ba/15-TableIX-1.png": "TABLE IX PERFORMANCE OF STATE OF THE ARTS ON MS-CELEB-1M DATASET", "21117380118ddce47b3c515c5228372c513e61ba/15-TableVIII-1.png": "TABLE VIII PERFORMANCE OF STATE OF THE ARTS ON MEGAFACE DATASET", "21117380118ddce47b3c515c5228372c513e61ba/15-TableX-1.png": "TABLE X FACE IDENTIFICATION AND VERIFICATION EVALUATION OF STATE OF THE ARTS ON IJB-A DATASET", "21117380118ddce47b3c515c5228372c513e61ba/18-Figure19-1.png": "Fig. 19. The different scenes of FR. We divide FR scenes into four categories: cross-factor FR, heterogenous FR, multiple (or single) media FR and FR in industry. There are many testing datasets and special FR methods for each scene.", "21117380118ddce47b3c515c5228372c513e61ba/18-Figure20-1.png": "Fig. 20. The architecture of the cross-age FR with LIA. [217]", "21117380118ddce47b3c515c5228372c513e61ba/18-Figure21-1.png": "Fig. 21. The architecture of BLAN. [119]", "21117380118ddce47b3c515c5228372c513e61ba/19-Figure22-1.png": "Fig. 22. The architecture of DualGAN. [244]", "21117380118ddce47b3c515c5228372c513e61ba/19-Figure23-1.png": "Fig. 23. The architecture of a single sample per person domain adaptation network (SSPP-DAN). [84]", "21117380118ddce47b3c515c5228372c513e61ba/2-Figure1-1.png": "Fig. 1. Milestones of face representation for recognition. The holistic approaches dominated the face recognition community in the 1990s. In the early 2000s, handcrafted local descriptors became popular, and the local feature learning approach were introduced in the late 2000s. In 2014, DeepFace [195] and DeepID [187] achieved a breakthrough on state-of-the-art performance, and research focus has shifted to deep-learning-based approaches. As the representation pipeline becomes deeper and deeper, the LFW (Labeled Face in-the-Wild) performance steadily improves from around 60% to above 90%, while deep learning boosts the performance to 99.80% in just three years.", "21117380118ddce47b3c515c5228372c513e61ba/2-Figure2-1.png": "Fig. 2. The hierarchical architecture that stitches together pixels into invariant face representation. Deep model consists of multiple layers of simulated neurons that convolute and pool input, during which the receptive-field size of simulated neurons are continually enlarged to integrate the low-level primary elements into multifarious facial attributes, finally feeding the data forward to one or more fully connected layer at the top of the network. The output is a compressed feature vector that represent the face. Such deep representation is widely considered the state-of-the-art technique for face recognition.", "21117380118ddce47b3c515c5228372c513e61ba/20-Figure24-1.png": "Fig. 24. The FR framework of NAN. [241]", "21117380118ddce47b3c515c5228372c513e61ba/4-Figure3-1.png": "Fig. 3. Deep FR system with face detector and alignment. First, a face detector is used to localize faces. Second, the faces are aligned to normalized canonical coordinates. Third, the FR module is implemented. In FR module, face anti-spoofing recognizes whether the face is live or spoofed; face processing is used to handle recognition difficulty before training and testing; different architectures and loss functions are used to extract discriminative deep feature when training; face matching methods are used to do feature classification when the deep feature of testing data are extracted.", "21117380118ddce47b3c515c5228372c513e61ba/4-TableII-1.png": "TABLE II DIFFERENT NETWORK ARCHITECTURES OF FR", "21117380118ddce47b3c515c5228372c513e61ba/5-Figure4-1.png": "Fig. 4. FR studies have begun with general scenario, then gradually increase difficulty levels and drive different solutions for specific scenarios to get close to reality, such as cross-pose FR, cross-age FR, video FR. In specific scenarios, targeted training and testing database are constructed, and the algorithms, e.g. face processing, architectures and loss functions are modified based on those of general solutions.", "21117380118ddce47b3c515c5228372c513e61ba/6-Figure5-1.png": "Fig. 5. The development of loss functions. It marks the beginning of deep FR that Deepface [195] and DeepID [191] were introduced in 2014. After that, Euclidean-distance-based loss always played the important role in loss function, such as contractive loss, triplet loss and center loss. In 2016 and 2017, L-softmax [126] and A-softmax [125] further promoted the development of the large-margin feature learning. In 2017, feature and weight normalization also begun to show excellent performance, which leads to the study on variations of softmax. Red, green, blue and yellow rectangles represent deep methods with softmax, Euclidean-distance-based loss, angular/cosine-margin-based loss and variations of softmax, respectively.", "21117380118ddce47b3c515c5228372c513e61ba/6-TableIV-1.png": "TABLE IV THE ACCURACY OF DIFFERENT VERIFICATION METHODS ON THE LFW DATASET.", "21117380118ddce47b3c515c5228372c513e61ba/7-Figure6-1.png": "Fig. 6. Geometry interpretation of A-Softmax loss. [125]", "21117380118ddce47b3c515c5228372c513e61ba/7-Figure7-1.png": "Fig. 7. 1:1M rank-1 identification results on MegaFace benchmark: (a) introducing label flips to training data, (b) introducing outliers to training data. [204]", "21117380118ddce47b3c515c5228372c513e61ba/7-TableV-1.png": "TABLE V DECISION BOUNDARIES FOR CLASS 1 UNDER BINARY CLASSIFICATION CASE, WHERE x\u0302 IS THE NORMALIZED FEATURE. [42]", "21117380118ddce47b3c515c5228372c513e61ba/8-Figure8-1.png": "Fig. 8. The top row presents the typical network architectures in object classification, and the bottom row describes the well-known algorithms of deep FR that use the typical architectures and achieve good performance. The same color rectangles mean the same architecture. It is easy to find that the architectures of deep FR have always followed those of deep object classification and evolved from AlexNet to SENet rapidly.", "21117380118ddce47b3c515c5228372c513e61ba/9-Figure10-1.png": "Fig. 10. Joint face registration and representation. [77]", "21117380118ddce47b3c515c5228372c513e61ba/9-Figure11-1.png": "Fig. 11. Reconstruction-based disentanglement for pose-invariant FR. [151]", "21117380118ddce47b3c515c5228372c513e61ba/9-Figure9-1.png": "Fig. 9. The architecture of Alexnet, VGGNet, GoogleNet, ResNet, SENet."}, "referred_figures_tables": [["21117380118ddce47b3c515c5228372c513e61ba/2-Figure1-1.png", "21117380118ddce47b3c515c5228372c513e61ba/10-Figure12-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/9-Figure9-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/2-Figure1-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/4-Figure3-1.png", "21117380118ddce47b3c515c5228372c513e61ba/4-Figure3-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/2-Figure2-1.png", "21117380118ddce47b3c515c5228372c513e61ba/2-Figure2-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/11-Figure13-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/11-Figure13-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/2-Figure1-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/7-Figure6-1.png", "21117380118ddce47b3c515c5228372c513e61ba/7-Figure7-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/15-TableIX-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/11-Figure13-1.png", "21117380118ddce47b3c515c5228372c513e61ba/12-Figure14-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/8-Figure8-1.png", "21117380118ddce47b3c515c5228372c513e61ba/9-Figure9-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/2-Figure1-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/6-Figure5-1.png", "21117380118ddce47b3c515c5228372c513e61ba/7-Figure6-1.png", "21117380118ddce47b3c515c5228372c513e61ba/7-Figure7-1.png"], ["21117380118ddce47b3c515c5228372c513e61ba/19-Figure22-1.png", "21117380118ddce47b3c515c5228372c513e61ba/19-Figure23-1.png", "21117380118ddce47b3c515c5228372c513e61ba/20-Figure24-1.png"]], "question_id": [0, 1, 2, 3, 4, 15, 16, 7, 9, 5, 14, 11, 12, 6, 18], "question": ["What are the different approaches that have been used in face recognition technology over the years?", "How has deep learning improved the accuracy of face recognition systems compared to traditional methods?", "In what areas has face recognition technology been commonly used?", "What are the three main modules of a face recognition system?", "How is deep learning used in the process of feature extraction in face recognition systems?", "What are some challenges that researchers have encountered when generating 3D face images from 2D images?", "How has the quality and diversity of generated 3D face images improved over time, and what advances have contributed to these improvements?", "What are some common methods used in facial recognition and how do they compare in terms of effectiveness and challenges?", "How do angular/cosine-margin-based loss functions improve the separability of learned features in deep face recognition?\t", "What are some common metrics used to evaluate the performance of face recognition systems?", "Can the methods of \"one-to-many augmentation\" like data augmentation and 3D face reconstruction effectively improve the performance of deep FR algorithms in terms of accuracy and diversity of training data?", "How has the evolution of network architectures in deep face recognition systems, such as the transition from AlexNet to ResNet and SENet, impacted the performance of these systems?\t", "How do feature-based methods work in face recognition?", "What are the main loss functions that have been explored for improving deep FR methods and how have they evolved over time?", "What are some of the specific challenges that FR models face in real-world applications and how have researchers attempted to address these challenges through the design of specialized algorithms?\t"], "question_section": ["I. INTRODUCTION", "I. INTRODUCTION", "I. INTRODUCTION", "II. OVERVIEW", "II. OVERVIEW", "IV. FACE PROCESSING FOR TRAINING AND RECOGNITION", "V. FACE DATABASES AND EVALUATION PROTOCOLS", "III. NETWORK ARCHITECTURE AND TRAINING LOSS", "III. NETWORK ARCHITECTURE AND TRAINING LOSS", "II. OVERVIEW", "IV. FACE PROCESSING FOR TRAINING AND RECOGNITION", "III. NETWORK ARCHITECTURE AND TRAINING LOSS", "III. NETWORK ARCHITECTURE AND TRAINING LOSS", "III. NETWORK ARCHITECTURE AND TRAINING LOSS", "VI. DIVERSE RECOGNITION SCENES OF DEEP LEARNING"], "question_trigger_sentence": ["The milestones of feature-based FR over the past years are presented in Fig. 1, in which the times of four major technical streams are highlighted. The holistic approaches derive the low-dimensional representation through certain distribution assumptions, such as linear subspace, manifold, and sparse representation. This idea dominated the FR community in the 1990s and 2000s. However, a well-known problem is that these theoretically plausible holistic methods fail to address the uncontrolled facial changes that deviate from their prior assumptions. In the early 2000s, this problem gave rise to local-feature-based FR. Gabor and LBP, as well as their multilevel and high-dimensional extensions, achieved robust performance through some invariant properties of local filtering. Unfortunately, handcrafted features suffered from a lack of distinctiveness and compactness. In the early 2010s, learning-based local descriptors were introduced to the FR community, in which local filters are learned for better distinctiveness and the encoding codebook is learned for better compactness.", "Deep learning methods, such as convolutional neural networks, use a cascade of multiple layers of processing units for feature extraction and transformation. They learn multiple levels of representations that correspond to different levels of abstraction. The levels form a hierarchy of concepts, showing strong invariance to the face pose, lighting, and expression changes... As a result, deep learning has greatly improved the accuracy of FR systems and has been widely used in various applications.\"", "FR has been widely used in many areas, such as military, finance, public security and daily life.", "There are three modules needed for FR system, as shown in Fig. 3. First, a face detector is used to localize faces in images or videos. Second, with the facial landmark detector, the faces are aligned to normalized canonical coordinates. Third, the FR module is implemented with these aligned face images.", "Deep feature extraction involves the use of deep learning techniques to extract identity information from a face image.", "There are several approaches to generating 3D face images from 2D images, including the use of 3D CNNs, autoencoder models, and GANs. These approaches have been successful in generating realistic and diverse 3D face images, but there is still room for improvement in terms of the quality and diversity of the generated images.\t", "In [65], two groups of units are embedded between encoder and decoder. The identity units remain unchanged and the rotation of images is achieved by taking actions to pose units at each time step.\t", "Contrary to contrastive loss that considers the absolute distances of the matching pairs and non-matching pairs, triplet loss considers the relative difference of the distances between them. Along with FaceNet [38] proposed by Google, Triplet loss [38], [37], [81], [80], [58], [60] was introduced into FR. It requires the face triplets, and then it minimizes the distance between an anchor and a positive sample of the same identity and maximizes the distance between the anchor and a negative sample of a different identity. FaceNet made kf(x a i ) \u2212 f(x p i )k 2 2 + \u03b1 < \u2212 kf(x a i ) \u2212 f(x n i )k 2 2 using hard triplet face samples, where x a i , x p i and x n i are the anchor, positive and negative samples, respectively, \u03b1 is a margin and f(\u00b7) represents a nonlinear transformation embedding an image into a feature space. Inspired by FaceNet [38], TPE [81] and TSE [80] learned a linear projection W to construct triplet loss. Other methods optimize deep models using both triplet loss and softmax loss [59], [58], [60], [121]. They first train networks with softmax and then fine-tune them with triplet loss.", "Angular/cosine-margin-based loss [104], [84], [105], [106], [108] is proposed to make learned features potentially separable with a larger angular/cosine distance\t", "Finally, face recognition systems can be evaluated using various metrics, such as accuracy, precision, and recall, to measure their effectiveness.", "Collecting a large database is extremely expensive and time consuming. The methods of \u201cone-to-many augmentation\u201d can mitigate the challenges of data collection, and they can be used to augment not only training data but also the gallery of test data.\t", "The commonly used network architectures of deep FR have always followed those of deep object classification and evolved from AlexNet to SENet rapidly.\t", "Feature-based methods extract features from the face image and use them to recognize the face.\t", "This becomes the hottest research topic in deep FR research, as illustrated in Fig. 5. Before 2017, Euclidean-distance-based loss played an important role; In 2017, angular/cosine-margin-based loss as well as feature and weight normalization became popular.", "Despite the high accuracy in the LFW [23] and Megaface [44], [164] benchmarks, the performance of FR models still hardly meets the requirements in real-world application. A conjecture in industry is made that results of generic deep models can be improved simply by collecting big datasets of the target scene. However, this holds only to a certain degree. More and more concerns on privacy may make the collection and human-annotation of face data become illegal in the future. Therefore, significant efforts have been paid to design excellent algorithms to address the specific problems with limited data in these realistic scenes.\t"], "question_type": ["Shallow question", "Deep/complex question", "Shallow question", "Shallow question", "Shallow question", "Shallow question", "Testing question", "Testing question", "Deep/complex question", "Testing question", "Shallow question", "Deep/complex question", "Shallow question", "Shallow question", "Shallow question"], "evidential_info": [[{"context": "Face recognition (FR) has been the prominent biometric technique for identity authentication and has been widely used in many areas, such as military, finance, public security and daily life. FR has been a long-standing research topic in the CVPR community. In the early 1990s, the study of FR became popular following the introduction of the historical Eigenface approach [1]. The milestones of feature-based FR over the past years are presented in Fig. 1, in which the times of four major technical streams are highlighted. The holistic approaches derive the low-dimensional representation through certain distribution assumptions, such as linear subspace [2][3][4], manifold [5][6][7], and sparse representation [8][9][10][11]. This idea dominated the FR community in the 1990s and 2000s. However, a well-known problem is that these theoretically plausible holistic methods fail to address the uncontrolled facial changes that deviate from their prior assumptions. In the early 2000s, this problem gave rise to local-feature-based FR. Gabor [12] and LBP [13], as well as their multilevel and high-dimensional extensions [14][15][16], achieved robust performance through some invariant properties of local filtering. Unfortunately, handcrafted features suffered from a lack of distinctiveness and compactness. In the early 2010s, learning-based local descriptors were introduced to the FR community [17][18][19], in which local filters are learned for better distinctiveness and the encoding codebook is learned for better compactness. However, these shallow representations still have an inevitable limitation on robustness against the complex nonlinear facial appearance variations.", "rationale": "There were 4 major technological streams of development of Face Recognition. In the 1990s and 2000s holistic approach was the most popular direction of research, however, these rather simple methods failed to address the uncontrolled facial changes. This problem led to different local feature-based face recognition. However, these features had to be handcrafted, thus in the 2010s, learning-based-local-descriptors were introduced (i.e shallow learning). But still, these methods could not handle the complexity and non-linearity of the face recognition task."}, {"context": "In general, traditional methods attempted to recognize human face by one or two layer representations, such as filtering responses, histogram of the feature codes, or distribution of the dictionary atoms. The research community studied intensively to separately improve the preprocessing, local descriptors, and feature transformation, but these approaches improved FR accuracy slowly. What\u2019s worse, most methods aimed to address one aspect of unconstrained facial changes only, such as lighting, pose, expression, or disguise. There was no any integrated technique to address these unconstrained challenges integrally. As a result, with continuous efforts of more than a decade, \u201cshallow\u201d methods only improved the accuracy of the LFW benchmark to about 95% [15], which indicates that \u201cshallow\u201d methods are insufficient to extract stable identity feature invariant to real-world changes. Due to the insufficiency of this technical, facial recognition systems were often reported with unstable performance or failures with countless false alarms in real-world applications.", "rationale": "The breakthrough happened in 2014, when DeepFace, the deep learning-based model, equaled the human capability in face recognition. Thus, other deep learning-based methods quickly followed reshaping the face recognition research."}, {"context": "In 2014, DeepFace [20] achieved the SOTA accuracy on the famous LFW benchmark [23], approaching human performance on the unconstrained condition for the first time (DeepFace: 97.35% vs. Human: 97.53%), by training a 9-layer model on 4 million facial images. Inspired by this work, research focus has shifted to deep-learning-based approaches, and the accuracy was dramatically boosted to above 99.80% in just three years. Deep learning technique has reshaped the research landscape of FR in almost all aspects such as algorithm designs, training/test datasets, application scenarios and even the evaluation protocols. Therefore, it is of great significance to review the breakthrough and rapid development process in recent years. There have been several surveys on FR [24, 25, 26, 27, 28] and its subdomains, and they mostly summarized and compared a diverse set of techniques related to a specific FR scene, such as illumination-invariant FR [29], 3D FR [28], pose-invariant FR [30][31]. Unfortunately, due to their earlier publication dates, none of them covered the deep learning methodology that is most successful nowadays. This survey focuses only on recognition problem, and one can refer to Ranjan et al. [32] for a brief review of a full deep FR pipeline with detection and alignment, or refer to Jin et al. [33] for a survey of face alignment. Specifically, the major contributions of this survey are as follows:", "rationale": "The traditional methods failed to address the face recognition task entirely, and most of the research was focused on improving face recognition separately in its different stages, thus there were no integrated techniques."}, {"context": "We present the development of face processing methods in chronological order in Fig. 12. As we can see from the figure, most papers attempted to perform face processing by autoencoder model in 2014 and 2015; while 3D model played an important role in 2016. GAN [40] has drawn substantial attention from the deep learning and computer vision community since it was first proposed by Goodfellow et al. It can be used in different fields and was also introduced into face processing in 2017. GAN can be used to perform \u201cone-to-many augmentation\u201d and \u201cmany-to-one normalization\u201d, and it broke the limit that face synthesis should be done under supervised way. Although GAN has not been widely used in face processing for training and recognition, it has great latent capacity for preprocessing, for example, Dual-Agent GANs (DA-GAN) [56] won the 1st places on verification and identification tracks in the NIST IJB-A 2017 FR competitions.", "rationale": "In 2014 and 2015 most research attempted to solve the face processing problem with autoencoders. In 2016, 3D models were important. However, the introduction of GANs in 2017 has been the latest trend in face processing."}], [{"context": "In general, traditional methods attempted to recognize human face by one or two layer representations, such as filtering responses, histogram of the feature codes, or distribution of the dictionary atoms. The research community studied intensively to separately improve the preprocessing, local descriptors, and feature transformation, but these approaches improved FR accuracy slowly. What\u2019s worse, most methods aimed to address one aspect of unconstrained facial changes only, such as lighting, pose, expression, or disguise. There was no any integrated technique to address these unconstrained challenges integrally. As a result, with continuous efforts of more than a decade, \u201cshallow\u201d methods only improved the accuracy of the LFW benchmark to about 95% [15], which indicates that \u201cshallow\u201d methods are insufficient to extract stable identity feature invariant to real-world changes. Due to the insufficiency of this technical, facial recognition systems were often reported with unstable performance or failures with countless false alarms in real-world applications.", "rationale": "In 2014, DeepFace achieved the SOTA by matching the human accuracy (DeepFace: 97.35% vs. Human: 97.53%) in Face Recognition. In just 3 years, deep learning approaches have achieved an accuracy of 99.8% surpassing humans."}, {"context": "With the evolved architectures and advanced training techniques, such as batch normalization (BN), the network becomes deeper and the training becomes more controllable. Following these architectures in object classification, the networks in deep FR are also developed step by step, and the performance of deep FR is continually improving. We present these mainstream architectures of deep FR in Fig. 9. In 2014, DeepFace [20] was the first to use a nine-layer CNN with several locally connected layers. With 3D alignment for face processing, it reaches an accuracy of 97.35% on LFW. In 2015, FaceNet [38] used a large private dataset to train a GoogleNet. It adopted a triplet loss function based on triplets of roughly aligned matching/nonmatching face patches generated by a novel online triplet mining method and achieved good performance of 99.63%. In the same year, VGGface [37] designed a procedure to collect a large-scale dataset from the Internet. It trained the VGGNet on this dataset and then fine-tuned the networks via a triplet loss function similar to FaceNet. VGGface obtains an accuracy of 98.95%. In 2017, SphereFace [84] used a 64-layer ResNet architecture and proposed the angular softmax (A-Softmax) loss to learn discriminative face features with angular margin. It boosts the achieves to 99.42% on LFW. In the end of 2017, a new large-scale face dataset, namely VGGface2 [39], was introduced, which consists of large variations in pose, age, illumination, ethnicity and profession. Cao et al. first trained a SENet with MS-celeb-1M dataset [45] and then fine-tuned the model with VGGface2 [39], and achieved the SOTA performance on the IJB-A [41] and IJB-B [42].", "rationale": "The traditional methods failed to address the face recognition task entirely, and most of the research was focused on improving face recognition separately in its different stages. Thus, the improvement was slow and the best accuracy achieved by such techniques was 95%. It was achieved by a shallow learning approach."}, {"context": "In 2014, DeepFace [20] achieved the SOTA accuracy on the famous LFW benchmark [23], approaching human performance on the unconstrained condition for the first time (DeepFace: 97.35% vs. Human: 97.53%), by training a 9-layer model on 4 million facial images. Inspired by this work, research focus has shifted to deep-learning-based approaches, and the accuracy was dramatically boosted to above 99.80% in just three years. Deep learning technique has reshaped the research landscape of FR in almost all aspects such as algorithm designs, training/test datasets, application scenarios and even the evaluation protocols. Therefore, it is of great significance to review the breakthrough and rapid development process in recent years. There have been several surveys on FR [24, 25, 26, 27, 28] and its subdomains, and they mostly summarized and compared a diverse set of techniques related to a specific FR scene, such as illumination-invariant FR [29], 3D FR [28], pose-invariant FR [30][31]. Unfortunately, due to their earlier publication dates, none of them covered the deep learning methodology that is most successful nowadays. This survey focuses only on recognition problem, and one can refer to Ranjan et al. [32] for a brief review of a full deep FR pipeline with detection and alignment, or refer to Jin et al. [33] for a survey of face alignment. Specifically, the major contributions of this survey are as follows:", "rationale": "As the datasets for training for deep learning based methods got bigger and bigger, the SOTA models kept increasing the accuracy of face recognition."}], [{"context": "Face recognition (FR) has been the prominent biometric technique for identity authentication and has been widely used in many areas, such as military, finance, public security and daily life. FR has been a long-standing research topic in the CVPR community. In the early 1990s, the study of FR became popular following the introduction of the historical Eigenface approach [1]. The milestones of feature-based FR over the past years are presented in Fig. 1, in which the times of four major technical streams are highlighted. The holistic approaches derive the low-dimensional representation through certain distribution assumptions, such as linear subspace [2][3][4], manifold [5][6][7], and sparse representation [8][9][10][11]. This idea dominated the FR community in the 1990s and 2000s. However, a well-known problem is that these theoretically plausible holistic methods fail to address the uncontrolled facial changes that deviate from their prior assumptions. In the early 2000s, this problem gave rise to local-feature-based FR. Gabor [12] and LBP [13], as well as their multilevel and high-dimensional extensions [14][15][16], achieved robust performance through some invariant properties of local filtering. Unfortunately, handcrafted features suffered from a lack of distinctiveness and compactness. In the early 2010s, learning-based local descriptors were introduced to the FR community [17][18][19], in which local filters are learned for better distinctiveness and the encoding codebook is learned for better compactness. However, these shallow representations still have an inevitable limitation on robustness against the complex nonlinear facial appearance variations.", "rationale": "Face recognition is being used in many user-cooperated applications."}, {"context": "\u2022Ubiquitous face recognition across applications and scenes. Deep face recognition has been successfully applied on many user-cooperated applications, but the ubiquitous recognition applications in everywhere are still an ambitious goal. In practice, it is difficult to collect and label sufficient samples for innumerable scenes in real world. One promising solution is to first learn a general model and then transfer it to an application-specific scene. While deep domain adaptation [145] has recently been applied to reduce the algorithm bias on different scenes [148], different races [173], general solution to transfer face recognition is largely open.", "rationale": "Face recognition is used in many areas such as military, finance, public security (surveillance), and daily life."}], [{"context": "Before a face image is fed to an FR module, face anti-spoofing, which recognizes whether the face is live or spoofed, is applied to avoid different types of attacks. Then, recognition can be performed. As shown in Fig. 3(c), an FR module consists of face processing, deep feature extraction and face matching, and it can be described as follows:", "rationale": "The 3 main modules of the face recognition system are: face detection used to localize faces, facial landmark detector which aligns the face to the normalized canonical coordinates, and the FR module which does the further processing."}, {"context": "As mentioned in [32], there are three modules needed for FR system, as shown in Fig. 3. First, a face detector is used to localize faces in images or videos. Second, with the facial landmark detector, the faces are aligned to normalized canonical coordinates. Third, the FR module is implemented with these aligned face images. We only focus on the FR module throughout the remainder of this paper.", "rationale": "Before feeding the image to a FR module, face anti-spoofing is implemented to prevent different types of attacks."}], [{"context": "For most applications, it is difficult to include the candidate faces during the training stage, which makes FR become a \u201czero-shot\u201d learning task. Fortunately, since all human faces share a similar shape and texture, the representation learned from a small proportion of faces can generalize well to the rest. Based on this theory, a straightforward way to improve generalized performance is to include as many IDs as possible in the training set. For example, Internet giants such as Facebook and Google have reported their deep FR system trained by 10^{6}-10^{7} IDs [38, 20].", "rationale": "By feeding the image to a deep learning model, the image goes through different layers that represent the different levels of abstraction. And through those layers, the transformations occur and the features are extracted. For example, early layers of a deep learning model tend to represent texture features, and in the later stages, more complex facial features such as smile, roar, etc appear. And after such transformations, in the end, an encoding of the face in the image can be obtained."}, {"context": "But all that changed in 2012 when AlexNet won the ImageNet competition by a large margin using a technique called deep learning [22]. Deep learning methods, such as convolutional neural networks, use a cascade of multiple layers of processing units for feature extraction and transformation. They learn multiple levels of representations that correspond to different levels of abstraction. The levels form a hierarchy of concepts, showing strong invariance to the face pose, lighting, and expression changes, as shown in Fig. 2. It can be seen from the figure that the first layer of the deep neural network is somewhat similar to the Gabor feature found by human scientists with years of experience. The second layer learns more complex texture features. The features of the third layer are more complex, and some simple structures have begun to appear, such as high-bridged nose and big eyes. In the fourth, the network output is enough to explain a certain facial attribute, which can make a special response to some clear abstract concepts such as smile, roar, and even blue eye. In conclusion, in deep convolutional neural networks (CNN), the lower layers automatically learn the features similar to Gabor and SIFT designed for years or even decades (such as initial layers in Fig. 2), and the higher layers further learn higher level abstraction. Finally, the combination of these higher level abstraction represents facial identity with unprecedented stability.", "rationale": "Deep learning techniques are also used for 3D face recognition, although the lack of large datasets hinders the overall research in this direction."}, {"context": "3D FR has inherent advantages over 2D methods, but 3D deep FR is not well developed due to the lack of large annotated 3D data. To enlarge 3D training datasets, most works use the methods of \u201cone-to-many augmentation\u201d to synthesize 3D faces. However, the effective methods for extracting deep features of 3D faces remain to be explored. Kim et al. [204] fine-tuned a 2D CNN with a small amount of 3D scans for 3D FR. Zulqarnain et al. [259] used a three-channel (corresponding to depth, azimuth and elevation angles of the normal vector) image as input and minimized the average prediction log-loss. Zhang et al. [260] first selected 30 feature points from the Candide-3 face model to characterize faces, then conducted the unsupervised pretraining of face depth data, and finally performed the supervised fine-tuning.", "rationale": "The deep learning model that was trained on a small portion of faces can still give a generalizable representation, as human faces are all similar to each other."}], [{"context": "3D model. 3D face reconstruction is also a way to enrich the diversity of training data. They utilize 3D structure information to model the transformation between poses. 3D models first use 3D face data to obtain morphable displacement fields and then apply them to obtain 2D face data in different pose angles. There is a large number of papers about this domain, but we only focus on the 3D face reconstruction using deep methods or used for deep FR. In [47], Masi et al. generated face images with new intra-class facial appearance variations, including pose, shape and expression, and then trained a 19-layer VGGNet with both real and augmented data. Masi et al. [48] used generic 3D faces and rendered fixed views to reduce much of the computational effort. Richardson et al. [49] employed an iterative 3D CNN by using a secondary input channel to represent the previous network\u2019s output as an image for reconstructing a 3D face as shown in Fig. 13. Dou et al. [51] used a multi-task CNN to divide 3D face reconstruction into neutral 3D reconstruction and expressive 3D reconstruction. Tran et al. [53] directly regressed 3D morphable face model (3DMM) [155] parameters from an input photo by a very deep CNN architecture. An et al. [156] synthesized face images with various poses and expressions using the 3DMM method, then reduced the gap between synthesized data and real data with the help of MMD.", "rationale": "Reconstructing 3D images from 2D images has been attempted to enlarge 3D face datasets, but still effective methods are yet to be discovered."}, {"context": "3D FR has inherent advantages over 2D methods, but 3D deep FR is not well developed due to the lack of large annotated 3D data. To enlarge 3D training datasets, most works use the methods of \u201cone-to-many augmentation\u201d to synthesize 3D faces. However, the effective methods for extracting deep features of 3D faces remain to be explored. Kim et al. [204] fine-tuned a 2D CNN with a small amount of 3D scans for 3D FR. Zulqarnain et al. [259] used a three-channel (corresponding to depth, azimuth and elevation angles of the normal vector) image as input and minimized the average prediction log-loss. Zhang et al. [260] first selected 30 feature points from the Candide-3 face model to characterize faces, then conducted the unsupervised pretraining of face depth data, and finally performed the supervised fine-tuning.", "rationale": "Mainly reconstructing 3D face data from 2D images were used to diversify the 2D face data. Some works focused on obtaining 2D face data in different poses, angles, shapes, and expressions using 3D reconstruction."}], [{"context": "3D model. 3D face reconstruction is also a way to enrich the diversity of training data. They utilize 3D structure information to model the transformation between poses. 3D models first use 3D face data to obtain morphable displacement fields and then apply them to obtain 2D face data in different pose angles. There is a large number of papers about this domain, but we only focus on the 3D face reconstruction using deep methods or used for deep FR. In [47], Masi et al. generated face images with new intra-class facial appearance variations, including pose, shape and expression, and then trained a 19-layer VGGNet with both real and augmented data. Masi et al. [48] used generic 3D faces and rendered fixed views to reduce much of the computational effort. Richardson et al. [49] employed an iterative 3D CNN by using a secondary input channel to represent the previous network\u2019s output as an image for reconstructing a 3D face as shown in Fig. 13. Dou et al. [51] used a multi-task CNN to divide 3D face reconstruction into neutral 3D reconstruction and expressive 3D reconstruction. Tran et al. [53] directly regressed 3D morphable face model (3DMM) [155] parameters from an input photo by a very deep CNN architecture. An et al. [156] synthesized face images with various poses and expressions using the 3DMM method, then reduced the gap between synthesized data and real data with the help of MMD.", "rationale": "Masi et al. were able to generate new images with different poses, shapes, and expressions. They also reduced the computational effort by rendering only fixed views. The iterative 3D CNN was used to reconstruct 3D images with a secondary input channel representing the output of the previous network. Multi-task CNN was used to divide the 3D reconstruction into neutral and expressive 3D face reconstruction. 3DMM model, which regressed 3D morphable face model parameters directly from an image, was used to synthesize images with different poses and expressions."}], [{"context": "Face recognition (FR) has been the prominent biometric technique for identity authentication and has been widely used in many areas, such as military, finance, public security and daily life. FR has been a long-standing research topic in the CVPR community. In the early 1990s, the study of FR became popular following the introduction of the historical Eigenface approach [1]. The milestones of feature-based FR over the past years are presented in Fig. 1, in which the times of four major technical streams are highlighted. The holistic approaches derive the low-dimensional representation through certain distribution assumptions, such as linear subspace [2][3][4], manifold [5][6][7], and sparse representation [8][9][10][11]. This idea dominated the FR community in the 1990s and 2000s. However, a well-known problem is that these theoretically plausible holistic methods fail to address the uncontrolled facial changes that deviate from their prior assumptions. In the early 2000s, this problem gave rise to local-feature-based FR. Gabor [12] and LBP [13], as well as their multilevel and high-dimensional extensions [14][15][16], achieved robust performance through some invariant properties of local filtering. Unfortunately, handcrafted features suffered from a lack of distinctiveness and compactness. In the early 2010s, learning-based local descriptors were introduced to the FR community [17][18][19], in which local filters are learned for better distinctiveness and the encoding codebook is learned for better compactness. However, these shallow representations still have an inevitable limitation on robustness against the complex nonlinear facial appearance variations.", "rationale": "Although deep face recognition models are surpassed humans, the identity capacity of deep representations is unknown and they are easily fooled by adversarial samples. These cannot be solved just by bigger and bigger datasets."}, {"context": "In general, traditional methods attempted to recognize human face by one or two layer representations, such as filtering responses, histogram of the feature codes, or distribution of the dictionary atoms. The research community studied intensively to separately improve the preprocessing, local descriptors, and feature transformation, but these approaches improved FR accuracy slowly. What\u2019s worse, most methods aimed to address one aspect of unconstrained facial changes only, such as lighting, pose, expression, or disguise. There was no any integrated technique to address these unconstrained challenges integrally. As a result, with continuous efforts of more than a decade, \u201cshallow\u201d methods only improved the accuracy of the LFW benchmark to about 95% [15], which indicates that \u201cshallow\u201d methods are insufficient to extract stable identity feature invariant to real-world changes. Due to the insufficiency of this technical, facial recognition systems were often reported with unstable performance or failures with countless false alarms in real-world applications.", "rationale": "The DeepFace, a deep learning-based method, surpassed humans for the first time in FR. Just in 3 years, the accuracy of deep learning-based methods reached 99.8%."}, {"context": "In 2014, DeepFace [20] achieved the SOTA accuracy on the famous LFW benchmark [23], approaching human performance on the unconstrained condition for the first time (DeepFace: 97.35% vs. Human: 97.53%), by training a 9-layer model on 4 million facial images. Inspired by this work, research focus has shifted to deep-learning-based approaches, and the accuracy was dramatically boosted to above 99.80% in just three years. Deep learning technique has reshaped the research landscape of FR in almost all aspects such as algorithm designs, training/test datasets, application scenarios and even the evaluation protocols. Therefore, it is of great significance to review the breakthrough and rapid development process in recent years. There have been several surveys on FR [24, 25, 26, 27, 28] and its subdomains, and they mostly summarized and compared a diverse set of techniques related to a specific FR scene, such as illumination-invariant FR [29], 3D FR [28], pose-invariant FR [30][31]. Unfortunately, due to their earlier publication dates, none of them covered the deep learning methodology that is most successful nowadays. This survey focuses only on recognition problem, and one can refer to Ranjan et al. [32] for a brief review of a full deep FR pipeline with detection and alignment, or refer to Jin et al. [33] for a survey of face alignment. Specifically, the major contributions of this survey are as follows:", "rationale": "In the 1990s there were holistic methods that designed low-dimensional representations of faces with distribution assumptions. However, these were too rigid and were not able to adapt to uncontrolled facial changes that did not fit the assumptions. Later, came the local-feature-based FR and its variations that tried to extract invariant properties with local filtering. But, the filters had to be handcrafted and lacked distinctiveness and compactness. In the 2010s, learning-based methods were used, but these shallow representations lacked the robustness to address the non-linearity and complexity of facial variations."}, {"context": "Despite the high accuracy in the LFW [23] and Megaface [44, 164] benchmarks, the performance of FR models still hardly meets the requirements in real-world application. A conjecture in industry is made that results of generic deep models can be improved simply by collecting big datasets of the target scene. However, this holds only to a certain degree. More and more concerns on privacy may make the collection and human-annotation of face data become illegal in the future. Therefore, significant efforts have been paid to design excellent algorithms to address the specific problems with limited data in these realistic scenes. In this section, we present several special algorithms of FR.", "rationale": "There were no traditional methods that addressed the FR problem integrally. Most methods tried to deal with one aspect of facial changes such as lighting, pose, expression, etc. Thus, \"shallow\" methods improved very slowly and could only reach 95% accuracy on the LFW benchmark."}, {"context": "\u2022Privacy-preserving face recognition. With the leakage of biological data, privacy concerns are raising nowadays. Facial images can predict not only demographic information such as gender, age, or race, but even the genetic information [297]. Recently, the pioneer works such as Semi-Adversarial Networks [298, 299, 285] have explored to generate a recognizable biometric templates that can hidden some of the private information presented in the facial images. Further research on the principles of visual cryptography, signal mixing and image perturbation to protect users\u2019 privacy on stored face templates are essential for addressing public concern on privacy.", "rationale": "Since deep learning-based models need large datasets, privacy is a big issue. Facial images are informative of private information that should be hidden."}, {"context": "\u2022Understanding deep face recognition. Deep face recognition systems are now believed to surpass human performance in most scenarios [300]. There are also some interesting attempts to apply deep models to assist human operators for face verification [183][300]. Despite this progress, many fundamental questions are still open, such as what is the \u201cidentity capacity\u201d of a deep representation [301]? Why deep neural networks, rather than humans, are easily fooled by adversarial samples? While bigger and bigger training dataset by itself cannot solve this problem, deeper understanding on these questions may help us to build robust applications in real world. Recently, a new benchmark called TALFW has been proposed to explore this issue [93].", "rationale": "Since privacy is the biggest hurdle in the path of collecting bigger and bigger datasets, the algorithms should be able to show great accuracy even with limited data."}], [{"context": "In 2017, people had a deeper understanding of loss function in deep FR and thought that samples should be separated more strictly to avoid misclassifying the difficult samples. Angular/cosine-margin-based loss [104, 84, 105, 106, 108] is proposed to make learned features potentially separable with a larger angular/cosine distance. The decision boundary in softmax loss is \\left(W_{1}-W_{2}\\right)x+b_{1}-b_{2}=0, where x is feature vector, W_{i} and b_{i} are weights and bias in softmax loss, respectively. Liu et al. [104] reformulated the original softmax loss into a large-margin softmax (L-Softmax) loss. They constrain b_{1}=b_{2}=0, so the decision boundaries for class 1 and class 2 become \\left\\|x\\right\\|\\left(\\left\\|W_{1}\\right\\|cos\\left(m\\theta_{1}\\right)-\\left\\|W_{2}\\right\\|cos\\left(\\theta_{2}\\right)\\right)=0 and \\left\\|x\\right\\|\\left(\\left\\|W_{1}\\right\\|\\left\\|W_{2}\\right\\|cos\\left(\\theta_{1}\\right)-cos\\left(m\\theta_{2}\\right)\\right)=0, respectively, where m is a positive integer introducing an angular margin, and \\theta_{i} is the angle between W_{i} and x. Due to the non-monotonicity of the cosine function, a piece-wise function is applied in L-softmax to guarantee the monotonicity. The loss function is defined as follows:\\mathcal{L}_{i}=-log\\left(\\frac{e^{\\left\\|W_{yi}\\right\\|\\left\\|x_{i}\\right\\|\\varphi(\\theta_{yi})}}{e^{\\left\\|W_{yi}\\right\\|\\left\\|x_{i}\\right\\|\\varphi(\\theta_{yi})+\\sum_{j\\neq y_{i}}e^{\\left\\|W_{yi}\\right\\|\\left\\|x_{i}\\right\\|cos(\\theta_{j})}}}\\right)(4)where\\varphi(\\theta)=(-1)^{k}cos(m\\theta)-2k,\\theta\\in\\left[\\frac{k\\pi}{m},\\frac{(k+1)\\pi}{m}\\right](5)Considering that L-Softmax is difficult to converge, it is always combined with softmax loss to facilitate and ensure the convergence. Therefore, the loss function is changed into: f_{y_{i}}=\\frac{\\lambda\\left\\|W_{y_{i}}\\right\\|\\left\\|x_{i}\\right\\|cos(\\theta_{y_{i}})+\\left\\|W_{y_{i}}\\right\\|\\left\\|x_{i}\\right\\|\\varphi(\\theta_{y_{i}})}{1+\\lambda}, where \\lambda is a dynamic hyper-parameter. Based on L-Softmax, A-Softmax loss [84] further normalized the weight W by L2 norm (\\left\\|W\\right\\|=1) such that the normalized vector will lie on a hypersphere, and then the discriminative face features can be learned on a hypersphere manifold with an angular margin (Fig. 6). Liu et al. [108] introduced a deep hyperspherical convolution network (SphereNet) that adopts hyperspherical convolution as its basic convolution operator and is supervised by angular-margin-based loss. To overcome the optimization difficulty of L-Softmax and A-Softmax, which incorporate the angular margin in a multiplicative manner, ArcFace [106] and CosFace [105], AMS loss [107] respectively introduced an additive angular/cosine margin cos(\\theta+m) and cos\\theta-m. They are extremely easy to implement without tricky hyper-parameters \\lambda, and are more clear and able to converge without the softmax supervision. The decision boundaries under the binary classification case are given in Table V. Based on large margin, FairLoss [122] and AdaptiveFace [123] further proposed to adjust the margins for different classes adaptively to address the problem of unbalanced data. Compared to Euclidean-distance-based loss, angular/cosine-margin-based loss explicitly adds discriminative constraints on a hypershpere manifold, which intrinsically matches the prior that human face lies on a manifold. However, Wang et al. [124] showed that angular/cosine-margin-based loss can achieve better results on a clean dataset, but is vulnerable to noise and becomes worse than center loss and softmax in the high-noise region as shown in Fig. 7.", "rationale": "Angular/cosine-margin-based loss allows the separation of learned features with larger angular/cosine distance. When the bias is removed and the weights are normalized in softmax, the outcome only depends on the angle between the weight and the features. Based on the prior that the human face lies on a manifold, the angular/cosine-margin-based loss explicitly adds discriminative constraints on a hypersphere manifold."}], [{"context": "\u2022We present a comparison and analysis on public available databases that are of vital importance for both model training and testing. Major FR benchmarks, such as LFW [23], IJB-A/B/C [41, 42, 43], Megaface [44], and MS-Celeb-1M [45], are reviewed and compared, in term of the four aspects: training methodology, evaluation tasks and metrics, and recognition scenes, which provides an useful reference for training and testing deep FR.", "rationale": "In Face Verification (whether the two images are of the same subject) usually measured using receiver operating characteristic (ROC) and estimated mean accuracy (Acc.). The ROC analysis measures the true accept rate (TAR) and false accept rate (FAR) of the model where a certain threshold is chosen. On the other hand, Acc calculates the percentage of correct classifications."}, {"context": "In order to evaluate whether our deep models can solve the different problems of FR in real life, many testing datasets are designed to evaluate the models in different tasks, i.e. face verification, close-set face identification and open-set face identification. In either task, a set of known subjects is initially enrolled in the system (the gallery), and during testing, a new subject (the probe) is presented. Face verification computes one-to-one similarity between the gallery and probe to determine whether the two images are of the same subject, whereas face identification computes one-to-many similarity to determine the specific identity of a probe face. When the probe appears in the gallery identities, this is referred to as closed-set identification; when the probes include those who are not in the gallery, this is open-set identification.", "rationale": "Popular benchmarks for FR are: LFW, IJB-A/B/C, Megaface, and MS-Celeb-1M."}, {"context": "Face verification is relevant to access control systems, re-identification, and application independent evaluations of FR algorithms. It is classically measured using the receiver operating characteristic (ROC) and estimated mean accuracy (Acc). At a given threshold (the independent variable), ROC analysis measures the true accept rate (TAR), which is the fraction of genuine comparisons that correctly exceed the threshold, and the false accept rate (FAR), which is the fraction of impostor comparisons that incorrectly exceed the threshold. And Acc is a simplified metric introduced by LFW [23], which represents the percentage of correct classifications. With the development of deep FR, more accurate recognitions are required. Customers concern more about the TAR when FAR is kept in a very low rate in most security certification scenario. PaSC [179] reports TAR at a FAR of 10^{-2}; IJB-A [41] evaluates TAR at a FAR of 10^{-3}; Megaface [44, 164] focuses on TAR@10^{-6}FAR; especially, in MS-celeb-1M challenge 3 [163], TAR@10^{-9}FAR is reported.", "rationale": "The benchmarks are designed to evaluate FR models on different tasks, for example, face verification, and open-set and close-set face identification."}, {"context": "Close-set face identification is relevant to user driven searches (e.g., forensic identification), rank-N and cumulative match characteristic (CMC) is commonly used metrics in this scenario. Rank-N is based on what percentage of probe searches return the probe\u2019s gallery mate within the top k rank-ordered results. The CMC curve reports the percentage of probes identified within a given rank (the independent variable). IJB-A/B/C [41, 42, 43] concern on the rank-1 and rank-5 recognition rate. The MegaFace challenge [44, 164] systematically evaluates rank-1 recognition rate function of increasing number of gallery distractors (going from 10 to 1 Million), the results of the SOTA evaluated on MegaFace challenge are listed in Table IX. Rather than rank-N and CMC, MS-Celeb-1M [45] further applies a precision-coverage curve to measure identification performance under a variable threshold t. The probe is rejected when its confidence score is lower than t. The algorithms are compared in term of what fraction of passed probes, i.e. coverage, with a high recognition precision, e.g. 95% or 99%, the results of the SOTA evaluated on MS-Celeb-1M challenge are listed in Table X.", "rationale": "To tackle difficult tasks of FR through ages/poses/sensors/styles new benchmarks are needed. It is also necessary to measure the age/racial/gender biases of the models."}, {"context": "\u2022Remaining challenges defined by non-saturated benchmark datasets. Three current major datasets, namely, MegaFace [44, 164] , MS-Celeb-1M [45] and IJB-A/B/C [41, 42, 43], are corresponding to large-scale FR with a very large number of candidates, low/one-shot FR and large pose-variance FR which will be the focus of research in the future. Although the SOTA algorithms can be over 99.9 percent accurate on LFW [23] and Megaface [44, 164] databases, fundamental challenges such as matching faces cross ages [181], poses [188], sensors, or styles still remain. For both datasets and algorithms, it is necessary to measure and address the racial/gender/age biases of deep FR in future research.", "rationale": "The smaller, faster, more efficient models are also needed for many applications."}, {"context": "\u2022Pursuit of extreme accuracy and efficiency. Many killer-applications, such as watch-list surveillance or financial identity verification, require high matching accuracy at very low alarm rate, e.g. 10^{-9}. It is still a big challenge even with deep learning on massive training data. Meanwhile, deploying deep face recognition on mobile devices pursues the minimum size of feature representation and compressed deep network. It is of great significance for both industry and academic to explore this extreme face-recognition performance beyond human imagination. It is also exciting to constantly push the performance limits of the algorithm after it has already surpassed human.", "rationale": "For close-set face identification rank-N (percentage of correct returns in top K rank-ordered results of the model) and cumulative match characteristic (CMC) (percentage of probes identified within a given rank) are used. Furthermore, MS-Celeb-1M uses a precision-coverage curve to measure the performance under the threshold."}, {"context": "Open-set face identification is relevant to high throughput face search systems (e.g., de-duplication, watch list identification), where the recognition system should reject unknown/unseen subjects (probes who do not present in gallery) at test time. At present, there are very few databases covering the task of open-set FR. IJB-A/B/C [41], [42], [43] benchmarks introduce a decision error tradeoff (DET) curve to characterize the the false negative identification rate (FNIR) as function of the false positive identification rate (FPIR). FPIR measures what fraction of comparisons between probe templates and non-mate gallery templates result in a match score exceeding T . At the same time, FNIR measures what fraction of probe searches will fail to match a mated gallery template above a score of T . The algorithms are compared in term of the FNIR at a low FPIR, e.g. 1% or 10%, the results of the SOTA evaluated on IJB-A dataset as listed in Table XI.", "rationale": "The open-set face identification uses a decision error tradeoff (DET) curve which shows the false negative identification rate (FNIR) as a function of the false positive identification rate (FPIR)."}], [{"context": "Collecting a large database is extremely expensive and time consuming. The methods of \u201cone-to-many augmentation\u201d can mitigate the challenges of data collection, and they can be used to augment not only training data but also the gallery of test data. we categorized them into four classes: data augmentation, 3D model, autoencoder model and GAN model.", "rationale": "The \"one-to-many augmentation\" methods can be used to augment the training data and test data, since collecting large databases is an expensive task."}, {"context": "Data augmentation. Common data augmentation methods consist of photometric transformations [75, 22] and geometric transformations, such as oversampling (multiple patches obtained by cropping at different scales) [22], mirroring [153], and rotating [154] the images. Recently, data augmentation has been widely used in deep FR algorithms [58, 59, 60, 35, 21, 36, 61, 62]. for example, Sun et al. [21] cropped 400 face patches varying in positions, scales, and color channels and mirrored the images. Liu et al. [58] generated seven overlapped image patches centered at different landmarks on the face region and trained them with seven CNNs with the same structure.", "rationale": "To increase the diversity of the dataset, some works focused on 3D model reconstruction. They can obtain 2D face data in different poses, shapes, angles, and expressions by reconstructing 3D models from 2D images."}, {"context": "3D model. 3D face reconstruction is also a way to enrich the diversity of training data. They utilize 3D structure information to model the transformation between poses. 3D models first use 3D face data to obtain morphable displacement fields and then apply them to obtain 2D face data in different pose angles. There is a large number of papers about this domain, but we only focus on the 3D face reconstruction using deep methods or used for deep FR. In [47], Masi et al. generated face images with new intra-class facial appearance variations, including pose, shape and expression, and then trained a 19-layer VGGNet with both real and augmented data. Masi et al. [48] used generic 3D faces and rendered fixed views to reduce much of the computational effort. Richardson et al. [49] employed an iterative 3D CNN by using a secondary input channel to represent the previous network\u2019s output as an image for reconstructing a 3D face as shown in Fig. 13. Dou et al. [51] used a multi-task CNN to divide 3D face reconstruction into neutral 3D reconstruction and expressive 3D reconstruction. Tran et al. [53] directly regressed 3D morphable face model (3DMM) [155] parameters from an input photo by a very deep CNN architecture. An et al. [156] synthesized face images with various poses and expressions using the 3DMM method, then reduced the gap between synthesized data and real data with the help of MMD.", "rationale": "A great number of works focused on data augmentation, in other words, doing photometric or geometric transformations to expand the dataset."}, {"context": "2) Assembled Networks : Multi-input networks. In \u201cone-to-many augmentation\u201d, multiple images with variety are generated from one image in order to augment training data. Taken these multiple images as input, multiple networks are also assembled together to extract and combine features of different type of inputs, which can outperform an individual network. In [58], [59], [60], [99], [34], [21], [35], assembled networks are built after different face patches are cropped, and then different types of patches are fed into different sub-networks for representation extraction. By combining the results of subnetworks, the performance can be improved. Other papers [96], [95], [98] used assembled networks to recognize images.", "rationale": "Assembled multi-input networks ([58], [59], [60], [99], [34], [21], [35]) showed better accuracy than individual networks when the \"one-to-many augmentation\" was used to generate multiple images from a single image."}, {"context": "Autoencoder model. Rather than reconstructing 3D models from a 2D image and projecting it back into 2D images of different poses, autoencoder models can generate 2D target images directly. Taken a face image and a pose code encoding a target pose as input, an encoder first learns pose-invariant face representation, and then a decoder generates a face image with the same identity viewed at the target pose by using the pose-invariant representation and the pose code. For example, given the target pose codes, multi-view perceptron (MVP) [55] trained some deterministic hidden neurons to learn pose-invariant face representations, and simultaneously trained some random hidden neurons to capture pose features, then a decoder generated the target images by combining pose-invariant representations with pose features. As shown in Fig. 14, Yim et al. [157] and Qian et al. [158] introduced an auxiliary CNN to generate better images viewed at the target poses. First, an autoencoder generated the desired pose image, then the auxiliary CNN reconstructed the original input image back from the generated target image, which guarantees that the generated image is identity-preserving. In [65], two groups of units are embedded between encoder and decoder. The identity units remain unchanged and the rotation of images is achieved by taking actions to pose units at each time step.", "rationale": "Autoencoder models are able to generate 2D faces at certain poses directly without reconstructing the 3D model. It is achieved through the pose-invariant encoding of the 2D image and decoding it along with pose features."}, {"context": "GAN model. In GAN models, a generator aims to fool a discriminator through generating images that resemble the real images, while the discriminator aims to discriminate the generated samples from the real ones. By this minimax game between generator and discriminator, GAN can successfully generate photo-realistic images with different poses. After using a 3D model to generate profile face images, DA-GAN [56] refined the images by a GAN, which combines prior knowledge of the data distribution and knowledge of faces (pose and identity perception loss). CVAE-GAN [159] combined a variational auto-encoder with a GAN for augmenting data, and took advantages of both statistic and pairwise feature matching to make the training process converge faster and more stably. In addition to synthesizing diverse faces from noise, some papers also explore to disentangle the identity and variation, and synthesize new faces by exchanging identity and variation from different people. In CG-GAN [160], a generator directly resolves each representation of input image into a variation code and an identity code and regroups these codes for cross-generating, simultaneously, a discriminator ensures the reality of generated images. Bao et al. [161] extracted identity representation of one input image and attribute representation of any other input face image, then synthesized new faces by recombining these representations. This work shows superior performance in generating realistic and identity preserving face images, even for identities outside the training dataset. Unlike previous methods that treat classifier as a spectator, FaceID-GAN [162] proposed a three-player GAN where the classifier cooperates together with the discriminator to compete with the generator from two different aspects, i.e. facial identity and image quality respectively.", "rationale": "GAN-based architectures were found to be effective in refining the 2D images from 3D reconstruction, and synthesizing new faces by combining identities and variation."}], [{"context": "Mainstream architectures. The commonly used network architectures of deep FR have always followed those of deep object classification and evolved from AlexNet to SENet rapidly. We present the most influential architectures of deep object classification and deep face recognition in chronological order 111The time we present is when the paper was published. in Fig. 8. ", "rationale": "The deep FR model architectures followed the deep object classification architectures starting from using the AlexNet and evolving to SeNet to boost the performance."}, {"context": "With the evolved architectures and advanced training techniques, such as batch normalization (BN), the network becomes deeper and the training becomes more controllable. Following these architectures in object classification, the networks in deep FR are also developed step by step, and the performance of deep FR is continually improving. We present these mainstream architectures of deep FR in Fig. 9. In 2014, DeepFace [20] was the first to use a nine-layer CNN with several locally connected layers. With 3D alignment for face processing, it reaches an accuracy of 97.35% on LFW. In 2015, FaceNet [38] used a large private dataset to train a GoogleNet. It adopted a triplet loss function based on triplets of roughly aligned matching/nonmatching face patches generated by a novel online triplet mining method and achieved good performance of 99.63%. In the same year, VGGface [37] designed a procedure to collect a large-scale dataset from the Internet. It trained the VGGNet on this dataset and then fine-tuned the networks via a triplet loss function similar to FaceNet. VGGface obtains an accuracy of 98.95%. In 2017, SphereFace [84] used a 64-layer ResNet architecture and proposed the angular softmax (A-Softmax) loss to learn discriminative face features with angular margin. It boosts the achieves to 99.42% on LFW. In the end of 2017, a new large-scale face dataset, namely VGGface2 [39], was introduced, which consists of large variations in pose, age, illumination, ethnicity and profession. Cao et al. first trained a SENet with MS-celeb-1M dataset [45] and then fine-tuned the model with VGGface2 [39], and achieved the SOTA performance on the IJB-A [41] and IJB-B [42].", "rationale": "As deep FR models followed the footsteps of deep object classification network architectures the performance got better, training got more controllable, and models got deeper. It started with DeepFace which was based on AlexNet that achieved 97.35% on the LFW benchmark. Then came the FaceNet based on GoogleNet which achieved 99.63%. VGGFace with a procedure to collect the large-scale dataset on the web and using the VGGNet architecture reached 98.95%. SphereFace used ResNet to achieve 99.42% accuracy. After the new VGGFace2 dataset was introduced Cao et al. trained a SENet-based architecture to achieve the SOTA for several datasets."}, {"context": "In 2014, DeepFace [20] achieved the SOTA accuracy on the famous LFW benchmark [23], approaching human performance on the unconstrained condition for the first time (DeepFace: 97.35% vs. Human: 97.53%), by training a 9-layer model on 4 million facial images. Inspired by this work, research focus has shifted to deep-learning-based approaches, and the accuracy was dramatically boosted to above 99.80% in just three years. Deep learning technique has reshaped the research landscape of FR in almost all aspects such as algorithm designs, training/test datasets, application scenarios and even the evaluation protocols. Therefore, it is of great significance to review the breakthrough and rapid development process in recent years. There have been several surveys on FR [24, 25, 26, 27, 28] and its subdomains, and they mostly summarized and compared a diverse set of techniques related to a specific FR scene, such as illumination-invariant FR [29], 3D FR [28], pose-invariant FR [30][31]. Unfortunately, due to their earlier publication dates, none of them covered the deep learning methodology that is most successful nowadays. This survey focuses only on recognition problem, and one can refer to Ranjan et al. [32] for a brief review of a full deep FR pipeline with detection and alignment, or refer to Jin et al. [33] for a survey of face alignment. Specifically, the major contributions of this survey are as follows:", "rationale": "Started with DeepFace with 97.35% LFW, in just 3 years deep learning-based models could achieve the accuracy of 99.8%."}], [{"context": "Face recognition (FR) has been the prominent biometric technique for identity authentication and has been widely used in many areas, such as military, finance, public security and daily life. FR has been a long-standing research topic in the CVPR community. In the early 1990s, the study of FR became popular following the introduction of the historical Eigenface approach [1]. The milestones of feature-based FR over the past years are presented in Fig. 1, in which the times of four major technical streams are highlighted. The holistic approaches derive the low-dimensional representation through certain distribution assumptions, such as linear subspace [2][3][4], manifold [5][6][7], and sparse representation [8][9][10][11]. This idea dominated the FR community in the 1990s and 2000s. However, a well-known problem is that these theoretically plausible holistic methods fail to address the uncontrolled facial changes that deviate from their prior assumptions. In the early 2000s, this problem gave rise to local-feature-based FR. Gabor [12] and LBP [13], as well as their multilevel and high-dimensional extensions [14][15][16], achieved robust performance through some invariant properties of local filtering. Unfortunately, handcrafted features suffered from a lack of distinctiveness and compactness. In the early 2010s, learning-based local descriptors were introduced to the FR community [17][18][19], in which local filters are learned for better distinctiveness and the encoding codebook is learned for better compactness. However, these shallow representations still have an inevitable limitation on robustness against the complex nonlinear facial appearance variations.", "rationale": "Local-feature-based methods were the prominent direction for FR in the early 2000s. Gabor and LBP tried using local filtering to extract invariant features, but the filters had to be handcrafted and they lacked distinctiveness and compactness."}], [{"context": "Inheriting from the object classification network such as AlexNet, the initial Deepface [20] and DeepID [34] adopted cross-entropy based softmax loss for feature learning. After that, people realized that the softmax loss is not sufficient by itself to learn discriminative features, and more researchers began to explore novel loss functions for enhanced generalization ability. This becomes the hottest research topic in deep FR research, as illustrated in Fig. 5. Before 2017, Euclidean-distance-based loss played an important role; In 2017, angular/cosine-margin-based loss as well as feature and weight normalization became popular. It should be noted that, although some loss functions share the similar basic idea, the new one is usually designed to facilitate the training procedure by easier parameter or sample selection.", "rationale": "Different loss functions for FR are categorized into 3 main approaches: Euclidean-distance-based loss, angular/cosine-margin-based loss, and softmax loss variations. Systematic evolution of these loss functions is provided."}, {"context": "\u2022A systematic review on the evolution of the network architectures and loss functions for deep FR is provided. Various loss functions are categorized into Euclidean-distance-based loss, angular/cosine-margin-based loss and softmax loss and its variations. Both the mainstream network architectures, such as Deepface [20], DeepID series [34, 35, 21, 36], VGGFace [37], FaceNet [38], and VGGFace2 [39], and other architectures designed for FR are covered.", "rationale": "Different loss functions for FR are categorized into 3 main approaches: Euclidean-distance-based loss, angular/cosine-margin-based loss, and softmax loss variations."}, {"context": "In this paper, we provide a comprehensive survey of deep FR from both data and algorithm aspects. For algorithms, mainstream and special network architectures are presented. Meanwhile, we categorize loss functions into Euclidean-distance-based loss, angular/cosine-margin-based loss and variable softmax loss. For data, we summarize some commonly used datasets. Moreover, the methods of face processing are introduced and categorized as \u201cone-to-many augmentation\u201d and \u201cmany-to-one normalization\u201d. Finally, the special scenes of deep FR, including video FR, 3D FR and cross-age FR, are briefly introduced.", "rationale": "Initially DeepFace and DeepID networks used cross-entropy softmax loss following AlexNet. However, it was not enough to learn the discriminative features."}, {"context": "1) Euclidean-distance-based Loss : Euclidean-distance-based loss is a metric learning method [118], [119] that embeds images into Euclidean space in which intra-variance is reduced and inter-variance is enlarged. The contrastive loss and the triplet loss are the commonly used loss functions. The contrastive loss [35], [21], [36], [61], [120] requires face image pairs, and then pulls together positive pairs and pushes apart negative pairs. L =yij max (0, \u2016f (xi) \u2212 f (xj )\u20162 \u2212 \u000f+) + (1 \u2212 yij )max (0, \u000f\u2212 \u2212 \u2016f (xi) \u2212 f (xj )\u20162 ) (2) where yij = 1 means xi and xj are matching samples and yij = 0 means non-matching samples. f (\u00b7) is the feature embedding, \u000f+ and \u000f\u2212 control the margins of the matching and non-matching pairs respectively. DeepID2 [21] combined the face identification (softmax) and verification (contrastive loss) supervisory signals to learn a discriminative representation, and joint Bayesian (JB) was applied to obtain a robust embedding space. Extending from DeepID2 [21], DeepID2+ [35] increased the dimension of hidden representations and added supervision to early convolutional layers. DeepID3 [36] further introduced VGGNet and GoogleNet to their work. However, the main problem with the contrastive loss is that the margin parameters are often difficult to choose.", "rationale": "The first type of loss is Euclidean-distance-based loss where images are embedded into Euclidean space and intra-variance is reduced and inter-variance is increased. The common loss functions are contrastive loss and triplet loss."}, {"context": "Contrary to contrastive loss that considers the absolute distances of the matching pairs and non-matching pairs, triplet loss considers the relative difference of the distances between them. Along with FaceNet [38] proposed by Google, Triplet loss [38], [37], [81], [80], [58], [60] was introduced into FR. It requires the face triplets, and then it minimizes the distance between an anchor and a positive sample of the same identity and maximizes the distance between the anchor and a negative sample of a different identity. FaceNet made \u2016f (xa i ) \u2212 f (xp i )\u20162 2 + \u03b1 < \u2212 \u2016f (xa i ) \u2212 f (xn i )\u20162 2 using hard triplet face samples, where xa i , xp i and xn i are the anchor, positive and negative samples, respectively, \u03b1 is a margin and f (\u00b7) represents a nonlinear transformation embedding an image into a feature space. Inspired by FaceNet [38], TPE [81] and TSE [80] learned a linear projection W to construct triplet loss. Other methods optimize deep models using both triplet loss and softmax loss [59], [58], [60], [121]. They first train networks with softmax and then fine-tune them with triplet loss.", "rationale": "Contrastive loss considers the absolute distances between matching and non-matching embeddings, whereas triplet loss requires face triplets, where the distance from an anchor to a positive sample is decreased and the distance to a negative sample is increased."}, {"context": "However, the contrastive loss and triplet loss occasionally encounter training instability due to the selection of effective training samples, some paper begun to explore simple alternatives. Center loss [101] and its variants [82], [116], [102] are good choices for reducing intra-variance. The center loss [101] learned a center for each class and penalized the distances between the deep features and their corresponding class centers. This loss can be defined as follows: LC = 1 2 m\u2211 i=1 \u2016xi \u2212 cyi \u20162 2 (3) where xi denotes the i-th deep feature belonging to the yi-th class and cyi denotes the yi-th class center of deep features. To handle the long-tailed data, a range loss [82], which is a variant of center loss, is used to minimize k greatest range\u2019s harmonic mean values in one class and maximize the shortest interclass distance within one batch. Wu et al. [102] proposed a center-invariant loss that penalizes the difference between each center of classes. Deng et al. [116] selected the farthest intraclass samples and the nearest inter-class samples to compute a margin loss. However, the center loss and its variants suffer from massive GPU memory consumption on the classification layer, and prefer balanced and sufficient training data for each identity.", "rationale": "The contrastive loss and triplet loss functions were prone to training instability, thus center loss and its variants (range loss, center-invariant loss) was a better choice. However, they suffer from high GPU memory consumption and prefer balanced training data for each identity."}, {"context": "2) Angular/cosine-margin-based Loss : In 2017, people had a deeper understanding of loss function in deep FR and thought that samples should be separated more strictly to avoid misclassifying the difficult samples. Angular/cosine-margin-based loss [104], [84], [105], [106], [108] is proposed to make learned features potentially separable with a larger angular/cosine distance. The decision boundary in softmax loss is (W1 \u2212 W2) x + b1 \u2212 b2 = 0, where x is feature vector, Wi and bi are weights and bias in softmax loss, respectively. Liu et al. [104] reformulated the original softmax loss into a large-margin softmax (L-Softmax) loss. They constrain b1 = b2 = 0, so the decision boundaries for class 1 and class 2 become \u2016x\u2016 (\u2016W1\u2016 cos (m\u03b81) \u2212 \u2016W2\u2016 cos (\u03b82)) = 0 and \u2016x\u2016 (\u2016W1\u2016 \u2016W2\u2016 cos (\u03b81) \u2212 cos (m\u03b82)) = 0, respectively, where m is a positive integer introducing an angular margin, and \u03b8i is the angle between Wi and x. Due to the nonmonotonicity of the cosine function, a piece-wise function is applied in L-softmax to guarantee the monotonicity. The loss function is defined as follows: Li = \u2212log ( e\u2016Wyi\u2016\u2016xi\u2016\u03c6(\u03b8yi) e\u2016Wyi\u2016\u2016xi\u2016\u03c6(\u03b8yi)+\u2211 j6 =yi e\u2016Wyi\u2016\u2016xi\u2016cos(\u03b8j ) ) (4) where \u03c6(\u03b8) = (\u22121)kcos(m\u03b8) \u2212 2k, \u03b8 \u2208 [ k\u03c0 m , (k + 1)\u03c0 m ] (5) Considering that L-Softmax is difficult to converge, it is always combined with softmax loss to facilitate and ensure the convergence. Therefore, the loss function is changed into: fyi = \u03bb\u2016Wyi \u2016\u2016xi\u2016cos(\u03b8yi )+\u2016Wyi \u2016\u2016xi\u2016\u03c6(\u03b8yi ) 1+\u03bb , where \u03bb is a dynamic hyper-parameter. Based on L-Softmax, A-Softmax loss [84] further normalized the weight W by L2 norm (\u2016W \u2016 = 1) such that the normalized vector will lie on a hypersphere, and then the discriminative face features can be learned on a hypersphere manifold with an angular margin (Fig. 6). Liu et al. [108] introduced a deep hyperspherical convolution network (SphereNet) that adopts hyperspherical convolution as its basic convolution operator and is supervised by angular-margin-based loss. To overcome the optimization difficulty of L-Softmax and A-Softmax, which incorporate the angular margin in a multiplicative manner, ArcFace [106] and CosFace [105], AMS loss [107] respectively introduced an additive angular/cosine margin cos(\u03b8 + m) and cos\u03b8 \u2212 m. They are extremely easy to implement without tricky hyperparameters \u03bb, and are more clear and able to converge without the softmax supervision. The decision boundaries under the binary classification case are given in Table V. Based on large margin, FairLoss [122] and AdaptiveFace [123] further proposed to adjust the margins for different classes adaptively to address the problem of unbalanced data. Compared to Euclidean-distance-based loss, angular/cosine-margin-based loss explicitly adds discriminative constraints on a hypershpere manifold, which intrinsically matches the prior that human face lies on a manifold. However, Wang et al. [124] showed that angular/cosine-margin-based loss can achieve better results on a clean dataset, but is vulnerable to noise and becomes worse than center loss and softmax in the high-noise region as shown in Fig. 7.", "rationale": "With a better understanding of loss functions for FR, researchers came to the angular/cosine-margin-based loss functions. It is a reformulation of the original softmax loss into large-margin softmax loss (L-Softmax), but it did not converge well, so it was combined with the softmax loss. Later, A-Softmax loss adopted the L-Softmax loss normalizing the weights. ArcFace, CosFace, and AMS loss functions further facilitated the convergence without softmax supervision. Furthermore, Fairloss and AdaptiveFace addressed the problem of unbalanced data. In general, angular/cosine-margin-based loss are better with clean training data but are vulnerable to noise."}, {"context": "3) Softmax Loss and its Variations : In 2017, in addition to reformulating softmax loss into an angular/cosine-margin-based loss as mentioned above, some works tries to normalize the features and weights in loss functions to improve the model performance, which can be written as follows: \u02c6W = W \u2016W \u2016 , \u02c6x = \u03b1 x \u2016x\u2016 (6) where \u03b1 is a scaling parameter, x is the learned feature vector, W is weight of last fully connected layer. Scaling x to a fixed radius \u03b1 is important, as Wang et al. [110] proved that normalizing both features and weights to 1 will make the softmax loss become trapped at a very high value on the training set. After that, the loss function, e.g. softmax, can be performed using the normalized features and weights.", "rationale": "Some works have tried to normalize the features and weights before performing the softmax."}, {"context": "Some papers [84], [108] first normalized the weights only and then added angular/cosine margin into loss functions to make the learned features be discriminative. In contrast, some works, such as [109], [111], adopted feature normalization only to overcome the bias to the sample distribution of the softmax. Based on the observation of [125] that the L2-norm of features learned using the softmax loss is informative of the quality of the face, L2-softmax [109] enforced all the features to have the same L2-norm by feature normalization such that similar attention is given to good quality frontal faces and blurry faces with extreme pose. Rather than scaling x to the parameter \u03b1, Hasnat et al. [111] normalized features with \u02c6x = x\u2212\u03bc\u221a\u03c32 , where \u03bc and \u03c32 are the mean and variance. Ring loss [117] encouraged the norm of samples being value R (a learned parameter) rather than explicit enforcing through a hard normalization operation. Moreover, normalizing both features and weights [110], [112], [115], [105], [106] has become a common strategy. Wang et al. [110] explained the necessity of this normalization operation from both analytic and geometric perspectives. After normalizing features and weights, CoCo loss [112] optimized the cosine distance among data features, and Hasnat et al. [115] used the von Mises-Fisher (vMF) mixture model as the theoretical basis to develop a novel vMF mixture loss and its corresponding vMF deep features.", "rationale": "There were works that normalized the weights only and then added angular/cosine-margin, others normalized only the features. Also, normalizing the L2-norms of features was found useful, as L2-norms were informative of the quality of the sample (L2-softmax). Ring loss did encourage the normalization of norms to learned parameter R. Other approaches like CoCo loss and vMF mixture loss normalized both weights and features."}], [{"context": "Despite the high accuracy in the LFW [23] and Megaface [44, 164] benchmarks, the performance of FR models still hardly meets the requirements in real-world application. A conjecture in industry is made that results of generic deep models can be improved simply by collecting big datasets of the target scene. However, this holds only to a certain degree. More and more concerns on privacy may make the collection and human-annotation of face data become illegal in the future. Therefore, significant efforts have been paid to design excellent algorithms to address the specific problems with limited data in these realistic scenes. In this section, we present several special algorithms of FR.", "rationale": "The performance of the FR models is still struggling to satisfy real-world applications. Despite the conjecture that larger and larger datasets will solve the problem, privacy concerns make data collection very difficult. Thus, brilliant algorithms that can work with limited data are needed."}, {"context": "With the emergence of mobile phones, tablets and augmented reality, FR has been applied in mobile devices. Due to computational limitations, the recognition tasks in these devices need to be carried out in a light but timely fashion. MobiFace [87] required efficient memory and low cost operators by adopting fast downsampling and bottleneck residual block, and achieves99.7% on LFW database and 91.3% on Megaface database. Tadmor et al. [263] proposed a multibatch method that first generates signatures for a minibatch of k face images and then constructs an unbiased estimate of the full gradient by relying on all k^{2}-k pairs from the minibatch. As mentioned in Section 3.2.1, light-weight deep networks [126, 127, 128, 129] perform excellently in the fundamental tasks of image classification and deserve further attention in FR tasks. Moreover, some well-known compressed networks such as Pruning [264, 265, 266], BinaryNets [267, 268, 269, 270], Mimic Networks [271, 272], also have potential to be introduced into FR.", "rationale": "Applying FR in mobile devices is an important problem that needs a solution under stricter conditions. Deep models like MobiFace and the multi-batch method are some of the work in this direction. However, the light networks and compressing methods as in image classification still need exploration in the FR context."}, {"context": "1) Cross-Pose Face Recognition: As [182] shows that many existing algorithms suffer a decrease of over 10% from frontal-frontal to frontal-profile verification, cross-pose FR is still an extremely challenging scene. In addition to the aforementioned methods, including \u201cone-to-many augmentation\u201d, \u201cmany-to-one normalization\u201d and assembled networks (Section IV and III-B.2), there are some other algorithms designed for cross-pose FR. Considering the extra burden of above methods, Cao et al. [215] attempted to perform frontalization in the deep feature space rather than the image space. A deep residual equivariant mapping (DREAM) block dynamically added residuals to an input representation to transform a profile face to a frontal image. Chen et al. [216] proposed to combine feature extraction with multi-view subspace learning to simultaneously make features be more pose-robust and discriminative. Pose Invariant Model (PIM) [217] jointly performed face frontalization and learned pose invariant representations end-to-end to allow them to mutually boost each other, and further introduced unsupervised cross-domain adversarial training and a learning to learn strategy to provide high-fidelity frontal reference face images.", "rationale": "Cross-pose FR is still a challenging problem for existing algorithms and over 10% decrease in accuracy was observed in frontal-frontal to frontal-profile verification. Techniques like DREAM and PIM were employed to perform frontalization in the deep face and learn pose-invariant representations."}, {"context": "2) Cross-Age Face Recognition: Cross-age FR is extremely challenging due to the changes in facial appearance by the aging process over time. One direct approach is to synthesize the desired image with target age such that the recognition can be performed in the same age group. A generative probabilistic model was used by [218] to model the facial aging process at each short-term stage. The identity-preserved conditional generative adversarial networks (IPCGANs) [219] framework utilized a conditional-GAN to generate a face in which an identity-preserved module preserved the identity information and an age classifier forced the generated face with the target age. Antipov et al. [220] proposed to age faces by GAN, but the synthetic faces cannot be directly used for face verification due to its imperfect preservation of identities. Then, they used a local manifold adaptation (LMA) approach [221] to solve the problem of [220]. In [222], high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales to generate more lifelike facial details. An alternative to address the cross-age problem is to decompose aging and identity components separately and extract age-invariant representations. Wen et al. [192] developed a latent identity analysis (LIA) layer to separate these two components, as shown in Fig. 22. In [193], age-invariant features were obtained by subtracting age-specific factors from the representations with the help of the age estimation task. In [124], face features are decomposed in the spherical coordinate system, in which the identity-related components are represented with angular coordinates and the age-related information is encoded with radial coordinate. Additionally, there are other methods designed for cross-age FR. For example, Bianco ett al. [223] and El et al. [224] fine-tuned the CNN to transfer knowledge across age. Wang et al. [225] proposed a siamese deep network to perform multi-task learning of FR and age estimation. Li et al. [226] integrated feature extraction and metric learning via a deep CNN.", "rationale": "Cross-age FR is also a natural problem as facial appearance changes over time. There were attempts to synthesize images from the same age group with a generative probabilistic model and conditional GANs were used to generate an identity-preserved face with a target age. Further, local manifold adaptation (LMA) and pyramidal adversarial discriminator approaches were tried to deal with the imperfect preservation of identities of GAN-synthesized images. Alternatively, decomposing the identity and age from each other was another direction. Latent identity analysis (LIA) and decomposing in a spherical coordinate system are some methods from that direction. Lastly, CNN fine-tuning, siamese deep network, feature extraction, and deep learning with CNN were some of the notable approaches."}, {"context": "3) Makeup Face Recognition: Makeup is widely used by the public today, but it also brings challenges for FR due to significant facial appearance changes. The research on matching makeup and nonmakeup face images is receiving increasing attention. Li et al. [208] generated nonmakeup images from makeup ones by a bi-level adversarial network (BLAN) and then used the synthesized nonmakeup images for verification as shown in Fig. 23. Sun et al. [227] pretrained a triplet network on videos and fine-tuned it on a small makeup datasets. Specially, facial disguise [214], [228], [229] is a challenging research topic in makeup face recognition. By using disguise accessories such as wigs, beard, hats, mustache, and heavy makeup, disguise introduces two variations: (i) when a person wants to obfuscate his/her own identity, and (ii) another individual impersonates someone else\u2019s identity. Obfuscation increases intra-class variations whereas impersonation reduces the inter-class dissimilarity, thereby affecting face recognition/verification task. To address this issue, a variety of methods are proposed. Zhang et al. [230] first trained two DCNNs for generic face recognition and then used Principal Components Analysis (PCA) to find the transformation matrix for disguised face recognition adaptation. Kohli et al. [231] finetuned models using disguised faces. Smirnov et al. [232] proposed a hard example mining method benefitted from class-wise (Doppelganger Mining [233]) and example-wise mining to learn useful deep embeddings for disguised face recognition. Suri et al. [234] learned the representations of images in terms of colors, shapes, and textures (COST) using an unsupervised dictionary learning method, and utilized the combination of COST features and CNN features to perform recognition.", "rationale": "Makeup FR is another real-world problem that needs a solution, as makeup can drastically change the appearance of the subject. Bi-level adversarial network (BLAN) was used to generate non makeup images from makeup images. Fine-tuning the triplet network with a small makeup dataset was another try. In particular, facial disguise is a big issue for FR as people can either want to hide their identity or impersonate another one. Identity hiding increases intra-class variation, while impersonation decreases inter-class distinction. Using DCNN and finding the transformation matrix with PCA for face disguise, fine-tuning models with disguised faces, hard example mining, and learning the representation of images in colors, shapes, and textures are some of the attempts to solve the issue."}, {"context": "1) NIR-VIS Face Recognition: Due to the excellent performance of the near-infrared spectrum (NIS) images under low-light scenarios, NIS images are widely applied in surveillance systems. Because most enrolled databases consist of visible light (VIS) spectrum images, how to recognize a NIR face from a gallery of VIS images has been a hot topic. Saxena et al. [235] and Liu et al. [236] transferred the VIS deep networks to the NIR domain by fine-tuning. Lezama et al. [237] used a VIS CNN to recognize NIR faces by transforming NIR images to VIS faces through cross-spectral hallucination and restoring a low-rank structure for features through low-rank embedding. Reale et al. [198] trained a VISNet (for visible images) and a NIRNet (for near-infrared images), and coupled their output features by creating a siamese network. He et al. [238], [239] divided the high layer of the network into a NIR layer, a VIS layer and a NIR-VIS shared layer, then, a modality-invariant feature can be learned by the NIR-VIS shared layer. Song et al. [240] embedded cross-spectral face hallucination and discriminative feature learning into an end-to-end adversarial network. In [196], the low-rank relevance and cross-modal ranking were used to alleviate the semantic gap.", "rationale": "NIR-VIS FR is needed to match the NIS images, (near-infrared spectrum) that usually come from surveillance contexts to VIS (visible light spectrum) images, as most of the available datasets contain VIS images. Transferring from VIS to NIR with fine-tuning, transforming NIR images to VIS with CNN, using the siamese network for each VIS and NIR respectively, dividing the network into NIR, VIS, and NIR-VIS layers to learn modality-invariant features, embedding cross-spectral face hallucination and discriminative features, and low-rank relevance and cross-modal ranking are some of the methods that were used to solve the issue."}, {"context": "2) Low-Resolution Face Recognition: Although deep networks are robust to low resolution to a great extent, there are still a few studies focused on promoting the performance of low-resolution FR. For example, Zangeneh et al. [241] proposed a CNN with a two-branch architecture (a super-resolution network and a feature extraction network) to map the high and low-resolution face images into a common space where the intra-person distance is smaller than the interperson distance. Shen et al. [242] exploited the face semantic information and local structural constraints to better restore the shape and detail of face images. In addition, they optimized the network with perceptual and adversarial losses to produce photo-realistic results.", "rationale": "Low-resolution FR needs addressing, although deep models are mostly robust to such cases. Mapping low and high-resolution faces into the same space with CNN, using face semantic information and local structural constraints to restore the shape and detail of the images are notable approaches in this direction."}, {"context": "3) Photo-Sketch Face Recognition: The photo-sketch FR may help law enforcement to quickly identify suspects. The commonly used methods can be categorized as two classes. One is to utilize transfer learning to directly match photos to sketches. Deep networks are first trained using a large face database of photos and are then fine-tuned using small sketch database [243], [244]. The other is to use the image-to-image translation, where the photo can be transformed to a sketch or the sketch to a photo; then, FR can be performed in one domain. Zhang et al. [200] developed a fully convolutional network with generative loss and a discriminative regularizer to transform photos to sketches. Zhang et al. [245] utilized a branched fully convolutional neural network (BFCN) to generate a structure-preserved sketch and a texture-preserved sketch, and then they fused them together via a probabilistic method. Recently, GANs have achieved impressive results in image generation. Yi et al. [246], Kim et al. [247] and Zhu et al. [248] used two generators, GA and GB , to generate sketches from photos and photos from sketches, respectively (Fig. 24). Based on [248], Wang et al. [202] proposed a multi-adversarial network to avoid artifacts by leveraging the implicit presence of feature maps of different resolutions in the generator subnetwork. Similar to photo-sketch FR, photocaricature FR is one kind of heterogenous FR scenes which is challenging and important to understanding of face perception. Huo et al. [213] built a large dataset of caricatures and photos, and provided several evaluation protocols and their baseline performances for comparison.", "rationale": "Photo-sketch FR can help find suspects effectively. Approaches usually either use transfer learning to directly match photos to sketches or perform image-to-image translation (image to sketch or sketch to an image). For the first type, training with images of faces and fine-tuning with sketches is one of the attempts. For the second type, branched fully convolution network (BFCN) and lately, GAN architectures were used to translate the image to sketch or vice versa."}, {"context": "1) Low-Shot Face Recognition: For many practical applications, such as surveillance and security, the FR system should recognize persons with a very limited number of training samples or even with only one sample. The methods of low-shot learning can be categorized as 1) synthesizing training data and 2) learning more powerful features. Hong et al. [249] generated images in various poses using a 3D face model and adopted deep domain adaptation to handle other variations, such as blur, occlusion, and expression (Fig. 25). Choe et al. [250] used data augmentation methods and a GAN for pose transition and attribute boosting to increase the size of the training dataset. Wu et al. [176] proposed a framework with hybrid classifiers using a CNN and a nearest neighbor (NN) model. Guo et al. [143] made the norms of the weight vectors of the one-shot classes and the normal classes aligned to address the data imbalance problem. Cheng et al. [137] proposed an enforced softmax that contains optimal dropout, selective attenuation, L2 normalization and model-level optimization. Yin et al. [251] augmented feature space of low-shot classes by transferring the principal components from regular to low-shot classes to encourage the variance of low-shot classes to mimic that of regular classes.", "rationale": "In many real-world scenarios low-shot FR is needed where only a few data points are available. Researchers tried to either synthesize more data or learn more meaningful features. 3D models, GANs, data augmentation, hybrid classifiers, and normalization are some of the attempts that were found useful."}, {"context": "2) Set/Template-Based Face Recognition: Different from traditional image-to-image recognition, set-to-set recognition takes a set (heterogeneous contents containing both images and videos) as the smallest unit of representation. This kind of setting does reflect the real-world biometric scenarios, thereby attracting a lot of attention. After learning face representations of media in each set, two strategies are generally adopted to perform set-to-set matching. One is to use these representations to perform pair-wise similarity comparison of two sets and aggregate the results into a single and final score by max score pooling [96], average score pooling [252] and its variations [253], [254]. The other strategy is feature pooling [96], [103], [81] which first aggregates face representations into a single representation for each set and then performs a comparison between two sets. In addition to the commonly used strategies, there are also some novel methods proposed for set/template-based FR. For example, Hayat et al. [255] proposed a deep heterogeneous feature fusion network to exploit the features\u2019 complementary information generated by different CNNs. Liu et al. [256] introduced the actor-critic reinforcement learning for set-based FR. They casted the inner-set dependency modeling to a Markov decision process in the latent space, and trained a dependency-aware attention control agent to make attention control for each image in each step.", "rationale": "Using not only a single image but a set of data as the smallest unit matches many of the biometric scenarios. There are 2 types of methods in set/template-based FR, either processing all the data in the set separately to find the matching score by combining the individual scores with a certain function or doing feature pooling which generates a single representation of the set and compares only them. Additionally, a deep heterogeneous feature fusion network and actor-critic reinforcement learning are some of the alternative attempts to deal with sets/templates."}, {"context": "3) Video Face Recognition: There are two key issues in video FR: one is to integrate the information across different frames together to build a representation of the video face, and the other is to handle video frames with severe blur, pose variations, and occlusions. For frame aggregation, Yang et al. [83] proposed a neural aggregation network (NAN) in which the aggregation module, consisting of two attention blocks driven by a memory, produces a 128-dimensional vector representation (Fig. 26). Rao et al. [187] aggregated raw video frames directly by combining the idea of metric learning and adversarial learning. For dealing with bad frames, Rao et al. [185] discarded the bad frames by treating this operation as a Markov decision process and trained the attention model through a deep reinforcement learning framework. Ding et al. [257] artificially blurred clear images for training to learn blur-robust face representations. Parchami et al. [258] used a CNN to reconstruct a lower-quality video into a high-quality face.", "rationale": "Video FR is also a complex problem consisting of combining the data across frames and handling individual frames with blur, pose variations, and occlusions. A neural aggregation network (NAN), combining metric and adversarial learning is some of the attempts to aggregate the frames. To deal with bad frames: deep reinforcement learning, learning blur-robust representations, and reconstruction of frames with CNN was tried."}, {"context": "1) 3D Face Recognition: 3D FR has inherent advantages over 2D methods, but 3D deep FR is not well developed due to the lack of large annotated 3D data. To enlarge 3D training datasets, most works use the methods of \u201cone-to-many augmentation\u201d to synthesize 3D faces. However, the effective methods for extracting deep features of 3D faces remain to be explored. Kim et al. [204] fine-tuned a 2D CNN with a small amount of 3D scans for 3D FR. Zulqarnain et al. [259] used a three-channel (corresponding to depth, azimuth and elevation angles of the normal vector) image as input and minimized the average prediction log-loss. Zhang et al. [260] first selected 30 feature points from the Candide-3 face model to characterize aces, then conducted the unsupervised pretraining of face depth data, and finally performed the supervised fine-tuning.", "rationale": "3D FR is underdeveloped due to a lack of good datasets. Despite attempts to enlarge such datasets with 3D reconstruction from 2D images, using 2D CNN, and using 3-channel inputs, the direction is still open for exploration."}, {"context": "2) Partial Face Recognition: Partial FR, in which only arbitrary-size face patches are presented, has become an emerging problem with increasing requirements of identification from CCTV cameras and embedded vision systems in mobile devices, robots and smart home facilities. He et al. [261] divided the aligned face image into several multi-scale patches, and the dissimilarity between two partial face images is calculated as the weighted L2 distance between corresponding patches. Dynamic feature matching (DFM) [262] utilized a sliding window of the same size as the probe feature maps to decompose the gallery feature maps into several gallery sub-feature maps, and the similarity-guided constraint imposed on sparse representation classification (SRC) provides an alignment-free matching.", "rationale": "Partial Face Recognition is emerging in several real-world scenarios where a decision should be made with only a part of the face available. Dividing the aligned image into multi-scale patches and Dynamic Feature Matching (DFM) are some of the approaches for Partial FR."}, {"context": "4) Face Anti-attack: With the success of FR techniques, various types of attacks, such as face spoofing and adversarial perturbations, are becoming large threats. Face spoofing involves presenting a fake face to the biometric sensor using a printed photograph, worn mask, or even an image displayed on another electronic device. In order to defense this type of attack, several methods are proposed [211], [273], [274], [275], [276], [277], [278], [279]. Atoum et al. [211] proposed a novel two-stream CNN in which the local features discriminate the spoof patches that are independent of the spatial face areas, and holistic depth maps ensure that the input live sample has a face-like depth. Yang et al. [273] trained a CNN using both a single frame and multiple frames with five scales as input, and using the live/spoof label as the output. Taken the sequence of video frames as input, Xu et al. [274] applied LSTM units on top of CNN to obtain end-to-end features to recognize spoofing faces which leveraged the local and dense property from convolution operation and learned the temporal structure using LSTM units. Li et al. [275] and Patel et al. [276] fine-tuned their networks from a pretrained model by training sets of real and fake images. Jourabloo et al. [277] proposed to inversely decompose a spoof face into the live face and the spoof noise pattern. Adversarial perturbation is the other type of attack which can be defined as the addition of a minimal vector r such that with addition of this vector into the input image x, i.e. (x + r), the deep learning models misclassifies the input while people will not. Recently, more and more work has begun to focus on solving this perturbation of FR. Goswami et al. [280] proposed to detect adversarial samples by characterizing abnormal filter response behavior in the hidden layers and increase the network\u2019s robustness by removing the most problematic filters. Goel et al. [281] provided an open source implementation of adversarial detection and mitigation algorithms. Despite of progresses of anti-attack algorithms, attack methods are updated as well and remind us the need to further increase security and robustness in FR systems, for example, Mai et al. [282] proposed a neighborly deconvolutional neural network (NbNet) to reconstruct a fake face using the stolen deep templates.", "rationale": "Face Anti-attack systems are needed to defend from face spoofing, adversarial perturbations, etc. For face spoofing, ensuring face-like depth with two-stream CNN, classification with CNN, and LSTM for sequences of frames were tried to resolve the issue. In terms of adversarial perturbation, detecting abnormal layers of the network to increase the robustness of the model was an idea. However, the attack methods evolve as well, thus continued work in this direction is necessary."}, {"context": "5) Debiasing face recognition: As described in Section V-A, existing datasets are highly biased in terms of the distribution of demographic cohorts, which may dramatically impact the fairness of deep models. To address this issue, there are some works that seek to introduce fairness into face recognition and mitigate demographic bias, e,g. unbalanced-training [283], attribute removal [284], [285], [286] and domain adaptation [173], [287], [147]. 1) Unbalanced-training methods mitigate the bias via model regularization, taking into consideration of the fairness goal in the overall model objective function. For example, RL-RBN [283] formulated the process of finding the optimal margins for non-Caucasians as a Markov decision process and employed deep Q-learning to learn policies based on large margin loss. 2) Attribute removal methods confound or remove demographic information of faces to learn attribute-invariant representations. For example, Alvi et al. [284] applied a confusion loss to make a classifier fail to distinguish attributes of examples so that multiple spurious variations are removed from the feature representation. SensitiveNets [288] proposed to introduce sensitive information into triplet loss. They minimized the sensitive information, while maintaining distances between positive and negative embeddings. 3) Domain adaptation methods propose to investigate data bias problem from a domain adaptation point of view and attempt to design domain-invariant feature representations to mitigate bias across domains. IMAN [173] simultaneously aligned global distribution to decrease race gap at domain-level, and learned the discriminative target representations at cluster level. Kan [147] directly converted the Caucasian data to non-Caucasian domain in the image space with the help of sparse reconstruction coefficients learnt in the common subspace.", "rationale": "Highly biased FR datasets impose fairness issues on FR models. Thus, debiasing attempts are made by unbalanced training, attribute removal, and domain adaptation. Unbalanced training, for example, RL-RBN,  tries to remove the bias of the model by regularization (i.e adjusting the objective function). The attribute-removal method tries to learn attribute-invariant representations by removing demographic information. Lastly, domain adaptation attempts to learn domain-invariant representations to avoid any domain bias."}]], "composition": ["Starting from the 1990s to the 2000s, holistic approaches were the most prominent direction in face recognition, Later on, local-feature-based face recognition was introduced. In the 2010s, shallow learning-based-local-descriptors were used. In 2014, the DeepFace, a deep learning-based model, was invented. And ever since, the state-of-the-art techniques were from deep learning-based approaches.", "The traditional methods (i.e holistic approaches, local-feature-based methods, shallow learning) were the approaches used before the boom of deep learning based techniques. They could achieve an accuracy of 95%, while the human accuracy was 97.53%. The rapid progress of deep learning methods quickly equaled human performance (DeepFace 97.35%) and later on surpassed them with 99.8%. These methods could achieve such numbers by using bigger and bigger datasets and new architectures.", "Face recognition is widely used in the military, finance, public security, and daily life.", "The 3 main modules are: face detection, facial landmark detector, and FR module.", "The deep learning model does feature extraction by processing the image through many layers and giving an encoding of the face that can be used to solve different FR tasks. The early layers of a deep learning model tend to represent simple textures that continuously evolve into facial structures in the later layers.", "The paper mainly talks about research on 3D face data reconstruction in terms of \"one-to-many augmentation\" methods. Also, attempts to enlarge 3D face datasets with the same method are mentioned. However, no challenges or specific issues during the process of 3D face reconstruction are included in the paper.", "The paper only talks about the line of work on 3D image reconstruction, in other words, the methods and approaches to reconstruct 3D face images. However, the quality and improvements that were made are not mentioned explicitly, but only referenced.", "There are broadly 4 methods that are used in FR. Holistic methods were the first-ever attempt to solve the FR problem. But they were too primitive and could not account for uncontrolled facial changes that did not fit its assumptions. Then, there are local feature-based methods that try to extract invariant properties with local filtering. However, although better than holistic methods, these are also short of complexity and capacity to address the vastness of facial appearances. The first learning-based methods also lacked the robustness to address the non-linearity and complexity of FR. Also, the efforts that were made in this direction were too scattered and there were no traditional methods that could address the FR problem entirely. Afterward, deep learning based methods were introduced which surpassed the human ability in FR. Unfortunately, these methods are prone to adversarial noises and need large datasets. In particular, designing bigger and bigger datasets is becoming a privacy issue, that is yet to be handled by deep FR models.", "Angular/cosine-margin-based loss allows the separation of learned features with larger angular/cosine distance. When the bias is removed and the weights are normalized in softmax, the outcome only depends on the angle between the weight and the features. Based on the prior that the human face lies on a manifold, the angular/cosine-margin-based loss explicitly adds discriminative constraints on a hypersphere manifold.", "Common metrics that are used to evaluate the accuracy of FR models are: ROC, Acc. for face verification; rank-N and CMC curve for closed-set face identification, and DET curve for open-set face identification. Also, the metrics for the complexity and size of FR models are important. Lastly, the metrics that measure the age/gender/racial bias of the FR models are becoming necessary.", "In terms of accuracy, the paper mentions a set of work done on assembled multi-input networks that used \"one-to-many augmentation\" methods to expand their dataset and achieve better results compared to individual networks. In terms of diversity, all data augmentation, 3D face reconstruction, autoencoders, and especially GANs were found to be effective in generating faces in certain poses, angles, with different expressions, etc.", "As deep FR models followed the footsteps of deep object classification network architectures the performance got better, training got more controllable, and models got deeper. It started with DeepFace which was based on AlexNet that achieved 97.35% on the LFW benchmark. Then came the FaceNet based on GoogleNet which achieved 99.63%. VGGFace with a procedure to collect the large-scale dataset on the web and using the VGGNet architecture reached 98.95%. SphereFace used ResNet to achieve 99.42% accuracy. After the new VGGFace2 dataset was introduced Cao et al. trained a SENet-based architecture to achieve the SOTA for several datasets.", "The only feature-based method that is mentioned in the paper is the local-feature-based methods from the 2000s of Gabor and LBP that tried local filtering to extract invariant properties. But they were too rigid and lacked distinctiveness and compactness.", "There are 3 categories of loss functions for FR: Euclidean-distance-based loss, angular/cosine-margin-based loss, and softmax loss variations. Initially, cross-entropy softmax loss was used, then some models tried using Euclidean-distance-based loss functions which started from contrastive loss and triplet loss. However, due to their instability, the center loss and its variants (range loss, center-invariant loss) were introduced. With a better understanding of loss functions for FR angular/cosine-margin-based loss functions were used. It began with a reformulation of a softmax loss called L-Softmax, later A-Softmax appeared which adopted the L-Softmax idea but tried normalizing the weights. Afterward, there were several improvements such as ArcFace, CosFace, and AMS which facilitated the convergence, while Fairloss and AdaptiveFace dealt with unbalanced data. Lastly, there are different variations of softmax that try to normalize the L2-norms (L2-softmax, Ring loss), the weights, the features, or both weights and features (CoCo loss and vMF mixture loss).", "Cross-pose FR is still a challenging problem for existing algorithms and over 10% decrease in accuracy was observed in frontal-frontal to frontal-profile verification. Techniques like DREAM and PIM were employed to perform frontalization in the deep face and learn pose-invariant representations. Cross-age FR is also a natural problem as facial appearance changes over time. There were attempts to synthesize images from the same age group with a generative probabilistic model and conditional GANs were used to generate an identity-preserved face with a target age. Further, local manifold adaptation (LMA) and pyramidal adversarial discriminator approaches were tried to deal with the imperfect preservation of identities of GAN-synthesized images. Alternatively, decomposing the identity and age from each other was another direction. Latent identity analysis (LIA) and decomposing in a spherical coordinate system are some methods from that direction. Lastly, CNN fine-tuning, siamese deep network, feature extraction, and deep learning with CNN were some of the notable approaches. Makeup FR is another real-world problem that needs a solution, as makeup can drastically change the appearance of the subject. Bi-level adversarial network (BLAN) was used to generate non makeup images from makeup images. Fine-tuning the triplet network with a small makeup dataset was another try. In particular, facial disguise is a big issue for FR as people can either want to hide their identity or impersonate another one. Identity hiding increases intra-class variation, while impersonation decreases inter-class distinction. Using DCNN and finding the transformation matrix with PCA for face disguise, fine-tuning models with disguised faces, hard example mining, and learning the representation of images in colors, shapes, and textures are some of the attempts to solve the issue. NIR-VIS FR is needed to match the NIS images, (near-infrared spectrum) that usually come from surveillance contexts to VIS (visible light spectrum) images, as most of the available datasets contain VIS images. Transferring from VIS to NIR with fine-tuning, transforming NIR images to VIS with CNN, using the siamese network for each VIS and NIR respectively, dividing the network into NIR, VIS, and NIR-VIS layers to learn modality-invariant features, embedding cross-spectral face hallucination and discriminative features, and low-rank relevance and cross-modal ranking are some of the methods that were used to solve the issue. Low-resolution FR needs addressing, although deep models are mostly robust to such cases. Mapping low and high-resolution faces into the same space with CNN, using face semantic information and local structural constraints to restore the shape and detail of the images are notable approaches in this direction. Photo-sketch FR can help find suspects effectively. Approaches usually either use transfer learning to directly match photos to sketches or perform image-to-image translation (image to sketch or sketch to an image). For the first type, training with images of faces and fine-tuning with sketches is one of the attempts. For the second type, branched fully convolution network (BFCN) and lately, GAN architectures were used to translate the image to sketch or vice versa. In many real-world scenarios low-shot FR is needed where only a few data points are available. Researchers tried to either synthesize more data or learn more meaningful features. 3D models, GANs, data augmentation, hybrid classifiers, and normalization are some of the attempts that were found useful. Using not only a single image but a set of data as the smallest unit matches many of the biometric scenarios. There are 2 types of methods in set/template-based FR, either processing all the data in the set separately to find the matching score by combining the individual scores with a certain function or doing feature pooling which generates a single representation of the set and compares only them. Additionally, a deep heterogeneous feature fusion network and actor-critic reinforcement learning are some of the alternative attempts to deal with sets/templates. Video FR is also a complex problem consisting of combining the data across frames and handling individual frames with blur, pose variations, and occlusions. A neural aggregation network (NAN), combining metric and adversarial learning is some of the attempts to aggregate the frames. To deal with bad frames: deep reinforcement learning, learning blur-robust representations, and reconstruction of frames with CNN was tried. 3D FR is underdeveloped due to a lack of good datasets. Despite attempts to enlarge such datasets with 3D reconstruction from 2D images, using 2D CNN, and using 3-channel inputs, the direction is still open for exploration. Partial Face Recognition is emerging in several real-world scenarios where a decision should be made with only a part of the face available. Dividing the aligned image into multi-scale patches and Dynamic Feature Matching (DFM) are some of the approaches for Partial FR. Applying FR in mobile devices is an important problem that needs a solution under stricter conditions. Deep models like MobiFace and the multi-batch method are some of the work in this direction. However, the light networks and compressing methods as in image classification still need exploration in the FR context. Face Anti-attack systems are needed to defend from face spoofing, adversarial perturbations, etc. For face spoofing, ensuring face-like depth with two-stream CNN, classification with CNN, and LSTM for sequences of frames were tried to resolve the issue. In terms of adversarial perturbation, detecting abnormal layers of the network to increase the robustness of the model was an idea. However, the attack methods evolve as well, thus continued work in this direction is necessary. Highly biased FR datasets impose fairness issues on FR models. Thus, debiasing attempts are made by unbalanced training, attribute removal, and domain adaptation. Unbalanced training, for example, RL-RBN,  tries to remove the bias of the model by regularization (i.e adjusting the objective function). The attribute-removal method tries to learn attribute-invariant representations by removing demographic information. Lastly, domain adaptation attempts to learn domain-invariant representations to avoid any domain bias."], "Is_figure_in_evidence": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true], "Is_table_in_evidence": [true, true, false, false, false, false, false, true, false, true, false, false, false, false, false], "question_key": ["799", "800", "801", "802", "803", "804", "805", "808", "810", "812", "813", "814", "815", "817", "818"], "passages": ["Face recognition (FR) has been the prominent biometric technique for identity authentication and has been widely used in many areas, such as military, finance, public security and daily life. FR has been a long-standing research topic in the CVPR community. In the early 1990s, the study of FR became popular following the introduction of the historical Eigenface approach [1]. The milestones of feature-based FR over the past years are presented in Fig. 1, in which the times of four major technical streams are highlighted. The holistic approaches derive the low-dimensional representation through certain distribution assumptions, such as linear subspace [2][3][4], manifold [5][6][7], and sparse representation [8][9][10][11]. This idea dominated the FR community in the 1990s and 2000s. However, a well-known problem is that these theoretically plausible holistic methods fail to address the uncontrolled facial changes that deviate from their prior assumptions. In the early 2000s, this problem gave rise to local-feature-based FR. Gabor [12] and LBP [13], as well as their multilevel and high-dimensional extensions [14][15][16], achieved robust performance through some invariant properties of local filtering. Unfortunately, handcrafted features suffered from a lack of distinctiveness and compactness. In the early 2010s, learning-based local descriptors were introduced to the FR community [17][18][19], in which local filters are learned for better distinctiveness and the encoding codebook is learned for better compactness. However, these shallow representations still have an inevitable limitation on robustness against the complex nonlinear facial appearance variations.", "In general, traditional methods attempted to recognize human face by one or two layer representations, such as filtering responses, histogram of the feature codes, or distribution of the dictionary atoms. The research community studied intensively to separately improve the preprocessing, local descriptors, and feature transformation, but these approaches improved FR accuracy slowly. What\u2019s worse, most methods aimed to address one aspect of unconstrained facial changes only, such as lighting, pose, expression, or disguise. There was no any integrated technique to address these unconstrained challenges integrally. As a result, with continuous efforts of more than a decade, \u201cshallow\u201d methods only improved the accuracy of the LFW benchmark to about 95% [15], which indicates that \u201cshallow\u201d methods are insufficient to extract stable identity feature invariant to real-world changes. Due to the insufficiency of this technical, facial recognition systems were often reported with unstable performance or failures with countless false alarms in real-world applications.", "But all that changed in 2012 when AlexNet won the ImageNet competition by a large margin using a technique called deep learning [22]. Deep learning methods, such as convolutional neural networks, use a cascade of multiple layers of processing units for feature extraction and transformation. They learn multiple levels of representations that correspond to different levels of abstraction. The levels form a hierarchy of concepts, showing strong invariance to the face pose, lighting, and expression changes, as shown in Fig. 2. It can be seen from the figure that the first layer of the deep neural network is somewhat similar to the Gabor feature found by human scientists with years of experience. The second layer learns more complex texture features. The features of the third layer are more complex, and some simple structures have begun to appear, such as high-bridged nose and big eyes. In the fourth, the network output is enough to explain a certain facial attribute, which can make a special response to some clear abstract concepts such as smile, roar, and even blue eye. In conclusion, in deep convolutional neural networks (CNN), the lower layers automatically learn the features similar to Gabor and SIFT designed for years or even decades (such as initial layers in Fig. 2), and the higher layers further learn higher level abstraction. Finally, the combination of these higher level abstraction represents facial identity with unprecedented stability.", "In 2014, DeepFace [20] achieved the SOTA accuracy on the famous LFW benchmark [23], approaching human performance on the unconstrained condition for the first time (DeepFace: 97.35% vs. Human: 97.53%), by training a 9-layer model on 4 million facial images. Inspired by this work, research focus has shifted to deep-learning-based approaches, and the accuracy was dramatically boosted to above 99.80% in just three years. Deep learning technique has reshaped the research landscape of FR in almost all aspects such as algorithm designs, training/test datasets, application scenarios and even the evaluation protocols. Therefore, it is of great significance to review the breakthrough and rapid development process in recent years. There have been several surveys on FR [24, 25, 26, 27, 28] and its subdomains, and they mostly summarized and compared a diverse set of techniques related to a specific FR scene, such as illumination-invariant FR [29], 3D FR [28], pose-invariant FR [30][31]. Unfortunately, due to their earlier publication dates, none of them covered the deep learning methodology that is most successful nowadays. This survey focuses only on recognition problem, and one can refer to Ranjan et al. [32] for a brief review of a full deep FR pipeline with detection and alignment, or refer to Jin et al. [33] for a survey of face alignment. Specifically, the major contributions of this survey are as follows:", "\u2022A systematic review on the evolution of the network architectures and loss functions for deep FR is provided. Various loss functions are categorized into Euclidean-distance-based loss, angular/cosine-margin-based loss and softmax loss and its variations. Both the mainstream network architectures, such as Deepface [20], DeepID series [34, 35, 21, 36], VGGFace [37], FaceNet [38], and VGGFace2 [39], and other architectures designed for FR are covered.", "\u2022We categorize the new face processing methods based on deep learning, such as those used to handle recognition difficulty on pose changes, into two classes: \u201cone-to-many augmentation\u201d and \u201cmany-to-one normalization\u201d, and discuss how emerging generative adversarial network (GAN) [40] facilitates deep FR.", "\u2022We present a comparison and analysis on public available databases that are of vital importance for both model training and testing. Major FR benchmarks, such as LFW [23], IJB-A/B/C [41, 42, 43], Megaface [44], and MS-Celeb-1M [45], are reviewed and compared, in term of the four aspects: training methodology, evaluation tasks and metrics, and recognition scenes, which provides an useful reference for training and testing deep FR.", "\u2022Besides the general purpose tasks defined by the major databases, we summarize a dozen scenario-specific databases and solutions that are still challenging for deep learning, such as anti-attack, cross-pose FR, and cross-age FR. By reviewing specially designed methods for these unsolved problems, we attempt to reveal the important issues for future research on deep FR, such as adversarial samples, algorithm/data biases, and model interpretability.", "The remainder of this survey is structured as follows. In Section II, we introduce some background concepts and terminologies, and then we briefly introduce each component of FR. In Section III, different network architectures and loss functions are presented. Then, we summarize the face processing algorithms and the datasets. In Section V, we briefly introduce several methods of deep FR used for different scenes. Finally, the conclusion of this paper and discussion of future works are presented in Section VI.", "As mentioned in [32], there are three modules needed for FR system, as shown in Fig. 3. First, a face detector is used to localize faces in images or videos. Second, with the facial landmark detector, the faces are aligned to normalized canonical coordinates. Third, the FR module is implemented with these aligned face images. We only focus on the FR module throughout the remainder of this paper.", "Before a face image is fed to an FR module, face anti-spoofing, which recognizes whether the face is live or spoofed, is applied to avoid different types of attacks. Then, recognition can be performed. As shown in Fig. 3(c), an FR module consists of face processing, deep feature extraction and face matching, and it can be described as follows:", "M[F(Pi(Ii)),F(Pj(Ij))]\ud835\udc40\ud835\udc39subscript\ud835\udc43\ud835\udc56subscript\ud835\udc3c\ud835\udc56\ud835\udc39subscript\ud835\udc43\ud835\udc57subscript\ud835\udc3c\ud835\udc57M[F(P_{i}(I_{i})),F(P_{j}(I_{j}))]italic_M [ italic_F ( italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) , italic_F ( italic_P start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ) ](1)where Iisubscript\ud835\udc3c\ud835\udc56I_{i}italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and Ijsubscript\ud835\udc3c\ud835\udc57I_{j}italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT are two face images, respectively. P\ud835\udc43Pitalic_P stands for face processing to handle intra-personal variations before training and testing, such as poses, illuminations, expressions and occlusions. F\ud835\udc39Fitalic_F denotes feature extraction, which encodes the identity information. The feature extractor is learned by loss functions when training, and is utilized to extract features of faces when testing. M\ud835\udc40Mitalic_M means a face matching algorithm used to compute similarity scores of features to determine the specific identity of faces. Different from object classification, the testing identities are usually disjoint from the training data in FR, which makes the learned classifier cannot be used to recognize testing faces. Therefore, face matching algorithm is an essential part in FR.", "Although deep-learning-based approaches have been widely used, Mehdipour et al. [46] proved that various conditions, such as poses, illuminations, expressions and occlusions, still affect the performance of deep FR. Accordingly, face processing is introduced to address this problem. The face processing methods are categorized as \u201cone-to-many augmentation\u201d and \u201cmany-to-one normalization\u201d, as shown in Table I.", "\u2022\u201cOne-to-many augmentation\u201d. These methods generate many patches or images of the pose variability from a single image to enable deep networks to learn pose-invariant representations.\u2022\u201cMany-to-one normalization\u201d. These methods recover the canonical view of face images from one or many images of a nonfrontal view; then, FR can be performed as if it were under controlled conditions.Note that we mainly focus on deep face processing method designed for pose variations in this paper, since pose is widely regarded as a major challenge in automatic FR applications and other variations can be solved by the similar methods.", "Network Architecture. The architectures can be categorized as backbone and assembled networks, as shown in Table II. Inspired by the extraordinary success on the ImageNet [74] challenge, the typical CNN architectures, e.g. AlexNet, VGGNet, GoogleNet, ResNet and SENet [22, 75, 76, 77, 78], are introduced and widely used as the baseline models in FR (directly or slightly modified). In addition to the mainstream, some assembled networks, e.g. multi-task networks and multi-input networks, are utilized in FR. Hu et al. [79] shows that accumulating the results of assembled networks provides an increase in performance compared with an individual network.", "Loss Function. The softmax loss is commonly used as the supervision signal in object recognition, and it encourages the separability of features. However, the softmax loss is not sufficiently effective for FR because intra-variations could be larger than inter-differences and more discriminative features are required when recognizing different people. Many works focus on creating novel loss functions to make features not only more separable but also discriminative, as shown in Table III.", "FR can be categorized as face verification and face identification. In either scenario, a set of known subjects is initially enrolled in the system (the gallery), and during testing, a new subject (the probe) is presented. After the deep networks are trained on massive data with the supervision of an appropriate loss function, each of the test images is passed through the networks to obtain a deep feature representation. Using cosine distance or L2 distance, face verification computes one-to-one similarity between the gallery and probe to determine whether the two images are of the same subject, whereas face identification computes one-to-many similarity to determine the specific identity of a probe face. In addition to these, other methods are introduced to postprocess the deep features such that the face matching is performed efficiently and accurately, such as metric learning, sparse-representation-based classifier (SRC), and so forth.", "To sum up, we present FR modules and their commonly-used methods in Fig. 4 to help readers to get a view of the whole FR. In deep FR, various training and testing face databases are constructed, and different architectures and losses of deep FR always follow those of deep object classification and are modified according to unique characteristics of FR. Moreover, in order to address unconstrained facial changes, face processing methods are further designed to handle poses, expressions and occlusions variations. Benefiting from these strategies, deep FR system significantly improves the SOTA and surpasses human performance. When the applications of FR becomes more and more mature in general scenario, recently, different solutions are driven for more difficult specific scenarios, such as cross-pose FR, cross-age FR, video FR. ", "For most applications, it is difficult to include the candidate faces during the training stage, which makes FR become a \u201czero-shot\u201d learning task. Fortunately, since all human faces share a similar shape and texture, the representation learned from a small proportion of faces can generalize well to the rest. Based on this theory, a straightforward way to improve generalized performance is to include as many IDs as possible in the training set. For example, Internet giants such as Facebook and Google have reported their deep FR system trained by 106\u2212107superscript106superscript10710^{6}-10^{7}10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT - 10 start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT IDs [38, 20].", "Unfortunately, these personal datasets, as well as prerequisite GPU clusters for distributed model training, are not accessible for academic community. Currently, public available training databases for academic research consist of only 103\u2212105superscript103superscript10510^{3}-10^{5}10 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT - 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT IDs. Instead, academic community makes effort to design effective loss functions and adopts efficient architectures to make deep features more discriminative using the relatively small training data sets. For instance, the accuracy of most popular LFW benchmark has been boosted from 97% to above 99.8% in the pasting four years, as enumerated in Table IV. In this section, we survey the research efforts on different loss functions and network architectures that have significantly improved deep FR methods.", "Inheriting from the object classification network such as AlexNet, the initial Deepface [20] and DeepID [34] adopted cross-entropy based softmax loss for feature learning. After that, people realized that the softmax loss is not sufficient by itself to learn discriminative features, and more researchers began to explore novel loss functions for enhanced generalization ability. This becomes the hottest research topic in deep FR research, as illustrated in Fig. 5. Before 2017, Euclidean-distance-based loss played an important role; In 2017, angular/cosine-margin-based loss as well as feature and weight normalization became popular. It should be noted that, although some loss functions share the similar basic idea, the new one is usually designed to facilitate the training procedure by easier parameter or sample selection.", "Euclidean-distance-based loss is a metric learning method [118, 119] that embeds images into Euclidean space in which intra-variance is reduced and inter-variance is enlarged. The contrastive loss and the triplet loss are the commonly used loss functions. The contrastive loss [35, 21, 36, 61, 120] requires face image pairs, and then pulls together positive pairs and pushes apart negative pairs.\u2112=yijmax(0,\u2016f(xi)\u2212f(xj)\u20162\u2212\u03f5+)+(1\u2212yij)max(0,\u03f5\u2212\u2212\u2016f(xi)\u2212f(xj)\u20162)\u2112subscript\ud835\udc66\ud835\udc56\ud835\udc57\ud835\udc5a\ud835\udc4e\ud835\udc650subscriptdelimited-\u2225\u2225\ud835\udc53subscript\ud835\udc65\ud835\udc56\ud835\udc53subscript\ud835\udc65\ud835\udc572superscriptitalic-\u03f51subscript\ud835\udc66\ud835\udc56\ud835\udc57\ud835\udc5a\ud835\udc4e\ud835\udc650superscriptitalic-\u03f5subscriptdelimited-\u2225\u2225\ud835\udc53subscript\ud835\udc65\ud835\udc56\ud835\udc53subscript\ud835\udc65\ud835\udc572\\begin{split}\\mathcal{L}=&y_{ij}max\\left(0,\\left\\|f(x_{i})-f(x_{j})\\right\\|_{2}-\\epsilon^{+}\\right)\\\\&+(1-y_{ij})max\\left(0,\\epsilon^{-}-\\left\\|f(x_{i})-f(x_{j})\\right\\|_{2}\\right)\\end{split}start_ROW start_CELL caligraphic_L = end_CELL start_CELL italic_y start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_m italic_a italic_x ( 0 , \u2225 italic_f ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_f ( italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_\u03f5 start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL + ( 1 - italic_y start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) italic_m italic_a italic_x ( 0 , italic_\u03f5 start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT - \u2225 italic_f ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_f ( italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) end_CELL end_ROW(2)where yij=1subscript\ud835\udc66\ud835\udc56\ud835\udc571y_{ij}=1italic_y start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1 means xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and xjsubscript\ud835\udc65\ud835\udc57x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT are matching samples and yij=0subscript\ud835\udc66\ud835\udc56\ud835\udc570y_{ij}=0italic_y start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 0 means non-matching samples. f(\u22c5)\ud835\udc53\u22c5f(\\cdot)italic_f ( \u22c5 ) is the feature embedding, \u03f5+superscriptitalic-\u03f5\\epsilon^{+}italic_\u03f5 start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT and \u03f5\u2212superscriptitalic-\u03f5\\epsilon^{-}italic_\u03f5 start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT control the margins of the matching and non-matching pairs respectively. DeepID2 [21] combined the face identification (softmax) and verification (contrastive loss) supervisory signals to learn a discriminative representation, and joint Bayesian (JB) was applied to obtain a robust embedding space. Extending from DeepID2 [21], DeepID2+ [35] increased the dimension of hidden representations and added supervision to early convolutional layers. DeepID3 [36] further introduced VGGNet and GoogleNet to their work. However, the main problem with the contrastive loss is that the margin parameters are often difficult to choose.", "Contrary to contrastive loss that considers the absolute distances of the matching pairs and non-matching pairs, triplet loss considers the relative difference of the distances between them. Along with FaceNet [38] proposed by Google, Triplet loss [38, 37, 81, 80, 58, 60] was introduced into FR. It requires the face triplets, and then it minimizes the distance between an anchor and a positive sample of the same identity and maximizes the distance between the anchor and a negative sample of a different identity. FaceNet made \u2016f(xia)\u2212f(xip)\u201622+\u03b1<\u2212\u2016f(xia)\u2212f(xin)\u201622superscriptsubscriptnorm\ud835\udc53superscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc4e\ud835\udc53superscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc5d22\ud835\udefcsuperscriptsubscriptnorm\ud835\udc53superscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc4e\ud835\udc53superscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc5b22\\left\\|f(x_{i}^{a})-f(x_{i}^{p})\\right\\|_{2}^{2}+\\alpha<-\\left\\|f(x_{i}^{a})-f(x_{i}^{n})\\right\\|_{2}^{2}\u2225 italic_f ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT ) - italic_f ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ) \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_\u03b1 < - \u2225 italic_f ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT ) - italic_f ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT using hard triplet face samples, where xiasuperscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc4ex_{i}^{a}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT, xipsuperscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc5dx_{i}^{p}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT and xinsuperscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc5bx_{i}^{n}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT are the anchor, positive and negative samples, respectively, \u03b1\ud835\udefc\\alphaitalic_\u03b1 is a margin and f(\u22c5)\ud835\udc53\u22c5f(\\cdot)italic_f ( \u22c5 ) represents a nonlinear transformation embedding an image into a feature space. Inspired by FaceNet [38], TPE [81] and TSE [80] learned a linear projection W\ud835\udc4aWitalic_W to construct triplet loss. Other methods optimize deep models using both triplet loss and softmax loss [59, 58, 60, 121]. They first train networks with softmax and then fine-tune them with triplet loss.", "However, the contrastive loss and triplet loss occasionally encounter training instability due to the selection of effective training samples, some paper begun to explore simple alternatives. Center loss [101] and its variants [82, 116, 102] are good choices for reducing intra-variance. The center loss [101] learned a center for each class and penalized the distances between the deep features and their corresponding class centers. This loss can be defined as follows:\u2112C=12\u2211i=1m\u2016xi\u2212cyi\u201622subscript\u2112\ud835\udc3612superscriptsubscript\ud835\udc561\ud835\udc5asuperscriptsubscriptnormsubscript\ud835\udc65\ud835\udc56subscript\ud835\udc50subscript\ud835\udc66\ud835\udc5622\\mathcal{L}_{C}=\\frac{1}{2}\\sum_{i=1}^{m}\\left\\|x_{i}-c_{y_{i}}\\right\\|_{2}^{2}caligraphic_L start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG 2 end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT \u2225 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_c start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT(3)where xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT denotes the i\ud835\udc56iitalic_i-th deep feature belonging to the yisubscript\ud835\udc66\ud835\udc56y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT-th class and cyisubscript\ud835\udc50subscript\ud835\udc66\ud835\udc56c_{y_{i}}italic_c start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT denotes the yisubscript\ud835\udc66\ud835\udc56y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT-th class center of deep features. To handle the long-tailed data, a range loss [82], which is a variant of center loss, is used to minimize k greatest range\u2019s harmonic mean values in one class and maximize the shortest inter-class distance within one batch. Wu et al. [102] proposed a center-invariant loss that penalizes the difference between each center of classes. Deng et al. [116] selected the farthest intra-class samples and the nearest inter-class samples to compute a margin loss. However, the center loss and its variants suffer from massive GPU memory consumption on the classification layer, and prefer balanced and sufficient training data for each identity.", "In 2017, people had a deeper understanding of loss function in deep FR and thought that samples should be separated more strictly to avoid misclassifying the difficult samples. Angular/cosine-margin-based loss [104, 84, 105, 106, 108] is proposed to make learned features potentially separable with a larger angular/cosine distance. The decision boundary in softmax loss is (W1\u2212W2)x+b1\u2212b2=0subscript\ud835\udc4a1subscript\ud835\udc4a2\ud835\udc65subscript\ud835\udc4f1subscript\ud835\udc4f20\\left(W_{1}-W_{2}\\right)x+b_{1}-b_{2}=0( italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) italic_x + italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0, where x\ud835\udc65xitalic_x is feature vector, Wisubscript\ud835\udc4a\ud835\udc56W_{i}italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and bisubscript\ud835\udc4f\ud835\udc56b_{i}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are weights and bias in softmax loss, respectively. Liu et al. [104] reformulated the original softmax loss into a large-margin softmax (L-Softmax) loss. They constrain b1=b2=0subscript\ud835\udc4f1subscript\ud835\udc4f20b_{1}=b_{2}=0italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0, so the decision boundaries for class 1 and class 2 become \u2016x\u2016(\u2016W1\u2016cos(m\u03b81)\u2212\u2016W2\u2016cos(\u03b82))=0norm\ud835\udc65normsubscript\ud835\udc4a1\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc5asubscript\ud835\udf031normsubscript\ud835\udc4a2\ud835\udc50\ud835\udc5c\ud835\udc60subscript\ud835\udf0320\\left\\|x\\right\\|\\left(\\left\\|W_{1}\\right\\|cos\\left(m\\theta_{1}\\right)-\\left\\|W_{2}\\right\\|cos\\left(\\theta_{2}\\right)\\right)=0\u2225 italic_x \u2225 ( \u2225 italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2225 italic_c italic_o italic_s ( italic_m italic_\u03b8 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - \u2225 italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2225 italic_c italic_o italic_s ( italic_\u03b8 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ) = 0 and \u2016x\u2016(\u2016W1\u2016\u2016W2\u2016cos(\u03b81)\u2212cos(m\u03b82))=0norm\ud835\udc65normsubscript\ud835\udc4a1normsubscript\ud835\udc4a2\ud835\udc50\ud835\udc5c\ud835\udc60subscript\ud835\udf031\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc5asubscript\ud835\udf0320\\left\\|x\\right\\|\\left(\\left\\|W_{1}\\right\\|\\left\\|W_{2}\\right\\|cos\\left(\\theta_{1}\\right)-cos\\left(m\\theta_{2}\\right)\\right)=0\u2225 italic_x \u2225 ( \u2225 italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2225 \u2225 italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2225 italic_c italic_o italic_s ( italic_\u03b8 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_c italic_o italic_s ( italic_m italic_\u03b8 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ) = 0, respectively, where m\ud835\udc5amitalic_m is a positive integer introducing an angular margin, and \u03b8isubscript\ud835\udf03\ud835\udc56\\theta_{i}italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the angle between Wisubscript\ud835\udc4a\ud835\udc56W_{i}italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and x\ud835\udc65xitalic_x. Due to the non-monotonicity of the cosine function, a piece-wise function is applied in L-softmax to guarantee the monotonicity. The loss function is defined as follows:\u2112i=\u2212log(e\u2016Wyi\u2016\u2016xi\u2016\u03c6(\u03b8yi)e\u2016Wyi\u2016\u2016xi\u2016\u03c6(\u03b8yi)+\u2211j\u2260yie\u2016Wyi\u2016\u2016xi\u2016cos(\u03b8j))subscript\u2112\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc54superscript\ud835\udc52normsubscript\ud835\udc4a\ud835\udc66\ud835\udc56normsubscript\ud835\udc65\ud835\udc56\ud835\udf11subscript\ud835\udf03\ud835\udc66\ud835\udc56superscript\ud835\udc52normsubscript\ud835\udc4a\ud835\udc66\ud835\udc56normsubscript\ud835\udc65\ud835\udc56\ud835\udf11subscript\ud835\udf03\ud835\udc66\ud835\udc56subscript\ud835\udc57subscript\ud835\udc66\ud835\udc56superscript\ud835\udc52normsubscript\ud835\udc4a\ud835\udc66\ud835\udc56normsubscript\ud835\udc65\ud835\udc56\ud835\udc50\ud835\udc5c\ud835\udc60subscript\ud835\udf03\ud835\udc57\\mathcal{L}_{i}=-log\\left(\\frac{e^{\\left\\|W_{yi}\\right\\|\\left\\|x_{i}\\right\\|\\varphi(\\theta_{yi})}}{e^{\\left\\|W_{yi}\\right\\|\\left\\|x_{i}\\right\\|\\varphi(\\theta_{yi})+\\sum_{j\\neq y_{i}}e^{\\left\\|W_{yi}\\right\\|\\left\\|x_{i}\\right\\|cos(\\theta_{j})}}}\\right)caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = - italic_l italic_o italic_g ( divide start_ARG italic_e start_POSTSUPERSCRIPT \u2225 italic_W start_POSTSUBSCRIPT italic_y italic_i end_POSTSUBSCRIPT \u2225 \u2225 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2225 italic_\u03c6 ( italic_\u03b8 start_POSTSUBSCRIPT italic_y italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT end_ARG start_ARG italic_e start_POSTSUPERSCRIPT \u2225 italic_W start_POSTSUBSCRIPT italic_y italic_i end_POSTSUBSCRIPT \u2225 \u2225 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2225 italic_\u03c6 ( italic_\u03b8 start_POSTSUBSCRIPT italic_y italic_i end_POSTSUBSCRIPT ) + \u2211 start_POSTSUBSCRIPT italic_j \u2260 italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT \u2225 italic_W start_POSTSUBSCRIPT italic_y italic_i end_POSTSUBSCRIPT \u2225 \u2225 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2225 italic_c italic_o italic_s ( italic_\u03b8 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT end_ARG )(4)where\u03c6(\u03b8)=(\u22121)kcos(m\u03b8)\u22122k,\u03b8\u2208[k\u03c0m,(k+1)\u03c0m]formulae-sequence\ud835\udf11\ud835\udf03superscript1\ud835\udc58\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc5a\ud835\udf032\ud835\udc58\ud835\udf03\ud835\udc58\ud835\udf0b\ud835\udc5a\ud835\udc581\ud835\udf0b\ud835\udc5a\\varphi(\\theta)=(-1)^{k}cos(m\\theta)-2k,\\theta\\in\\left[\\frac{k\\pi}{m},\\frac{(k+1)\\pi}{m}\\right]italic_\u03c6 ( italic_\u03b8 ) = ( - 1 ) start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_c italic_o italic_s ( italic_m italic_\u03b8 ) - 2 italic_k , italic_\u03b8 \u2208 [ divide start_ARG italic_k italic_\u03c0 end_ARG start_ARG italic_m end_ARG , divide start_ARG ( italic_k + 1 ) italic_\u03c0 end_ARG start_ARG italic_m end_ARG ](5)Considering that L-Softmax is difficult to converge, it is always combined with softmax loss to facilitate and ensure the convergence. Therefore, the loss function is changed into: fyi=\u03bb\u2016Wyi\u2016\u2016xi\u2016cos(\u03b8yi)+\u2016Wyi\u2016\u2016xi\u2016\u03c6(\u03b8yi)1+\u03bbsubscript\ud835\udc53subscript\ud835\udc66\ud835\udc56\ud835\udf06normsubscript\ud835\udc4asubscript\ud835\udc66\ud835\udc56normsubscript\ud835\udc65\ud835\udc56\ud835\udc50\ud835\udc5c\ud835\udc60subscript\ud835\udf03subscript\ud835\udc66\ud835\udc56normsubscript\ud835\udc4asubscript\ud835\udc66\ud835\udc56normsubscript\ud835\udc65\ud835\udc56\ud835\udf11subscript\ud835\udf03subscript\ud835\udc66\ud835\udc561\ud835\udf06f_{y_{i}}=\\frac{\\lambda\\left\\|W_{y_{i}}\\right\\|\\left\\|x_{i}\\right\\|cos(\\theta_{y_{i}})+\\left\\|W_{y_{i}}\\right\\|\\left\\|x_{i}\\right\\|\\varphi(\\theta_{y_{i}})}{1+\\lambda}italic_f start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT = divide start_ARG italic_\u03bb \u2225 italic_W start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2225 \u2225 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2225 italic_c italic_o italic_s ( italic_\u03b8 start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) + \u2225 italic_W start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2225 \u2225 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2225 italic_\u03c6 ( italic_\u03b8 start_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) end_ARG start_ARG 1 + italic_\u03bb end_ARG, where \u03bb\ud835\udf06\\lambdaitalic_\u03bb is a dynamic hyper-parameter. Based on L-Softmax, A-Softmax loss [84] further normalized the weight W\ud835\udc4aWitalic_W by L2 norm (\u2016W\u2016=1norm\ud835\udc4a1\\left\\|W\\right\\|=1\u2225 italic_W \u2225 = 1) such that the normalized vector will lie on a hypersphere, and then the discriminative face features can be learned on a hypersphere manifold with an angular margin (Fig. 6). Liu et al. [108] introduced a deep hyperspherical convolution network (SphereNet) that adopts hyperspherical convolution as its basic convolution operator and is supervised by angular-margin-based loss. To overcome the optimization difficulty of L-Softmax and A-Softmax, which incorporate the angular margin in a multiplicative manner, ArcFace [106] and CosFace [105], AMS loss [107] respectively introduced an additive angular/cosine margin cos(\u03b8+m)\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udf03\ud835\udc5acos(\\theta+m)italic_c italic_o italic_s ( italic_\u03b8 + italic_m ) and cos\u03b8\u2212m\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udf03\ud835\udc5acos\\theta-mitalic_c italic_o italic_s italic_\u03b8 - italic_m. They are extremely easy to implement without tricky hyper-parameters \u03bb\ud835\udf06\\lambdaitalic_\u03bb, and are more clear and able to converge without the softmax supervision. The decision boundaries under the binary classification case are given in Table V. Based on large margin, FairLoss [122] and AdaptiveFace [123] further proposed to adjust the margins for different classes adaptively to address the problem of unbalanced data. Compared to Euclidean-distance-based loss, angular/cosine-margin-based loss explicitly adds discriminative constraints on a hypershpere manifold, which intrinsically matches the prior that human face lies on a manifold. However, Wang et al. [124] showed that angular/cosine-margin-based loss can achieve better results on a clean dataset, but is vulnerable to noise and becomes worse than center loss and softmax in the high-noise region as shown in Fig. 7.", "In 2017, in addition to reformulating softmax loss into an angular/cosine-margin-based loss as mentioned above, some works tries to normalize the features and weights in loss functions to improve the model performance, which can be written as follows:W^=W\u2016W\u2016,x^=\u03b1x\u2016x\u2016formulae-sequence^\ud835\udc4a\ud835\udc4anorm\ud835\udc4a^\ud835\udc65\ud835\udefc\ud835\udc65norm\ud835\udc65\\hat{W}=\\frac{W}{\\left\\|W\\right\\|},\\hat{x}=\\alpha\\frac{x}{\\left\\|x\\right\\|}over^ start_ARG italic_W end_ARG = divide start_ARG italic_W end_ARG start_ARG \u2225 italic_W \u2225 end_ARG , over^ start_ARG italic_x end_ARG = italic_\u03b1 divide start_ARG italic_x end_ARG start_ARG \u2225 italic_x \u2225 end_ARG(6)where \u03b1\ud835\udefc\\alphaitalic_\u03b1 is a scaling parameter, x\ud835\udc65xitalic_x is the learned feature vector, W\ud835\udc4aWitalic_W is weight of last fully connected layer. Scaling x\ud835\udc65xitalic_x to a fixed radius \u03b1\ud835\udefc\\alphaitalic_\u03b1 is important, as Wang et al. [110] proved that normalizing both features and weights to 1 will make the softmax loss become trapped at a very high value on the training set. After that, the loss function, e.g. softmax, can be performed using the normalized features and weights. ", "Some papers [84, 108] first normalized the weights only and then added angular/cosine margin into loss functions to make the learned features be discriminative. In contrast, some works, such as [109, 111], adopted feature normalization only to overcome the bias to the sample distribution of the softmax. Based on the observation of [125] that the L2-norm of features learned using the softmax loss is informative of the quality of the face, L2-softmax [109] enforced all the features to have the same L2-norm by feature normalization such that similar attention is given to good quality frontal faces and blurry faces with extreme pose. Rather than scaling x\ud835\udc65xitalic_x to the parameter \u03b1\ud835\udefc\\alphaitalic_\u03b1, Hasnat et al. [111] normalized features with x^=x\u2212\u03bc\u03c32^\ud835\udc65\ud835\udc65\ud835\udf07superscript\ud835\udf0e2\\hat{x}=\\frac{x-\\mu}{\\sqrt{\\sigma^{2}}}over^ start_ARG italic_x end_ARG = divide start_ARG italic_x - italic_\u03bc end_ARG start_ARG square-root start_ARG italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG, where \u03bc\ud835\udf07\\muitalic_\u03bc and \u03c32superscript\ud835\udf0e2\\sigma^{2}italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT are the mean and variance. Ring loss [117] encouraged the norm of samples being value R\ud835\udc45Ritalic_R (a learned parameter) rather than explicit enforcing through a hard normalization operation. Moreover, normalizing both features and weights [110, 112, 115, 105, 106] has become a common strategy. Wang et al. [110] explained the necessity of this normalization operation from both analytic and geometric perspectives. After normalizing features and weights, CoCo loss [112] optimized the cosine distance among data features, and Hasnat et al. [115] used the von Mises-Fisher (vMF) mixture model as the theoretical basis to develop a novel vMF mixture loss and its corresponding vMF deep features.", "Mainstream architectures. The commonly used network architectures of deep FR have always followed those of deep object classification and evolved from AlexNet to SENet rapidly. We present the most influential architectures of deep object classification and deep face recognition in chronological order 111The time we present is when the paper was published. in Fig. 8. ", "In 2012, AlexNet [22] was reported to achieve the SOTA recognition accuracy in the ImageNet large-scale visual recognitioncompetition (ILSVRC) 2012, exceeding the previous best results by a large margin. AlexNet consists of five convolutional layers and three fully connected layers, and it also integrates various techniques, such as rectified linear unit (ReLU), dropout, data augmentation, and so forth. ReLU was widely regarded as the most essential component for making deep learning possible. Then, in 2014, VGGNet [75] proposed a standard network architecture that used very small 3\u00d73333\\times 33 \u00d7 3 convolutional filters throughout and doubled the number of feature maps after the 2\u00d7\\times\u00d72 pooling. It increased the depth of the network to 16-19 weight layers, which further enhanced the flexibility to learn progressive nonlinear mappings by deep architectures. In 2015, the 22-layer GoogleNet [76] introduced an \u201cinception module\u201d with the concatenation of hybrid feature maps, as well as two additional intermediate softmax supervised signals. It performs several convolutions with different receptive fields (1\u00d71111\\times 11 \u00d7 1, 3\u00d73333\\times 33 \u00d7 3 and 5\u00d75555\\times 55 \u00d7 5) in parallel, and concatenates all feature maps to merge the multi-resolution information. In 2016, ResNet [77] proposed to make layers learn a residual mapping with reference to the layer inputs \u2131(x):=\u210b(x)\u2212xassign\u2131\ud835\udc65\u210b\ud835\udc65\ud835\udc65\\mathcal{F}(x):=\\mathcal{H}(x)-xcaligraphic_F ( italic_x ) := caligraphic_H ( italic_x ) - italic_x rather than directly learning a desired underlying mapping \u210b(x)\u210b\ud835\udc65\\mathcal{H}(x)caligraphic_H ( italic_x ) to ease the training of very deep networks (up to 152 layers). The original mapping is recast into \u2131(x)+x\u2131\ud835\udc65\ud835\udc65\\mathcal{F}(x)+xcaligraphic_F ( italic_x ) + italic_x and can be realized by \u201cshortcut connections\u201d. As the champion of ILSVRC 2017, SENet [78] introduced a \u201cSqueeze-and-Excitation\u201d (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. These blocks can be integrated with modern architectures, such as ResNet, and improves their representational power.", "With the evolved architectures and advanced training techniques, such as batch normalization (BN), the network becomes deeper and the training becomes more controllable. Following these architectures in object classification, the networks in deep FR are also developed step by step, and the performance of deep FR is continually improving. We present these mainstream architectures of deep FR in Fig. 9. In 2014, DeepFace [20] was the first to use a nine-layer CNN with several locally connected layers. With 3D alignment for face processing, it reaches an accuracy of 97.35% on LFW. In 2015, FaceNet [38] used a large private dataset to train a GoogleNet. It adopted a triplet loss function based on triplets of roughly aligned matching/nonmatching face patches generated by a novel online triplet mining method and achieved good performance of 99.63%. In the same year, VGGface [37] designed a procedure to collect a large-scale dataset from the Internet. It trained the VGGNet on this dataset and then fine-tuned the networks via a triplet loss function similar to FaceNet. VGGface obtains an accuracy of 98.95%. In 2017, SphereFace [84] used a 64-layer ResNet architecture and proposed the angular softmax (A-Softmax) loss to learn discriminative face features with angular margin. It boosts the achieves to 99.42% on LFW. In the end of 2017, a new large-scale face dataset, namely VGGface2 [39], was introduced, which consists of large variations in pose, age, illumination, ethnicity and profession. Cao et al. first trained a SENet with MS-celeb-1M dataset [45] and then fine-tuned the model with VGGface2 [39], and achieved the SOTA performance on the IJB-A [41] and IJB-B [42].", "Light-weight networks. Using deeper neural network with hundreds of layers and millions of parameters to achieve higher accuracy comes at cost. Powerful GPUs with larger memory size are needed, which makes the applications on many mobiles and embedded devices impractical. To address this problem, light-weight networks are proposed. Light CNN [85, 86] proposed a max-feature-map (MFM) activation function that introduces the concept of maxout in the fully connected layer to CNN. The MFM obtains a compact representation and reduces the computational cost. Sun et al. [61] proposed to sparsify deep networks iteratively from the previously learned denser models based on a weight selection criterion. MobiFace [87] adopted fast downsampling and bottleneck residual block with the expansion layers and achievedhigh performance with 99.7% on LFW database. Although some other light-weight CNNs, such as SqueezeNet, MobileNet, ShuffleNet and Xception [126, 127, 128, 129], are still not widely used in FR, they deserve more attention.", "Adaptive-architecture networks. Considering that designing architectures manually by human experts are time-consuming and error-prone processes, there is growing interest in adaptive-architecture networks which can find well-performing architectures, e.g. the type of operation every layer executes (pooling, convolution, etc) and hyper-parameters associated with the operation (number of filters, kernel size and strides for a convolutional layer, etc), according to the specific requirements of training and testing data. Currently, neural architecture search (NAS) [130] is one of the promising methodologies, which has outperformed manually designed architectures on some tasks such as image classification [131] or semantic segmentation [132]. Zhu et al. [88] integrated NAS technology into face recognition. They used reinforcement learning [133] algorithm (policy gradient) to guide the controller network to train the optimal child architecture. Besides NAS, there are some other explorations to learn optimal architectures adaptively. For example, conditional convolutional neural network (c-CNN) [89] dynamically activated sets of kernels according to modalities of samples; Han et al. [90] proposed a novel contrastive convolution consisted of a trunk CNN and a kernel generator, which is beneficial owing to its dynamistic generation of contrastive kernels based on the pair of faces being compared.", "Joint alignment-recognition networks. Recently, an end-to-end system [91, 92, 93, 94] was proposed to jointly train FR with several modules (face detection, alignment, and so forth) together. Compared to the existing methods in which each module is generally optimized separately according to different objectives, this end-to-end system optimizes each module according to the recognition objective, leading to more adequate and robust inputs for the recognition model. For example, inspired by spatial transformer [134], Hayat et al. [91] proposed a CNN-based data-driven approach that learns to simultaneously register and represent faces (Fig. 10), while Wu et al. [92] designed a novel recursive spatial transformer (ReST) module for CNN allowing face alignment and recognition to be jointly optimized.", "Multi-input networks. In \u201cone-to-many augmentation\u201d, multiple images with variety are generated from one image in order to augment training data. Taken these multiple images as input, multiple networks are also assembled together to extract and combine features of different type of inputs, which can outperform an individual network. In [58, 59, 60, 99, 34, 21, 35], assembled networks are built after different face patches are cropped, and then different types of patches are fed into different sub-networks for representation extraction. By combining the results of sub-networks, the performance can be improved. Other papers [96, 95, 98] used assembled networks to recognize images with different poses. For example, Masi et al. [96] adjusted the pose to frontal (0\u2218superscript00^{\\circ}0 start_POSTSUPERSCRIPT \u2218 end_POSTSUPERSCRIPT), half-profile (40\u2218superscript4040^{\\circ}40 start_POSTSUPERSCRIPT \u2218 end_POSTSUPERSCRIPT) and full-profile views (75\u2218superscript7575^{\\circ}75 start_POSTSUPERSCRIPT \u2218 end_POSTSUPERSCRIPT) and then addressed pose variation by assembled pose networks. A multi-view deep network (MvDN) [95] consists of view-specific subnetworks and common subnetworks; the former removes view-specific variations, and the latter obtains common representations. ", "Multi-task networks. FR is intertwined with various factors, such as pose, illumination, and age. To solve this problem, multitask learning is introduced to transfer knowledge from other relevant tasks and to disentangle nuisance factors. In multi-task networks, identity classification is the main task and the side tasks are pose, illumination, and expression estimations, among others. The lower layers are shared among all the tasks, and the higher layers are disentangled into different sub-networks to generate the task-specific outputs. In [100], the task-specific sub-networks are branched out to learn face detection, face alignment, pose estimation, gender recognition, smile detection, age estimation and FR. Yin et al. [97] proposed to automatically assign the dynamic loss weights for each side task. Peng et al. [135] used a feature reconstruction metric learning to disentangle a CNN into sub-networks for jointly learning the identity and non-identity features as shown in Fig. 11.", "During testing, the cosine distance and L2 distance are generally employed to measure the similarity between the deep features x1subscript\ud835\udc651x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and x2subscript\ud835\udc652x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT; then, threshold comparison and the nearest neighbor (NN) classifier are used to make decision for verification and identification. In addition to these common methods, there are some other explorations.", "Metric learning, which aims to find a new metric to make two classes more separable, can also be used for face matching based on extracted deep features. The JB [136] model is a well-known metric learning method [35, 21, 36, 34, 120], and Hu et al. [79] proved that it can improve the performance greatly. In the JB model, a face feature x\ud835\udc65xitalic_x is modeled as x=\u03bc+\u03b5\ud835\udc65\ud835\udf07\ud835\udf00x=\\mu+\\varepsilonitalic_x = italic_\u03bc + italic_\u03b5, where \u03bc\ud835\udf07\\muitalic_\u03bc and \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 are identity and intra-personal variations, respectively. The similarity score r(x1,x2)\ud835\udc5fsubscript\ud835\udc651subscript\ud835\udc652r(x_{1},x_{2})italic_r ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) can be represented as follows:r(x1,x2)=logP(x1,x2|HI)P(x1,x2|HE)\ud835\udc5fsubscript\ud835\udc651subscript\ud835\udc652\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc43subscript\ud835\udc651conditionalsubscript\ud835\udc652subscript\ud835\udc3b\ud835\udc3c\ud835\udc43subscript\ud835\udc651conditionalsubscript\ud835\udc652subscript\ud835\udc3b\ud835\udc38r(x_{1},x_{2})=log\\frac{P\\left(x_{1},x_{2}|H_{I}\\right)}{P\\left(x_{1},x_{2}|H_{E}\\right)}italic_r ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = italic_l italic_o italic_g divide start_ARG italic_P ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_H start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ) end_ARG start_ARG italic_P ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_H start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT ) end_ARG(7)where P(x1,x2|HI)\ud835\udc43subscript\ud835\udc651conditionalsubscript\ud835\udc652subscript\ud835\udc3b\ud835\udc3cP(x_{1},x_{2}|H_{I})italic_P ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_H start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ) is the probability that two faces belong to the same identity and P(x1,x2|HE)\ud835\udc43subscript\ud835\udc651conditionalsubscript\ud835\udc652subscript\ud835\udc3b\ud835\udc38P(x_{1},x_{2}|H_{E})italic_P ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_H start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT ) is the probability that two faces belong to different identities.", "After cosine distance was computed, Cheng et al. [137] proposed a heuristic voting strategy at the similarity score level to combine the results of multiple CNN models and won first place in Challenge 2 of MS-celeb-1M 2017. Yang et al. [138] extracted the local adaptive convolution features from the local regions of the face image and used the extended SRC for FR with a single sample per person. Guo et al. [139] combined deep features and the SVM classifier to perform recognition. Wang et al. [62] first used product quantization (PQ) [140] to directly retrieve the top-k most similar faces and re-ranked these faces by combining similarities from deep features and the COTS matcher [141]. In addition, Softmax can be also used in face matching when the identities of training set and test set overlap. For example, in Challenge 2 of MS-celeb-1M, Ding et al. [142] trained a 21,000-class softmax classifier to directly recognize faces of one-shot classes and normal classes after augmenting feature by a conditional GAN; Guo et al. [143] trained the softmax classifier combined with underrepresented-classes promotion (UP) loss term to enhance the performance on one-shot classes.", "When the distributions of training data and testing data are the same, the face matching methods mentioned above are effective. However, there is always a distribution change or domain shift between two data domains that can degrade the performance on test data. Transfer learning [144, 145] has recently been introduced into deep FR to address the problem of domain shift. It learns transferable features using a labeled source domain (training data) and an unlabeled target domain (testing data) such that domain discrepancy is reduced and models trained on source domain will also perform well on target domain. Sometimes, this technology is applied to face matching. For example, Crosswhite et al. [121] and Xiong et al. [146] adopted template adaptation to the set of media in a template by combining CNN features with template-specific linear SVMs. But most of the time, it is not enough to do transfer learning only at face matching stage. Transfer learning should be embedded in deep models to learn more transferable representations. Kan et al. [147] proposed a bi-shifting autoencoder network (BAE) for domain adaptation across view angle, ethnicity, and imaging sensor; while Luo et al. [148] utilized the multi-kernels maximum mean discrepancy (MMD) to reduce domain discrepancies. Sohn et al. [149] used adversarial learning [150] to transfer knowledge from still image FR to video FR. Moreover, fine-tuning the CNN parameters from a prelearned model using a target training dataset is a particular type of transfer learning, and is commonly employed by numerous methods [151, 152, 103].", "We present the development of face processing methods in chronological order in Fig. 12. As we can see from the figure, most papers attempted to perform face processing by autoencoder model in 2014 and 2015; while 3D model played an important role in 2016. GAN [40] has drawn substantial attention from the deep learning and computer vision community since it was first proposed by Goodfellow et al. It can be used in different fields and was also introduced into face processing in 2017. GAN can be used to perform \u201cone-to-many augmentation\u201d and \u201cmany-to-one normalization\u201d, and it broke the limit that face synthesis should be done under supervised way. Although GAN has not been widely used in face processing for training and recognition, it has great latent capacity for preprocessing, for example, Dual-Agent GANs (DA-GAN) [56] won the 1st places on verification and identification tracks in the NIST IJB-A 2017 FR competitions.", "Collecting a large database is extremely expensive and time consuming. The methods of \u201cone-to-many augmentation\u201d can mitigate the challenges of data collection, and they can be used to augment not only training data but also the gallery of test data. we categorized them into four classes: data augmentation, 3D model, autoencoder model and GAN model.", "Data augmentation. Common data augmentation methods consist of photometric transformations [75, 22] and geometric transformations, such as oversampling (multiple patches obtained by cropping at different scales) [22], mirroring [153], and rotating [154] the images. Recently, data augmentation has been widely used in deep FR algorithms [58, 59, 60, 35, 21, 36, 61, 62]. for example, Sun et al. [21] cropped 400 face patches varying in positions, scales, and color channels and mirrored the images. Liu et al. [58] generated seven overlapped image patches centered at different landmarks on the face region and trained them with seven CNNs with the same structure.", "3D model. 3D face reconstruction is also a way to enrich the diversity of training data. They utilize 3D structure information to model the transformation between poses. 3D models first use 3D face data to obtain morphable displacement fields and then apply them to obtain 2D face data in different pose angles. There is a large number of papers about this domain, but we only focus on the 3D face reconstruction using deep methods or used for deep FR. In [47], Masi et al. generated face images with new intra-class facial appearance variations, including pose, shape and expression, and then trained a 19-layer VGGNet with both real and augmented data. Masi et al. [48] used generic 3D faces and rendered fixed views to reduce much of the computational effort. Richardson et al. [49] employed an iterative 3D CNN by using a secondary input channel to represent the previous network\u2019s output as an image for reconstructing a 3D face as shown in Fig. 13. Dou et al. [51] used a multi-task CNN to divide 3D face reconstruction into neutral 3D reconstruction and expressive 3D reconstruction. Tran et al. [53] directly regressed 3D morphable face model (3DMM) [155] parameters from an input photo by a very deep CNN architecture. An et al. [156] synthesized face images with various poses and expressions using the 3DMM method, then reduced the gap between synthesized data and real data with the help of MMD.", "Autoencoder model. Rather than reconstructing 3D models from a 2D image and projecting it back into 2D images of different poses, autoencoder models can generate 2D target images directly. Taken a face image and a pose code encoding a target pose as input, an encoder first learns pose-invariant face representation, and then a decoder generates a face image with the same identity viewed at the target pose by using the pose-invariant representation and the pose code. For example, given the target pose codes, multi-view perceptron (MVP) [55] trained some deterministic hidden neurons to learn pose-invariant face representations, and simultaneously trained some random hidden neurons to capture pose features, then a decoder generated the target images by combining pose-invariant representations with pose features. As shown in Fig. 14, Yim et al. [157] and Qian et al. [158] introduced an auxiliary CNN to generate better images viewed at the target poses. First, an autoencoder generated the desired pose image, then the auxiliary CNN reconstructed the original input image back from the generated target image, which guarantees that the generated image is identity-preserving. In [65], two groups of units are embedded between encoder and decoder. The identity units remain unchanged and the rotation of images is achieved by taking actions to pose units at each time step.", "GAN model. In GAN models, a generator aims to fool a discriminator through generating images that resemble the real images, while the discriminator aims to discriminate the generated samples from the real ones. By this minimax game between generator and discriminator, GAN can successfully generate photo-realistic images with different poses. After using a 3D model to generate profile face images, DA-GAN [56] refined the images by a GAN, which combines prior knowledge of the data distribution and knowledge of faces (pose and identity perception loss). CVAE-GAN [159] combined a variational auto-encoder with a GAN for augmenting data, and took advantages of both statistic and pairwise feature matching to make the training process converge faster and more stably. In addition to synthesizing diverse faces from noise, some papers also explore to disentangle the identity and variation, and synthesize new faces by exchanging identity and variation from different people. In CG-GAN [160], a generator directly resolves each representation of input image into a variation code and an identity code and regroups these codes for cross-generating, simultaneously, a discriminator ensures the reality of generated images. Bao et al. [161] extracted identity representation of one input image and attribute representation of any other input face image, then synthesized new faces by recombining these representations. This work shows superior performance in generating realistic and identity preserving face images, even for identities outside the training dataset. Unlike previous methods that treat classifier as a spectator, FaceID-GAN [162] proposed a three-player GAN where the classifier cooperates together with the discriminator to compete with the generator from two different aspects, i.e. facial identity and image quality respectively.", "In contrast to \u201cone-to-many augmentation\u201d, the methods of \u201cmany-to-one normalization\u201d produce frontal faces and reduce appearance variability of test data to make faces align and compare easily. It can be categorized as autoencoder model, CNN model and GAN model.", "Autoencoder model. Autoencoder can also be applied to \u201cmany-to-one normalization\u201d. Different from the autoencoder model in \u201cone-to-many augmentation\u201d which generates the desired pose images with the help of pose codes, autoencoder model here learns pose-invariant face representation by an encoder and directly normalizes faces by a decoder without pose codes. Zhu et al. [66, 67] selected canonical-view images according to the face images\u2019 symmetry and sharpness and then adopted an autoencoder to recover the frontal view images by minimizing the reconstruction loss error. The proposed stacked progressive autoencoders (SPAE) [63] progressively map the nonfrontal face to the frontal face through a stack of several autoencoders. Each shallow autoencoders of SPAE is designed to convert the input face images at large poses to a virtual view at a smaller pose, so the pose variations are narrowed down gradually layer by layer along the pose manifold. Zhang et al. [64] built a sparse many-to-one encoder to enhance the discriminant of the pose free feature by using multiple random faces as the target values for multiple encoders.", "CNN model. CNN models usually directly learn the 2D mappings between non-frontal face images and frontal images, and utilize these mapping to normalize images in pixel space. The pixels in normalized images are either directly the pixels or the combinations of the pixels in non-frontal images. In LDF-Net [68], the displacement field network learns the shifting relationship of two pixels, and the translation layer transforms the input non-frontal face image into a frontal one with this displacement field. In GridFace [69] shown in Fig. 15, first, the rectification network normalizes the images by warping pixels from the original image to the canonical one according to the computed homography matrix, then the normalized output is regularized by an implicit canonical view face prior, finally, with the normalized faces as input, the recognition network learns discriminative face representation via metric learning.", "GAN model. Huang et al. [70] proposed a two-pathway generative adversarial network (TP-GAN) that contains four landmark-located patch networks and a global encoder-decoder network. Through combining adversarial loss, symmetry loss and identity-preserving loss, TP-GAN generates a frontal view and simultaneously preserves global structures and local details as shown in Fig. 16. In a disentangled representation learning generative adversarial network (DR-GAN) [71], the generator serves as a face rotator, in which an encoder produces an identity representation, and a decoder synthesizes a face at the specified pose using this representation and a pose code. And the discriminator is trained to not only distinguish real vs. synthetic images, but also predict the identity and pose of a face. Yin et al. [73] incorporated 3DMM into the GAN structure to provide shape and appearance priors to guide the generator to frontalization.", "In the past three decades, many face databases have been constructed with a clear tendency from small-scale to large-scale, from single-source to diverse-sources, and from lab-controlled to real-world unconstrained condition, as shown in Fig. 17. As the performance of some simple databases become saturated, e.g. LFW [23], more and more complex databases were continually developed to facilitate the FR research. It can be said without exaggeration that the development process of the face databases largely leads the direction of FR research. In this section, we review the development of major training and testing academic databases for the deep FR.", "The prerequisite of effective deep FR is a sufficiently large training dataset. Zhou et al. [59] suggested that large amounts of data with deep learning improve the performance of FR. The results of Megaface Challenge also revealed that premier deep FR methods were typically trained on data larger than 0.5M images and 20K people. The early works of deep FR were usually trained on private training datasets. Facebook\u2019s Deepface [20] model was trained on 4M images of 4K people; Google\u2019s FaceNet [38] was trained on 200M images of 3M people; DeepID serial models [34, 35, 21, 36] were trained on 0.2M images of 10K people. Although they reported ground-breaking performance at this stage, researchers cannot accurately reproduce or compare their models without public training datasets.", "To address this issue, CASIA-Webface [120] provided the first widely-used public training dataset for the deep model training purpose, which consists of 0.5M images of 10K celebrities collected from the web. Given its moderate size and easy usage, it has become a great resource for fair comparisons for academic deep models. However, its relatively small data and ID size may not be sufficient to reflect the power of many advanced deep learning methods. Currently, there have been more databases providing public available large-scale training dataset (Table VI), especially three databases with over 1M images, namely MS-Celeb-1M [45], VGGface2 [39], and Megaface [44, 164], and we summary some interesting findings about these training sets, as shown in Fig. 18.", "Depth v.s. breadth. These large training sets are expanded from depth or breadth. VGGface2 provides a large-scale training dataset of depth, which have limited number of subjects but many images for each subjects. The depth of dataset enforces the trained model to address a wide range intra-class variations, such as lighting, age, and pose. In contrast, MS-Celeb-1M and Mageface (Challenge 2) offers large-scale training datasets of breadth, which contains many subject but limited images for each subjects. The breadth of dataset ensures the trained model to cover the sufficiently variable appearance of various people. Cao et al. [39] conducted a systematic studies on model training using VGGface2 and MS-Celeb-1M, and found an optimal model by first training on MS-Celeb-1M (breadth) and then fine-tuning on VGGface2 (depth).", "Long tail distribution. The utilization of long tail distribution is different among datasets. For example, in Challenge 2 of MS-Celeb-1M, the novel set specially uses the tailed data to study low-shot learning; central part of the long tail distribution is used by the Challenge 1 of MS-Celeb-1M and images\u2019 number is approximately limited to 100 for each celebrity; VGGface and VGGface2 only use the head part to construct deep databases; Megaface utilizes the whole distribution to contain as many images as possible, the minimal number of images is 3 per person and the maximum is 2469.", "Data engineering. Several popular benchmarks, such as LFW unrestricted protocol, Megaface Challenge 1, MS-Celeb-1M Challenge 1&2, explicitly encourage researchers to collect and clean a large-scale data set for enhancing the capability of deep neural network. Although data engineering is a valuable problem to computer vision researchers, this protocol is more incline to the industry participants. As evidence, the leaderboards of these experiments are mostly occupied by the companies holding invincible hardwares and data scales. This phenomenon may not be beneficial for developments of new models in academic community.", "Data noise. Owing to data source and collecting strategies, existing large-scale datasets invariably contain label noises. Wang et al. [124] profiled the noise distribution in existing datasets in Fig. 19 and showed that the noise percentage increases dramatically along the scale of data. Moreover, they found that noise is more lethal on a 10,000-class problem of FR than on a 10-class problem of object classification and that label flip noise severely deteriorates the performance of a model, especially the model using A-softmax [84]. Therefore, building a sufficiently large and clean dataset for academic research is very meaningful. Deng et al. [106] found there are serious label noise in MS-Celeb-1M [45], and they cleaned the noise of MS-Celeb-1M, and made the refined dataset public available. Microsoft and Deepglint jointly released the largest public data set [163] with cleaned labels, which includes 4M images cleaned from MS-Celeb-1M dataset and 2.8M aligned images of 100K Asian celebrities. Moreover, Zhan et al. [167] shifted the focus from cleaning the datasets to leveraging more unlabeled data. Through automatically assigning pseudo labels to unlabeled data with the help of relational graphs, they obtained competitive or even better results over the fully-supervised counterpart.", "Data bias. Large-scale training datasets, such as CASIA-WebFace [120], VGGFace2 [39] and MS-Celeb-1M [45], are typically constructed by scraping websites like Google Images, and consist of celebrities on formal occasions: smiling, make-up, young, and beautiful. They are largely different from databases captured in the daily life (e.g. Megaface). The biases can be attributed to many exogenous factors in data collection, such as cameras, lightings, preferences over certain types of backgrounds, or annotator tendencies. Dataset biases adversely affect cross-dataset generalization; that is, the performance of the model trained on one dataset drops significantly when applied to another one. One persuasive evidence is presented by P.J. Phillips\u2019 study [168] which conducted a cross benchmark assessment of VGGFace model [37] for face recognition. The VGGFace model achieves 98.95% on LFW [23] and 97.30% on YTF [169], but only obtains 26%, 52% and 85% on Ugly, Bad and Good partition of GBU database [170]. ", "Demographic bias (e.g., race/ethnicity, gender, age) in datasets is a universal but urgent issue to be solved in data bias field. In existing training and testing datasets, the male, White, and middle-aged cohorts always appear more frequently, as shown in Table VII, which inevitably causes deep learning models to replicate and even amplify these biases resulting in significantly different accuracies when deep models are applied to different demographic groups. Some researches [145, 171, 172] showed that the female, Black, and younger cohorts are usually more difficult to recognize in FR systems trained with commonly-used datasets. For example, Wang et al. [173] proposed a Racial Faces in-the-Wild (RFW) database and proved that existing commercial APIs and the SOTA algorithms indeed work unequally for different races and the maximum difference in error rate between the best and worst groups is 12%, as shown in Table VIII. Hupont et al. [171] showed that SphereFace has a TAR of 0.87 for White males which drops to 0.28 for Asian females, at a FAR of 1e\u221241\ud835\udc5241e-41 italic_e - 4. Such bias can result in mistreatment of certain demographic groups, by either exposing them to a higher risk of fraud, or by making access to services more difficult. Therefore, addressing data bias and enhancing fairness of FR systems in real life are urgent and necessary tasks. Collecting balanced data to train a fair model or designing some debiasing algorithms are effective way. ", "In terms of training protocol, FR can be categorized into subject-dependent and subject-independent settings, as illustrated in Fig. 20.", "Subject-dependent protocol. For subject-dependent protocol, all testing identities are predefined in training set, it is natural to classify testing face images to the given identities. Therefore, subject-dependent FR can be well addressed as a classification problem, where features are expected to be separable. The protocol is mostly adopted by the early-stage (before 2010) FR studies on FERET [177], AR [178], and is suitable only for some small-scale applications. The Challenge 2 of MS-Celeb-1M is the only large-scale database using subject-dependent training protocol.", "Subject-independent protocol. For subject-independent protocol, the testing identities are usually disjoint from the training set, which makes FR more challenging yet close to practice. Because it is impossible to classify faces to known identities in training set, generalized representation is essential. Due to the fact that human faces exhibit similar intra-subject variations, deep models can display transcendental generalization ability when training with a sufficiently large set of generic subjects, where the key is to learn discriminative large-margin deep features. This generalization ability makes subject-independent FR possible. Almost all major face-recognition benchmarks, such as LFW [23], PaSC [179], IJB-A/B/C [41, 42, 43] and Megaface [44, 164], require the tested models to be trained under subject-independent protocol.", "In order to evaluate whether our deep models can solve the different problems of FR in real life, many testing datasets are designed to evaluate the models in different tasks, i.e. face verification, close-set face identification and open-set face identification. In either task, a set of known subjects is initially enrolled in the system (the gallery), and during testing, a new subject (the probe) is presented. Face verification computes one-to-one similarity between the gallery and probe to determine whether the two images are of the same subject, whereas face identification computes one-to-many similarity to determine the specific identity of a probe face. When the probe appears in the gallery identities, this is referred to as closed-set identification; when the probes include those who are not in the gallery, this is open-set identification.", "Face verification is relevant to access control systems, re-identification, and application independent evaluations of FR algorithms. It is classically measured using the receiver operating characteristic (ROC) and estimated mean accuracy (Acc). At a given threshold (the independent variable), ROC analysis measures the true accept rate (TAR), which is the fraction of genuine comparisons that correctly exceed the threshold, and the false accept rate (FAR), which is the fraction of impostor comparisons that incorrectly exceed the threshold. And Acc is a simplified metric introduced by LFW [23], which represents the percentage of correct classifications. With the development of deep FR, more accurate recognitions are required. Customers concern more about the TAR when FAR is kept in a very low rate in most security certification scenario. PaSC [179] reports TAR at a FAR of 10\u22122superscript10210^{-2}10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT; IJB-A [41] evaluates TAR at a FAR of 10\u22123superscript10310^{-3}10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT; Megaface [44, 164] focuses on TAR@10\u22126superscript10610^{-6}10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPTFAR; especially, in MS-celeb-1M challenge 3 [163], TAR@10\u22129superscript10910^{-9}10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPTFAR is reported.", "Close-set face identification is relevant to user driven searches (e.g., forensic identification), rank-N and cumulative match characteristic (CMC) is commonly used metrics in this scenario. Rank-N is based on what percentage of probe searches return the probe\u2019s gallery mate within the top k\ud835\udc58kitalic_k rank-ordered results. The CMC curve reports the percentage of probes identified within a given rank (the independent variable). IJB-A/B/C [41, 42, 43] concern on the rank-1 and rank-5 recognition rate. The MegaFace challenge [44, 164] systematically evaluates rank-1 recognition rate function of increasing number of gallery distractors (going from 10 to 1 Million), the results of the SOTA evaluated on MegaFace challenge are listed in Table IX. Rather than rank-N and CMC, MS-Celeb-1M [45] further applies a precision-coverage curve to measure identification performance under a variable threshold t\ud835\udc61titalic_t. The probe is rejected when its confidence score is lower than t\ud835\udc61titalic_t. The algorithms are compared in term of what fraction of passed probes, i.e. coverage, with a high recognition precision, e.g. 95% or 99%, the results of the SOTA evaluated on MS-Celeb-1M challenge are listed in Table X.", "Open-set face identification is relevant to high throughput face search systems (e.g., de-duplication, watch list identification), where the recognition system should reject unknown/unseen subjects (probes who do not present in gallery) at test time. At present, there are very few databases covering the task of open-set FR. IJB-A/B/C [41, 42, 43] benchmarks introduce a decision error tradeoff (DET) curve to characterize the the false negative identification rate (FNIR) as function of the false positive identification rate (FPIR). FPIR measures what fraction of comparisons between probe templates and non-mate gallery templates result in a match score exceeding T\ud835\udc47Titalic_T. At the same time, FNIR measures what fraction of probe searches will fail to match a mated gallery template above a score of T\ud835\udc47Titalic_T. The algorithms are compared in term of the FNIR at a low FPIR, e.g. 1% or 10%, the results of the SOTA evaluated on IJB-A dataset as listed in Table XI.", "Public available training databases are mostly collected from the photos of celebrities due to privacy issue, it is far from images captured in the daily life with diverse scenes. In order to study different specific scenarios, more difficult and realistic datasets are constructed accordingly, as shown in Table XII. According to their characteristics, we divide these scenes into four categories: cross-factor FR, heterogenous FR, multiple (or single) media FR and FR in industry (Fig. 21).", "\u2022Cross-factor FR. Due to the complex nonlinear facial appearance, some variations will be caused by people themselves, such as cross-pose, cross-age, make-up, and disguise. For example, CALFW [188], MORPH [189], CACD [191] and FG-NET [194] are commonly used datasets with different age range; CFP [182] only focuses on frontal and profile face, CPLFW [181] is extended from LFW and contains different poses. Disguised faces in the wild (DFW) [214] evaluates face recognition across disguise.\u2022Heterogenous FR. It refers to the problem of matching faces across different visual domains. The domain gap is mainly caused by sensory devices and cameras settings, e.g. visual light vs. near-infrared and photo vs. sketch. For example, CUFSF [201] and CUFS [199] are commonly used photo-sketch datasets and CUFSF dataset is harder due to lighting variation and shape exaggeration.\u2022Multiple (or single) media FR. Ideally, in FR, many images of each subject are provided in training datasets and image-to-image recognitions are performed when testing. But the situation will be different in reality. Sometimes, the number of images per person in training set could be very small, such as MS-Celeb-1M challenge 2 [45]. This challenge is often called low- shot or few-shot FR. Moreover, each subject face in test set may be enrolled with a set of images and videos and set-to-set recognition should be performed, such as IJB-A [41] and PaSC [179].\u2022FR in industry. Although deep FR has achieved beyond human performance on some standard benchmarks, but some other factors should be given more attention rather than accuracy when deep FR is adopted in industry, e.g. anti-attack (CASIA-FASD [210]) and 3D FR (Bosphorus [203], BU-3DFE [205] and FRGCv2 [206]). Compared to publicly available 2D face databases, 3D scans are hard to acquire, and the number of scans and subjects in public 3D face databases is still limited, which hinders the development of 3D deep FR.", "Despite the high accuracy in the LFW [23] and Megaface [44, 164] benchmarks, the performance of FR models still hardly meets the requirements in real-world application. A conjecture in industry is made that results of generic deep models can be improved simply by collecting big datasets of the target scene. However, this holds only to a certain degree. More and more concerns on privacy may make the collection and human-annotation of face data become illegal in the future. Therefore, significant efforts have been paid to design excellent algorithms to address the specific problems with limited data in these realistic scenes. In this section, we present several special algorithms of FR.", "As [182] shows that many existing algorithms suffer a decrease of over 10% from frontal-frontal to frontal-profile verification, cross-pose FR is still an extremely challenging scene. In addition to the aforementioned methods, including \u201cone-to-many augmentation\u201d, \u201cmany-to-one normalization\u201d and assembled networks (Section 4 and 3.2.2), there are some other algorithms designed for cross-pose FR. Considering the extra burden of above methods, Cao et al. [215] attempted to perform frontalization in the deep feature space rather than the image space. A deep residual equivariant mapping (DREAM) block dynamically added residuals to an input representation to transform a profile face to a frontal image. Chen et al. [216] proposed to combine feature extraction with multi-view subspace learning to simultaneously make features be more pose-robust and discriminative. Pose Invariant Model (PIM) [217] jointly performed face frontalization and learned pose invariant representations end-to-end to allow them to mutually boost each other, and further introduced unsupervised cross-domain adversarial training and a learning to learn strategy to provide high-fidelity frontal reference face images.", "Cross-age FR is extremely challenging due to the changes in facial appearance by the aging process over time. One direct approach is to synthesize the desired image with target age such that the recognition can be performed in the same age group. A generative probabilistic model was used by [218] to model the facial aging process at each short-term stage. The identity-preserved conditional generative adversarial networks (IPCGANs) [219] framework utilized a conditional-GAN to generate a face in which an identity-preserved module preserved the identity information and an age classifier forced the generated face with the target age. Antipov et al. [220] proposed to age faces by GAN, but the synthetic faces cannot be directly used for face verification due to its imperfect preservation of identities. Then, they used a local manifold adaptation (LMA) approach [221] to solve the problem of [220]. In [222], high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales to generate more lifelike facial details. An alternative to address the cross-age problem is to decompose aging and identity components separately and extract age-invariant representations. Wen et al. [192] developed a latent identity analysis (LIA) layer to separate these two components, as shown in Fig. 22. In [193], age-invariant features were obtained by subtracting age-specific factors from the representations with the help of the age estimation task. In [124], face features are decomposed in the spherical coordinate system, in which the identity-related components are represented with angular coordinates and the age-related information is encoded with radial coordinate. Additionally, there are other methods designed for cross-age FR. For example, Bianco ett al. [223] and El et al. [224] fine-tuned the CNN to transfer knowledge across age. Wang et al. [225] proposed a siamese deep network to perform multi-task learning of FR and age estimation. Li et al. [226] integrated feature extraction and metric learning via a deep CNN.", "Makeup is widely used by the public today, but it also brings challenges for FR due to significant facial appearance changes. The research on matching makeup and nonmakeupface images is receiving increasing attention. Li et al. [208] generated nonmakeup images from makeup ones by a bi-level adversarial network (BLAN) and then used the synthesized nonmakeup images for verification as shown in Fig. 23. Sun et al. [227] pretrained a triplet network on videos and fine-tuned it on a small makeup datasets. Specially, facial disguise [214, 228, 229] is a challenging research topic in makeup face recognition. By using disguise accessories such as wigs, beard, hats, mustache, and heavy makeup, disguise introduces two variations: (i) when a person wants to obfuscate his/her own identity, and (ii) another individual impersonates someone else\u2019s identity. Obfuscation increases intra-class variations whereas impersonation reduces the inter-class dissimilarity, thereby affecting face recognition/verification task. To address this issue, a variety of methods are proposed. Zhang et al. [230] first trained two DCNNs for generic face recognition and then used Principal Components Analysis (PCA) to find the transformation matrix for disguised face recognition adaptation. Kohli et al. [231] finetuned models using disguised faces. Smirnov et al. [232] proposed a hard example mining method benefitted from class-wise (Doppelganger Mining [233]) and example-wise mining to learn useful deep embeddings for disguised face recognition. Suri et al. [234] learned the representations of images in terms of colors, shapes, and textures (COST) using an unsupervised dictionary learning method, and utilized the combination of COST features and CNN features to perform recognition.", "Due to the excellent performance of the near-infrared spectrum (NIS) images under low-light scenarios, NIS images are widely applied in surveillance systems. Because most enrolled databases consist of visible light (VIS) spectrum images, how to recognize a NIR face from a gallery of VIS images has been a hot topic. Saxena et al. [235] and Liu et al. [236] transferred the VIS deep networks to the NIR domain by fine-tuning. Lezama et al. [237] used a VIS CNN to recognize NIR faces by transforming NIR images to VIS faces through cross-spectral hallucination and restoring a low-rank structure for features through low-rank embedding. Reale et al. [198] trained a VISNet (for visible images) and a NIRNet (for near-infrared images), and coupled their output features by creating a siamese network. He et al. [238, 239] divided the high layer of the network into a NIR layer, a VIS layer and a NIR-VIS shared layer, then, a modality-invariant feature can be learned by the NIR-VIS shared layer. Song et al. [240] embedded cross-spectral face hallucination and discriminative feature learning into an end-to-end adversarial network. In [196], the low-rank relevance and cross-modal ranking were used to alleviate the semantic gap.", "Although deep networks are robust to low resolution to a great extent, there are still a few studies focused on promoting the performance of low-resolution FR. For example, Zangeneh et al. [241] proposed a CNN with a two-branch architecture (a super-resolution network and a feature extraction network) to map the high- and low-resolution face images into a common space where the intra-person distance is smaller than the inter-person distance. Shen et al. [242] exploited the face semantic information and local structural constraints to better restore the shape and detail of face images. In addition, they optimized the network with perceptual and adversarial losses to produce photo-realistic results.", "The photo-sketch FR may help law enforcement to quickly identify suspects. The commonly used methods can be categorized as two classes. One is to utilize transfer learning to directly match photos to sketches. Deep networks are first trained using a large face database of photos and are then fine-tuned using small sketch database [243, 244]. The other is to use the image-to-image translation, where the photo can be transformed to a sketch or the sketch to a photo; then, FR can be performed in one domain. Zhang et al. [200] developed a fully convolutional network with generative loss and a discriminative regularizer to transform photos to sketches. Zhang et al. [245] utilized a branched fully convolutional neural network (BFCN) to generate a structure-preserved sketch and a texture-preserved sketch, and then they fused them together via a probabilistic method. Recently, GANs have achieved impressive results in image generation. Yi et al. [246], Kim et al. [247] and Zhu et al. [248] used two generators, GAsubscript\ud835\udc3a\ud835\udc34G_{A}italic_G start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT and GBsubscript\ud835\udc3a\ud835\udc35G_{B}italic_G start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT, to generate sketches from photos and photos from sketches, respectively (Fig. 24). Based on [248], Wang et al. [202] proposed a multi-adversarial network to avoid artifacts by leveraging the implicit presence of feature maps of different resolutions in the generator subnetwork. Similar to photo-sketch FR, photo-caricature FR is one kind of heterogenous FR scenes which is challenging and important to understanding of face perception. Huo et al. [213] built a large dataset of caricatures and photos, and provided several evaluation protocols and their baseline performances for comparison.", "For many practical applications, such as surveillance and security, the FR system should recognize persons with a very limited number of training samples or even with only one sample. The methods of low-shot learning can be categorized as 1) synthesizing training data and 2) learning more powerful features. Hong et al. [249] generated images in various poses using a 3D face model and adopted deep domain adaptation to handle other variations, such as blur, occlusion, and expression (Fig. 25). Choe et al. [250] used data augmentation methods and a GAN for pose transition and attribute boosting to increase the size of the training dataset. Wu et al. [176] proposed a framework with hybrid classifiers using a CNN and a nearest neighbor (NN) model. Guo et al. [143] made the norms of the weight vectors of the one-shot classes and the normal classes aligned to address the data imbalance problem. Cheng et al. [137] proposed an enforced softmax that contains optimal dropout, selective attenuation, L2 normalization and model-level optimization. Yin et al. [251] augmented feature space of low-shot classes by transferring the principal components from regular to low-shot classes to encourage the variance of low-shot classes to mimic that of regular classes.", "Different from traditional image-to-image recognition, set-to-set recognition takes a set (heterogeneous contents containing both images and videos) as the smallest unit of representation. This kind of setting does reflect the real-world biometric scenarios, thereby attracting a lot of attention. After learning face representations of media in each set, two strategies are generally adopted to perform set-to-set matching. One is to use these representations to perform pair-wise similarity comparison of two sets and aggregate the results into a single and final score by max score pooling [96], average score pooling [252] and its variations [253, 254]. The other strategy is feature pooling [96, 103, 81] which first aggregates face representations into a single representation for each set and then performs a comparison between two sets. In addition to the commonly used strategies, there are also some novel methods proposed for set/template-based FR. For example, Hayat et al. [255] proposed a deep heterogeneous feature fusion network to exploit the features\u2019 complementary information generated by different CNNs. Liu et al. [256] introduced the actor-critic reinforcement learning for set-based FR. They casted the inner-set dependency modeling to a Markov decision process in the latent space, and trained a dependency-aware attention control agent to make attention control for each image in each step.", "There are two key issues in video FR: one is to integrate the information across different frames together to build a representation of the video face, and the other is to handle video frames with severe blur, pose variations, and occlusions. For frame aggregation, Yang et al. [83] proposed a neural aggregation network (NAN) in which the aggregation module, consisting of two attention blocks driven by a memory, produces a 128-dimensional vector representation (Fig. 26). Rao et al. [187] aggregated raw video frames directly by combining the idea of metric learning and adversarial learning. For dealing with bad frames, Rao et al. [185] discarded the bad frames by treating this operation as a Markov decision process and trained the attention model through a deep reinforcement learning framework. Ding et al. [257] artificially blurred clear images for training to learn blur-robust face representations. Parchami et al. [258] used a CNN to reconstruct a lower-quality video into a high-quality face.", "3D FR has inherent advantages over 2D methods, but 3D deep FR is not well developed due to the lack of large annotated 3D data. To enlarge 3D training datasets, most works use the methods of \u201cone-to-many augmentation\u201d to synthesize 3D faces. However, the effective methods for extracting deep features of 3D faces remain to be explored. Kim et al. [204] fine-tuned a 2D CNN with a small amount of 3D scans for 3D FR. Zulqarnain et al. [259] used a three-channel (corresponding to depth, azimuth and elevation angles of the normal vector) image as input and minimized the average prediction log-loss. Zhang et al. [260] first selected 30 feature points from the Candide-3 face model to characterize faces, then conducted the unsupervised pretraining of face depth data, and finally performed the supervised fine-tuning.", "Partial FR, in which only arbitrary-size face patches are presented, has become an emerging problem with increasing requirements of identification from CCTV cameras and embedded vision systems in mobile devices, robots and smart home facilities. He et al. [261] divided the aligned face image into several multi-scale patches, and the dissimilarity between two partial face images is calculated as the weighted L2 distance between corresponding patches. Dynamic feature matching (DFM) [262] utilized a sliding window of the same size as the probe feature maps to decompose the gallery feature maps into several gallery sub-feature maps, and the similarity-guided constraint imposed on sparse representation classification (SRC) provides an alignment-freematching.", "With the emergence of mobile phones, tablets and augmented reality, FR has been applied in mobile devices. Due to computational limitations, the recognition tasks in these devices need to be carried out in a light but timely fashion. MobiFace [87] required efficient memory and low cost operators by adopting fast downsampling and bottleneck residual block, and achieves99.7% on LFW database and 91.3% on Megaface database. Tadmor et al. [263] proposed a multibatch method that first generates signatures for a minibatch of k\ud835\udc58kitalic_k face images and then constructs an unbiased estimate of the full gradient by relying on all k2\u2212ksuperscript\ud835\udc582\ud835\udc58k^{2}-kitalic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_k pairs from the minibatch. As mentioned in Section 3.2.1, light-weight deep networks [126, 127, 128, 129] perform excellently in the fundamental tasks of image classification and deserve further attention in FR tasks. Moreover, some well-known compressed networks such as Pruning [264, 265, 266], BinaryNets [267, 268, 269, 270], Mimic Networks [271, 272], also have potential to be introduced into FR.", "With the success of FR techniques, various types of attacks, such as face spoofing and adversarial perturbations, are becoming large threats. Face spoofing involves presenting a fake face to the biometric sensor using a printed photograph, worn mask, or even an image displayed on another electronic device. In order to defense this type of attack, several methods are proposed [211, 273, 274, 275, 276, 277, 278, 279]. Atoum et al. [211] proposed a novel two-stream CNN in which the local features discriminate the spoof patches that are independent of the spatial face areas, and holistic depth maps ensure that the input live sample has a face-like depth. Yang et al. [273] trained a CNN using both a single frame and multiple frames with five scales as input, and using the live/spoof label as the output. Taken the sequence of video frames as input, Xu et al. [274] applied LSTM units on top of CNN to obtain end-to-end features to recognize spoofing faces which leveraged the local and dense property from convolution operation and learned the temporal structure using LSTM units. Li et al. [275] and Patel et al. [276] fine-tuned their networks from a pretrained model by training sets of real and fake images. Jourabloo et al. [277] proposed to inversely decompose a spoof face into the live face and the spoof noise pattern. Adversarial perturbation is the other type of attack which can be defined as the addition of a minimal vector r\ud835\udc5fritalic_r such that with addition of this vector into the input image x\ud835\udc65xitalic_x, i.e. (x+r)\ud835\udc65\ud835\udc5f(x+r)( italic_x + italic_r ), the deep learning models misclassifies the input while people will not. Recently, more and more work has begun to focus on solving this perturbation of FR. Goswami et al. [280] proposed to detect adversarial samples by characterizing abnormal filter response behavior in the hidden layers and increase the network\u2019s robustness by removing the most problematic filters. Goel et al. [281] provided an open source implementation of adversarial detection and mitigation algorithms. Despite of progresses of anti-attack algorithms, attack methods are updated as well and remind us the need to further increase security and robustness in FR systems, for example, Mai et al. [282] proposed a neighborly de-convolutional neural network (NbNet) to reconstruct a fake face using the stolen deep templates.", "As described in Section 5.1, existing datasets are highly biased in terms of the distribution of demographic cohorts, which may dramatically impact the fairness of deep models. To address this issue, there are some works that seek to introduce fairness into face recognition and mitigate demographic bias, e,g. unbalanced-training [283], attribute removal [284, 285, 286] and domain adaptation [173, 287, 147]. 1) Unbalanced-training methods mitigate the bias via model regularization, taking into consideration of the fairness goal in the overall model objective function. For example, RL-RBN [283] formulated the process of finding the optimal margins for non-Caucasians as a Markov decision process and employed deep Q-learning to learn policies based on large margin loss. 2) Attribute removal methods confound or remove demographic information of faces to learn attribute-invariant representations. For example, Alvi et al. [284] applied a confusion loss to make a classifier fail to distinguish attributes of examples so that multiple spurious variations are removed from the feature representation. SensitiveNets [288] proposed to introduce sensitive information into triplet loss. They minimized the sensitive information, while maintaining distances between positive and negative embeddings. 3) Domain adaptation methods propose to investigate data bias problem from a domain adaptation point of view and attempt to design domain-invariant feature representations to mitigate bias across domains. IMAN [173] simultaneously aligned global distribution to decrease race gap at domain-level, and learned the discriminative target representations at cluster level. Kan [147] directly converted the Caucasian data to non-Caucasian domain in the image space with the help of sparse reconstruction coefficients learnt in the common subspace.", "In this paper, we provide a comprehensive survey of deep FR from both data and algorithm aspects. For algorithms, mainstream and special network architectures are presented. Meanwhile, we categorize loss functions into Euclidean-distance-based loss, angular/cosine-margin-based loss and variable softmax loss. For data, we summarize some commonly used datasets. Moreover, the methods of face processing are introduced and categorized as \u201cone-to-many augmentation\u201d and \u201cmany-to-one normalization\u201d. Finally, the special scenes of deep FR, including video FR, 3D FR and cross-age FR, are briefly introduced.", "Taking advantage of big annotated data and revolutionary deep learning techniques, deep FR has dramatically improved the SOTA performance and fostered successful real-world applications. With the practical and commercial use of this technology, many ideal assumptions of academic research were broken, and more and more real-world issues are emerging. To the best our knowledge, major technical challenges include the following aspects.", "\u2022Security issues. Presentation attack [289], adversarial attack [280, 281, 290], template attack [291] and digital manipulation attack [292, 293] are developing to threaten the security of deep face recognition systems. 1) Presentation attack with 3D silicone mask, which exhibits skin-like appearance and facial motion, challenges current anti-sproofing methods [294]. 2) Although adversarial perturbation detection and mitigation methods are recently proposed [280][281], the root cause of adversarial vulnerability is unclear and thus new types of adversarial attacks are still upgraded continuously [295, 296]. 3) The stolen deep feature template can be used to recover its facial appearance, and how to generate cancelable template without loss of accuracy is another important issue. 4) Digital manipulation attack, made feasible by GANs, can generate entirely or partially modified photorealistic faces by expression swap, identity swap, attribute manipulation and entire face synthesis, which remains a main challenge for the security of deep FR.", "\u2022Privacy-preserving face recognition. With the leakage of biological data, privacy concerns are raising nowadays. Facial images can predict not only demographic information such as gender, age, or race, but even the genetic information [297]. Recently, the pioneer works such as Semi-Adversarial Networks [298, 299, 285] have explored to generate a recognizable biometric templates that can hidden some of the private information presented in the facial images. Further research on the principles of visual cryptography, signal mixing and image perturbation to protect users\u2019 privacy on stored face templates are essential for addressing public concern on privacy.", "\u2022Understanding deep face recognition. Deep face recognition systems are now believed to surpass human performance in most scenarios [300]. There are also some interesting attempts to apply deep models to assist human operators for face verification [183][300]. Despite this progress, many fundamental questions are still open, such as what is the \u201cidentity capacity\u201d of a deep representation [301]? Why deep neural networks, rather than humans, are easily fooled by adversarial samples? While bigger and bigger training dataset by itself cannot solve this problem, deeper understanding on these questions may help us to build robust applications in real world. Recently, a new benchmark called TALFW has been proposed to explore this issue [93].", "\u2022Remaining challenges defined by non-saturated benchmark datasets. Three current major datasets, namely, MegaFace [44, 164] , MS-Celeb-1M [45] and IJB-A/B/C [41, 42, 43], are corresponding to large-scale FR with a very large number of candidates, low/one-shot FR and large pose-variance FR which will be the focus of research in the future. Although the SOTA algorithms can be over 99.9 percent accurate on LFW [23] and Megaface [44, 164] databases, fundamental challenges such as matching faces cross ages [181], poses [188], sensors, or styles still remain. For both datasets and algorithms, it is necessary to measure and address the racial/gender/age biases of deep FR in future research.", "\u2022Ubiquitous face recognition across applications and scenes. Deep face recognition has been successfully applied on many user-cooperated applications, but the ubiquitous recognition applications in everywhere are still an ambitious goal. In practice, it is difficult to collect and label sufficient samples for innumerable scenes in real world. One promising solution is to first learn a general model and then transfer it to an application-specific scene. While deep domain adaptation [145] has recently been applied to reduce the algorithm bias on different scenes [148], different races [173], general solution to transfer face recognition is largely open.", "\u2022Pursuit of extreme accuracy and efficiency. Many killer-applications, such as watch-list surveillance or financial identity verification, require high matching accuracy at very low alarm rate, e.g. 10\u22129superscript10910^{-9}10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPT. It is still a big challenge even with deep learning on massive training data. Meanwhile, deploying deep face recognition on mobile devices pursues the minimum size of feature representation and compressed deep network. It is of great significance for both industry and academic to explore this extreme face-recognition performance beyond human imagination. It is also exciting to constantly push the performance limits of the algorithm after it has already surpassed human.", "\u2022Fusion issues. Face recognition by itself is far from sufficient to solve all biometric and forensic tasks, such as distinguishing identical twins and matching faces before and after surgery [302]. A reliable solution is to consolidate multiple sources of biometric evidence [303]. These sources of information may correspond to different biometric traits (e.g., face + hand [304]), sensors (e.g., 2D + 3D face cameras), feature extraction and matching techniques, or instances (e.g., a face sequence of various poses). It is beneficial for face biometric and forensic applications to perform information fusion at the data level, feature level, score level, rank level, and decision level [305].", "This work was partially supported by National Key R&D Program of China (2019YFB1406504) and BUPT Excellent Ph.D. Students Foundation CX2020207."], "figure_types": {"21117380118ddce47b3c515c5228372c513e61ba/10-Figure12-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/11-Figure13-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/12-Figure14-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/13-Figure15-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/13-Figure16-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/13-Figure17-1.png": "plot", "21117380118ddce47b3c515c5228372c513e61ba/13-TableVI-1.png": "table", "21117380118ddce47b3c515c5228372c513e61ba/14-TableVII-1.png": "table", "21117380118ddce47b3c515c5228372c513e61ba/15-Figure18-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/15-TableIX-1.png": "table", "21117380118ddce47b3c515c5228372c513e61ba/15-TableVIII-1.png": "table", "21117380118ddce47b3c515c5228372c513e61ba/15-TableX-1.png": "table", "21117380118ddce47b3c515c5228372c513e61ba/18-Figure19-1.png": "photograph(s)", "21117380118ddce47b3c515c5228372c513e61ba/18-Figure20-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/18-Figure21-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/19-Figure22-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/19-Figure23-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/2-Figure1-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/2-Figure2-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/20-Figure24-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/4-Figure3-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/4-TableII-1.png": "table", "21117380118ddce47b3c515c5228372c513e61ba/5-Figure4-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/6-Figure5-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/6-TableIV-1.png": "table", "21117380118ddce47b3c515c5228372c513e61ba/7-Figure6-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/7-Figure7-1.png": "plot", "21117380118ddce47b3c515c5228372c513e61ba/7-TableV-1.png": "table", "21117380118ddce47b3c515c5228372c513e61ba/8-Figure8-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/9-Figure10-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/9-Figure11-1.png": "schematic", "21117380118ddce47b3c515c5228372c513e61ba/9-Figure9-1.png": "schematic"}}, "1606.04797": {"paper_id": "paper_35", "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "arxiv_url": "https://arxiv.org/abs/1606.04797", "s2orc_url": "https://www.semanticscholar.org/paper/50004c086ffd6a201a4b782281aaa930fbfe6ecf", "all_figures_tables": {"50004c086ffd6a201a4b782281aaa930fbfe6ecf/2-Figure1-1.png": "Fig. 1. Slices from MRI volumes depicting prostate. This data is part of the PROMISE2012 challenge dataset [7].", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/3-Figure2-1.png": "Fig. 2. Schematic representation of our network architecture. Our custom implementation of Caffe [5] processes 3D data by performing volumetric convolutions. Best viewed in electronic format.", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Figure3-1.png": "Fig. 3. Convolutions with appropriate stride can be used to reduce the size of the data. Conversely, de-convolutions increase the data size by projecting each input voxel to a bigger region through the kernel.", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Table1-1.png": "Table 1. Theoretical receptive field of the 3\u00d7 3\u00d7 3 convolutional layers of the network.", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/7-Figure4-1.png": "Fig. 4. Qualitative results on the PROMISE 2012 dataset [7].", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/8-Figure5-1.png": "Fig. 5. Distribution of volumes with respect to the Dice coefficient achieved during segmentation.", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/8-Table2-1.png": "Table 2. Quantitative comparison between the proposed approach and the current best results on the PROMISE 2012 challenge dataset.", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/9-Figure6-1.png": "Fig. 6. Qualitative comparison between the results obtained using the Dice coefficient based loss (green) and re-weighted soft-max with loss (yellow)."}, "referred_figures_tables": [["50004c086ffd6a201a4b782281aaa930fbfe6ecf/2-Figure1-1.png"], ["50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Figure3-1.png", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Table1-1.png"], ["50004c086ffd6a201a4b782281aaa930fbfe6ecf/2-Figure1-1.png", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Table1-1.png"], ["50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Table1-1.png"], ["50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Figure3-1.png"], ["50004c086ffd6a201a4b782281aaa930fbfe6ecf/3-Figure2-1.png", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Figure3-1.png", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Table1-1.png"], ["50004c086ffd6a201a4b782281aaa930fbfe6ecf/2-Figure1-1.png"], ["50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Figure3-1.png"], ["50004c086ffd6a201a4b782281aaa930fbfe6ecf/3-Figure2-1.png"], ["50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Table1-1.png"]], "question_id": [2, 3, 9, 5, 0, 1, 6, 11, 12, 13], "question": ["Is the segmented training data 2d or 3d ?", "What is the difference between foreground and background voxels?", "Does it have anything to do with the nature and complexity of data we are working with ?", "Going deep through network layers makes it harder to remember shallower local information, wouldn't that make segmentation harder?", "Why it is needed to have a two channel volumetric segmentation in the output?", "Is the increase in receptive field of the features being computed in subsequent network layers due to the downsampling mentioned by the authors, or is it the result of subsequent convolutions as the network goes deeper?", "Can we enclose all appearances of prostate MRI volumes?", "Does deconvolution and unpooling conduct the same goal in the network?", "Does using horizontal connections depend on the amount and complexity of the data wanted to be segmented?", "Does the phrase \"data with larger spatial support than the typical size of the anatomy\" refer to feature maps with a larger number of channels than the input map at the deepest layer, or does it refer to something else?"], "question_section": ["Abstract", "Abstract", "Method", "Introductin and Related Work", "Method", "Method", "Introduction and Related Work", "Method", "Method", "Method"], "question_trigger_sentence": ["Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once", " In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels.", "This strategy serves a similar purpose as pooling layers that, motivated by and other works discouraging the use of max-pooling operations in CNNs, have been replaced in our approach by convolutional ones", "Shallower layers grasp local information while deeper layers use filters whose receptive fields are much broader that therefore capture global information [", "The right portion of the network extracts features and expands the spatial support of the lower resolution feature maps in order to gather and assemble the necessary information to output a two channel volumetric segmentation.", "Downsampling allows us to reduce the size of the signal presented as input and to increase the receptive field of the features being computed in subsequent network layers. Each of the stages of the left part of the network, computes a number of features which is two times higher than the one of the previous layer.", "In this work, we aim to segment prostate MRI volumes. This is a challenging task due to the wide range of appearance the prostate can assume in different scans due to deformations and variations of the intensity distribution. ", " After each stage of the right portion of the CNN, a de-convolution operation is employed in order increase the size of the inputs followed by one to three convolutional layers involving half the number of 5 \u00d7 5 \u00d7 5 kernels employed in the previous layer.", "We also observed that when these connections improve the convergence time of the model.", "We believe that this characteristic is important during segmentation of poorly visible anatomy: the features computed in the deepest layer perceive the whole anatomy of interest at once, since they are computed from data having a spatial support much larger than the typical size of the anatomy we seek to delineate, and therefore impose global constraints."], "question_type": ["Shallow question", "Testing question", "Deep/complex question", "Deep/complex question", "Deep/complex question", "Deep/complex question", "Shallow question", "Shallow question", "Shallow question", "Shallow question"], "evidential_info": [[{"context": "Segmentation is a highly relevant task in medical image analysis.Automatic delineation of organs and structures of interest is often necessary to perform tasks such as visual augmentation [10], computer assisted diagnosis [12], interventions [20] and extraction of quantitative indices from images [1].In particular, since diagnostic and interventional imagery often consists of 3D images, being able to perform volumetric segmentations by taking into account the whole volume content at once, has a particular relevance.In this work, we aim to segment prostate MRI volumes. This is a challenging task due to the wide range of appearance the prostate can assume in different scans due to deformations and variations of the intensity distribution. Moreover, MRI volumes are often affected by artefacts and distortions due to field inhomogeneity. Prostate segmentation is nevertheless an important task having clinical relevance both during diagnosis, where the volume of the prostate needs to be assessed [13], and during treatment planning, where the estimate of the anatomical boundary needs to be accurate [4, 20].", "rationale": "V-Net is trained on 50 MRI volumes,"}, {"context": "Our CNN is trained end-to-end on a dataset of prostate scans in MRI. An example of the typical content of such volumes is shown in Figure 1. All the volumes processed by the network have fixed size of 128\\times 128\\times 64 voxels and a spatial resolution of 1\\times 1\\times 1.5 millimeters.", "rationale": "Table 1 shows the increase in receptive fields of features in the deeper layers"}, {"context": "We trained our method on 50 MRI volumes, and the relative manual ground truth annotation, obtained from the \u201dPROMISE2012\u201d challenge dataset [7]. This dataset contains medical data acquired in different hospitals, using different equipment and different acquisition protocols. The data in this dataset is representative of the clinical variability and challenges encountered in clinical settings. As previously stated we massively augmented this dataset through random transformation performed in each training iteration, for each mini-batch fed to the network. The mini-batches used in our implementation contained two volumes each, mainly due to the high memory requirement of the model during training. We used a momentum of 0.99 and a initial learning rate of 0.0001 which decreases by one order of magnitude every 25K iterations.", "rationale": "V-Net performs segmentation of MRI prostate volumes."}, {"context": "We tested V-Net on 30 MRI volumes depicting prostate whose ground truth annotation was secret. All the results reported in this section of the paper were obtained directly from the organisers of the challenge after submitting the segmentation obtained through our approach. The test set was representative of the clinical variability encountered in prostate scans in real clinical settings [7].", "rationale": "V-Net processes fixed size of volumes of size 128\\times 128\\times 64 voxels."}, {"context": "We presented and approach based on a volumetric convolutional neural network that performs segmentation of MRI prostate volumes in a fast and accurate manner. We introduced a novel objective function that we optimise during training based on the Dice overlap coefficient between the predicted segmentation and the ground truth annotation. Our Dice loss layer does not need sample re-weighting when the amount of background and foreground pixels is strongly unbalanced and is indicated for binary segmentation tasks. Although we inspired our architecture to the one proposed in [14], we divided it into stages that learn residuals and, as empirically observed, improve both results and convergence time. Future works will aim at segmenting volumes containing multiple regions in other modalities such as ultrasound and at higher resolutions by splitting the network over multiple GPUs.", "rationale": "Proposed network processes MRI volumes."}, {"context": "Fully convolutional network trained end-to-end were so far applied only to 2D images both in computer vision [11, 8] and microscopy image analysis [14]. These models, which served as an inspiration for our work, employed different network architectures and were trained to predict a segmentation mask, delineating the structures of interest, for the whole image. In [11] a pre-trained VGG network architecture [15] was used in conjunction with its mirrored, de-convolutional, equivalent to segment RGB images by leveraging the descriptive power of the features extracted by the innermost layer. In [8] three fully convolutional deep neural networks, pre-trained on a classification task, were refined to produce segmentations while in [14] a brand new CNN model, especially tailored to tackle biomedical image analysis problems in 2D, was proposed.", "rationale": "V-Net is tested on 30 MRI volumes."}, {"context": "In this work we present our approach to medical image segmentation that leverages the power of a fully convolutional neural networks, trained end-to-end, to process MRI volumes.Differently from other recent approaches we refrain from processing the input volumes slice-wise and we propose to use volumetric convolutions instead. We propose a novel objective function based on Dice coefficient maximisation, that we optimise during training.We demonstrate fast and accurate results on prostate MRI test volumes and we provide direct comparison with other methods which were evaluated on the same test data 111Detailed results available on http://promise12.grand-challenge.org/results/.", "rationale": "proposed model aim to segment prostate MRI volumes."}], [{"context": "The right portion of the network extracts features and expands the spatial support of the lower resolution feature maps in order to gather and assemble the necessary information to output a two channel volumetric segmentation. The two features maps computed by the very last convolutional layer, having 1\\times 1\\times 1 kernel size and producing outputs of the same size as the input volume, are converted to probabilistic segmentations of the foreground and background regions by applying soft-max voxelwise.After each stage of the right portion of the CNN, a de-convolution operation is employed in order increase the size of the inputs (Figure 3) followed by one to three convolutional layers involving half the number of 5\\times 5\\times 5 kernels employed in the previous layer. Similar to the left part of the network, also in this case we resort to learn residual functions in the convolutional stages.", "rationale": "A probability map is generated after applying a soft-max on the output of the last convolution layer. This map consists of voxel probability where higher probability (>0.5) belongs to the foreground and is considered part of the anatomy."}, {"context": "We report in Table 1 the receptive fields of each network layer, showing the fact that the innermost portion of our CNN already captures the content of the whole input volume. We believe that this characteristic is important during segmentation of poorly visible anatomy: the features computed in the deepest layer perceive the whole anatomy of interest at once, since they are computed from data having a spatial support much larger than the typical size of the anatomy we seek to delineate, and therefore impose global constraints.", "rationale": "Anatomy of interest for the V-Net occurs in very regions. The regions are part of foreground voxels."}, {"context": "The network predictions, which consist of two volumes having the same resolution as the original input data, are processed through a soft-max layer which outputs the probability of each voxel to belong to foreground and to background. In medical volumes such as the ones we are processing in this work, it is not uncommon that the anatomy of interest occupies only a very small region of the scan. This often causes the learning process to get trapped in local minima of the loss function yielding a network whose predictions are strongly biased towards background. As a result the foreground region is often missing or only partially detected. Several previous approaches resorted to loss functions based on sample re-weighting where foreground regions are given more importance than background ones during learning. In this work we propose a novel objective function based on dice coefficient, which is a quantity ranging between 0 and 1 which we aim to maximise. The dice coefficient D between two binary volumes can be written asD=\\frac{2\\sum_{i}^{N}p_{i}g_{i}}{\\sum_{i}^{N}p_{i}^{2}+\\sum_{i}^{N}g_{i}^{2}}", "rationale": "Foreground and background voxels can be used to segment different classes."}, {"context": "A Previously unseen MRI volume can be segmented by processing it in a feed-forward manner through the network. The output of the last convolutional layer, after soft-max, consists of a probability map for background and foreground. The voxels having higher probability (>0.5) to belong to the foreground than to the background are considered part of the anatomy.", "rationale": "foreground and background classes are strongly unbalanced."}, {"context": "We tested V-Net on 30 MRI volumes depicting prostate whose ground truth annotation was secret. All the results reported in this section of the paper were obtained directly from the organisers of the challenge after submitting the segmentation obtained through our approach. The test set was representative of the clinical variability encountered in prostate scans in real clinical settings [7].", "rationale": "V-Net is trained on 30 MRI volumes depicting prostate which each voxel can be labeled with or without prostate."}, {"context": "We presented and approach based on a volumetric convolutional neural network that performs segmentation of MRI prostate volumes in a fast and accurate manner. We introduced a novel objective function that we optimise during training based on the Dice overlap coefficient between the predicted segmentation and the ground truth annotation. Our Dice loss layer does not need sample re-weighting when the amount of background and foreground pixels is strongly unbalanced and is indicated for binary segmentation tasks. Although we inspired our architecture to the one proposed in [14], we divided it into stages that learn residuals and, as empirically observed, improve both results and convergence time. Future works will aim at segmenting volumes containing multiple regions in other modalities such as ultrasound and at higher resolutions by splitting the network over multiple GPUs.", "rationale": "V-Net is tested on 30 MRI volumes."}, {"context": "In this work we present our approach to medical image segmentation that leverages the power of a fully convolutional neural networks, trained end-to-end, to process MRI volumes.Differently from other recent approaches we refrain from processing the input volumes slice-wise and we propose to use volumetric convolutions instead. We propose a novel objective function based on Dice coefficient maximisation, that we optimise during training.We demonstrate fast and accurate results on prostate MRI test volumes and we provide direct comparison with other methods which were evaluated on the same test data 111Detailed results available on http://promise12.grand-challenge.org/results/.", "rationale": "proposed model aim to segment prostate MRI volumes."}], [{"context": "We report in Table 1 the receptive fields of each network layer, showing the fact that the innermost portion of our CNN already captures the content of the whole input volume. We believe that this characteristic is important during segmentation of poorly visible anatomy: the features computed in the deepest layer perceive the whole anatomy of interest at once, since they are computed from data having a spatial support much larger than the typical size of the anatomy we seek to delineate, and therefore impose global constraints.", "rationale": "Sometimes the anatomy is poorly visible in the MRI volumes."}, {"context": "Our CNN is trained end-to-end on a dataset of prostate scans in MRI. An example of the typical content of such volumes is shown in Figure 1. All the volumes processed by the network have fixed size of 128\\times 128\\times 64 voxels and a spatial resolution of 1\\times 1\\times 1.5 millimeters.", "rationale": "V-Net is trained end-to-end on a dataset of prostate scans in MRI."}, {"context": "Annotated medical volumes are not easy to obtain due to the fact that one or more experts are required to manually trace a reliable ground truth annotation and that there is a cost associated with their acquisition. In this work we found necessary to augment the original training dataset in order to obtain robustness and increased precision on the test dataset.", "rationale": "Data used for the segmentation task is complex because it is obtained from different hospitals, using different equipment and different acquisition protocols."}, {"context": "We tested V-Net on 30 MRI volumes depicting prostate whose ground truth annotation was secret. All the results reported in this section of the paper were obtained directly from the organisers of the challenge after submitting the segmentation obtained through our approach. The test set was representative of the clinical variability encountered in prostate scans in real clinical settings [7].", "rationale": "The V-Net is tested on 30 MRI volumes depicting prostate whose ground truth annotation was secret."}], [{"context": "Recent research in computer vision and pattern recognition has highlighted the capabilities of Convolutional Neural Networks (CNNs) to solve challenging tasks such as classification, segmentation and object detection, achieving state-of-the-art performances.This success has been attributed to the ability of CNNs to learn a hierarchical representation of raw input data, without relying on handcrafted features.As the inputs are processed through the network layers, the level of abstraction of the resulting features increases.Shallower layers grasp local information while deeper layers use filters whose receptive fields are much broader that therefore capture global information [19].", "rationale": "Features computed in the deepest layer of CNN can perceive the whole anatomy of interest at once. In general non anatomy part has a much larger spatial support than the anatomy. Therefore global constraint will work fine for this problem."}, {"context": "We report in Table 1 the receptive fields of each network layer, showing the fact that the innermost portion of our CNN already captures the content of the whole input volume. We believe that this characteristic is important during segmentation of poorly visible anatomy: the features computed in the deepest layer perceive the whole anatomy of interest at once, since they are computed from data having a spatial support much larger than the typical size of the anatomy we seek to delineate, and therefore impose global constraints.", "rationale": "Recent research in computer vision and pattern recognition has proved that CNNs achieve state of the art performance in image segmentation."}, {"context": "CNNs have been recently used for medical image segmentation.Early approaches obtain anatomy delineation in images or volumes by performing patch-wise image classification. Such segmentations are obtained by only considering local context and therefore are prone to failure, especially in challenging modalities such as ultrasound, where a high number of mis-classified voxel are to be expected.Post-processing approaches such as connected components analysis normally yield no improvement and therefore, more recent works, propose to use the network predictions in combination with Markov random fields [6], voting strategies [9] or more traditional approaches such as level-sets [2].Patch-wise approaches also suffer from efficiency issues. When densely extracted patches are processed in a CNN, a high number of computations is redundant and therefore the total algorithm runtime is high. In this case, more efficient computational schemes can be adopted.", "rationale": "patch-wise image classification using CNN suffer from inaccurate local segmentation and will have gaps in the anatomy regions."}, {"context": "Downsampling allows us to reduce the size of the signal presented as input and to increase the receptive field of the features being computed in subsequent network layers. Each of the stages of the left part of the network, computes a number of features which is two times higher than the one of the previous layer.", "rationale": "As we move down the layers in a CNN the receptive field of the features increases."}], [{"context": "Segmentation is a highly relevant task in medical image analysis.Automatic delineation of organs and structures of interest is often necessary to perform tasks such as visual augmentation [10], computer assisted diagnosis [12], interventions [20] and extraction of quantitative indices from images [1].In particular, since diagnostic and interventional imagery often consists of 3D images, being able to perform volumetric segmentations by taking into account the whole volume content at once, has a particular relevance.In this work, we aim to segment prostate MRI volumes. This is a challenging task due to the wide range of appearance the prostate can assume in different scans due to deformations and variations of the intensity distribution. Moreover, MRI volumes are often affected by artefacts and distortions due to field inhomogeneity. Prostate segmentation is nevertheless an important task having clinical relevance both during diagnosis, where the volume of the prostate needs to be assessed [13], and during treatment planning, where the estimate of the anatomical boundary needs to be accurate [4, 20].", "rationale": "The two features maps at the output having the  same size as the input volume, are used to generate probabilistic segmentations of the foreground and background regions by applying soft-max function."}, {"context": "The right portion of the network extracts features and expands the spatial support of the lower resolution feature maps in order to gather and assemble the necessary information to output a two channel volumetric segmentation. The two features maps computed by the very last convolutional layer, having 1\\times 1\\times 1 kernel size and producing outputs of the same size as the input volume, are converted to probabilistic segmentations of the foreground and background regions by applying soft-max voxelwise.After each stage of the right portion of the CNN, a de-convolution operation is employed in order increase the size of the inputs (Figure 3) followed by one to three convolutional layers involving half the number of 5\\times 5\\times 5 kernels employed in the previous layer. Similar to the left part of the network, also in this case we resort to learn residual functions in the convolutional stages.", "rationale": "Paper solve the problem of segmenting prostate MRI volumes which is a binary classification problem."}, {"context": "The network predictions, which consist of two volumes having the same resolution as the original input data, are processed through a soft-max layer which outputs the probability of each voxel to belong to foreground and to background. In medical volumes such as the ones we are processing in this work, it is not uncommon that the anatomy of interest occupies only a very small region of the scan. This often causes the learning process to get trapped in local minima of the loss function yielding a network whose predictions are strongly biased towards background. As a result the foreground region is often missing or only partially detected. Several previous approaches resorted to loss functions based on sample re-weighting where foreground regions are given more importance than background ones during learning. In this work we propose a novel objective function based on dice coefficient, which is a quantity ranging between 0 and 1 which we aim to maximise. The dice coefficient D between two binary volumes can be written asD=\\frac{2\\sum_{i}^{N}p_{i}g_{i}}{\\sum_{i}^{N}p_{i}^{2}+\\sum_{i}^{N}g_{i}^{2}}", "rationale": "Last layer output volumes are used to generate output segmentation probability maps by using a soft-max function."}, {"context": "A Previously unseen MRI volume can be segmented by processing it in a feed-forward manner through the network. The output of the last convolutional layer, after soft-max, consists of a probability map for background and foreground. The voxels having higher probability (>0.5) to belong to the foreground than to the background are considered part of the anatomy.", "rationale": "The network prediction volumes are used to generate output segmentation probability maps  for foreground and background classes by using a soft-max function."}], [{"context": "Recent research in computer vision and pattern recognition has highlighted the capabilities of Convolutional Neural Networks (CNNs) to solve challenging tasks such as classification, segmentation and object detection, achieving state-of-the-art performances.This success has been attributed to the ability of CNNs to learn a hierarchical representation of raw input data, without relying on handcrafted features.As the inputs are processed through the network layers, the level of abstraction of the resulting features increases.Shallower layers grasp local information while deeper layers use filters whose receptive fields are much broader that therefore capture global information [19].", "rationale": "Due to the reduced size because of Downsampling receptive field of the features can be increased."}, {"context": "We report in Table 1 the receptive fields of each network layer, showing the fact that the innermost portion of our CNN already captures the content of the whole input volume. We believe that this characteristic is important during segmentation of poorly visible anatomy: the features computed in the deepest layer perceive the whole anatomy of interest at once, since they are computed from data having a spatial support much larger than the typical size of the anatomy we seek to delineate, and therefore impose global constraints.", "rationale": "Table 1 shows the increase in receptive fields of features in the deeper layers"}, {"context": "In Figure 2 we provide a schematic representation of our convolutional neural network.We perform convolutions aiming to both extract features from the data and, at the end of each stage, to reduce its resolution by using appropriate stride. The left part of the network consists of a compression path, while the right part decompresses the signal until its original size is reached. Convolutions are all applied with appropriate padding.", "rationale": "Number of feature channels doubles at each stage during  the compression path of the V-Net."}, {"context": "The convolutions performed in each stage use volumetric kernels having size 5\\times 5\\times 5 voxels.As the data proceeds through different stages along the compression path, its resolution is reduced. This is performed through convolution with 2\\times 2\\times 2 voxels wide kernels applied with stride 2 (Figure 3). Since the second operation extracts features by considering only non overlapping 2\\times 2\\times 2 volume patches, the size of the resulting feature maps is halved.This strategy serves a similar purpose as pooling layers that, motivated by [16] and other works discouraging the use of max-pooling operations in CNNs, have been replaced in our approach by convolutional ones. Moreover, since the number of feature channels doubles at each stage of the compression path of the V-Net, and due to the formulation of the model as a residual network, we resort to these convolution operations to double the number of feature maps as we reduce their resolution. PReLu non linearities are applied throughout the network.", "rationale": "Convolutions operations with appropriate strides are used in each layer to both extract features and also to reduce its resolution."}, {"context": "Downsampling allows us to reduce the size of the signal presented as input and to increase the receptive field of the features being computed in subsequent network layers. Each of the stages of the left part of the network, computes a number of features which is two times higher than the one of the previous layer.", "rationale": "receptive field of the features becomes much broader as we go deeper into CNNs layers."}], [{"context": "Our CNN is trained end-to-end on a dataset of prostate scans in MRI. An example of the typical content of such volumes is shown in Figure 1. All the volumes processed by the network have fixed size of 128\\times 128\\times 64 voxels and a spatial resolution of 1\\times 1\\times 1.5 millimeters.", "rationale": "V-Net is trained on the diverse set of prostate scans in MRI."}, {"context": "During every training iteration, we fed as input to the network randomly deformed versions of the training images by using a dense deformation field obtained through a 2\\times 2\\times 2 grid of control-points and B-spline interpolation. This augmentation has been performed \u201don-the-fly\u201d, prior to each optimisation iteration, in order to alleviate the otherwise excessive storage requirements. Additionally we vary the intensity distribution of the data by adapting, using histogram matching, the intensity distributions of the training volumes used in each iteration, to the ones of other randomly chosen scans belonging to the dataset.", "rationale": "model performed well on a test set of 30 MRI volumes depicting prostate whose ground truth annotation was secret.\\"}, {"context": "We trained our method on 50 MRI volumes, and the relative manual ground truth annotation, obtained from the \u201dPROMISE2012\u201d challenge dataset [7]. This dataset contains medical data acquired in different hospitals, using different equipment and different acquisition protocols. The data in this dataset is representative of the clinical variability and challenges encountered in clinical settings. As previously stated we massively augmented this dataset through random transformation performed in each training iteration, for each mini-batch fed to the network. The mini-batches used in our implementation contained two volumes each, mainly due to the high memory requirement of the model during training. We used a momentum of 0.99 and a initial learning rate of 0.0001 which decreases by one order of magnitude every 25K iterations.", "rationale": ".V-Net shows the fast and accurate results on prostate MRI test volumes in comparison with other methods which were evaluated on the same test data."}, {"context": "We tested V-Net on 30 MRI volumes depicting prostate whose ground truth annotation was secret. All the results reported in this section of the paper were obtained directly from the organisers of the challenge after submitting the segmentation obtained through our approach. The test set was representative of the clinical variability encountered in prostate scans in real clinical settings [7].", "rationale": "V-Net is trained on dataset which contains medical data acquired in different hospitals, using different equipment and different acquisition protocols."}, {"context": "In this work we present our approach to medical image segmentation that leverages the power of a fully convolutional neural networks, trained end-to-end, to process MRI volumes.Differently from other recent approaches we refrain from processing the input volumes slice-wise and we propose to use volumetric convolutions instead. We propose a novel objective function based on Dice coefficient maximisation, that we optimise during training.We demonstrate fast and accurate results on prostate MRI test volumes and we provide direct comparison with other methods which were evaluated on the same test data 111Detailed results available on http://promise12.grand-challenge.org/results/.", "rationale": "Data augmentation has also been performed to increase the diversity of the prostate appearances in MRI volumes."}], [{"context": "The right portion of the network extracts features and expands the spatial support of the lower resolution feature maps in order to gather and assemble the necessary information to output a two channel volumetric segmentation. The two features maps computed by the very last convolutional layer, having 1\\times 1\\times 1 kernel size and producing outputs of the same size as the input volume, are converted to probabilistic segmentations of the foreground and background regions by applying soft-max voxelwise.After each stage of the right portion of the CNN, a de-convolution operation is employed in order increase the size of the inputs (Figure 3) followed by one to three convolutional layers involving half the number of 5\\times 5\\times 5 kernels employed in the previous layer. Similar to the left part of the network, also in this case we resort to learn residual functions in the convolutional stages.", "rationale": "In V-Net pooling operations are replaced by the convolution operations for smaller memory footprints. Decovoluton is also used for un-pooling."}, {"context": "Replacing pooling operations with convolutional ones results also to networks that, depending on the specific implementation, can have a smaller memory footprint during training, due to the fact that no switches mapping the output of pooling layers back to their inputs are needed for back-propagation, and that can be better understood and analysed [19] by applying only de-convolutions instead of un-pooling operations.", "rationale": "After each stage of the right portion of the V-Net, a de-convolution operation is employed in order to increase the size of the inputs."}], [{"context": "Similarly to [14], we forward the features extracted from early stages of the left part of the CNN to the right part. This is schematically represented in Figure 2 by horizontal connections. In this way we gather fine grained detail that would be otherwise lost in the compression path and we improve the quality of the final contour prediction. We also observed that when these connections improve the convergence time of the model.", "rationale": "V-Net improves the quality of the final contour prediction by using horizontal connections from left part features to the right part."}], [{"context": "We report in Table 1 the receptive fields of each network layer, showing the fact that the innermost portion of our CNN already captures the content of the whole input volume. We believe that this characteristic is important during segmentation of poorly visible anatomy: the features computed in the deepest layer perceive the whole anatomy of interest at once, since they are computed from data having a spatial support much larger than the typical size of the anatomy we seek to delineate, and therefore impose global constraints.", "rationale": "In V-Net features computed in the deepest layer perceive the whole anatomy of interest at once because they are computed have larger size than anatomy."}]], "composition": ["V-Net is trained on the 3D MRI prostate volumes.", "At the output of the V-Net foreground voxels represents the score for the anatomy and background voxels represents score for not having the anatomy at a region.", "In general MRI 3D volume data is complex. Prostate anomaly segmentation also makes the data in consideration unique.", "Since non anatomy part has a much larger spatial support than the anatomy and As we move down the layers in a CNN the receptive field of the features increases therefore proposed CNN would work fine for the local information.", "Two channel volumetric segmentation is used at the output to perform binary classification of foreground and background classes using soft-max. Each volume represent the logits for each class at each pixel location.", "The increase in receptive field of the features being computed in subsequent network layers is the result of convolutions layers as the network goes deeper?", "Due to the training/testing and augmentation on the diverse set of prostate scans all appearances of prostate can be encoded with V-Net.", "Yes the purpose of the de-convolution layer is to increase the size similar to un-pooling operation.", "Yes, more complex data can be finely segmented by using horizontal connections in the CNN network.", "Yes the phrase refer to feature maps with a larger number of channels than the input map at the deepest layer. As we move deeper the network will capture more features.."], "Is_figure_in_evidence": [true, true, true, false, true, true, true, true, true, false], "Is_table_in_evidence": [false, true, true, true, false, true, false, false, false, true], "question_key": ["819", "820", "821", "822", "823", "824", "826", "829", "830", "831"], "passages": ["Recent research in computer vision and pattern recognition has highlighted the capabilities of Convolutional Neural Networks (CNNs) to solve challenging tasks such as classification, segmentation and object detection, achieving state-of-the-art performances.This success has been attributed to the ability of CNNs to learn a hierarchical representation of raw input data, without relying on handcrafted features.As the inputs are processed through the network layers, the level of abstraction of the resulting features increases.Shallower layers grasp local information while deeper layers use filters whose receptive fields are much broader that therefore capture global information [19].", "Segmentation is a highly relevant task in medical image analysis.Automatic delineation of organs and structures of interest is often necessary to perform tasks such as visual augmentation [10], computer assisted diagnosis [12], interventions [20] and extraction of quantitative indices from images [1].In particular, since diagnostic and interventional imagery often consists of 3D images, being able to perform volumetric segmentations by taking into account the whole volume content at once, has a particular relevance.In this work, we aim to segment prostate MRI volumes. This is a challenging task due to the wide range of appearance the prostate can assume in different scans due to deformations and variations of the intensity distribution. Moreover, MRI volumes are often affected by artefacts and distortions due to field inhomogeneity. Prostate segmentation is nevertheless an important task having clinical relevance both during diagnosis, where the volume of the prostate needs to be assessed [13], and during treatment planning, where the estimate of the anatomical boundary needs to be accurate [4, 20].", "CNNs have been recently used for medical image segmentation.Early approaches obtain anatomy delineation in images or volumes by performing patch-wise image classification. Such segmentations are obtained by only considering local context and therefore are prone to failure, especially in challenging modalities such as ultrasound, where a high number of mis-classified voxel are to be expected.Post-processing approaches such as connected components analysis normally yield no improvement and therefore, more recent works, propose to use the network predictions in combination with Markov random fields [6], voting strategies [9] or more traditional approaches such as level-sets [2].Patch-wise approaches also suffer from efficiency issues. When densely extracted patches are processed in a CNN, a high number of computations is redundant and therefore the total algorithm runtime is high. In this case, more efficient computational schemes can be adopted.", "Fully convolutional network trained end-to-end were so far applied only to 2D images both in computer vision [11, 8] and microscopy image analysis [14]. These models, which served as an inspiration for our work, employed different network architectures and were trained to predict a segmentation mask, delineating the structures of interest, for the whole image. In [11] a pre-trained VGG network architecture [15] was used in conjunction with its mirrored, de-convolutional, equivalent to segment RGB images by leveraging the descriptive power of the features extracted by the innermost layer. In [8] three fully convolutional deep neural networks, pre-trained on a classification task, were refined to produce segmentations while in [14] a brand new CNN model, especially tailored to tackle biomedical image analysis problems in 2D, was proposed.", "In this work we present our approach to medical image segmentation that leverages the power of a fully convolutional neural networks, trained end-to-end, to process MRI volumes.Differently from other recent approaches we refrain from processing the input volumes slice-wise and we propose to use volumetric convolutions instead. We propose a novel objective function based on Dice coefficient maximisation, that we optimise during training.We demonstrate fast and accurate results on prostate MRI test volumes and we provide direct comparison with other methods which were evaluated on the same test data 111Detailed results available on http://promise12.grand-challenge.org/results/.", "In Figure 2 we provide a schematic representation of our convolutional neural network.We perform convolutions aiming to both extract features from the data and, at the end of each stage, to reduce its resolution by using appropriate stride. The left part of the network consists of a compression path, while the right part decompresses the signal until its original size is reached. Convolutions are all applied with appropriate padding.", "The left side of the network is divided in different stages that operate at different resolutions. Each stage comprises one to three convolutional layers. Similarly to the approach presented in [3], we formulate each stage such that it learns a residual function: the input of each stage is (a) used in the convolutional layers and processed through the non-linearities and (b) added to the output of the last convolutional layer of that stage in order to enable learning a residual function. As confirmed by our empirical observations, this architecture ensures convergence in a fraction of the time required by a similar network that does not learn residual functions.", "The convolutions performed in each stage use volumetric kernels having size 5\u00d75\u00d755555\\times 5\\times 55 \u00d7 5 \u00d7 5 voxels.As the data proceeds through different stages along the compression path, its resolution is reduced. This is performed through convolution with 2\u00d72\u00d722222\\times 2\\times 22 \u00d7 2 \u00d7 2 voxels wide kernels applied with stride 2222 (Figure 3). Since the second operation extracts features by considering only non overlapping 2\u00d72\u00d722222\\times 2\\times 22 \u00d7 2 \u00d7 2 volume patches, the size of the resulting feature maps is halved.This strategy serves a similar purpose as pooling layers that, motivated by [16] and other works discouraging the use of max-pooling operations in CNNs, have been replaced in our approach by convolutional ones. Moreover, since the number of feature channels doubles at each stage of the compression path of the V-Net, and due to the formulation of the model as a residual network, we resort to these convolution operations to double the number of feature maps as we reduce their resolution. PReLu non linearities are applied throughout the network.", "Replacing pooling operations with convolutional ones results also to networks that, depending on the specific implementation, can have a smaller memory footprint during training, due to the fact that no switches mapping the output of pooling layers back to their inputs are needed for back-propagation, and that can be better understood and analysed [19] by applying only de-convolutions instead of un-pooling operations.", "Downsampling allows us to reduce the size of the signal presented as input and to increase the receptive field of the features being computed in subsequent network layers. Each of the stages of the left part of the network, computes a number of features which is two times higher than the one of the previous layer.", "The right portion of the network extracts features and expands the spatial support of the lower resolution feature maps in order to gather and assemble the necessary information to output a two channel volumetric segmentation. The two features maps computed by the very last convolutional layer, having 1\u00d71\u00d711111\\times 1\\times 11 \u00d7 1 \u00d7 1 kernel size and producing outputs of the same size as the input volume, are converted to probabilistic segmentations of the foreground and background regions by applying soft-max voxelwise.After each stage of the right portion of the CNN, a de-convolution operation is employed in order increase the size of the inputs (Figure 3) followed by one to three convolutional layers involving half the number of 5\u00d75\u00d755555\\times 5\\times 55 \u00d7 5 \u00d7 5 kernels employed in the previous layer. Similar to the left part of the network, also in this case we resort to learn residual functions in the convolutional stages.", "Similarly to [14], we forward the features extracted from early stages of the left part of the CNN to the right part. This is schematically represented in Figure 2 by horizontal connections. In this way we gather fine grained detail that would be otherwise lost in the compression path and we improve the quality of the final contour prediction. We also observed that when these connections improve the convergence time of the model.", "We report in Table 1 the receptive fields of each network layer, showing the fact that the innermost portion of our CNN already captures the content of the whole input volume. We believe that this characteristic is important during segmentation of poorly visible anatomy: the features computed in the deepest layer perceive the whole anatomy of interest at once, since they are computed from data having a spatial support much larger than the typical size of the anatomy we seek to delineate, and therefore impose global constraints.", "The network predictions, which consist of two volumes having the same resolution as the original input data, are processed through a soft-max layer which outputs the probability of each voxel to belong to foreground and to background. In medical volumes such as the ones we are processing in this work, it is not uncommon that the anatomy of interest occupies only a very small region of the scan. This often causes the learning process to get trapped in local minima of the loss function yielding a network whose predictions are strongly biased towards background. As a result the foreground region is often missing or only partially detected. Several previous approaches resorted to loss functions based on sample re-weighting where foreground regions are given more importance than background ones during learning. In this work we propose a novel objective function based on dice coefficient, which is a quantity ranging between 00 and 1111 which we aim to maximise. The dice coefficient D\ud835\udc37Ditalic_D between two binary volumes can be written asD=2\u2211iNpigi\u2211iNpi2+\u2211iNgi2\ud835\udc372superscriptsubscript\ud835\udc56\ud835\udc41subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc54\ud835\udc56superscriptsubscript\ud835\udc56\ud835\udc41superscriptsubscript\ud835\udc5d\ud835\udc562superscriptsubscript\ud835\udc56\ud835\udc41superscriptsubscript\ud835\udc54\ud835\udc562D=\\frac{2\\sum_{i}^{N}p_{i}g_{i}}{\\sum_{i}^{N}p_{i}^{2}+\\sum_{i}^{N}g_{i}^{2}}italic_D = divide start_ARG 2 \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG", "where the sums run over the N\ud835\udc41Nitalic_N voxels, of the predicted binary segmentation volume pi\u2208Psubscript\ud835\udc5d\ud835\udc56\ud835\udc43p_{i}\\in{P}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 italic_P and the ground truth binary volume gi\u2208Gsubscript\ud835\udc54\ud835\udc56\ud835\udc3ag_{i}\\in{G}italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 italic_G. This formulation of Dice can be differentiated yielding the gradient\u2202D\u2202pj=2[gj(\u2211iNpi2+\u2211iNgi2)\u22122pj(\u2211iNpigi)(\u2211iNpi2+\u2211iNgi2)2]\ud835\udc37subscript\ud835\udc5d\ud835\udc572delimited-[]subscript\ud835\udc54\ud835\udc57superscriptsubscript\ud835\udc56\ud835\udc41superscriptsubscript\ud835\udc5d\ud835\udc562superscriptsubscript\ud835\udc56\ud835\udc41superscriptsubscript\ud835\udc54\ud835\udc5622subscript\ud835\udc5d\ud835\udc57superscriptsubscript\ud835\udc56\ud835\udc41subscript\ud835\udc5d\ud835\udc56subscript\ud835\udc54\ud835\udc56superscriptsuperscriptsubscript\ud835\udc56\ud835\udc41superscriptsubscript\ud835\udc5d\ud835\udc562superscriptsubscript\ud835\udc56\ud835\udc41superscriptsubscript\ud835\udc54\ud835\udc5622\\frac{\\partial D}{\\partial p_{j}}=2\\left[\\frac{g_{j}\\left(\\sum_{i}^{N}p_{i}^{2}+\\sum_{i}^{N}g_{i}^{2}\\right)-2p_{j}\\left(\\sum_{i}^{N}p_{i}g_{i}\\right)}{\\left(\\sum_{i}^{N}p_{i}^{2}+\\sum_{i}^{N}g_{i}^{2}\\right)^{2}}\\right]divide start_ARG \u2202 italic_D end_ARG start_ARG \u2202 italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG = 2 [ divide start_ARG italic_g start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) - 2 italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG ( \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ]computed with respect to the j\ud835\udc57jitalic_j-th voxel of the prediction. Using this formulation we do not need to assign weights to samples of different classes to establish the right balance between foreground and background voxels, and we obtain results that we experimentally observed are much better than the ones computed through the same network trained optimising a multinomial logistic loss with sample re-weighting (Fig. 6).", "Our CNN is trained end-to-end on a dataset of prostate scans in MRI. An example of the typical content of such volumes is shown in Figure 1. All the volumes processed by the network have fixed size of 128\u00d7128\u00d76412812864128\\times 128\\times 64128 \u00d7 128 \u00d7 64 voxels and a spatial resolution of 1\u00d71\u00d71.5111.51\\times 1\\times 1.51 \u00d7 1 \u00d7 1.5 millimeters.", "Annotated medical volumes are not easy to obtain due to the fact that one or more experts are required to manually trace a reliable ground truth annotation and that there is a cost associated with their acquisition. In this work we found necessary to augment the original training dataset in order to obtain robustness and increased precision on the test dataset.", "During every training iteration, we fed as input to the network randomly deformed versions of the training images by using a dense deformation field obtained through a 2\u00d72\u00d722222\\times 2\\times 22 \u00d7 2 \u00d7 2 grid of control-points and B-spline interpolation. This augmentation has been performed \u201don-the-fly\u201d, prior to each optimisation iteration, in order to alleviate the otherwise excessive storage requirements. Additionally we vary the intensity distribution of the data by adapting, using histogram matching, the intensity distributions of the training volumes used in each iteration, to the ones of other randomly chosen scans belonging to the dataset.", "A Previously unseen MRI volume can be segmented by processing it in a feed-forward manner through the network. The output of the last convolutional layer, after soft-max, consists of a probability map for background and foreground. The voxels having higher probability (>0.5absent0.5>0.5> 0.5) to belong to the foreground than to the background are considered part of the anatomy.", "We trained our method on 50505050 MRI volumes, and the relative manual ground truth annotation, obtained from the \u201dPROMISE2012\u201d challenge dataset [7]. This dataset contains medical data acquired in different hospitals, using different equipment and different acquisition protocols. The data in this dataset is representative of the clinical variability and challenges encountered in clinical settings. As previously stated we massively augmented this dataset through random transformation performed in each training iteration, for each mini-batch fed to the network. The mini-batches used in our implementation contained two volumes each, mainly due to the high memory requirement of the model during training. We used a momentum of 0.990.990.990.99 and a initial learning rate of 0.00010.00010.00010.0001 which decreases by one order of magnitude every 25252525K iterations.", "We tested V-Net on 30303030 MRI volumes depicting prostate whose ground truth annotation was secret. All the results reported in this section of the paper were obtained directly from the organisers of the challenge after submitting the segmentation obtained through our approach. The test set was representative of the clinical variability encountered in prostate scans in real clinical settings [7].", "We evaluated the approach performance in terms of Dice coefficient, Hausdorff distance of the predicted delineation to the ground truth annotation and in terms of score obtained on the challenge data as computed by the organisers of \u201dPROMISE 2012\u201d [7]. The results are shown in Table 2 and Fig. 5.", "Our implementation222Implementation available at https://github.com/faustomilletari/VNet was realised in python, using a custom version of the Caffe333Implementation available at https://github.com/faustomilletari/3D-Caffe [5] framework which was enabled to perform volumetric convolutions via CuDNN v3. All the trainings and experiments were ran on a standard workstation equipped with 64646464 GB of memory, an Intel(R) Core(TM) i7-5820K CPU working at 3.30GHz, and a NVidia GTX 1080 with 8888 GB of video memory. We let our model train for 48484848 hours, or 30303030K iterations circa, and we were able to segment a previously unseen volume in circa 1111 second. The datasets were first normalised using the N4 bias filed correction function of the ANTs framework [17] and then resampled to a common resolution of 1\u00d71\u00d71.5111.51\\times 1\\times 1.51 \u00d7 1 \u00d7 1.5 mm. We applied random deformations to the scans used for training by varying the position of the control points with random quantities obtained from gaussian distribution with zero mean and 15151515 voxels standard deviation. Qualitative results can be seen in Fig. 4.", "We presented and approach based on a volumetric convolutional neural network that performs segmentation of MRI prostate volumes in a fast and accurate manner. We introduced a novel objective function that we optimise during training based on the Dice overlap coefficient between the predicted segmentation and the ground truth annotation. Our Dice loss layer does not need sample re-weighting when the amount of background and foreground pixels is strongly unbalanced and is indicated for binary segmentation tasks. Although we inspired our architecture to the one proposed in [14], we divided it into stages that learn residuals and, as empirically observed, improve both results and convergence time. Future works will aim at segmenting volumes containing multiple regions in other modalities such as ultrasound and at higher resolutions by splitting the network over multiple GPUs.", "We would like to acknowledge NVidia corporation, that donated a Tesla K40 GPU to our group enabling this research, Dr. Geert Litjens who dedicated some of his time to evaluate our results against the ground truth of the PROMISE 2012 dataset and Ms. Iro Laina for her support to this project."], "figure_types": {"50004c086ffd6a201a4b782281aaa930fbfe6ecf/2-Figure1-1.png": "photograph(s)", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/3-Figure2-1.png": "schematic", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Figure3-1.png": "schematic", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/5-Table1-1.png": "table", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/7-Figure4-1.png": "photograph(s)", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/8-Figure5-1.png": "plot", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/8-Table2-1.png": "table", "50004c086ffd6a201a4b782281aaa930fbfe6ecf/9-Figure6-1.png": "photograph(s)"}}, "2004.04092": {"paper_id": "paper_4", "title": "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space", "arxiv_url": "https://arxiv.org/abs/2004.04092", "s2orc_url": "https://www.semanticscholar.org/paper/00696ba295d66f049d70272219f7fea4266171be", "all_figures_tables": {"00696ba295d66f049d70272219f7fea4266171be/13-Figure5-1.png": "Figure 5: Illustration of three different schemes to inject latent vector into GPT-2 for guided language generation: (a) Yelp and (b) PTB. The learning curves for reconstruction error per word is considered. Emb indicates latent vector is used as additional embedding to add into other embeddings, and Mem indicates latent vector is used as additional memory token for GPT2 to attend. Mem+Emb indicates the integration of two schemes.", "00696ba295d66f049d70272219f7fea4266171be/13-Figure6-1.png": "Figure 6: Illustration of sentence distribution in Wikipedia dataset: (a) Frequency distribution and (b) Cumulative Frequency distribution. We choose maximum length as 64 to construct the pre-training dataset. It leads to 1990K sentences, which is 96.45% of entire Wikipedia dataset.", "00696ba295d66f049d70272219f7fea4266171be/14-Table8-1.png": "Table 8: Comparison on PTB dataset.", "00696ba295d66f049d70272219f7fea4266171be/14-Table9-1.png": "Table 9: Comparison on Yelp dataset. For LSTM-LM and GPT-2, we report the exact negative log likelihood.", "00696ba295d66f049d70272219f7fea4266171be/15-Table10-1.png": "Table 10: Comparison on Yahoo dataset.", "00696ba295d66f049d70272219f7fea4266171be/15-Table11-1.png": "Table 11: Comparison on SNLI dataset. For LSTM-LM and GPT-2, we report the exact negative log likelihood.", "00696ba295d66f049d70272219f7fea4266171be/17-Table12-1.png": "Table 12: Dialog response generation on DailyDialog dataset. All numbers are from (Gu et al., 2019) except that iVAEMI is from (Fang et al., 2019).", "00696ba295d66f049d70272219f7fea4266171be/17-Table14-1.png": "Table 14: Interpolating latent representation from short sentence to long sentence. Each row show \u03c4 and the sentence generated from the latent vector z\u03c4 .", "00696ba295d66f049d70272219f7fea4266171be/18-Table15-1.png": "Table 15: Interpolating latent representation within the same sentiment. Each row show \u03c4 and the sentence generated from the latent vector z\u03c4 .", "00696ba295d66f049d70272219f7fea4266171be/18-Table16-1.png": "Table 16: Comparison of VAE and AE objective for latent space interpolation. VAE shows smoother interpolation results than AE.", "00696ba295d66f049d70272219f7fea4266171be/18-Table17-1.png": "Table 17: Sentence transfer via arithmetic operation in the latent space. The output sentences are in blue. In this example, we see content transition from relaxing to working.", "00696ba295d66f049d70272219f7fea4266171be/19-Table18-1.png": "Table 18: Sentence transfer via arithmetic operation in the latent space. The output sentences are in blue. In this example, we see two type of style transition: (1) from singular to plural subject, and (2) from daily-life activity to sport.", "00696ba295d66f049d70272219f7fea4266171be/19-Table19-1.png": "Table 19: Sentence transfer via arithmetic operation in the latent space. The output sentences are in blue. In this example, we see two type of style transition: (1) from plural/old to singular/young subject, or and (2) sentences are expended.", "00696ba295d66f049d70272219f7fea4266171be/20-Table20-1.png": "Table 20: Label-conditional text generation on Yelp dataset. The top block shows the positive reviews, and bottom block shows the negative reviews.", "00696ba295d66f049d70272219f7fea4266171be/20-Table21-1.png": "Table 21: Ablation study on the AE and VAE objective of OPTIMUS. Comparison is on the validation set of GLUE. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks.", "00696ba295d66f049d70272219f7fea4266171be/4-Figure1-1.png": "Figure 1: Illustration of OPTIMUS architecture.", "00696ba295d66f049d70272219f7fea4266171be/4-Figure2-1.png": "Figure 2: Illustration of two schemes to inject latent vector. (a) Memory: xt attends both x&lt;t and hMem; (b) Embedding: latent embedding is added into old embeddings to construct new token embedding h\u2032Emb.", "00696ba295d66f049d70272219f7fea4266171be/6-Table1-1.png": "Table 1: Comparison on language modeling tasks on four datasets. Best values are in blue. \u03bb = 0.50 is a good trade-off to achieve the best values on all metrics compared with small VAEs. \u201c-\u201d indicates the models are improper to report these values; Empty cells indicate the results were not reported in the literature.", "00696ba295d66f049d70272219f7fea4266171be/7-Table2-1.png": "Table 2: Sentence transfer via arithmetic operation in the latent space. The output sentences are in blue.", "00696ba295d66f049d70272219f7fea4266171be/7-Table3-1.png": "Table 3: Interpolating latent space. Each row shows \u03c4 , and the generated sentence (in blue) conditioned on z\u03c4 .", "00696ba295d66f049d70272219f7fea4266171be/8-Figure3-1.png": "Figure 3: Testing accuracy with a varying number of labeled training samples per class on the Yelp dataset.", "00696ba295d66f049d70272219f7fea4266171be/8-Figure4-1.png": "Figure 4: Comparison of tSNE visualization for the self-supervised feature learning results. The colors indicate different labels.", "00696ba295d66f049d70272219f7fea4266171be/8-Table4-1.png": "Table 4: Dialog response generation on DailyDialog dataset. All numbers are from (Gu et al., 2019) except that iVAEMI is from (Fang et al., 2019).", "00696ba295d66f049d70272219f7fea4266171be/8-Table6-1.png": "Table 6: Label-conditional text generation on Yelp.", "00696ba295d66f049d70272219f7fea4266171be/9-Table7-1.png": "Table 7: Comparison on the validation set of GLUE. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks."}, "referred_figures_tables": [["00696ba295d66f049d70272219f7fea4266171be/8-Table4-1.png"]], "question_id": [8], "question": ["How does training pre-training a latent space in Optimus lead to higher performance for dialog generation? Is it because the whole dialog can be encoded in the latent space?"], "question_section": ["Experimental Results"], "question_trigger_sentence": ["We measure the performance using Bleu Chen and Cherry (2014), and compute the precision, recall and F1 in Table LABEL:table:dialogue."], "question_type": ["Deep/complex question"], "evidential_info": [[{"context": "Dialog response generation The open-domain dialog response generation task is considered: generating responses z given a dialog history c. Following (Gao et al, 2019a), we embed the history and response in a joint latent space as z_{S2S} and z_{ae}, respectively. A fusion regularization is used to match the responses to the context. We consider Dailydialog (Li et al, 2017) used in (Gu et al, 2019), which has 13,118 daily conversations. Each utterance is processed as the response of previous 10 context utterances from both speakers. The baseline methods are described in Appendix. We measure the performance using Bleu (Chen and Cherry, 2014), and compute the precision, recall and Fl in Table 4. OPTIMUS shows higher Bleu scores than all existing baselines.", "rationale": "This paragraph mentions that the authors\u2019 proposed approach, OPTIMUS, outperforms all existing baselines on dialog response generation tasks."}]], "composition": ["There is no information about dialog generation specifically, to explain if this outperformance by OPTIMUS can be attributed specifically to being able to encode the entire dialog in latent space."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["882"], "passages": ["Pre-trained language models (PLMs) have substantially advanced the state-of-the-art across a variety of natural language processing (NLP) tasks\u00a0Peters et\u00a0al. (2018); Devlin et\u00a0al. (2019); Yang et\u00a0al. (2019); Radford et\u00a0al. (2019); Liu et\u00a0al. (2019); Keskar et\u00a0al. (2019); Shoeybi et\u00a0al. (2019). PLMs are often trained to predict words based on their context on massive text data, and the learned models can be fine-tuned to adapt to various downstream tasks.", "PLMs can generally play two different roles:(\ud835\udc56)\ud835\udc56(\\textup{\\it i})( i )a generic encoder such as BERT\u00a0Devlin et\u00a0al. (2019) to provide contextualized representations for language understanding tasks, and(\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56(\\textup{\\it ii})( ii )a powerful decoder such as GPT-2\u00a0Radford et\u00a0al. (2019) to generate text sequences in an auto-regressive manner. In a bid to combine language understanding and generation tasks in one unified framework, several model variants have been proposed, including UniLM\u00a0Dong et\u00a0al. (2019), BART\u00a0Lewis et\u00a0al. (2019), and T5\u00a0Raffel et\u00a0al. (2019). Although significant performance improvement has been reported on a wide range of NLP tasks, these models lack of explicit modeling of structures in a compact latent space, rendering it difficult to control language generation/representation from an abstract level.", "Variational Autoencoders (VAEs)\u00a0Kingma and Welling (2013); Rezende et\u00a0al. (2014) provide a tractable method to train latent-variable generative models. In NLP, latent variables may assume the role ofhigher-level sentence representations, which govern a lower-level word-by-word generation process, thus facilitating controlled text generation\u00a0Bowman et\u00a0al. (2016); Hu et\u00a0al. (2017). By representing sentences in a low-dimensional latent space, VAEs allow easy manipulation of sentences using the corresponding compact vector representations, such as feature regularization specified by prior distributions, and guided sentence generation with interpretable vector operators.Despite the attractive theoretical strengths, the current language VAEs are often built with shallow network architectures, such as two-layer LSTMs\u00a0Hochreiter and Schmidhuber (1997). This limits the model\u2019s capacity and leads to sub-optimal performance.", "In this paper, we propose Optimus, the first large-scale pre-trained deep latent variable models for natural language. Optimus is pre-trained using the sentence-level (variational) auto-encoder objectives on large text corpus. This leads to a universal latent space to organize sentences (hence named Optimus).Optimus enjoys several favorable properties:(\ud835\udc56)\ud835\udc56(\\textup{\\it i})( i )It combines the strengths of VAE, BERT and GPT, and supports both natural language understanding and generation tasks.(\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56(\\textup{\\it ii})( ii )Comparing to BERT, Optimus learns a more structured semantic space due to the use of the prior distribution in training. As a result, the language representations learned by Optimus are more universal / general in that they can be more easily adapted to a new domain/task.(\ud835\udc56\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56\ud835\udc56(\\textup{\\it iii})( iii )Different from GPT-2, which generates human-like text but may lack effective means of controlling its high-level semantics (such as tense, topics, sentiment), Optimus can be easily deployed for guided text generation.The effectiveness of Optimus has been demonstrated with extensive experiments on language modeling, dialog response generation, text style transfer and low-resource language understanding. It achieves lower perplexity than GPT-2 on standard benchmarks, produces strong performance on guided text generation, and improves BERT on feature-based language understanding tasks.The code and pre-trained models are released on Github222https://github.com/ChunyuanLI/Optimus.", "Along the way to build the first big VAE language model, there are several technical contributions/implications that are novel: (\ud835\udc56)\ud835\udc56(\\textup{\\it i})( i ) Latent vector injection: this work demonstrates two schemes to discuss how to effectively inject conditioning vectors into GPT-2 without re-training it.(\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56(\\textup{\\it ii})( ii ) The design idea to combine BERT/GPT-2 serves as a practical recipe to inspire people to integrate and reuse existing PLMs for larger and complex models.(\ud835\udc56\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56\ud835\udc56(\\textup{\\it iii})( iii ) Pre-training on massive datasets itself is an effective approach to reduce KL vanishing, as demonstrated by the state of-the-art performance on four VAE language modeling datasets. (\ud835\udc56\ud835\udc63)\ud835\udc56\ud835\udc63(\\textup{\\it iv})( iv ) The proof of VAE objective from the lens of IB, showing that VAE is a principled approach to balance the compactness and usability of learned representations.(\ud835\udc63)\ud835\udc63(\\textup{\\it v})( v ) Improved performance on several language tasks shows the importance and necessity of pre-training a latent space.", "Large-scale Transformer-based PLMs have recently achieved state-of-the-art performance on various natural language understanding and generation tasks\u00a0Devlin et\u00a0al. (2019); Yang et\u00a0al. (2019); Radford et\u00a0al. (2019); Liu et\u00a0al. (2019); Keskar et\u00a0al. (2019).Prior to Transformer-based PLMs, non-generative methods have seen some early success in pre-training sequence models for supervised downstream tasksincluding standard sequence auto-encoders\u00a0Dai and Le (2015); Li et\u00a0al. (2015), skip-thought models\u00a0Kiros et\u00a0al. (2015) and paragraph vector models\u00a0Le and Mikolov (2014) etc.\u00a0However, all of these models do not generally learn a smooth, interpretable feature space for sentence encoding, or generating novel sentences. In this work, we aim to fill the gap to learn such a universal latent space in the field of Transformer-based PLMs.", "Language VAEs have inspired new applications in NLP, via exploiting many interestingproperties of the model\u2019s latent space\u00a0Bowman et\u00a0al. (2016); Kim et\u00a0al. (2018b).Its modeling capacity and empirical performance is somewhat limited, partially due to the KL vanishing issue described in Section\u00a04.3. Several attempts have been made to alleviate this issue, including different KL annealing/thresholding schemes\u00a0Bowman et\u00a0al. (2016); Fu et\u00a0al. (2019); Higgins et\u00a0al. (2017); Li et\u00a0al. (2019), decoder architectures\u00a0Yang et\u00a0al. (2017); Dieng et\u00a0al. (2018), auxiliary loss\u00a0Zhao et\u00a0al. (2017), semi-amortized inference\u00a0Kim et\u00a0al. (2018a), aggressive encoder training schedule\u00a0He et\u00a0al. (2019), batch normalized inference\u00a0Zhu et\u00a0al. (2020) and flexible posterior\u00a0Fang et\u00a0al. (2019). Subramanian et\u00a0al. (2018) have shown some promise that general encoder can benefit language generation. Transformers\u00a0Vaswani et\u00a0al. (2017) are recently considered in VAEs for classification\u00a0Gururangan et\u00a0al. (2019) and storytelling\u00a0Wang and Wan (2019). Pre-training VAEs has been recently considered in conditional text generation to amortize the training of decoders and to allow easy adaptation in new generation tasks\u00a0Duan et\u00a0al. (2019).", "All these efforts utilize simple LSTM\u00a0Hochreiter and Schmidhuber (1997) and shallow Transformer\u00a0Vaswani et\u00a0al. (2017) architectures, thus with limited capacity. Our paper is the first big VAE model at the same scale of recent PLMs such as BERT and GPT-2. More importantly, we show that pre-training a meaningful latent space on a large text corpus can largely reduce the KL vanishing issue, and lead to new state-of-the-art performance.", "To generate a text sequence of length T\ud835\udc47Titalic_T, \ud835\udc99=[x1,\u22ef,xT]\ud835\udc99subscript\ud835\udc651\u22efsubscript\ud835\udc65\ud835\udc47\\boldsymbol{x}=[x_{1},\\cdots,x_{T}]bold_italic_x = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22ef , italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ], neural language models (NLM)\u00a0Mikolov et\u00a0al. (2010) generate every token xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT conditioned on the previous word tokens:p(\ud835\udc99)=\u220ft=1Tp\ud835\udf3d(xt|x<t),\ud835\udc5d\ud835\udc99superscriptsubscriptproduct\ud835\udc611\ud835\udc47subscript\ud835\udc5d\ud835\udf3dconditionalsubscript\ud835\udc65\ud835\udc61subscript\ud835\udc65absent\ud835\udc61\\displaystyle\\vspace{-2mm}p(\\boldsymbol{x})=\\prod_{t=1}^{T}p_{\\boldsymbol{\\theta}}(x_{t}|x_{<t}),\\vspace{-2mm}italic_p ( bold_italic_x ) = \u220f start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT ) ,(1)where x<tsubscript\ud835\udc65absent\ud835\udc61x_{<t}italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT indicates all tokens before t\ud835\udc61titalic_t, and \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8 is the model parameter.In NLMs, each one-step-ahead conditional in\u00a0(1) is modeled by an expressive family of neural networks, and is typically trained via maximum likelihood estimate (MLE).Perhaps the most well-known NLM instance is GPT-2\u00a0Radford et\u00a0al. (2019), which employs Transformers\u00a0Vaswani et\u00a0al. (2017) for each conditional, and \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8 is learned on a huge amount of OpenWeb text corpus. GPT-2 has shown surprisingly realistic text generation results, and low perplexity on several benchmarks. GPT-3\u00a0Brown et\u00a0al. (2020) was recently proposed to further scale up NLMs to 175 billion parameters, showing impressive results on few-shot learning on multiple language tasks.", "However, the only source of variation in NLMs, GPT2 and GPT3 is modeled in the conditionals at every step: the text generation process only depends on previous word tokens, and there is limited capacity for the generation to be guided by the higher-level structures that are likely presented in natural language, such as tense, topics or sentiment.", "To facilitate high-level guidance in sentence generation,Optimus organizes sentences in a universal latent (or semantic) space, via pre-training on large text corpora. Each sample in this space can be interpreted as outlines of the corresponding sentences, guiding the language generation process performed in the symbolic space\u00a0Subramanian et\u00a0al. (2018). This naturally fits within the learning paradigm of latent variable models such as VAEs\u00a0Kingma and Welling (2013); Bowman et\u00a0al. (2016), where the latent representations capture the high-level semantics/patterns.It consists of two parts, generation and inference, enabling a bidirectional mapping between the latent space and symbolic space.", "The generative model (decoder) draws a latent vector \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z from the continuous latent space with priorp(\ud835\udc9b)\ud835\udc5d\ud835\udc9bp(\\boldsymbol{z})italic_p ( bold_italic_z ), and generates the text sequence \ud835\udc99\ud835\udc99\\boldsymbol{x}bold_italic_x from a conditional distribution p\ud835\udf3d(\ud835\udc99|\ud835\udc9b)subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc99\ud835\udc9bp_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_z ); p(\ud835\udc9b)\ud835\udc5d\ud835\udc9bp(\\boldsymbol{z})italic_p ( bold_italic_z ) is typically assumed a multivariate Gaussian, and \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8 represents the neural network parameters. The following auto-regressive decoding process is usually used:p\ud835\udf3d(\ud835\udc99|\ud835\udc9b)=\u220ft=1Tp\ud835\udf3d(xt|x<t,\ud835\udc9b).subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc99\ud835\udc9bsuperscriptsubscriptproduct\ud835\udc611\ud835\udc47subscript\ud835\udc5d\ud835\udf3dconditionalsubscript\ud835\udc65\ud835\udc61subscript\ud835\udc65absent\ud835\udc61\ud835\udc9b\\displaystyle\\vspace{-2mm}p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{z})=\\prod_{t=1}^{T}p_{\\boldsymbol{\\theta}}(x_{t}|x_{<t},\\boldsymbol{z}).\\vspace{-4mm}italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_z ) = \u220f start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT < italic_t end_POSTSUBSCRIPT , bold_italic_z ) .(2)Intuitively, VAE provides a \u201chierachical\u201d generation procedure: \ud835\udc9b\u223cp(\ud835\udc9b)similar-to\ud835\udc9b\ud835\udc5d\ud835\udc9b\\boldsymbol{z}\\sim p(\\boldsymbol{z})bold_italic_z \u223c italic_p ( bold_italic_z ) determines the high-level semantics, followed by (2) to produce the output sentences with low-level syntactic and lexical details. This contrasts with\u00a0(1) in the explicit dependency on \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z.", "Similar to GPT-2, parameters \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8 are typically learned by maximizing the marginal log likelihood log\u2061p\ud835\udf3d(\ud835\udc99)=log\u222bp(\ud835\udc9b)p\ud835\udf3d(\ud835\udc99|\ud835\udc9b)d\ud835\udc9bsubscript\ud835\udc5d\ud835\udf3d\ud835\udc99\ud835\udc5d\ud835\udc9bsubscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc99\ud835\udc9bd\ud835\udc9b\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})=\\log\\int p(\\boldsymbol{z})p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{z})\\mbox{d}\\boldsymbol{z}roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x ) = roman_log \u222b italic_p ( bold_italic_z ) italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_z ) d bold_italic_z.However, this marginal term is intractable to compute for many decoder choices. Thus, variational inference is considered, and the true posterior p\ud835\udf3d(\ud835\udc9b|\ud835\udc99)\u221dp\ud835\udf3d(\ud835\udc99|\ud835\udc9b)p(\ud835\udc9b)proportional-tosubscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc9b\ud835\udc99subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc99\ud835\udc9b\ud835\udc5d\ud835\udc9bp_{\\boldsymbol{\\theta}}(\\boldsymbol{z}|\\boldsymbol{x})\\propto p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{z})p(\\boldsymbol{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) \u221d italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_z ) italic_p ( bold_italic_z ) is approximated viathe variational distribution q\u03d5(\ud835\udc9b|\ud835\udc99)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc9b\ud835\udc99q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}|\\boldsymbol{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) is (often known as the inference model or encoder), implemented via a \u03d5bold-italic-\u03d5\\boldsymbol{\\phi}bold_italic_\u03d5-parameterized neural network.It yields the evidence lower bound objective (ELBO):log\u2061p\ud835\udf3d(\ud835\udc99)\u2265\u2112ELBO=subscript\ud835\udc5d\ud835\udf3d\ud835\udc99subscript\u2112ELBOabsent\\displaystyle\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\geq\\mathcal{L}_{\\text{ELBO}}=roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x ) \u2265 caligraphic_L start_POSTSUBSCRIPT ELBO end_POSTSUBSCRIPT =(3)\ud835\udd3cq\u03d5(\ud835\udc9b|\ud835\udc99)[logp\ud835\udf3d(\ud835\udc99|\ud835\udc9b)]\u2212KL(q\u03d5(\ud835\udc9b|\ud835\udc99)||p(\ud835\udc9b))\\displaystyle\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}|\\boldsymbol{x})}\\big{[}\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{z})\\big{]}-\\mbox{KL}(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}|\\boldsymbol{x})||p(\\boldsymbol{z}))blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_z ) ] - KL ( italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) | | italic_p ( bold_italic_z ) )", "Typically, q\u03d5(\ud835\udc9b|\ud835\udc99)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc9b\ud835\udc99q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}|\\boldsymbol{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) is modeled as a Gaussian distribution, and the re-parametrization trick is used for efficient learning\u00a0Kingma and Welling (2013).", "There is an alternative interpretation of the ELBO: the VAE objective can be viewed as a regularized version of the autoencoder (AE)\u00a0Goodfellow et\u00a0al. (2016).It is thus natural to extend the negative of \u2112ELBOsubscript\u2112ELBO\\mathcal{L}_{\\text{ELBO}}caligraphic_L start_POSTSUBSCRIPT ELBO end_POSTSUBSCRIPT in (3) by introducing a hyper-parameter \u03b2\ud835\udefd\\betaitalic_\u03b2 to control the strength of regularization:\u2112\u03b2subscript\u2112\ud835\udefd\\displaystyle\\mathcal{L}_{\\beta}caligraphic_L start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT=\u2112E+\u03b2\u2112R,withabsentsubscript\u2112\ud835\udc38\ud835\udefdsubscript\u2112\ud835\udc45with\\displaystyle=\\mathcal{L}_{E}+\\beta\\mathcal{L}_{R},~{}~{}\\text{with}= caligraphic_L start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT + italic_\u03b2 caligraphic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT , with(4)\u2112Esubscript\u2112\ud835\udc38\\displaystyle\\mathcal{L}_{E}caligraphic_L start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT=\u2212\ud835\udd3cq\u03d5(\ud835\udc9b|\ud835\udc99)[log\u2061p\ud835\udf3d(\ud835\udc99|\ud835\udc9b)]absentsubscript\ud835\udd3csubscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc9b\ud835\udc99delimited-[]subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc99\ud835\udc9b\\displaystyle=-\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}|\\boldsymbol{x})}\\big{[}\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}|\\boldsymbol{z})\\big{]}= - blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_z ) ](5)\u2112Rsubscript\u2112\ud835\udc45\\displaystyle\\mathcal{L}_{R}caligraphic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT=KL(q\u03d5(\ud835\udc9b|\ud835\udc99)||p(\ud835\udc9b))\\displaystyle=\\mbox{KL}(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}|\\boldsymbol{x})||p(\\boldsymbol{z}))= KL ( italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) | | italic_p ( bold_italic_z ) )(6)where \u2112Esubscript\u2112\ud835\udc38\\mathcal{L}_{E}caligraphic_L start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT is the reconstruction error (or negative log-likelihood (NLL)), and \u2112Rsubscript\u2112\ud835\udc45\\mathcal{L}_{R}caligraphic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT is a KL regularizer.The cost function \u2112\u03b2subscript\u2112\ud835\udefd\\mathcal{L}_{\\beta}caligraphic_L start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT provides a unified perspective for understanding various autoencoder variants and training methods. We consider two types of latent space with the following objectives:", "\u2022AE. Only \u2112Esubscript\u2112\ud835\udc38\\mathcal{L}_{E}caligraphic_L start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT is considered (\u03b2=0\ud835\udefd0\\beta=0italic_\u03b2 = 0), while the Gaussian sampling in q\u03d5(\ud835\udc9b|\ud835\udc99)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc9b\ud835\udc99q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}|\\boldsymbol{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) remains. In other words, the regularization is removed, and a point-estimate is likely to be learned to represent the text sequence\u2019s latent feature.Note our reconstruction is on sentence-level, while other PLMs\u00a0Devlin et\u00a0al. (2019); Yang et\u00a0al. (2019) employ masked LM loss, performing token-level reconstruction. \u2022VAE. The full VAE objective is considered (\u03b2>0\ud835\udefd0\\beta>0italic_\u03b2 > 0). It tends to learn a smooth latent space due to \u2112Rsubscript\u2112\ud835\udc45\\mathcal{L}_{R}caligraphic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT.", "From an information theory perspective, information bottleneck (IB) provides a principled approach to find the trade-off between predictive power and complexity (compactness) when summarizing observed data in learned representations. We show that our Optimus pre-training objectives effectively practice the IB principle as follows.", "The objective in\u00a0(4) shows the \u03b2\ud835\udefd\\betaitalic_\u03b2-VAE loss for one single sentence \ud835\udc99\ud835\udc99\\boldsymbol{x}bold_italic_x. The training objective over the dataset q(\ud835\udc99)\ud835\udc5e\ud835\udc99q(\\boldsymbol{x})italic_q ( bold_italic_x ) can be written as:\u2131\u03b2=\u2212\u2131E+\u03b2\u2131Rsubscript\u2131\ud835\udefdsubscript\u2131\ud835\udc38\ud835\udefdsubscript\u2131\ud835\udc45\\displaystyle\\mathcal{F}_{\\beta}=-\\mathcal{F}_{E}+\\beta\\mathcal{F}_{R}\\vspace{-2mm}caligraphic_F start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT = - caligraphic_F start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT + italic_\u03b2 caligraphic_F start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT(7)where \u2131E=Eq(\ud835\udc99),\ud835\udc9b\u223cq(\ud835\udc9b|\ud835\udc99)[log\u2061p(\ud835\udc99~|\ud835\udc9b)]subscript\u2131\ud835\udc38subscript\ud835\udc38similar-to\ud835\udc5e\ud835\udc99\ud835\udc9b\ud835\udc5econditional\ud835\udc9b\ud835\udc99delimited-[]\ud835\udc5dconditional~\ud835\udc99\ud835\udc9b\\mathcal{F}_{E}=E_{q(\\boldsymbol{x}),\\boldsymbol{z}\\sim q(\\boldsymbol{z}|\\boldsymbol{x})}[\\log p(\\tilde{\\boldsymbol{x}}|\\boldsymbol{z})]caligraphic_F start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT = italic_E start_POSTSUBSCRIPT italic_q ( bold_italic_x ) , bold_italic_z \u223c italic_q ( bold_italic_z | bold_italic_x ) end_POSTSUBSCRIPT [ roman_log italic_p ( over~ start_ARG bold_italic_x end_ARG | bold_italic_z ) ] is the aggregated reconstruction term (\ud835\udc99~~\ud835\udc99\\tilde{\\boldsymbol{x}}over~ start_ARG bold_italic_x end_ARG is the reconstruction target), and \u2131R=\ud835\udd3cq(\ud835\udc99)[KL(q(\ud835\udc9b|\ud835\udc99)||p(\ud835\udc9b))]\\mathcal{F}_{R}=\\mathbb{E}_{q(\\boldsymbol{x})}[\\mbox{KL}(q(\\boldsymbol{z}|\\boldsymbol{x})||p(\\boldsymbol{z}))]caligraphic_F start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT italic_q ( bold_italic_x ) end_POSTSUBSCRIPT [ KL ( italic_q ( bold_italic_z | bold_italic_x ) | | italic_p ( bold_italic_z ) ) ] is the aggregated KL term. With the detailed proof shown in Section\u00a0A of Appendix, we see that \u2131\u03b2subscript\u2131\ud835\udefd\\mathcal{F}_{\\beta}caligraphic_F start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT is an upper bound of IB:\u2131\u03b2\u2265\u2212Iq(\ud835\udc9b,\ud835\udc99~)+\u03b2Iq(\ud835\udc9b,\ud835\udc99)=\u2112IB,subscript\u2131\ud835\udefdsubscript\ud835\udc3c\ud835\udc5e\ud835\udc9b~\ud835\udc99\ud835\udefdsubscript\ud835\udc3c\ud835\udc5e\ud835\udc9b\ud835\udc99subscript\u2112IB\\displaystyle\\mathcal{F}_{\\beta}\\geq-I_{q}(\\boldsymbol{z},\\tilde{\\boldsymbol{x}})+\\beta I_{q}(\\boldsymbol{z},\\boldsymbol{x})=\\mathcal{L}_{\\text{IB}},caligraphic_F start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT \u2265 - italic_I start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( bold_italic_z , over~ start_ARG bold_italic_x end_ARG ) + italic_\u03b2 italic_I start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( bold_italic_z , bold_italic_x ) = caligraphic_L start_POSTSUBSCRIPT IB end_POSTSUBSCRIPT ,(8)where \u2112IBsubscript\u2112IB\\mathcal{L}_{\\text{IB}}caligraphic_L start_POSTSUBSCRIPT IB end_POSTSUBSCRIPT is the Lagrange relaxation form of IB presented by\u00a0Tishby et\u00a0al. (2000), Iq(\u22c5,\u22c5)subscript\ud835\udc3c\ud835\udc5e\u22c5\u22c5I_{q}(\\cdot,\\cdot)italic_I start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( \u22c5 , \u22c5 ) is the mutual information (MI) measured by probability q\ud835\udc5eqitalic_q. The goal of IB is to maximize the predictive power of \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z on target \ud835\udc99~~\ud835\udc99\\tilde{\\boldsymbol{x}}over~ start_ARG bold_italic_x end_ARG, subject to the constraint on the amount of information about original \ud835\udc99\ud835\udc99\\boldsymbol{x}bold_italic_x that \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z carries.When \u03b2=0\ud835\udefd0\\beta=0italic_\u03b2 = 0, we have the AE variant of our Optimus, the model fully focuses on maximizing the MI to recover sentences from the latent space. As \u03b2\ud835\udefd\\betaitalic_\u03b2 increases, the model gradually transits towards fitting the aggregated latent distribution q(\ud835\udc9b)=\u222b\ud835\udc99q(\ud835\udc9b|\ud835\udc99)q(\ud835\udc99)\ud835\udc51\ud835\udc99\ud835\udc5e\ud835\udc9bsubscript\ud835\udc99\ud835\udc5econditional\ud835\udc9b\ud835\udc99\ud835\udc5e\ud835\udc99differential-d\ud835\udc99q(\\boldsymbol{z})=\\int_{\\boldsymbol{x}}q(\\boldsymbol{z}|\\boldsymbol{x})q(\\boldsymbol{x})d\\boldsymbol{x}italic_q ( bold_italic_z ) = \u222b start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT italic_q ( bold_italic_z | bold_italic_x ) italic_q ( bold_italic_x ) italic_d bold_italic_x to the given prior p(\ud835\udc9b)\ud835\udc5d\ud835\udc9bp(\\boldsymbol{z})italic_p ( bold_italic_z ), leading the VAE variant of our Optimus.", "The model architecture of Optimus is composed of multi-layer Transformer-based encoder and decoder, based on the original implementation described in\u00a0Vaswani et\u00a0al. (2017). The overall architecture is illustrated in Figure\u00a01.To leverage the expressiveness power of existing PLMs, we initialize our encoder and decoder with weights of BERT \u03d5BERTsubscriptbold-italic-\u03d5BERT\\boldsymbol{\\phi}_{\\text{BERT}}bold_italic_\u03d5 start_POSTSUBSCRIPT BERT end_POSTSUBSCRIPT and GPT-2 \ud835\udf3dGPT-2subscript\ud835\udf3dGPT-2\\boldsymbol{\\theta}_{\\text{GPT-2}}bold_italic_\u03b8 start_POSTSUBSCRIPT GPT-2 end_POSTSUBSCRIPT, respectively. This procedure is seamless, as all of these models are trained in a self-supervised/unsupervised manner.", "We denote the number of layers (i.e., Transformer blocks) as L\ud835\udc3fLitalic_L, the hidden size as H\ud835\udc3bHitalic_H, and the number of self-attention heads as A\ud835\udc34Aitalic_A. Specifically, we consider BERTBASEBASE{}_{\\text{BASE}}start_FLOATSUBSCRIPT BASE end_FLOATSUBSCRIPT (L=12, H=768, A=12, Total Parameters=110M) and GPT-2 (L=12, H=768, A=12, Total Parameters=117M).We hope that our approach can provide a practical recipe to inspire future work to integrate larger pre-trained encoder and decoder for higher performance models.", "Two technical questions remain, when pre-training Optimus from BERT & GPT-2:(\ud835\udc56)\ud835\udc56(\\textup{\\it i})( i ) How to represent sentences, since the two PLMs employ different tokenization schemes?(\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56(\\textup{\\it ii})( ii ) How to adapt a pre-trained GPT-2 to arbitrary conditional input without re-training the model again? Controllable GPT-2 models have been studied in\u00a0Keskar et\u00a0al. (2019); Zellers et\u00a0al. (2019); Peng et\u00a0al. (2020a, b) when prescribed control codes/tokens are provided, but it is still unknown how to ground GPT-2 to arbitrary conditional inputs.", "In BERT, WordPiece Embeddings (WPE) is used for tokenization (vocabulary size is 28996 for the cased version). In GPT-2, the modified Byte Pair Encoding (BPE)\u00a0Radford et\u00a0al. (2019) is used for tokenization (vocabulary size is 50260). A given token is represented as \ud835\udc89\ud835\ude74\ud835\ude96\ud835\ude8bsubscript\ud835\udc89\ud835\ude74\ud835\ude96\ud835\ude8b{\\boldsymbol{h}}_{\\texttt{Emb}}bold_italic_h start_POSTSUBSCRIPT Emb end_POSTSUBSCRIPT, by summing the corresponding token, position and segment embeddings\u00a0333Optimus does not require segment embeddings, but we remain it due to BERT initialization..For a sentence, we present it in both types of tokenization: the input of encoder is WPE, and the output of decoder is BPE to compute the reconstruction loss.", "Similar to BERT, the first token of every sentence is always a special classification token ([CLS]). The last-layer hidden state \ud835\udc89[CLS]\u2208\u211dHsubscript\ud835\udc89[CLS]superscript\u211d\ud835\udc3b{\\boldsymbol{h}}_{\\texttt{[CLS]}}\\in\\mathbb{R}^{H}bold_italic_h start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT corresponding to this token is used as the sentence-level representation. It further constructs the latent representation \ud835\udc9b=\ud835\udc16E\ud835\udc89[CLS]\ud835\udc9bsubscript\ud835\udc16Esubscript\ud835\udc89[CLS]\\boldsymbol{z}={{\\bf W}}_{\\text{E}}{\\boldsymbol{h}}_{\\texttt{[CLS]}}bold_italic_z = bold_W start_POSTSUBSCRIPT E end_POSTSUBSCRIPT bold_italic_h start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT, where \ud835\udc9b\u2208\u211dP\ud835\udc9bsuperscript\u211d\ud835\udc43\\boldsymbol{z}\\in\\mathbb{R}^{P}bold_italic_z \u2208 blackboard_R start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT is a P\ud835\udc43Pitalic_P-dimensional vector and \ud835\udc16E\u2208\u211dP\u00d7Hsubscript\ud835\udc16Esuperscript\u211d\ud835\udc43\ud835\udc3b{{\\bf W}}_{\\text{E}}\\in\\mathbb{R}^{P\\times H}bold_W start_POSTSUBSCRIPT E end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_P \u00d7 italic_H end_POSTSUPERSCRIPT is the weight matrix. To facilitate \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z in GPT-2 decoding without re-training the weights, we consider two schemes, illustrated in Figure\u00a02:\u2022Memory:\u00a0\ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z plays the role of an additional memory vector \ud835\udc89\ud835\ude7c\ud835\ude8e\ud835\ude96subscript\ud835\udc89\ud835\ude7c\ud835\ude8e\ud835\ude96{\\boldsymbol{h}}_{\\texttt{Mem}}bold_italic_h start_POSTSUBSCRIPT Mem end_POSTSUBSCRIPT for GPT2 to attend. Specifically, \ud835\udc89\ud835\ude7c\ud835\ude8e\ud835\ude96=\ud835\udc16M\ud835\udc9bsubscript\ud835\udc89\ud835\ude7c\ud835\ude8e\ud835\ude96subscript\ud835\udc16M\ud835\udc9b{\\boldsymbol{h}}_{\\texttt{Mem}}={{\\bf W}}_{\\text{M}}\\boldsymbol{z}bold_italic_h start_POSTSUBSCRIPT Mem end_POSTSUBSCRIPT = bold_W start_POSTSUBSCRIPT M end_POSTSUBSCRIPT bold_italic_z, where \ud835\udc16M\u2208\u211dLH\u00d7Psubscript\ud835\udc16Msuperscript\u211d\ud835\udc3f\ud835\udc3b\ud835\udc43{{\\bf W}}_{\\text{M}}\\in\\mathbb{R}^{LH\\times P}bold_W start_POSTSUBSCRIPT M end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_L italic_H \u00d7 italic_P end_POSTSUPERSCRIPT is the weight matrix. \ud835\udc89\ud835\ude7c\ud835\ude8e\ud835\ude96\u2208\u211dLHsubscript\ud835\udc89\ud835\ude7c\ud835\ude8e\ud835\ude96superscript\u211d\ud835\udc3f\ud835\udc3b{\\boldsymbol{h}}_{\\texttt{Mem}}\\in\\mathbb{R}^{LH}bold_italic_h start_POSTSUBSCRIPT Mem end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_L italic_H end_POSTSUPERSCRIPT is separated into L\ud835\udc3fLitalic_L vectors of length H\ud835\udc3bHitalic_H, each of which is attended by GPT-2 in one layer.\u2022Embedding:\u00a0\ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z is added on the original embedding layer, and directly used in every decoding step. The new embedding representation is \ud835\udc89\ud835\ude74\ud835\ude96\ud835\ude8b\u2032=\ud835\udc89\ud835\ude74\ud835\ude96\ud835\ude8b+\ud835\udc16D\ud835\udc9bsuperscriptsubscript\ud835\udc89\ud835\ude74\ud835\ude96\ud835\ude8b\u2032subscript\ud835\udc89\ud835\ude74\ud835\ude96\ud835\ude8bsubscript\ud835\udc16D\ud835\udc9b{\\boldsymbol{h}}_{\\texttt{Emb}}^{\\prime}={\\boldsymbol{h}}_{\\texttt{Emb}}+{{\\bf W}}_{\\text{D}}\\boldsymbol{z}bold_italic_h start_POSTSUBSCRIPT Emb end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = bold_italic_h start_POSTSUBSCRIPT Emb end_POSTSUBSCRIPT + bold_W start_POSTSUBSCRIPT D end_POSTSUBSCRIPT bold_italic_z, where \ud835\udc16D\u2208\u211dH\u00d7Psubscript\ud835\udc16Dsuperscript\u211d\ud835\udc3b\ud835\udc43{{\\bf W}}_{\\text{D}}\\in\\mathbb{R}^{H\\times P}bold_W start_POSTSUBSCRIPT D end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_H \u00d7 italic_P end_POSTSUPERSCRIPT.We study their empirical performance in Section B.1 of Appendix, and observe that Memory is significantly more effective than Embedding, and the integration of both schemes yields slightly better results. We hypothesize that the reason why Memory is superior is because it allows the decoder to attend the latent information at every layer ofthe network directly, while the Embedding method only allows the decoder to see the latentinformation at the input and output layer.In our experiments, we use the integration scheme by default.In summary, the encoder parameters \u03d5={\u03d5BERT,\ud835\udc16E}bold-italic-\u03d5subscriptbold-italic-\u03d5BERTsubscript\ud835\udc16E\\boldsymbol{\\phi}=\\{\\boldsymbol{\\phi}_{\\text{BERT}},{{\\bf W}}_{\\text{E}}\\}bold_italic_\u03d5 = { bold_italic_\u03d5 start_POSTSUBSCRIPT BERT end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT E end_POSTSUBSCRIPT }, and decoder parameters \ud835\udf3d={\ud835\udf3dGPT-2,\ud835\udc16M,\ud835\udc16D}\ud835\udf3dsubscript\ud835\udf3dGPT-2subscript\ud835\udc16Msubscript\ud835\udc16D\\boldsymbol{\\theta}=\\{\\boldsymbol{\\theta}_{\\text{GPT-2}},{{\\bf W}}_{\\text{M}},{{\\bf W}}_{\\text{D}}\\}bold_italic_\u03b8 = { bold_italic_\u03b8 start_POSTSUBSCRIPT GPT-2 end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT M end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT D end_POSTSUBSCRIPT }.", "We train the model parameters {\u03d5,\ud835\udf3d}bold-italic-\u03d5\ud835\udf3d\\{\\boldsymbol{\\phi},\\boldsymbol{\\theta}\\}{ bold_italic_\u03d5 , bold_italic_\u03b8 } using two objectives: AE and VAE, discussed in Section\u00a04.1. Pre-training AE using \u00a0(5) is straightforward. However, pre-training VAE can be challenging due to the notorious KL vanishing issue\u00a0Bowman et\u00a0al. (2016), where(\ud835\udc56)\ud835\udc56(\\textup{\\it i})( i )an encoder that produces posteriors almost identical to the Gaussian prior for all sentences (rather than a more interesting posterior); and(\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56(\\textup{\\it ii})( ii )a decoder that completely ignores \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z in\u00a0(2), and a learned model that reduces to a simpler NLM.", "To reduce this issue, we follow the intuition that if the encoder is providing useful information from the beginning of decoder training, the decoder is more likely to make use of \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z\u00a0Fu et\u00a0al. (2019); He et\u00a0al. (2019). Specifically, we use the cyclical schedule to anneal \u03b2\ud835\udefd\\betaitalic_\u03b2 for 10 periods\u00a0Fu et\u00a0al. (2019).Within one period, there are three consecutive stages: Training AE (\u03b2=0\ud835\udefd0\\beta=0italic_\u03b2 = 0) for 0.5 proportion, annealing \u03b2\ud835\udefd\\betaitalic_\u03b2 from 0 to 1 for 0.25 proportion, and fixing \u03b2=1\ud835\udefd1\\beta=1italic_\u03b2 = 1 for 0.25 proportion. When \u03b2>0\ud835\udefd0\\beta>0italic_\u03b2 > 0, we use the KL thresholding scheme\u00a0Li et\u00a0al. (2019); Kingma et\u00a0al. (2016), and replace the KL term \u2112Rsubscript\u2112\ud835\udc45\\mathcal{L}_{R}caligraphic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT in\u00a0(6) with a hinge loss term that maxes each component of the original KL with a constant \u03bb\ud835\udf06\\lambdaitalic_\u03bb:\u2112R\u2032=\u2211imax[\u03bb,KL(q\u03d5(zi|\ud835\udc99)||p(zi))]\\displaystyle\\mathcal{L}_{R}^{\\prime}=\\sum_{i}\\max[\\lambda,\\mbox{KL}(q_{\\boldsymbol{\\phi}}(z_{i}|\\boldsymbol{x})||p(z_{i}))]caligraphic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_max [ italic_\u03bb , KL ( italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | bold_italic_x ) | | italic_p ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ](9)Here, zisubscript\ud835\udc67\ud835\udc56z_{i}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT denotes the i\ud835\udc56iitalic_ith dimension of \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z. Usingthe thresholding objective causes learning to give up driving down KL for dimensions of \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z that are already beneath the target compression rate.", "The pre-training procedure largely follows the existing literature on language model pre-training. We use English Wikipedia to pre-train our AE and VAE objectives. As our main interest is to model sentences (rather than text sequences of a fixed length), we pre-process Wikipedia with maximum sentences length 64. It leads to 1990K sentences, which accounts 96.45% Wikipedia sentences used in BERT. More data pre-processing details are in Section\u00a0B.2 of Appendix.", "We consider to apply the pre-trained Optimus models to three types of downstream tasks:(\ud835\udc56)\ud835\udc56(\\textup{\\it i})( i ) language modeling, where Optimus is compared with SoTA VAE methods and GPT-2.(\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56(\\textup{\\it ii})( ii ) Guided language generation, where Optimus shows its unique advantage in producing controllable sentences in contrast to GPT-2.(\ud835\udc56\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56\ud835\udc56(\\textup{\\it iii})( iii ) Low-resource language understanding, where the learned structured latent features can be used for fast adaptation in new tasks.", "Fine-tuning LM on new datasets is straightforward. We load the pre-trained Optimus, and update the model with one additional \u03b2\ud835\udefd\\betaitalic_\u03b2 scheduling cycle for one epoch. The semantic latent vectors are first pre-trained off-the-shelf, and then easily leveraged to train the decoder on downstream datasets. From this perspective, our pre-training can be viewed as an effective approach to reduce KL vanishing.", "We consider four datasets: the Penn Treebank (\ud835\ude7f\ud835\ude83\ud835\ude71\ud835\ude7f\ud835\ude83\ud835\ude71\\mathtt{PTB}typewriter_PTB) Marcus et\u00a0al. (1993), \ud835\ude82\ud835\ude7d\ud835\ude7b\ud835\ude78\ud835\ude82\ud835\ude7d\ud835\ude7b\ud835\ude78\\mathtt{SNLI}typewriter_SNLI\u00a0Bowman et\u00a0al. (2015), \ud835\ude88\ud835\ude8a\ud835\ude91\ud835\ude98\ud835\ude98\ud835\ude88\ud835\ude8a\ud835\ude91\ud835\ude98\ud835\ude98\\mathtt{Yahoo}typewriter_Yahoo, and \ud835\ude88\ud835\ude8e\ud835\ude95\ud835\ude99\ud835\ude88\ud835\ude8e\ud835\ude95\ud835\ude99\\mathtt{Yelp}typewriter_Yelp corpora\u00a0Yang et\u00a0al. (2017); He et\u00a0al. (2019).", "There are two types of metrics to evaluate language VAEs.(\ud835\udc56)\ud835\udc56(\\textup{\\it i})( i ) Generation capability: we use perplexity (PPL). Note that NLM and GPT-2 has exactly PPL, while VAEs does not. Following\u00a0He et\u00a0al. (2019), we use the importance weighted bound in\u00a0Burda et\u00a0al. (2015) to approximate log\u2061p(\ud835\udc99)\ud835\udc5d\ud835\udc99\\log p(\\boldsymbol{x})roman_log italic_p ( bold_italic_x ), and report PPL.(\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56(\\textup{\\it ii})( ii ) Representation learning capability: Active units (AU) of \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z and its Mutual Information (MI) with \ud835\udc99\ud835\udc99\\boldsymbol{x}bold_italic_x.We report the full results with ELBO, KL and Reconstruction in Appendix, but note that higher ELBO does not necessarily yield better language modeling.", "(\ud835\udc56)\ud835\udc56(\\textup{\\it i})( i ) GPT-2. A large-scale LM trained on OpoenWebText\u00a0Radford et\u00a0al. (2019). We load the pre-trained GPT-2 weights, and refine the model for 1 epoch on the new datasets.(\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56(\\textup{\\it ii})( ii ) Annealing. \u03b2\ud835\udefd\\betaitalic_\u03b2 is gradually annealed from 0 to 1. This annealing procedure can be used once (M.A.)\u00a0Bowman et\u00a0al. (2016) or multiple times (C.A.)\u00a0Fu et\u00a0al. (2019).(\ud835\udc56\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56\ud835\udc56(\\textup{\\it iii})( iii ) Aggressive Training\u00a0He et\u00a0al. (2019). Training the encoder multiple times per decoder update.(\ud835\udc56\ud835\udc63)\ud835\udc56\ud835\udc63(\\textup{\\it iv})( iv ) AE-FB\u00a0Li et\u00a0al. (2019). Training AE, and then VAE using the KL thresholding in\u00a0(9), the results on \u03bb=0.50\ud835\udf060.50\\lambda\\!=\\!0.50italic_\u03bb = 0.50 are reported as a good trade-off.", "The results are shown in Table\u00a01. Various \u03bb\ud835\udf06\\lambdaitalic_\u03bb values are used, we observe a trade-off between language modeling and representation learning, controlled by \u03bb\ud835\udf06\\lambdaitalic_\u03bb. Compared with existing VAE methods, Optimus achieve significantly lower perplexity, and higher MI/AU. This indicates that our pre-training method is an effective approach to reduce KL vanishing issue and training VAEs, especially given the fact that we only fine-tune on these datasets for one epoch. Optimus achieves lower perplexity compared with GPT-2 on three out of four datasets. Intuitively, this is because the model can leverage the prior language knowledge encoded in \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z. This gap is larger, when the sentences in the dataset exhibit common regularities, such as \ud835\ude82\ud835\ude7d\ud835\ude7b\ud835\ude78\ud835\ude82\ud835\ude7d\ud835\ude7b\ud835\ude78\\mathtt{SNLI}typewriter_SNLI, where the prior plays a more important/effective role in this scenario.Though the form of our model is simple, Optimus shows stronger empirical performance than sophisticated models that are particularly designed for long-text, such as hVAE in\u00a0Shen et\u00a0al. (2019). For example, the KL and PPL ofOptimus (15.09 and 22.79) are much better than hVAE (6.8 and 45.8) on Yelp dataset. This verifies the importance of pre-training a latent space.The full experimental results are shown in Table\u00a08, 9, 10 and 11 of Appendix.", "Different from the traditional NLMs or GPT-2, VAEs learns bidirectional mappings between the latent and symbolic space. It enables high-level sentence editing as arithmetic latent vector operations, and thus allows guided language generation.The reason that Optimus supports arithmetic operations are two-fold: (1) Pre-training on large datasetswith large networks allows all sentences to be densely and faithfully represented in the latentspace. (2) The continuity property of neural nets and KL regularization of VAE encourage latentvectors with similar semantics are smoothly organized together.", "This is demonstrated with two simple schemes to manipulate pre-trained latent spaces: sentence transfer and interpolation, with results in Table\u00a02 and Table\u00a03, respectively.Details and more results are shown in Appendix.They showcase that Optimus enables new ways that one can play with language generation using pre-trained models, compared with GPT-2 that can only fulfill text sequences with given prompts. A website demo444http://aka.ms/optimus is released to the public to interact with the model, exhibiting the power of latent-vector-based controllable text generation.We demonstrate more sophisticated ways to manipulate pre-trained latent spaces in three real applications as follows.", "The open-domain dialog response generation task is considered: generating responses \ud835\udc99\ud835\udc99\\boldsymbol{x}bold_italic_x given a dialog history \ud835\udc84\ud835\udc84{\\boldsymbol{c}}bold_italic_c. Following\u00a0Gao et\u00a0al. (2019a), we embed the history and response in a joint latent space as \ud835\udc9bS2Ssubscript\ud835\udc9bS2S\\boldsymbol{z}_{\\text{S2S}}bold_italic_z start_POSTSUBSCRIPT S2S end_POSTSUBSCRIPT and \ud835\udc9bAEsubscript\ud835\udc9bAE\\boldsymbol{z}_{\\text{AE}}bold_italic_z start_POSTSUBSCRIPT AE end_POSTSUBSCRIPT, respectively. A fusion regularization is used to match the responses to the context.We consider \ud835\ude73\ud835\ude8a\ud835\ude92\ud835\ude95\ud835\udea2\ud835\ude8d\ud835\ude92\ud835\ude8a\ud835\ude95\ud835\ude98\ud835\ude90\ud835\ude73\ud835\ude8a\ud835\ude92\ud835\ude95\ud835\udea2\ud835\ude8d\ud835\ude92\ud835\ude8a\ud835\ude95\ud835\ude98\ud835\ude90\\mathtt{Dailydialog}typewriter_Dailydialog Li et\u00a0al. (2017c) used in\u00a0Gu et\u00a0al. (2019), which has 13,118 daily conversations. Each utterance is processed as the response of previous 10 context utterances from both speakers. The baseline methods are described in Appendix. We measure the performance using Bleu\u00a0Chen and Cherry (2014), and compute the precision, recall and F1 in Table\u00a04. Optimus shows higher Bleu scores than all existing baselines.", "Following StyleFusion\u00a0Gao et\u00a0al. (2019b), we consider generating responses for \ud835\ude73\ud835\ude8a\ud835\ude92\ud835\ude95\ud835\udea2\ud835\ude8d\ud835\ude92\ud835\ude8a\ud835\ude95\ud835\ude98\ud835\ude90\ud835\ude73\ud835\ude8a\ud835\ude92\ud835\ude95\ud835\udea2\ud835\ude8d\ud835\ude92\ud835\ude8a\ud835\ude95\ud835\ude98\ud835\ude90\\mathtt{Dailydialog}typewriter_Dailydialog in the style of Holmes. The comparison is shown in Table\u00a05. In addition to Bleu, we use neural and N-gram classifier scores to evaluate the accuracy of the generated responses that belong to the desired style. Optimus achieves better performance on all metrics.", "The short \ud835\ude88\ud835\ude8e\ud835\ude95\ud835\ude99\ud835\ude88\ud835\ude8e\ud835\ude95\ud835\ude99\\mathtt{Yelp}typewriter_Yelp dataset collected in\u00a0Shen et\u00a0al. (2017) is used. It contains 444K training sentences, and we use separated datasets of 10K sentences for validation/testing, respectively. The goal is to generate text reviews given the positive/negative sentiment. We fine-tune Optimus using the VAE objective on the dataset, then freeze backbone weights. A conditional GAN\u00a0Mirza and Osindero (2014) is trained on the fixed latent space. The generation process is to first produce a latent vector \ud835\udc9bysubscript\ud835\udc9b\ud835\udc66\\boldsymbol{z}_{y}bold_italic_z start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT based on a given label y\ud835\udc66yitalic_y using conditional GAN, then generate sentences conditioned on \ud835\udc9bysubscript\ud835\udc9b\ud835\udc66\\boldsymbol{z}_{y}bold_italic_z start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT using the decoder.The baselines are described in Appendix. G-score computes the geometric mean of Accuracy and Bleu, measuring the comprehensive quality of both content and style.Self-Bleu measures the diversity of the generated sentences. The results are shown in Table\u00a06,\u00a0Optimus achieves the best performance on all metrics.This verifies the importance of learning a smooth and meaningful latent space.The conditional generated sentences are shown in Appendix.", "Due to the regularization term \u2112Rsubscript\u2112\ud835\udc45\\mathcal{L}_{R}caligraphic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT, Optimus can organize sentences in the way specified by the prior distribution. For basic VAEs, a smooth feature space is learned, which is specifically beneficial for better generalization when the number of task-specific labeled data is low.To have a fair comparison, we follow the BERT paper, where the hidden feature of [CLS] is used as thesentence-level representation. In this way, the linear classifiers for both models have the same number of trainableparameters. Though the latent vector \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z is typically used as sentence-level representation in VAE literature, we argue that the KL regularization applied on \ud835\udc9b\ud835\udc9b\\boldsymbol{z}bold_italic_z has a large impact on the preceding layer feature \ud835\udc89[CLS]subscript\ud835\udc89[CLS]{\\boldsymbol{h}}_{\\texttt{[CLS]}}bold_italic_h start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT.Specifically, \ud835\udc89[CLS]subscript\ud835\udc89[CLS]{\\boldsymbol{h}}_{\\texttt{[CLS]}}bold_italic_h start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT is fed into an linear classifier \ud835\udc16C\u2208\u211dK\u00d7Hsubscript\ud835\udc16Csuperscript\u211d\ud835\udc3e\ud835\udc3b{{\\bf W}}_{\\text{C}}\\in\\mathbb{R}^{K\\times H}bold_W start_POSTSUBSCRIPT C end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_H end_POSTSUPERSCRIPT, where K\ud835\udc3eKitalic_K is the number of classes, with objective \u2212log\u2061(softmax(\ud835\udc89[CLS]\ud835\udc16C\u22a4))softmaxsubscript\ud835\udc89[CLS]superscriptsubscript\ud835\udc16Ctop-\\log(\\text{softmax}({\\boldsymbol{h}}_{\\texttt{[CLS]}}{{\\bf W}}_{\\text{C}}^{\\top}))- roman_log ( softmax ( bold_italic_h start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT bold_W start_POSTSUBSCRIPT C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT ) ). Two schemes are used:(\ud835\udc56)\ud835\udc56(\\textup{\\it i})( i ) Fine-tuning, where both the pre-trained model and the classifier are updated;(\ud835\udc56\ud835\udc56)\ud835\udc56\ud835\udc56(\\textup{\\it ii})( ii ) Feature-based, where pre-trained model weights are frozen to provide embeddings for the classifier update.", "A varying number of training samples are randomly chosen, ranging from 1 to 10K per class. 10 trials are used when the number of available training samples are small, each is trained in 100 training epochs.The results are shown in Figure\u00a03.When pre-trained models are used to provide sentence embeddings, the proposed Optimus consistently outperforms BERT. It demonstrates that the latent structure learned by Optimus is more separated, and helps generalize better. When the entire network is fine-tuned, Optimus can adapt faster than BERT, when the available number of training samples is small. The two methods perform quite similarly when more training data is provided. This is because the pre-trained backbone network size is much larger than the classifier, where the performance is dominated by the backbone networks.", "We use tSNE\u00a0Maaten and Hinton (2008) to visualize the learned feature on a 2D map. The validation set of Yelp is used to extract the latent features.Compared with BERT, Optimus learns a smoother space and more structured latent patterns, which explains why Optimus can yield better classification performance and faster adaptation.", "We further consider the GLUE benchmark\u00a0Wang et\u00a0al. (2019), which consists of nine datasets for general language understanding.Following the finetuning schedule in\u00a0Devlin et\u00a0al. (2019), we use learning rate [2,3,4,5]\u00d710\u221252345superscript105[2,3,4,5]\\times 10^{-5}[ 2 , 3 , 4 , 5 ] \u00d7 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT and train the model for 3 epochs. We select the best performance among different runs. We show the results on the validation set in Table\u00a07.With the feature-based scheme, Optimus yields higher performance than BERT, especially on the large datasets such as MNLI, QQP and QNLI. When the full models are fine-tuned, the two methods perform quite similarly.", "In summary, the scenarios that Optimus fit the low-resource settings are two-fold: (1) The required computing resource is low: the feature-based approach only updates the classifier, whosecomputing requirement is much lower than full-model fine-tuning; (2) The number of required labelled data is low: when labelled data is rare, Optimus adapts better.The results confirm that Optimus can maintain and exploit the structures learned in pre-training, and presents a more general representation that can be adapted to new tasks more easily than BERT \u2013 feature-based adaption is much faster and easier to perform than fine-tuning.", "We present Optimus, a large-scale pre-trained deep latent variable model for natural language. It introduces a smooth and universal latent space, by combining the advantages of VAEs, BERT and GPT-2 in one model. Experimental results on a wide range of tasks and datasets have demonstrated the strong performance of Optimus, including new state-of-the-art for language VAEs.", "There are several limitations in current Optimus. First, our pre-trained language VAE is still under-trained due to limited compute resource, as the training reconstruction loss can still decrease. One may further train the models with higher latent dimension and longer time to fully release the power of pre-trained latent spaces. Second, the current model can only control sentences of moderate length. One future direction is to consider more sophisticated mechanisms to gain stronger control-ability over longer sentences while maintaining the compactness of latent representations.", "While deep generative models (DGMs) such as VAEs are theoretically attractive due to its principle nature, it is now rarely used by practitioners in the modern pre-trained language modeling era where BERT/GPT dominate with strong empirical performance. That\u2019s why this paper makes a timely contribution to making DGMs practical for NLP. We hope that this paper will help renew interest in DGMs for this purpose.Hence, we deliberately keep a simple model, believing that the first pre-trained big VAE model itself and its implications are novel: it helps the community to recognize the importance of DGMs in the pre-training era, and revisit DGMs to make it more practical.Indeed, Optimus is uniquely positioned to learn a smooth latent space to organize sentences, which can enable guided language generation compared with GPT-2, and yield better generalization in low-resource language understanding tasks than BERT."], "figure_types": {"00696ba295d66f049d70272219f7fea4266171be/13-Figure5-1.png": "plot", "00696ba295d66f049d70272219f7fea4266171be/13-Figure6-1.png": "plot", "00696ba295d66f049d70272219f7fea4266171be/14-Table8-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/14-Table9-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/15-Table10-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/15-Table11-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/17-Table12-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/17-Table14-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/18-Table15-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/18-Table16-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/18-Table17-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/19-Table18-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/19-Table19-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/20-Table20-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/20-Table21-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/4-Figure1-1.png": "schematic", "00696ba295d66f049d70272219f7fea4266171be/4-Figure2-1.png": "schematic", "00696ba295d66f049d70272219f7fea4266171be/6-Table1-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/7-Table2-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/7-Table3-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/8-Figure3-1.png": "plot", "00696ba295d66f049d70272219f7fea4266171be/8-Figure4-1.png": "plot", "00696ba295d66f049d70272219f7fea4266171be/8-Table4-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/8-Table6-1.png": "table", "00696ba295d66f049d70272219f7fea4266171be/9-Table7-1.png": "table"}}, "1706.03762": {"paper_id": "paper_41", "title": "Attention Is All You Need", "arxiv_url": "https://arxiv.org/abs/1706.03762", "s2orc_url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "all_figures_tables": {"204e3073870fae3d05bcbc2f6a8e263d9b72e776/13-Figure3-1.png": "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for the word \u2018making\u2019. Different colors represent different heads. Best viewed in color.", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/14-Figure4-1.png": "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5 and 6. Note that the attentions are very sharp for this word.", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/15-Figure5-1.png": "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/3-Figure1-1.png": "Figure 1: The Transformer - model architecture.", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/4-Figure2-1.png": "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/6-Table1-1.png": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/8-Table2-1.png": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/9-Table3-1.png": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/9-Table4-1.png": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)"}, "referred_figures_tables": [["204e3073870fae3d05bcbc2f6a8e263d9b72e776/6-Table1-1.png"], ["204e3073870fae3d05bcbc2f6a8e263d9b72e776/6-Table1-1.png"], ["204e3073870fae3d05bcbc2f6a8e263d9b72e776/6-Table1-1.png"]], "question_id": [17, 18, 19], "question": ["Can't we use parallelization with RNN layers approach with any possible way?", "How could restricting self attention to some window with size r be useful with long term dependencies?", "How would be the results and performance considering accuracy and losses while using window-with-size r self-attention approach with shorter sequences?"], "question_section": ["Why Self-Attention", "Why Self-Attention", "Why Self-Attention"], "question_trigger_sentence": ["One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.", " To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position.", "This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work."], "question_type": ["Shallow question", "Shallow question", "Deep/complex question"], "evidential_info": [[{"context": "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h_{t}, as a function of the previous hidden state h_{t-1} and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved significant improvements in computational efficiency through factorization tricks (Kuchaiev2017Factorization, ) and conditional computation (shazeer2017outrageously, ), while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.", "rationale": "Recurrent models generate a sequence of hidden states h_{t}, as a function of the previous hidden state h_{t-1} and the input for position t. This inherently sequential nature prevent parallelization within training examples, which becomes critical at longer sequence lengths and also result in memory constraints."}, {"context": "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece (wu2016google, ) and byte-pair (sennrich2015neural, ) representations.To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.", "rationale": "In contrast to RNN for the translation tasks, the Transformer model can be trained significantly faster than architectures based on RNN layers."}, {"context": "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. ", "rationale": "Self-attention layer in transformers connects all positions with a constant number of sequentially executed operations, whereas RNN layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than RNN layers."}], [{"context": "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece (wu2016google, ) and byte-pair (sennrich2015neural, ) representations.To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.", "rationale": "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. Authors plan to investigate this approach further in future work."}], [{"context": "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece (wu2016google, ) and byte-pair (sennrich2015neural, ) representations.To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.", "rationale": "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence."}]], "composition": ["Because hidden state of each input position depends on previous hidden state therefore RNN can not be parallelized. Whereas Transformer due to attention layers are highly parallel.", "Restricting self attention to some window with size r does improve computational performance but its effect on long term dependencies have not been explored in the paper.", "Window-with-size r self-attention approach is only recommended to only improve computational performance for tasks involving very long sequences."], "Is_figure_in_evidence": [false, false, false], "Is_table_in_evidence": [true, true, true], "question_key": ["900", "901", "902"], "passages": ["Recurrent neural networks, long short-term memory (hochreiter1997, ) and gated recurrent (gruEval14, ) neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation (sutskever14, ; bahdanau2014neural, ; cho2014learning, ). Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures (wu2016google, ; luong2015effective, ; jozefowicz2016exploring, ).", "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states htsubscript\u210e\ud835\udc61h_{t}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, as a function of the previous hidden state ht\u22121subscript\u210e\ud835\udc611h_{t-1}italic_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT and the input for position t\ud835\udc61titalic_t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved significant improvements in computational efficiency through factorization tricks (Kuchaiev2017Factorization, ) and conditional computation (shazeer2017outrageously, ), while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.", "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences (bahdanau2014neural, ; structuredAttentionNetworks, ). In all but a few cases (decomposableAttnModel, ), however, such attention mechanisms are used in conjunction with a recurrent network.", "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.", "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU (extendedngpu, ), ByteNet (NalBytenet2017, ) and ConvS2S (JonasFaceNet2017, ), all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions (hochreiter2001gradient, ). In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section\u00a03.2.", "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations (cheng2016long, ; decomposableAttnModel, ; paulus2017deep, ; lin2017structured, ).", "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks (sukhbaatar2015, ).", "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as (neural_gpu, ; NalBytenet2017, ) and (JonasFaceNet2017, ).", "Most competitive neural sequence transduction models have an encoder-decoder structure (cho2014learning, ; bahdanau2014neural, ; sutskever14, ). Here, the encoder maps an input sequence of symbol representations (x1,\u2026,xn)subscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc5b(x_{1},...,x_{n})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) to a sequence of continuous representations \ud835\udc33=(z1,\u2026,zn)\ud835\udc33subscript\ud835\udc671\u2026subscript\ud835\udc67\ud835\udc5b\\mathbf{z}=(z_{1},...,z_{n})bold_z = ( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ). Given \ud835\udc33\ud835\udc33\\mathbf{z}bold_z, the decoder then generates an output sequence (y1,\u2026,ym)subscript\ud835\udc661\u2026subscript\ud835\udc66\ud835\udc5a(y_{1},...,y_{m})( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) of symbols one element at a time. At each step the model is auto-regressive (graves2013generating, ), consuming the previously generated symbols as additional input when generating the next.", "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure\u00a01, respectively.", "The encoder is composed of a stack of N=6\ud835\udc416N=6italic_N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection (he2016deep, ) around each of the two sub-layers, followed by layer normalization layernorm2016 . That is, the output of each sub-layer is LayerNorm(x+Sublayer(x))LayerNorm\ud835\udc65Sublayer\ud835\udc65\\mathrm{LayerNorm}(x+\\mathrm{Sublayer}(x))roman_LayerNorm ( italic_x + roman_Sublayer ( italic_x ) ), where Sublayer(x)Sublayer\ud835\udc65\\mathrm{Sublayer}(x)roman_Sublayer ( italic_x ) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel=512subscript\ud835\udc51model512d_{\\text{model}}=512italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT = 512.", "The decoder is also composed of a stack of N=6\ud835\udc416N=6italic_N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i\ud835\udc56iitalic_i can depend only on the known outputs at positions less than i\ud835\udc56iitalic_i.", "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.", "We call our particular attention \"Scaled Dot-Product Attention\" (Figure\u00a02). The input consists of queries and keys of dimension dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, and values of dimension dvsubscript\ud835\udc51\ud835\udc63d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. We compute the dot products of the query with all keys, divide each by dksubscript\ud835\udc51\ud835\udc58\\sqrt{d_{k}}square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG, and apply a softmax function to obtain the weights on the values.", "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q\ud835\udc44Qitalic_Q. The keys and values are also packed together into matrices K\ud835\udc3eKitalic_K and V\ud835\udc49Vitalic_V. We compute the matrix of outputs as:", "Attention(Q,K,V)=softmax(QKTdk)VAttention\ud835\udc44\ud835\udc3e\ud835\udc49softmax\ud835\udc44superscript\ud835\udc3e\ud835\udc47subscript\ud835\udc51\ud835\udc58\ud835\udc49\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})Vroman_Attention ( italic_Q , italic_K , italic_V ) = roman_softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) italic_V(1)", "The two most commonly used attention functions are additive attention (bahdanau2014neural, ), and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1dk1subscript\ud835\udc51\ud835\udc58\\frac{1}{\\sqrt{d_{k}}}divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.", "While for small values of dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (DBLP:journals/corr/BritzGLL17, ). We suspect that for large values of dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 111To illustrate why the dot products get large, assume that the components of q\ud835\udc5eqitalic_q and k\ud835\udc58kitalic_k are independent random variables with mean 00 and variance 1111. Then their dot product, q\u22c5k=\u2211i=1dkqiki\u22c5\ud835\udc5e\ud835\udc58superscriptsubscript\ud835\udc561subscript\ud835\udc51\ud835\udc58subscript\ud835\udc5e\ud835\udc56subscript\ud835\udc58\ud835\udc56q\\cdot k=\\sum_{i=1}^{d_{k}}q_{i}k_{i}italic_q \u22c5 italic_k = \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, has mean 00 and variance dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT.. To counteract this effect, we scale the dot products by 1dk1subscript\ud835\udc51\ud835\udc58\\frac{1}{\\sqrt{d_{k}}}divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG.", "Instead of performing a single attention function with dmodelsubscript\ud835\udc51modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h\u210ehitalic_h times with different, learned linear projections to dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and dvsubscript\ud835\udc51\ud835\udc63d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT dimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dvsubscript\ud835\udc51\ud835\udc63d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure\u00a02.", "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.", "MultiHead(Q,K,V)MultiHead\ud835\udc44\ud835\udc3e\ud835\udc49\\displaystyle\\mathrm{MultiHead}(Q,K,V)roman_MultiHead ( italic_Q , italic_K , italic_V )=Concat(head1,\u2026,headh)WOabsentConcatsubscripthead1\u2026subscriptheadhsuperscript\ud835\udc4a\ud835\udc42\\displaystyle=\\mathrm{Concat}(\\mathrm{head_{1}},...,\\mathrm{head_{h}})W^{O}= roman_Concat ( roman_head start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , roman_head start_POSTSUBSCRIPT roman_h end_POSTSUBSCRIPT ) italic_W start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPTwhereheadiwheresubscriptheadi\\displaystyle\\text{where}~{}\\mathrm{head_{i}}where roman_head start_POSTSUBSCRIPT roman_i end_POSTSUBSCRIPT=Attention(QWiQ,KWiK,VWiV)absentAttention\ud835\udc44subscriptsuperscript\ud835\udc4a\ud835\udc44\ud835\udc56\ud835\udc3esubscriptsuperscript\ud835\udc4a\ud835\udc3e\ud835\udc56\ud835\udc49subscriptsuperscript\ud835\udc4a\ud835\udc49\ud835\udc56\\displaystyle=\\mathrm{Attention}(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})= roman_Attention ( italic_Q italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_K italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_V italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )", "Where the projections are parameter matrices WiQ\u2208\u211ddmodel\u00d7dksubscriptsuperscript\ud835\udc4a\ud835\udc44\ud835\udc56superscript\u211dsubscript\ud835\udc51modelsubscript\ud835\udc51\ud835\udc58W^{Q}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, WiK\u2208\u211ddmodel\u00d7dksubscriptsuperscript\ud835\udc4a\ud835\udc3e\ud835\udc56superscript\u211dsubscript\ud835\udc51modelsubscript\ud835\udc51\ud835\udc58W^{K}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, WiV\u2208\u211ddmodel\u00d7dvsubscriptsuperscript\ud835\udc4a\ud835\udc49\ud835\udc56superscript\u211dsubscript\ud835\udc51modelsubscript\ud835\udc51\ud835\udc63W^{V}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{v}}italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and WO\u2208\u211dhdv\u00d7dmodelsuperscript\ud835\udc4a\ud835\udc42superscript\u211d\u210esubscript\ud835\udc51\ud835\udc63subscript\ud835\udc51modelW^{O}\\in\\mathbb{R}^{hd_{v}\\times d_{\\text{model}}}italic_W start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_h italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT.", "In this work we employ h=8\u210e8h=8italic_h = 8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h=64subscript\ud835\udc51\ud835\udc58subscript\ud835\udc51\ud835\udc63subscript\ud835\udc51model\u210e64d_{k}=d_{v}=d_{\\text{model}}/h=64italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT / italic_h = 64.Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.", "The Transformer uses multi-head attention in three different ways:\u2022In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as (wu2016google, ; bahdanau2014neural, ; JonasFaceNet2017, ).\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to \u2212\u221e-\\infty- \u221e) all values in the input of the softmax which correspond to illegal connections. See Figure\u00a02.", "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.", "FFN(x)=max\u2061(0,xW1+b1)W2+b2FFN\ud835\udc650\ud835\udc65subscript\ud835\udc4a1subscript\ud835\udc4f1subscript\ud835\udc4a2subscript\ud835\udc4f2\\mathrm{FFN}(x)=\\max(0,xW_{1}+b_{1})W_{2}+b_{2}roman_FFN ( italic_x ) = roman_max ( 0 , italic_x italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT(2)", "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel=512subscript\ud835\udc51model512d_{\\text{model}}=512italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT = 512, and the inner-layer has dimensionality dff=2048subscript\ud835\udc51\ud835\udc53\ud835\udc532048d_{ff}=2048italic_d start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT = 2048.", "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodelsubscript\ud835\udc51modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to (press2016using, ). In the embedding layers, we multiply those weights by dmodelsubscript\ud835\udc51model\\sqrt{d_{\\text{model}}}square-root start_ARG italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_ARG.", "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodelsubscript\ud835\udc51modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed (JonasFaceNet2017, ).", "In this work, we use sine and cosine functions of different frequencies:", "PE(pos,2i)=sin(pos/100002i/dmodel)\ud835\udc43subscript\ud835\udc38\ud835\udc5d\ud835\udc5c\ud835\udc602\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc5c\ud835\udc60superscript100002\ud835\udc56subscript\ud835\udc51model\\displaystyle PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\\text{model}}})italic_P italic_E start_POSTSUBSCRIPT ( italic_p italic_o italic_s , 2 italic_i ) end_POSTSUBSCRIPT = italic_s italic_i italic_n ( italic_p italic_o italic_s / 10000 start_POSTSUPERSCRIPT 2 italic_i / italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT )PE(pos,2i+1)=cos(pos/100002i/dmodel)\ud835\udc43subscript\ud835\udc38\ud835\udc5d\ud835\udc5c\ud835\udc602\ud835\udc561\ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc5d\ud835\udc5c\ud835\udc60superscript100002\ud835\udc56subscript\ud835\udc51model\\displaystyle PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\\text{model}}})italic_P italic_E start_POSTSUBSCRIPT ( italic_p italic_o italic_s , 2 italic_i + 1 ) end_POSTSUBSCRIPT = italic_c italic_o italic_s ( italic_p italic_o italic_s / 10000 start_POSTSUPERSCRIPT 2 italic_i / italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT )", "where pos\ud835\udc5d\ud835\udc5c\ud835\udc60positalic_p italic_o italic_s is the position and i\ud835\udc56iitalic_i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c02\ud835\udf0b2\\pi2 italic_\u03c0 to 10000\u22c52\u03c0\u22c5100002\ud835\udf0b10000\\cdot 2\\pi10000 \u22c5 2 italic_\u03c0. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k\ud835\udc58kitalic_k, PEpos+k\ud835\udc43subscript\ud835\udc38\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc58PE_{pos+k}italic_P italic_E start_POSTSUBSCRIPT italic_p italic_o italic_s + italic_k end_POSTSUBSCRIPT can be represented as a linear function of PEpos\ud835\udc43subscript\ud835\udc38\ud835\udc5d\ud835\udc5c\ud835\udc60PE_{pos}italic_P italic_E start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPT.", "We also experimented with using learned positional embeddings (JonasFaceNet2017, ) instead, and found that the two versions produced nearly identical results (see Table\u00a03 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.", "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1,\u2026,xn)subscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc5b(x_{1},...,x_{n})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) to another sequence of equal length (z1,\u2026,zn)subscript\ud835\udc671\u2026subscript\ud835\udc67\ud835\udc5b(z_{1},...,z_{n})( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ), with xi,zi\u2208\u211ddsubscript\ud835\udc65\ud835\udc56subscript\ud835\udc67\ud835\udc56superscript\u211d\ud835\udc51x_{i},z_{i}\\in\\mathbb{R}^{d}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.", "One is the total computational complexity per layer.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.", "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies (hochreiter2001gradient, ). Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.", "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)\ud835\udc42\ud835\udc5bO(n)italic_O ( italic_n ) sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n\ud835\udc5bnitalic_n is smaller than the representation dimensionality d\ud835\udc51ditalic_d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece (wu2016google, ) and byte-pair (sennrich2015neural, ) representations.To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r\ud835\udc5fritalic_r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r)\ud835\udc42\ud835\udc5b\ud835\udc5fO(n/r)italic_O ( italic_n / italic_r ). We plan to investigate this approach further in future work.", "A single convolutional layer with kernel width k<n\ud835\udc58\ud835\udc5bk<nitalic_k < italic_n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k)\ud835\udc42\ud835\udc5b\ud835\udc58O(n/k)italic_O ( italic_n / italic_k ) convolutional layers in the case of contiguous kernels, or O(logk(n))\ud835\udc42\ud835\udc59\ud835\udc5csubscript\ud835\udc54\ud835\udc58\ud835\udc5bO(log_{k}(n))italic_O ( italic_l italic_o italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_n ) ) in the case of dilated convolutions (NalBytenet2017, ), increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k\ud835\udc58kitalic_k. Separable convolutions (xception2016, ), however, decrease the complexity considerably, to O(k\u22c5n\u22c5d+n\u22c5d2)\ud835\udc42\u22c5\ud835\udc58\ud835\udc5b\ud835\udc51\u22c5\ud835\udc5bsuperscript\ud835\udc512O(k\\cdot n\\cdot d+n\\cdot d^{2})italic_O ( italic_k \u22c5 italic_n \u22c5 italic_d + italic_n \u22c5 italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Even with k=n\ud835\udc58\ud835\udc5bk=nitalic_k = italic_n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.", "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.", "This section describes the training regime for our models.", "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding (DBLP:journals/corr/BritzGLL17, ), which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary (wu2016google, ). Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.", "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).", "We used the Adam optimizer\u00a0(kingma2014adam, ) with \u03b21=0.9subscript\ud835\udefd10.9\\beta_{1}=0.9italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, \u03b22=0.98subscript\ud835\udefd20.98\\beta_{2}=0.98italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.98 and \u03f5=10\u22129italic-\u03f5superscript109\\epsilon=10^{-9}italic_\u03f5 = 10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPT. We varied the learning rate over the course of training, according to the formula:", "lrate=dmodel\u22120.5\u22c5min\u2061(step_num\u22120.5,step_num\u22c5warmup_steps\u22121.5)\ud835\udc59\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52\u22c5superscriptsubscript\ud835\udc51model0.5\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5d_\ud835\udc5b\ud835\udc62superscript\ud835\udc5a0.5\u22c5\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5d_\ud835\udc5b\ud835\udc62\ud835\udc5a\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc5a\ud835\udc62\ud835\udc5d_\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5dsuperscript\ud835\udc601.5lrate=d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})italic_l italic_r italic_a italic_t italic_e = italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 0.5 end_POSTSUPERSCRIPT \u22c5 roman_min ( italic_s italic_t italic_e italic_p _ italic_n italic_u italic_m start_POSTSUPERSCRIPT - 0.5 end_POSTSUPERSCRIPT , italic_s italic_t italic_e italic_p _ italic_n italic_u italic_m \u22c5 italic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s start_POSTSUPERSCRIPT - 1.5 end_POSTSUPERSCRIPT )(3)", "This corresponds to increasing the learning rate linearly for the first warmup_steps\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc5a\ud835\udc62\ud835\udc5d_\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5d\ud835\udc60warmup\\_stepsitalic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps=4000\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc5a\ud835\udc62\ud835\udc5d_\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5d\ud835\udc604000warmup\\_steps=4000italic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s = 4000.", "We employ three types of regularization during training:", "We apply dropout (srivastava2014dropout, ) to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop=0.1subscript\ud835\udc43\ud835\udc51\ud835\udc5f\ud835\udc5c\ud835\udc5d0.1P_{drop}=0.1italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT = 0.1.", "During training, we employed label smoothing of value \u03f5ls=0.1subscriptitalic-\u03f5\ud835\udc59\ud835\udc600.1\\epsilon_{ls}=0.1italic_\u03f5 start_POSTSUBSCRIPT italic_l italic_s end_POSTSUBSCRIPT = 0.1 (DBLP:journals/corr/SzegedyVISW15, ). This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.", "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table\u00a02) outperforms the best previously reported models (including ensembles) by more than 2.02.02.02.0 BLEU, establishing a new state-of-the-art BLEU score of 28.428.428.428.4. The configuration of this model is listed in the bottom line of Table\u00a03. Training took 3.53.53.53.5 days on 8888 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.", "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.041.041.041.0, outperforming all of the previously published single models, at less than 1/4141/41 / 4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop=0.1subscript\ud835\udc43\ud835\udc51\ud835\udc5f\ud835\udc5c\ud835\udc5d0.1P_{drop}=0.1italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT = 0.1, instead of 0.30.30.30.3.", "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4444 and length penalty \u03b1=0.6\ud835\udefc0.6\\alpha=0.6italic_\u03b1 = 0.6 (wu2016google, ). These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50505050, but terminate early when possible (wu2016google, ).", "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 222We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively..", "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table\u00a03.", "In Table\u00a03 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.", "In Table\u00a03 rows (B), we observe that reducing the attention key size dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings (JonasFaceNet2017, ), and observe nearly identical results to the base model.", "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes KVparse15 .", "We trained a 4-layer transformer with dmodel=1024subscript\ud835\udc51\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc591024d_{model}=1024italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank (marcus1993building, ), about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences (KVparse15, ). We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.", "We performed only a small number of experiments to select the dropout, both attention and residual (section\u00a05.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300300300300. We used a beam size of 21212121 and \u03b1=0.3\ud835\udefc0.3\\alpha=0.3italic_\u03b1 = 0.3 for both WSJ only and the semi-supervised setting.", "Our results in Table\u00a04 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar dyer-rnng:16 .", "In contrast to RNN sequence-to-sequence models (KVparse15, ), the Transformer outperforms the BerkeleyParser petrov-EtAl:2006:ACL  even when training only on the WSJ training set of 40K sentences.", "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.", "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. ", "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video.Making generation less sequential is another research goals of ours.", "The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.", "We are grateful to Nal Kalchbrenner and Stephan Gouws fortheir fruitful comments, corrections and inspiration."], "figure_types": {"204e3073870fae3d05bcbc2f6a8e263d9b72e776/13-Figure3-1.png": "other", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/14-Figure4-1.png": "schematic", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/15-Figure5-1.png": "other", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/3-Figure1-1.png": "schematic", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/4-Figure2-1.png": "schematic", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/6-Table1-1.png": "table", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/8-Table2-1.png": "table", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/9-Table3-1.png": "table", "204e3073870fae3d05bcbc2f6a8e263d9b72e776/9-Table4-1.png": "table"}}, "1704.04861": {"paper_id": "paper_42", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications", "arxiv_url": "https://arxiv.org/abs/1704.04861", "s2orc_url": "https://www.semanticscholar.org/paper/3647d6d0f151dc05626449ee09cc7bce55be497e", "all_figures_tables": {"3647d6d0f151dc05626449ee09cc7bce55be497e/2-Figure1-1.png": "Figure 1. MobileNet models can be applied to various recognition tasks for efficient on device intelligence.", "3647d6d0f151dc05626449ee09cc7bce55be497e/3-Figure2-1.png": "Figure 2. The standard convolutional filters in (a) are replaced by two layers: depthwise convolution in (b) and pointwise convolution in (c) to build a depthwise separable filter.", "3647d6d0f151dc05626449ee09cc7bce55be497e/4-Figure3-1.png": "Figure 3. Left: Standard convolutional layer with batchnorm and ReLU. Right: Depthwise Separable convolutions with Depthwise and Pointwise layers followed by batchnorm and ReLU.", "3647d6d0f151dc05626449ee09cc7bce55be497e/4-Table1-1.png": "Table 1. MobileNet Body Architecture", "3647d6d0f151dc05626449ee09cc7bce55be497e/4-Table2-1.png": "Table 2. Resource Per Layer Type", "3647d6d0f151dc05626449ee09cc7bce55be497e/5-Table3-1.png": "Table 3. Resource usage for modifications to standard convolution. Note that each row is a cumulative effect adding on top of the previous row. This example is for an internal MobileNet layer with DK = 3, M = 512, N = 512, DF = 14.", "3647d6d0f151dc05626449ee09cc7bce55be497e/5-Table6-1.png": "Table 6. MobileNet Width Multiplier", "3647d6d0f151dc05626449ee09cc7bce55be497e/6-Figure4-1.png": "Figure 4. This figure shows the trade off between computation (Mult-Adds) and accuracy on the ImageNet benchmark. Note the log linear dependence between accuracy and computation.", "3647d6d0f151dc05626449ee09cc7bce55be497e/6-Figure5-1.png": "Figure 5. This figure shows the trade off between the number of parameters and accuracy on the ImageNet benchmark. The colors encode input resolutions. The number of parameters do not vary based on the input resolution.", "3647d6d0f151dc05626449ee09cc7bce55be497e/6-Table10-1.png": "Table 10. MobileNet for Stanford Dogs", "3647d6d0f151dc05626449ee09cc7bce55be497e/6-Table11-1.png": "Table 11. Performance of PlaNet using the MobileNet architecture. Percentages are the fraction of the Im2GPS test dataset that were localized within a certain distance from the ground truth. The numbers for the original PlaNet model are based on an updated version that has an improved architecture and training dataset.", "3647d6d0f151dc05626449ee09cc7bce55be497e/6-Table8-1.png": "Table 8. MobileNet Comparison to Popular Models", "3647d6d0f151dc05626449ee09cc7bce55be497e/7-Figure6-1.png": "Figure 6. Example objection detection results using MobileNet SSD.", "3647d6d0f151dc05626449ee09cc7bce55be497e/7-Table12-1.png": "Table 12. Face attribute classification using the MobileNet architecture. Each row corresponds to a different hyper-parameter setting (width multiplier \u03b1 and image resolution).", "3647d6d0f151dc05626449ee09cc7bce55be497e/8-Table14-1.png": "Table 14. MobileNet Distilled from FaceNet"}, "referred_figures_tables": [["3647d6d0f151dc05626449ee09cc7bce55be497e/3-Figure2-1.png"], ["3647d6d0f151dc05626449ee09cc7bce55be497e/3-Figure2-1.png"], ["3647d6d0f151dc05626449ee09cc7bce55be497e/3-Figure2-1.png"], ["3647d6d0f151dc05626449ee09cc7bce55be497e/4-Figure3-1.png", "3647d6d0f151dc05626449ee09cc7bce55be497e/4-Table1-1.png"], ["3647d6d0f151dc05626449ee09cc7bce55be497e/4-Figure3-1.png", "3647d6d0f151dc05626449ee09cc7bce55be497e/4-Table1-1.png", "3647d6d0f151dc05626449ee09cc7bce55be497e/5-Table3-1.png"], ["3647d6d0f151dc05626449ee09cc7bce55be497e/4-Figure3-1.png", "3647d6d0f151dc05626449ee09cc7bce55be497e/4-Table1-1.png"], ["3647d6d0f151dc05626449ee09cc7bce55be497e/6-Table11-1.png"]], "question_id": [4, 5, 7, 8, 12, 13, 19], "question": ["What is a depthwise separable convolution means?", "Describe how mobile net use depthwise separable convolution to reduce computation and the model size", "What are the layers of depthwise separable convolution and discuss the function of each of them.", "What types of non-linearities is used for both layers of the depthwise separable convolution?", "Does all the layers of the MobileNet use depthwise separable convolution?", "How many layers does the MobileNet has?", "What characteristics did MobileNet showed better performance when compared to other models."], "question_section": ["MobileNet Architecture", "MobileNet Architecture", "MobileNet Architecture", "MobileNet Architecture", "MobileNet Architecture", "MobileNet Architecture", "Conclusion"], "question_trigger_sentence": ["The MobileNet model is based on depthwise separable convolutions", "This factorization has the effect of drastically reducing computation and model size", "Depthwise separable convolution are made up of two layers:", "nonlinearities for both layers", "The MobileNet structure is built on", "Counting depthwise and pointwise convolutions", "We then compared different MobileNets to popular models"], "question_type": ["Testing question ", "Deep/complex question", "Deep/complex question", "Shallow question", "Shallow question", "Testing question ", "Shallow question"], "evidential_info": [[{"context": "The standard convolution operation has the effect of filtering features based on theconvolutional kernels and combining features in order to produce a new representation.The filtering and combination steps can be split into two steps via the use offactorized convolutions called depthwise separable convolutions for substantial reduction in computational cost.", "rationale": "The combination of depthwise convolution and 1\\times 1 (pointwise) convolution is called depthwise separable convolution which was originally introduced in [26]."}, {"context": "Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions.We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, asimple 1\\times 1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets use bothbatchnorm and ReLU nonlinearities for both layers.", "rationale": "Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions.We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, a simple 1\\times 1 convolution, is then used to create a linear combination of the output of the depthwise layer."}, {"context": "Depthwise convolution is extremely efficient relative to standard convolution. However it only filters input channels, it does not combine them to create new features. So an additional layer that computes a linear combination of the output of depthwise convolution via 1\\times 1 convolution is needed in order to generate these new features.", "rationale": "The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining."}, {"context": "The combination of depthwise convolution and 1\\times 1 (pointwise) convolution is called depthwise separable convolution which was originally introduced in [26].", "rationale": "Depthwise convolution is extremely efficient relative to standard convolution. However it only filters input channels, it does not combine them to create new features. So an additional layer that computes a linear combination of the output of depthwise convolution via 1\\times 1 convolution is needed in order to generate these new features."}, {"context": "The MobileNet model is based on depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1\\times 1 convolution called a pointwise convolution. For MobileNets the depthwise convolution applies a single filter to each input channel. The pointwise convolution then applies a 1\\times 1 convolution to combine the outputs the depthwise convolution. A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This factorization has the effect of drastically reducing computation and model size. Figure 2 shows how a standard convolution 2(a) is factorized into a depthwise convolution 2(b) and a 1\\times 1 pointwise convolution 2(c).", "rationale": "The filtering and combination steps can be split into two steps via the use offactorized convolutions called depthwise separable convolutions for substantial reduction in computational cost."}], [{"context": "The MobileNet model is based on depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1\\times 1 convolution called a pointwise convolution. For MobileNets the depthwise convolution applies a single filter to each input channel. The pointwise convolution then applies a 1\\times 1 convolution to combine the outputs the depthwise convolution. A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This factorization has the effect of drastically reducing computation and model size. Figure 2 shows how a standard convolution 2(a) is factorized into a depthwise convolution 2(b) and a 1\\times 1 pointwise convolution 2(c).", "rationale": "The MobileNet model is based on depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1\\times 1 convolution called a pointwise convolution. For MobileNets the depthwise convolution applies a single filter to each input channel. The pointwise convolution then applies a 1\\times 1 convolution to combine the outputs the depthwise convolution. A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This factorization has the effect of drastically reducing computation and model size."}], [{"context": "Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions.We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, asimple 1\\times 1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets use bothbatchnorm and ReLU nonlinearities for both layers.", "rationale": "Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions.We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, a simple 1\\times 1 convolution, is then used to create a linear combination of the output of the depthwise layer."}, {"context": "The MobileNet model is based on depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1\\times 1 convolution called a pointwise convolution. For MobileNets the depthwise convolution applies a single filter to each input channel. The pointwise convolution then applies a 1\\times 1 convolution to combine the outputs the depthwise convolution. A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This factorization has the effect of drastically reducing computation and model size. Figure 2 shows how a standard convolution 2(a) is factorized into a depthwise convolution 2(b) and a 1\\times 1 pointwise convolution 2(c).", "rationale": "The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This factorization has the effect of drastically reducing computation and model size."}], [{"context": "Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions.We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, asimple 1\\times 1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets use bothbatchnorm and ReLU nonlinearities for both layers.", "rationale": "MobileNets use bothbatchnorm and ReLU nonlinearities for both layers."}, {"context": "The MobileNet structure is built on depthwise separable convolutions as mentioned in the previous section except for the first layer which is a full convolution. By defining the network in such simple terms we are able to easily explore network topologies to find a good network. The MobileNet architecture is defined in Table 1. All layers are followed by a batchnorm [13] and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification. Figure 3 contrasts a layer with regular convolutions, batchnorm and ReLU nonlinearity to the factorized layer with depthwise convolution, 1\\times 1 pointwise convolution as well as batchnorm and ReLU after each convolutional layer. Down sampling is handled with strided convolution in the depthwise convolutions as well as in the first layer. A final average pooling reduces the spatial resolution to 1 before the fully connected layer. Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers.", "rationale": "Figure 3 contrasts a layer with regular convolutions, batchnorm and ReLU nonlinearity to the factorized layer with depthwise convolution, 1\\times 1 pointwise convolution as well as batchnorm and ReLU after each convolutional layer."}], [{"context": "The MobileNet structure is built on depthwise separable convolutions as mentioned in the previous section except for the first layer which is a full convolution. By defining the network in such simple terms we are able to easily explore network topologies to find a good network. The MobileNet architecture is defined in Table 1. All layers are followed by a batchnorm [13] and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification. Figure 3 contrasts a layer with regular convolutions, batchnorm and ReLU nonlinearity to the factorized layer with depthwise convolution, 1\\times 1 pointwise convolution as well as batchnorm and ReLU after each convolutional layer. Down sampling is handled with strided convolution in the depthwise convolutions as well as in the first layer. A final average pooling reduces the spatial resolution to 1 before the fully connected layer. Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers.", "rationale": "The MobileNet structure is built on depthwise separable convolutions as mentioned in the previous section except for the first layer which is a full convolution."}, {"context": "As an example we can look at a typical layer in MobileNet and see how depthwise separable convolutions, width multiplier and resolution multiplier reduce the cost and parameters. Table 3 shows the computation and number of parameters for a layer as architecture shrinking methods are sequentially applied to the layer. The first row shows the Mult-Adds and parameters for a full convolutional layer with an input feature map of size 14\\times 14\\times 512 with a kernel K of size 3\\times 3\\times 512\\times 512. We will look in detail in the next section at the trade offs between resources and accuracy.", "rationale": "The first row shows the Mult-Adds and parameters for a full convolutional layer with an input feature map of size 14\\times 14\\times 512 with a kernel K of size 3\\times 3\\times 512\\times 512."}], [{"context": "The MobileNet structure is built on depthwise separable convolutions as mentioned in the previous section except for the first layer which is a full convolution. By defining the network in such simple terms we are able to easily explore network topologies to find a good network. The MobileNet architecture is defined in Table 1. All layers are followed by a batchnorm [13] and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification. Figure 3 contrasts a layer with regular convolutions, batchnorm and ReLU nonlinearity to the factorized layer with depthwise convolution, 1\\times 1 pointwise convolution as well as batchnorm and ReLU after each convolutional layer. Down sampling is handled with strided convolution in the depthwise convolutions as well as in the first layer. A final average pooling reduces the spatial resolution to 1 before the fully connected layer. Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers.", "rationale": "Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers."}], [{"context": "We re-train PlaNet using the MobileNet architecture on the same data. While the full PlaNet model based on the Inception V3 architecture [31] has 52 million parameters and 5.74 billion mult-adds. The MobileNet model has only 13 million parameters with the usual 3 million for the body and 10 million for the final layer and 0.58 Million mult-adds. As shown in Tab.\u00a011, the MobileNet version delivers only slightly decreased performance compared to PlaNet despite being much more compact. Moreover, it still outperforms Im2GPS by a large margin.", "rationale": "We proposed a new model architecture called MobileNets based on depthwise separable convolutions. We investigated some of the important design decisions leading to an efficient model. We then demonstrated how to build smaller and faster MobileNets using width multiplier and resolution multiplier by trading off a reasonable amount of accuracy to reduce size and latency. We then compared different MobileNets to popular models demonstrating superior size, speed and accuracy characteristics."}, {"context": "MobileNet can also be deployed as an effective base network in modern object detection systems.We report results for MobileNet trained for object detection on COCO data based on the recent work that won the 2016 COCO challenge [10].In table 13, MobileNet is compared to VGG and Inception V2 [13] under both Faster-RCNN [23] and SSD [21] framework.In our experiments, SSD is evaluated with 300 input resolution (SSD 300) and Faster-RCNN is compared with both 300 and 600 input resolution (Faster-RCNN 300, Faster-RCNN 600).The Faster-RCNN model evaluates 300 RPN proposal boxes per image. The models are trained on COCO train+val excluding 8k minival images and evaluated on minival.For both frameworks, MobileNet achieves comparable results to other networks with only a fraction of computational complexity and model size.", "rationale": "MobileNet achieves comparable results to other networks with only a fraction of computational complexity and model size."}, {"context": "We proposed a new model architecture called MobileNets based on depthwise separable convolutions. We investigated some of the important design decisions leading to an efficient model. We then demonstrated how to build smaller and faster MobileNets using width multiplier and resolution multiplier by trading off a reasonable amount of accuracy to reduce size and latency. We then compared different MobileNets to popular models demonstrating superior size, speed and accuracy characteristics. We concluded by demonstrating MobileNet\u2019s effectiveness when applied to a wide variety of tasks. As a next step to help adoption and exploration of MobileNets, we plan on releasing models in Tensor Flow.", "rationale": "As shown in Tab. 11, the MobileNet version delivers only slightly decreased performance compared to PlaNet despite being much more compact."}]], "composition": ["Depthwise separable convolution is made up of two layers: depthwise convolutions and pointwise convolutions where depthwise convolutions apply a single filter per each input channel and a Pointwise convolution creates a linear combination of the output of the depthwise layer.", "MobileNets use depthwise convolution with one filter per input channel. The pointwise convolution then combines the depthwise convolution outputs with a 1\\times 1 convolution. This factorization greatly reduces computation and model size.", "Depthwise separable convolutions have two layers\u2014depthwise and pointwise. Depthwise convolutions apply one filter per input channel (input depth). The depthwise layer output is linearly combined using pointwise convolution which is a 1\\times 1 convolution.", "MobileNet layers use batchnorm and ReLU nonlinearities.", "The first layer of MobileNet is a full convolution, and the rest are depthwise separable convolutions.", "MobileNet has 28 layers.", "MobileNets showed better performance at reducing model size, computational complexity and latency while maintaining comparable accuracy when compared with the other models."], "Is_figure_in_evidence": [true, true, true, true, true, true, false], "Is_table_in_evidence": [false, false, false, false, true, false, true], "question_key": ["926", "927", "929", "930", "934", "935", "939"], "passages": ["Convolutional neural networks have become ubiquitous in computer vision ever since AlexNet [19] popularized deep convolutional neural networks by winning the ImageNet Challenge: ILSVRC 2012 [24]. The general trend has been to make deeper and more complicated networks in order to achieve higher accuracy [27, 31, 29, 8]. However, these advances to improve accuracy are not necessarily making networks more efficient with respect to size and speed. In many real world applications such as robotics, self-driving car and augmented reality, the recognition tasks need to be carried out in a timely fashion on a computationally limited platform.", "This paper describes an efficient network architecture and a set of two hyper-parameters in order to build very small, low latency models that can be easily matched to the design requirements for mobile and embedded vision applications. Section 2 reviews prior work in building small models. Section 3 describes the MobileNet architecture and two hyper-parameters width multiplier and resolution multiplier to define smaller and more efficient MobileNets. Section 4 describes experiments on ImageNet as well a variety of different applications and use cases. Section 5 closes with a summary and conclusion.", "There has been rising interest in building small and efficient neural networks in the recent literature, e.g. [16, 34, 12, 36, 22]. Many different approaches can be generally categorized into either compressing pretrained networks or training small networks directly. This paper proposes a class of network architectures that allows a model developer to specifically choose a small network that matches the resource restrictions (latency, size) for their application. MobileNets primarily focus on optimizing for latency but also yield small networks. Many papers on small networks focus only on size but do not consider speed.", "MobileNets are built primarily from depthwise separable convolutions initially introduced in [26] and subsequently used in Inception models [13] to reduce the computation in the first few layers. Flattened networks [16] build a network out of fully factorized convolutions and showed the potential of extremely factorized networks. Independent of this current paper, Factorized Networks[34] introduces a similar factorized convolution as well as the use of topological connections. Subsequently, the Xception network [3] demonstrated how to scale up depthwise separable filters to out perform Inception V3 networks. Another small network is Squeezenet [12] which uses a bottleneck approach to design a very small network. Other reduced computation networks include structured transform networks [28] and deep fried convnets [37].", "A different approach for obtaining small networks is shrinking, factorizing or compressing pretrained networks. Compression based on product quantization [36], hashing [2], and pruning, vector quantization and Huffman coding [5] have been proposed in the literature. Additionally various factorizations have been proposed to speed up pretrained networks [14, 20]. Another method for training small networks is distillation [9] which uses a larger network to teach a smaller network. It is complementary to our approach and is covered in some of our use cases in section 4. Another emerging approach is low bit networks [4, 22, 11].", "In this section we first describe the core layers that MobileNet is built on which are depthwise separable filters. We then describe the MobileNet network structure and conclude with descriptions of the two model shrinking hyper-parameters width multiplier and resolution multiplier.", "The MobileNet model is based on depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1\u00d71111\\times 11 \u00d7 1 convolution called a pointwise convolution. For MobileNets the depthwise convolution applies a single filter to each input channel. The pointwise convolution then applies a 1\u00d71111\\times 11 \u00d7 1 convolution to combine the outputs the depthwise convolution. A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This factorization has the effect of drastically reducing computation and model size. Figure 2 shows how a standard convolution 2(a) is factorized into a depthwise convolution 2(b) and a 1\u00d71111\\times 11 \u00d7 1 pointwise convolution 2(c).", "A standard convolutional layer takes as input a DF\u00d7DF\u00d7Msubscript\ud835\udc37\ud835\udc39subscript\ud835\udc37\ud835\udc39\ud835\udc40D_{F}\\times D_{F}\\times Mitalic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u00d7 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u00d7 italic_M feature map \ud835\udc05\ud835\udc05\\mathbf{F}bold_F and produces a DF\u00d7DF\u00d7Nsubscript\ud835\udc37\ud835\udc39subscript\ud835\udc37\ud835\udc39\ud835\udc41D_{F}\\times D_{F}\\times Nitalic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u00d7 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u00d7 italic_N feature map \ud835\udc06\ud835\udc06\\mathbf{G}bold_G where DFsubscript\ud835\udc37\ud835\udc39D_{F}italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT is the spatial width and height of a square input feature map111We assume that the output feature map has the same spatial dimensions as the input and both feature maps are square. Our model shrinking results generalize to feature maps with arbitrary sizes and aspect ratios., M\ud835\udc40Mitalic_M is the number of input channels (input depth), DGsubscript\ud835\udc37\ud835\udc3aD_{G}italic_D start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT is the spatial width and height of a square output feature map and N\ud835\udc41Nitalic_N is the number of output channel (output depth).", "The standard convolutional layer is parameterized by convolution kernel \ud835\udc0a\ud835\udc0a\\mathbf{K}bold_K of size DK\u00d7DK\u00d7M\u00d7Nsubscript\ud835\udc37\ud835\udc3esubscript\ud835\udc37\ud835\udc3e\ud835\udc40\ud835\udc41D_{K}\\times D_{K}\\times M\\times Nitalic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u00d7 italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u00d7 italic_M \u00d7 italic_N where DKsubscript\ud835\udc37\ud835\udc3eD_{K}italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT is the spatial dimension of the kernel assumed to be square and M\ud835\udc40Mitalic_M is number of input channels and N\ud835\udc41Nitalic_N is the number of output channels as defined previously.", "The output feature map for standard convolution assuming stride one and padding is computed as:", "\ud835\udc06k,l,n=\u2211i,j,m\ud835\udc0ai,j,m,n\u22c5\ud835\udc05k+i\u22121,l+j\u22121,msubscript\ud835\udc06\ud835\udc58\ud835\udc59\ud835\udc5bsubscript\ud835\udc56\ud835\udc57\ud835\udc5a\u22c5subscript\ud835\udc0a\ud835\udc56\ud835\udc57\ud835\udc5a\ud835\udc5bsubscript\ud835\udc05\ud835\udc58\ud835\udc561\ud835\udc59\ud835\udc571\ud835\udc5a\\mathbf{G}_{k,l,n}=\\sum_{i,j,m}\\mathbf{K}_{i,j,m,n}\\cdot\\mathbf{F}_{k+i-1,l+j-1,m}bold_G start_POSTSUBSCRIPT italic_k , italic_l , italic_n end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_i , italic_j , italic_m end_POSTSUBSCRIPT bold_K start_POSTSUBSCRIPT italic_i , italic_j , italic_m , italic_n end_POSTSUBSCRIPT \u22c5 bold_F start_POSTSUBSCRIPT italic_k + italic_i - 1 , italic_l + italic_j - 1 , italic_m end_POSTSUBSCRIPT(1)", "Standard convolutions have the computational cost of:", "DK\u22c5DK\u22c5M\u22c5N\u22c5DF\u22c5DF\u22c5subscript\ud835\udc37\ud835\udc3esubscript\ud835\udc37\ud835\udc3e\ud835\udc40\ud835\udc41subscript\ud835\udc37\ud835\udc39subscript\ud835\udc37\ud835\udc39D_{K}\\cdot D_{K}\\cdot M\\cdot N\\cdot D_{F}\\cdot D_{F}italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_M \u22c5 italic_N \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT(2)where the computational cost depends multiplicatively on the number of input channels M\ud835\udc40Mitalic_M, the number of output channels N\ud835\udc41Nitalic_N the kernel size Dk\u00d7Dksubscript\ud835\udc37\ud835\udc58subscript\ud835\udc37\ud835\udc58D_{k}\\times D_{k}italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u00d7 italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and the feature map size DF\u00d7DFsubscript\ud835\udc37\ud835\udc39subscript\ud835\udc37\ud835\udc39D_{F}\\times D_{F}italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u00d7 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT. MobileNet models address each of these terms and their interactions. First it uses depthwise separable convolutions to break the interaction between the number of output channels and the size of the kernel.", "The standard convolution operation has the effect of filtering features based on theconvolutional kernels and combining features in order to produce a new representation.The filtering and combination steps can be split into two steps via the use offactorized convolutions called depthwise separable convolutions for substantial reduction in computational cost.", "Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions.We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, asimple 1\u00d71111\\times 11 \u00d7 1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets use bothbatchnorm and ReLU nonlinearities for both layers.", "Depthwise convolution with one filter per input channel (input depth) can be written as:", "\ud835\udc06^k,l,m=\u2211i,j\ud835\udc0a^i,j,m\u22c5\ud835\udc05k+i\u22121,l+j\u22121,msubscript^\ud835\udc06\ud835\udc58\ud835\udc59\ud835\udc5asubscript\ud835\udc56\ud835\udc57\u22c5subscript^\ud835\udc0a\ud835\udc56\ud835\udc57\ud835\udc5asubscript\ud835\udc05\ud835\udc58\ud835\udc561\ud835\udc59\ud835\udc571\ud835\udc5a\\hat{\\mathbf{G}}_{k,l,m}=\\sum_{i,j}\\hat{\\mathbf{K}}_{i,j,m}\\cdot\\mathbf{F}_{k+i-1,l+j-1,m}over^ start_ARG bold_G end_ARG start_POSTSUBSCRIPT italic_k , italic_l , italic_m end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT over^ start_ARG bold_K end_ARG start_POSTSUBSCRIPT italic_i , italic_j , italic_m end_POSTSUBSCRIPT \u22c5 bold_F start_POSTSUBSCRIPT italic_k + italic_i - 1 , italic_l + italic_j - 1 , italic_m end_POSTSUBSCRIPT(3)where \ud835\udc0a^^\ud835\udc0a\\hat{\\mathbf{K}}over^ start_ARG bold_K end_ARG is the depthwise convolutional kernel of size DK\u00d7DK\u00d7Msubscript\ud835\udc37\ud835\udc3esubscript\ud835\udc37\ud835\udc3e\ud835\udc40D_{K}\\times D_{K}\\times Mitalic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u00d7 italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u00d7 italic_M where the mthsubscript\ud835\udc5a\ud835\udc61\u210em_{th}italic_m start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT filter in \ud835\udc0a^^\ud835\udc0a\\hat{\\mathbf{K}}over^ start_ARG bold_K end_ARG is applied to the mthsubscript\ud835\udc5a\ud835\udc61\u210em_{th}italic_m start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT channel in \ud835\udc05\ud835\udc05\\mathbf{F}bold_F to produce the mthsubscript\ud835\udc5a\ud835\udc61\u210em_{th}italic_m start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT channel of the filtered output feature map \ud835\udc06^^\ud835\udc06\\hat{\\mathbf{G}}over^ start_ARG bold_G end_ARG.", "Depthwise convolution has a computational cost of:", "DK\u22c5DK\u22c5M\u22c5DF\u22c5DF\u22c5subscript\ud835\udc37\ud835\udc3esubscript\ud835\udc37\ud835\udc3e\ud835\udc40subscript\ud835\udc37\ud835\udc39subscript\ud835\udc37\ud835\udc39D_{K}\\cdot D_{K}\\cdot M\\cdot D_{F}\\cdot D_{F}italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_M \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT(4)", "Depthwise convolution is extremely efficient relative to standard convolution. However it only filters input channels, it does not combine them to create new features. So an additional layer that computes a linear combination of the output of depthwise convolution via 1\u00d71111\\times 11 \u00d7 1 convolution is needed in order to generate these new features.", "The combination of depthwise convolution and 1\u00d71111\\times 11 \u00d7 1 (pointwise) convolution is called depthwise separable convolution which was originally introduced in [26].", "Depthwise separable convolutions cost:", "DK\u22c5DK\u22c5M\u22c5DF\u22c5DF+M\u22c5N\u22c5DF\u22c5DF\u22c5subscript\ud835\udc37\ud835\udc3esubscript\ud835\udc37\ud835\udc3e\ud835\udc40subscript\ud835\udc37\ud835\udc39subscript\ud835\udc37\ud835\udc39\u22c5\ud835\udc40\ud835\udc41subscript\ud835\udc37\ud835\udc39subscript\ud835\udc37\ud835\udc39D_{K}\\cdot D_{K}\\cdot M\\cdot D_{F}\\cdot D_{F}+M\\cdot N\\cdot D_{F}\\cdot D_{F}italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_M \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT + italic_M \u22c5 italic_N \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT(5)which is the sum of the depthwise and 1\u00d71111\\times 11 \u00d7 1 pointwise convolutions.", "By expressing convolution as a two step process of filtering and combining we get a reduction in computation of:", "DK\u22c5DK\u22c5M\u22c5DF\u22c5DF+M\u22c5N\u22c5DF\u22c5DFDK\u22c5DK\u22c5M\u22c5N\u22c5DF\u22c5DF\u22c5subscript\ud835\udc37\ud835\udc3esubscript\ud835\udc37\ud835\udc3e\ud835\udc40subscript\ud835\udc37\ud835\udc39subscript\ud835\udc37\ud835\udc39\u22c5\ud835\udc40\ud835\udc41subscript\ud835\udc37\ud835\udc39subscript\ud835\udc37\ud835\udc39\u22c5subscript\ud835\udc37\ud835\udc3esubscript\ud835\udc37\ud835\udc3e\ud835\udc40\ud835\udc41subscript\ud835\udc37\ud835\udc39subscript\ud835\udc37\ud835\udc39\\displaystyle\\frac{D_{K}\\cdot D_{K}\\cdot M\\cdot D_{F}\\cdot D_{F}+M\\cdot N\\cdot D_{F}\\cdot D_{F}}{D_{K}\\cdot D_{K}\\cdot M\\cdot N\\cdot D_{F}\\cdot D_{F}}divide start_ARG italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_M \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT + italic_M \u22c5 italic_N \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT end_ARG start_ARG italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_M \u22c5 italic_N \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT end_ARG=\\displaystyle==1N+1DK21\ud835\udc411superscriptsubscript\ud835\udc37\ud835\udc3e2\\displaystyle\\frac{1}{N}+\\frac{1}{D_{K}^{2}}divide start_ARG 1 end_ARG start_ARG italic_N end_ARG + divide start_ARG 1 end_ARG start_ARG italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG", "MobileNet uses 3\u00d73333\\times 33 \u00d7 3 depthwise separable convolutions which uses between 8 to 9 times less computation than standard convolutions at only a small reduction in accuracy as seen in Section 4.", "Additional factorization in spatial dimension such as in [16, 31] does not save much additional computation as very little computation is spent in depthwise convolutions.", "The MobileNet structure is built on depthwise separable convolutions as mentioned in the previous section except for the first layer which is a full convolution. By defining the network in such simple terms we are able to easily explore network topologies to find a good network. The MobileNet architecture is defined in Table 1. All layers are followed by a batchnorm [13] and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification. Figure 3 contrasts a layer with regular convolutions, batchnorm and ReLU nonlinearity to the factorized layer with depthwise convolution, 1\u00d71111\\times 11 \u00d7 1 pointwise convolution as well as batchnorm and ReLU after each convolutional layer. Down sampling is handled with strided convolution in the depthwise convolutions as well as in the first layer. A final average pooling reduces the spatial resolution to 1 before the fully connected layer. Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers.", "It is not enough to simply define networks in terms of a small number of Mult-Adds. It is also important to make sure these operations can be efficiently implementable. For instance unstructured sparse matrix operations are not typically faster than dense matrix operations until a very high level of sparsity. Our model structure puts nearly all of the computation into dense 1\u00d71111\\times 11 \u00d7 1 convolutions. This can be implemented with highly optimized general matrix multiply (GEMM) functions. Often convolutions are implemented by a GEMM but require an initial reordering in memory called im2col in order to map it to a GEMM. For instance, this approach is used in the popular Caffe package [15]. 1\u00d71111\\times 11 \u00d7 1 convolutions do not require this reordering in memory and can be implemented directly with GEMM which is one of the most optimized numerical linear algebra algorithms. MobileNet spends 95%percent9595\\%95 % of it\u2019s computation time in 1\u00d71111\\times 11 \u00d7 1 convolutions which also has 75%percent7575\\%75 % of the parameters as can be seen in Table 2. Nearly all of the additional parameters are in the fully connected layer.", "MobileNet models were trained in TensorFlow [1] using RMSprop [33] with asynchronous gradient descent similar to Inception V3 [31]. However, contrary to training large models we use less regularization and data augmentation techniques because small models have less trouble with overfitting. When training MobileNets we do not use side heads or label smoothing and additionally reduce the amount image of distortions by limiting the size of small crops that are used in large Inception training [31]. Additionally, we found that it was important to put very little or no weight decay (l2 regularization) on the depthwise filters since their are so few parameters in them. For the ImageNet benchmarks in the next section all models were trained with same training parameters regardless of the size of the model.", "Although the base MobileNet architecture is already small and low latency, many times a specific use case or application may require the model to be smaller and faster. In order to construct these smaller and less computationally expensive models we introduce a very simple parameter \u03b1\ud835\udefc\\alphaitalic_\u03b1 called width multiplier. The role of the width multiplier \u03b1\ud835\udefc\\alphaitalic_\u03b1 is to thin a network uniformly at each layer. For a given layer and width multiplier \u03b1\ud835\udefc\\alphaitalic_\u03b1, the number of input channels M\ud835\udc40Mitalic_M becomes \u03b1M\ud835\udefc\ud835\udc40\\alpha Mitalic_\u03b1 italic_M and the number of output channels N\ud835\udc41Nitalic_N becomes \u03b1N\ud835\udefc\ud835\udc41\\alpha Nitalic_\u03b1 italic_N.", "The computational cost of a depthwise separable convolution with width multiplier \u03b1\ud835\udefc\\alphaitalic_\u03b1 is:DK\u22c5DK\u22c5\u03b1M\u22c5DF\u22c5DF+\u03b1M\u22c5\u03b1N\u22c5DF\u22c5DF\u22c5\u22c5subscript\ud835\udc37\ud835\udc3esubscript\ud835\udc37\ud835\udc3e\ud835\udefc\ud835\udc40subscript\ud835\udc37\ud835\udc39subscript\ud835\udc37\ud835\udc39\u22c5\u22c5\ud835\udefc\ud835\udc40\ud835\udefc\ud835\udc41subscript\ud835\udc37\ud835\udc39subscript\ud835\udc37\ud835\udc39D_{K}\\cdot D_{K}\\cdot\\alpha M\\cdot D_{F}\\cdot D_{F}+\\alpha M\\cdot\\alpha N\\cdot D_{F}\\cdot D_{F}italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_\u03b1 italic_M \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT + italic_\u03b1 italic_M \u22c5 italic_\u03b1 italic_N \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT(6)where \u03b1\u2208(0,1]\ud835\udefc01\\alpha\\in(0,1]italic_\u03b1 \u2208 ( 0 , 1 ] with typical settings of 1, 0.75, 0.5 and 0.25. \u03b1=1\ud835\udefc1\\alpha=1italic_\u03b1 = 1 is the baseline MobileNet and \u03b1<1\ud835\udefc1\\alpha<1italic_\u03b1 < 1are reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly \u03b12superscript\ud835\udefc2\\alpha^{2}italic_\u03b1 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Width multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.", "The second hyper-parameter to reduce the computational cost of a neural network is a resolution multiplier \u03c1\ud835\udf0c\\rhoitalic_\u03c1. We apply this to the input image and the internal representation of every layer is subsequently reduced by the same multiplier. In practice we implicitly set \u03c1\ud835\udf0c\\rhoitalic_\u03c1 by setting the input resolution.", "We can now express the computational cost for the core layers of our network as depthwise separable convolutions with width multiplier \u03b1\ud835\udefc\\alphaitalic_\u03b1 and resolution multiplier \u03c1\ud835\udf0c\\rhoitalic_\u03c1:DK\u22c5DK\u22c5\u03b1M\u22c5\u03c1DF\u22c5\u03c1DF+\u03b1M\u22c5\u03b1N\u22c5\u03c1DF\u22c5\u03c1DF\u22c5\u22c5\u22c5subscript\ud835\udc37\ud835\udc3esubscript\ud835\udc37\ud835\udc3e\ud835\udefc\ud835\udc40\ud835\udf0csubscript\ud835\udc37\ud835\udc39\ud835\udf0csubscript\ud835\udc37\ud835\udc39\u22c5\u22c5\u22c5\ud835\udefc\ud835\udc40\ud835\udefc\ud835\udc41\ud835\udf0csubscript\ud835\udc37\ud835\udc39\ud835\udf0csubscript\ud835\udc37\ud835\udc39D_{K}\\cdot D_{K}\\cdot\\alpha M\\cdot\\rho D_{F}\\cdot\\rho D_{F}+\\alpha M\\cdot\\alpha N\\cdot\\rho D_{F}\\cdot\\rho D_{F}italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u22c5 italic_\u03b1 italic_M \u22c5 italic_\u03c1 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u22c5 italic_\u03c1 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT + italic_\u03b1 italic_M \u22c5 italic_\u03b1 italic_N \u22c5 italic_\u03c1 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT \u22c5 italic_\u03c1 italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT(7)where \u03c1\u2208(0,1]\ud835\udf0c01\\rho\\in(0,1]italic_\u03c1 \u2208 ( 0 , 1 ] which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128. \u03c1=1\ud835\udf0c1\\rho=1italic_\u03c1 = 1 is the baseline MobileNet and \u03c1<1\ud835\udf0c1\\rho<1italic_\u03c1 < 1are reduced computation MobileNets. Resolution multiplier has the effect of reducing computational cost by \u03c12superscript\ud835\udf0c2\\rho^{2}italic_\u03c1 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT.", "As an example we can look at a typical layer in MobileNet and see how depthwise separable convolutions, width multiplier and resolution multiplier reduce the cost and parameters. Table 3 shows the computation and number of parameters for a layer as architecture shrinking methods are sequentially applied to the layer. The first row shows the Mult-Adds and parameters for a full convolutional layer with an input feature map of size 14\u00d714\u00d7512141451214\\times 14\\times 51214 \u00d7 14 \u00d7 512 with a kernel K\ud835\udc3eKitalic_K of size 3\u00d73\u00d7512\u00d7512335125123\\times 3\\times 512\\times 5123 \u00d7 3 \u00d7 512 \u00d7 512. We will look in detail in the next section at the trade offs between resources and accuracy.", "In this section we first investigate the effects of depthwise convolutions as well as the choice of shrinking by reducing the width of the network rather than the number of layers. We then show the trade offs of reducing the network based on the two hyper-parameters: width multiplier and resolution multiplier and compare results to a number of popular models. We then investigate MobileNets applied to a number of different applications.", "First we show results for MobileNet with depthwise separable convolutions compared to a model built with full convolutions. In Table 4 we see that using depthwise separable convolutions compared to full convolutions only reduces accuracy by 1%percent11\\%1 % on ImageNet was saving tremendously on mult-adds and parameters.", "We next show results comparing thinner models with width multiplier to shallower models using less layers. To make MobileNet shallower, the 5555 layers of separable filters with feature size 14\u00d714\u00d7512141451214\\times 14\\times 51214 \u00d7 14 \u00d7 512 in Table 1 are removed. Table 5 shows that at similar computation and number of parameters, that making MobileNets thinner is 3%percent33\\%3 % better than making them shallower.", "Table 6 shows the accuracy, computation and size trade offs of shrinking the MobileNet architecture with the width multiplier \u03b1\ud835\udefc\\alphaitalic_\u03b1.Accuracy drops off smoothly until the architecture is made too small at \u03b1=0.25\ud835\udefc0.25\\alpha=0.25italic_\u03b1 = 0.25.", "Table 7 shows the accuracy, computation and size trade offs for different resolution multipliers bytraining MobileNets with reduced input resolutions. Accuracy drops off smoothly across resolution.", "Figure 4 shows the trade off between ImageNet Accuracy and computation for the 16 models made from the cross product of width multiplier \u03b1\u2208{1,0.75,0.5,0.25}\ud835\udefc10.750.50.25\\alpha\\in\\{1,0.75,0.5,0.25\\}italic_\u03b1 \u2208 { 1 , 0.75 , 0.5 , 0.25 } and resolutions {224,192,160,128}224192160128\\{224,192,160,128\\}{ 224 , 192 , 160 , 128 }. Results are log linear with a jump when models get very small at \u03b1=0.25\ud835\udefc0.25\\alpha=0.25italic_\u03b1 = 0.25.", "Figure 5 shows the trade off between ImageNet Accuracy and number of parameters for the 16 models made from the cross product of width multiplier \u03b1\u2208{1,0.75,0.5,0.25}\ud835\udefc10.750.50.25\\alpha\\in\\{1,0.75,0.5,0.25\\}italic_\u03b1 \u2208 { 1 , 0.75 , 0.5 , 0.25 } and resolutions {224,192,160,128}224192160128\\{224,192,160,128\\}{ 224 , 192 , 160 , 128 }. ", "Table 8 compares full MobileNet to the original GoogleNet [30] and VGG16 [27]. MobileNet is nearly as accurate as VGG16 while being 32 times smaller and 27 times less compute intensive. It is more accurate than GoogleNet while being smaller and more than 2.5 times less computation.", "Table 9 compares a reduced MobileNet with width multiplier \u03b1=0.5\ud835\udefc0.5\\alpha=0.5italic_\u03b1 = 0.5 and reduced resolution 160\u00d7160160160160\\times 160160 \u00d7 160. Reduced MobileNet is 4%percent44\\%4 % better than AlexNet [19] while being 45\u00d745\\times45 \u00d7 smaller and 9.4\u00d79.4\\times9.4 \u00d7 less compute than AlexNet. It is also 4%percent44\\%4 % better than Squeezenet [12] at about the same size and 22\u00d722\\times22 \u00d7 less computation.", "We train MobileNet for fine grained recognition on the Stanford Dogs dataset [17].We extend the approach of [18] and collect an even larger but noisy training set than [18] from the web.We use the noisy web data to pretrain a fine grained dog recognition model and then fine tune the model on the Stanford Dogs training set.Results on Stanford Dogs test set are in Table 10. MobileNet can almost achieve the state of the art results from[18] at greatly reduced computation and size.", "PlaNet [35] casts the task of determining where on earth a photo was taken as a classification problem. The approach divides the earth into a grid of geographic cells that serve as the target classes and trains a convolutional neural network on millions of geo-tagged photos. PlaNet has been shown to successfully localize a large variety of photos and to outperform Im2GPS [6, 7] that addresses the same task.", "We re-train PlaNet using the MobileNet architecture on the same data. While the full PlaNet model based on the Inception V3 architecture [31] has 52 million parameters and 5.74 billion mult-adds. The MobileNet model has only 13 million parameters with the usual 3 million for the body and 10 million for the final layer and 0.58 Million mult-adds. As shown in Tab.\u00a011, the MobileNet version delivers only slightly decreased performance compared to PlaNet despite being much more compact. Moreover, it still outperforms Im2GPS by a large margin.", "Another use-case for MobileNet is compressing large systems with unknown or esoteric training procedures. In a face attribute classification task, we demonstrate a synergistic relationship between MobileNet and distillation\u00a0[9], a knowledge transfer technique for deep networks. We seek to reduce a large face attribute classifier with 75757575 million parameters and 1600160016001600 million Mult-Adds. The classifier is trained on a multi-attribute dataset similar to YFCC100M\u00a0[32].", "We distill a face attribute classifier using the MobileNet architecture. Distillation\u00a0[9] works by training the classifier to emulate the outputs of a larger model222The emulation quality is measured by averaging the per-attribute cross-entropy over all attributes. instead of the ground-truth labels, hence enabling training from large (and potentially infinite) unlabeled datasets. Marrying the scalability of distillation training and the parsimonious parameterization of MobileNet, the end system not only requires no regularization (e.g. weight-decay and early-stopping), but also demonstrates enhanced performances. It is evident from Tab.\u00a012 that the MobileNet-based classifier is resilient to aggressive model shrinking: it achieves a similar mean average precision across attributes (mean AP) as the in-house while consuming only 1%percent11\\%1 % the Multi-Adds.", "MobileNet can also be deployed as an effective base network in modern object detection systems.We report results for MobileNet trained for object detection on COCO data based on the recent work that won the 2016 COCO challenge [10].In table 13, MobileNet is compared to VGG and Inception V2 [13] under both Faster-RCNN [23] and SSD [21] framework.In our experiments, SSD is evaluated with 300 input resolution (SSD 300) and Faster-RCNN is compared with both 300 and 600 input resolution (Faster-RCNN 300, Faster-RCNN 600).The Faster-RCNN model evaluates 300 RPN proposal boxes per image. The models are trained on COCO train+val excluding 8k minival images and evaluated on minival.For both frameworks, MobileNet achieves comparable results to other networks with only a fraction of computational complexity and model size.", "The FaceNet model is a state of the art face recognition model [25]. It builds face embeddings based on the triplet loss. To build a mobile FaceNet model we use distillationto train by minimizing the squared differences of the output of FaceNet and MobileNet on the training data. Results for very small MobileNet models can be found in table 14.", "We proposed a new model architecture called MobileNets based on depthwise separable convolutions. We investigated some of the important design decisions leading to an efficient model. We then demonstrated how to build smaller and faster MobileNets using width multiplier and resolution multiplier by trading off a reasonable amount of accuracy to reduce size and latency. We then compared different MobileNets to popular models demonstrating superior size, speed and accuracy characteristics. We concluded by demonstrating MobileNet\u2019s effectiveness when applied to a wide variety of tasks. As a next step to help adoption and exploration of MobileNets, we plan on releasing models in Tensor Flow."], "figure_types": {"3647d6d0f151dc05626449ee09cc7bce55be497e/2-Figure1-1.png": "schematic", "3647d6d0f151dc05626449ee09cc7bce55be497e/3-Figure2-1.png": "schematic", "3647d6d0f151dc05626449ee09cc7bce55be497e/4-Figure3-1.png": "schematic", "3647d6d0f151dc05626449ee09cc7bce55be497e/4-Table1-1.png": "table", "3647d6d0f151dc05626449ee09cc7bce55be497e/4-Table2-1.png": "table", "3647d6d0f151dc05626449ee09cc7bce55be497e/5-Table3-1.png": "table", "3647d6d0f151dc05626449ee09cc7bce55be497e/5-Table6-1.png": "table", "3647d6d0f151dc05626449ee09cc7bce55be497e/6-Figure4-1.png": "plot", "3647d6d0f151dc05626449ee09cc7bce55be497e/6-Figure5-1.png": "plot", "3647d6d0f151dc05626449ee09cc7bce55be497e/6-Table10-1.png": "table", "3647d6d0f151dc05626449ee09cc7bce55be497e/6-Table11-1.png": "table", "3647d6d0f151dc05626449ee09cc7bce55be497e/6-Table8-1.png": "table", "3647d6d0f151dc05626449ee09cc7bce55be497e/7-Figure6-1.png": "photograph(s)", "3647d6d0f151dc05626449ee09cc7bce55be497e/7-Table12-1.png": "table", "3647d6d0f151dc05626449ee09cc7bce55be497e/8-Table14-1.png": "table"}}, "1602.03409": {"paper_id": "paper_43", "title": "Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning", "arxiv_url": "https://arxiv.org/abs/1602.03409", "s2orc_url": "https://www.semanticscholar.org/paper/a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f", "all_figures_tables": {"a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/10-Figure10-1.png": "Fig. 10. Visual examples of misclassified ILD 64x64 patches (in axial view), with their ground truth labels and inaccurately classified labels.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/10-TableVI-1.png": "TABLE VI CLASSIFICATION RESULTS ON ILD AND LN DETECTION WITH LOO.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/10-TableVII-1.png": "TABLE VII TRAINING TIME AND MEMORY REQUIREMENTS OF THE FIVE CNN ARCHITECTURES ON ILD PATCH-BASED CLASSIFICATION UP TO 90 EPOCHS.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/11-TableVIII-1.png": "TABLE VIII CLASSIFICATION ACCURACIES FOR ILD SLICE AND LN PATCH-LEVEL DETECTION WITH \u201cEQUAL PRIOR\u201d AND \u201cBIASED PRIOR\u201d, USING GOOGLENET-TL.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/12-Figure11-1.png": "Fig. 11. Traces of training and validation loss (blue and green lines) and validation accuracy (orange lines) during (a) training AlexNet from random initialization and (b) fine-tuning from ImageNet pre-trained CNN, for ILD classification.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/12-Figure12-1.png": "Fig. 12. Visualization of first layer convolution filters of CNNs trained on abdominal and mediastinal LNs in RGB color, from random initialization (AlexNet-RI (256x256), AlexNet-RI (64x64), GoogLeNet-RI (256x256) and GoogLeNet-RI (64x64)) and with transfer learning (AlexNet-TL (256x256)).", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/13-Figure13-1.png": "Fig. 13. Visualization of the last pooling layer (pool-5) activations (top). Pooling units where the relative image location of the disease region is located in the image are highlighted with green boxes. The original images reconstructed from the units are shown in the bottom [72]. The examples in (a) and (b) are computed from the input ILD images in Figure 2 (b) and (c), respectively.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/3-Figure1-1.png": "Fig. 1. Some examples of abdominal and mediastinal lymph nodes sampled on axial (ax), coronal (co), and sagittal (sa) views, with four different fields-ofviews (30mm: orange; 45mm: red; 85mm: green; 128mm: blue) surrounding lymph nodes.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/3-Figure2-1.png": "Fig. 2. Some examples of CT image slices with six lung tissue types in the ILD dataset [37]. Disease tissue types are located with dark orange arrows.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/4-Figure3-1.png": "Fig. 3. Some examples of 64\u00d7 64 pixel CT image patches for (a) NM, (b) EM, (c) GG, (d) FB, (e) MN (f) CD.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/4-Figure4-1.png": "Fig. 4. An example of lung/high-attenuation/low-attenuation CT windowing for an axis lung CT slice. We encode the lung/high-attenuation/low-attenuation CT windowing into red/green/blue channels.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/5-Figure5-1.png": "Fig. 5. A simplified illustration of the CNN architectures used. GoogLeNet [33] contains two convolution layers, three pooling layers, and nine inception layers. Each of the inception layer of GoogLeNet consists of six convolution layers and one pooling layer.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/5-Figure6-1.png": "Fig. 6. Illustration of inception3a layer of GoogLeNet. Inception layers of GoogLeNet consist of six convolution layers with different kernel sizes and one pooling layer.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/5-Figure7-1.png": "Fig. 7. Some examples of Cifar10 dataset and some images of \u201ctennis ball\u201d class from ImageNet dataset. Images of Cifar10 dataset are small (32 \u00d7 32) images with object of the image class category in the center. Images of ImageNet dataset are larger (256\u00d7256), where object of the image class category can be small, obscure, partial, and sometimes in a cluttered environment.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/7-Figure8-1.png": "Fig. 8. FROC curves averaged on three-fold CV for the abdominal (left) and mediastinal (right) lymph nodes using different CNN models.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/7-TableII-1.png": "TABLE II COMPARISON OF MEDIASTINAL AND ABDOMINAL LN DETECTION RESULTS USING VARIOUS CNN MODELS. NUMBERS IN BOLD INDICATE THE BEST PERFORMANCE VALUES ON CLASSIFICATION ACCURACY.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/8-Figure9-1.png": "Fig. 9. Examples of misclassified lymph nodes (in axial view) of both false negatives (Left) and false positives (Right). Mediastinal LN examples are shown in the upper row, and abdominal LN examples in the bottom row.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/8-TableIV-1.png": "TABLE IV COMPARISON OF INTERSTITIAL LUNG DISEASE CLASSIFICATION RESULTS USING F-SCORES: NM, EM, GG, FB, MN AND CD.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/8-TableV-1.png": "TABLE V CONFUSION MATRIX FOR ILD CLASSIFICATION (PATCH-LEVEL) WITH FIVE-FOLD CV USING GOOGLENET-TL.", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/9-TableIII-1.png": "TABLE III COMPARISON OF INTERSTITIAL LUNG DISEASE CLASSIFICATION ACCURACIES ON BOTH SLICE-LEVEL (SLICE-CV5) AND PATCH-BASED (PATCH-CV5) CLASSIFICATION USING FIVE-FOLD CV. BOLD NUMBERS INDICATE THE BEST PERFORMANCE VALUES ON CLASSIFICATION ACCURACY."}, "referred_figures_tables": [["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/5-Figure7-1.png"], ["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/5-Figure5-1.png"], ["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/4-Figure3-1.png"], ["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/4-Figure3-1.png"], ["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/4-Figure4-1.png"], ["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/13-Figure13-1.png"], ["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/5-Figure5-1.png"], ["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/5-Figure7-1.png"], ["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/7-Figure8-1.png", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/7-TableII-1.png"], ["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/12-Figure11-1.png", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/9-TableIII-1.png"], ["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/9-TableIII-1.png"], ["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/3-Figure2-1.png", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/13-Figure13-1.png", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/13-Figure13-1.png", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/3-Figure2-1.png", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/13-Figure13-1.png", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/3-Figure2-1.png"], ["a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/10-Figure10-1.png", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/9-TableIII-1.png"]], "question_id": [3, 6, 12, 13, 14, 15, 16, 17, 19, 20, 22, 23, 24], "question": ["What is the number of images and classes does the ImageNet dataset have?", "What are the CNN architectures that were explored in this paper?", "How many images did the dataset consist of and the number of unique patients ?", "What are the six classes of the data used for training ?", "How did the authors leverage the CNN architectures designed for color images and to transfer CNN parameters pre-trained on ImageNet to be able to use it on the medical dataset ?", "How does CNN model learns to ignore areas that appear in both healthy and diseased lungs?", "What was the goal behind reducing the filter size and stride of the ALexNet and GoogLeNet ?", "What is CifarNet?", "What are the models that yielded the least competitive detection accuracy results on the Thoracoabdominal Lymph Node Detection?", "Why did the GoogLENet-RI-H performs poorly in the Thoracoabdominal Lymph Node Detection task?", "What is the difference between five-fold cross validation and leave-one-patient out?", "How is the original ILD images were reconstructed ?", "Was Transfer learning beneficial on the CADe process? "], "question_section": ["Introduction ", "Introduction ", "DATASETS AND RELATED WORK", "DATASETS AND RELATED WORK", "DATASETS AND RELATED WORK", "DATASETS AND RELATED WORK", "METHODS", "METHODS", "EVALUATIONS AND DISCUSSIONS", "EVALUATIONS AND DISCUSSIONS", "EVALUATIONS AND DISCUSSIONS", "ANALYSIS VIA CNN LEARNING TRACES AND LULVISUALIZATION", "ANALYSIS VIA CNN LEARNING TRACES AND LULVISUALIZATION"], "question_trigger_sentence": ["ImageNet offers a very comprehensive database of", "We also explore CNN architectures", "utilize the publicly available dataset", "six lung tissue types an Notations containing at least one of the following\r\n", "To leverage the CNN architectures designed for color images and to transfer CNN parameters pre-trained on ImageNet", "CNN training learns to ignore", "We also reduced the filter size", "CifarNet, introduced in", "Of the nine investigated CNN", "GoogLeNet-RI-H performs poorly", "five-fold cross-validation", "Next, we reconstruct the original ILD images", "Transfer learning from the large scale annotated natural image datasets (ImageNet) to CADe problems"], "question_type": ["Testing question ", "Shallow", "Testing", "Shallow", "Deep / Complex", "Deep/complex question", "Deep / Complex", "Shallow question", "Shallow", "Deep/complex question", "Testing", "Shallow", "Deep / Complex question"], "evidential_info": [[{"context": "Tremendous progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets (i.e. ImageNet [1, 2]) and the recent revival of deep convolutional neural networks (CNN) [3, 4].For data-driven learning, large-scale well-annotated datasets with representative data distribution characteristics are crucial to learning more accurate or generalizable models [5, 4].Unlike previous image datasets used in computer vision, ImageNet [1] offers a very comprehensive database of more than 1.2 million categorized natural images of 1000+ classes.The CNN models trained upon this database serve as the backbone for significantly improving many object detection and image segmentation problems using other datasets [6, 7], e.g., PASCAL [8] and medical image categorization [9, 10, 11, 12].However, there exists no large-scale annotated medical image dataset comparable to ImageNet, as data acquisition is difficult, and quality annotation is costly.", "rationale": "ImageNet has more than 1.2 million images and about 1000 classes."}, {"context": "The AlexNet architecture was published in [4], achieved significantly improved performance over the other non-deep learning methods for ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012.This success has revived the interest in CNNs [3] in computer vision.ImageNet consists of 1.2 million 256\\times 256 images belonging to 1000 categories.At times, the objects in the image are small and obscure, and thus pose more challenges for learning a successful classification model.More details about the ImageNet dataset will be discussed in Sec. III-B.AlexNet has five convolution layers, three pooling layers, and two fully-connected layers with approximately 60 million free parameters.AlexNet is our default CNN architecture for evaluation and analysis in the remainder of the paper.", "rationale": "ImageNet has more than 1.2 million images and about 1000 classes."}, {"context": "ImageNet [1] has more than 1.2 million 256\\times 256 images categorized under 1000 object class categories.There are more than 1000 training images per class.The database is organized according to the WordNet [55] hierarchy, which currently contains only nouns in 1000 object categories.The image-object labels are obtained largely through crowd-sourcing, e.g., Amazon Mechanical Turk, and human inspection.Some examples of object categories in ImageNet are \u201csea snake\u201d, \u201csandwich\u201d, \u201cvase\u201d, \u201cleopard\u201d, etc.ImageNet is currently the largest image dataset among other standard datasets for visual recognition.Indeed, the Caltech101, Caltech256 and Cifar10 dataset merely contain 60000 32\\times 32 images and 10 object classes.Furthermore, due to the large number (1000+) of object classes, the objects belonging to each ImageNet class category can be occluded, partial and small, relative to those in the previous public image datasets.This significant intra-class variation poses greater challenges to any data-driven learning system that builds a classifier to fit given data and generalize to unseen data.For comparison, some example images of Cifar10 dataset and ImageNet images in the \u201ctennis ball\u201d class category are shown in Figure 7.The ImageNet dataset is publicly available, and the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) has become the standard benchmark for large-scale object recognition.", "rationale": "ImageNet has more than 1.2 million images and about 1000 classes."}], [{"context": "We mainly explore three convolutional neural network architectures (CifarNet [5, 22], AlexNet [4] and GoogLeNet [33]) with different model training parameter values.The current deep learning models [22, 52, 53] in medical image tasks are at least 2\\sim 5 orders of magnitude smaller than even AlexNet [4].More complex CNN models [22, 52] have only about 150K or 15K parameters.Roth et al. [22] adopt the CNN architecture tailored to the Cifar-10 dataset [5] and operate on image windows of 32\\times 32\\times 3 pixels for lymph node detection, while the simplest CNN in [54] has only one convolutional, pooling, and FC layer, respectively.", "rationale": "The paper uses AlexNet, CifarNet, and GoogLeNet with various numbers of parameters."}, {"context": "We use CifarNet [5] as used in [22] as a baseline for the LN detection.AlexNet [4] and GoogLeNet [33] are also modified to evaluate these state-of-the-art CNN architecture from ImageNet classification task [2] to our CADe problems and datasets.A simplified illustration of three CNN architectures exploited is shown in Figure 5.CifarNet always takes 32\\times 32\\times 3 image patches as input while AlexNet and GoogLeNet are originally designed for the fixed image dimension of 256\\times 256\\times 3 pixels.We also reduced the filter size, stride and pooling parameters of AlexNet and GoogLeNet to accommodate a smaller input size of 64\\times 64\\times 3 pixels.We do so to produce and evaluate \u201csimplified\u201d AlexNet and GoogLeNet versions that are better suited to the smaller scale training datasets common in CADe problems.Throughout the paper, we refer to the models as CifarNet (32x32) or CifarNet (dropping 32x32); AlexNet (256x256) or AlexNet-H (high resolution); AlexNet (64x64) or AlexNet-L (low resolution); GoogLeNet (256x256) or GoogLeNet-H and GoogLeNet (64x64) or GoogLeNet-L (dropping 3 since all image inputs are three channels).", "rationale": "The paper uses AlexNet."}, {"context": "The AlexNet architecture was published in [4], achieved significantly improved performance over the other non-deep learning methods for ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012.This success has revived the interest in CNNs [3] in computer vision.ImageNet consists of 1.2 million 256\\times 256 images belonging to 1000 categories.At times, the objects in the image are small and obscure, and thus pose more challenges for learning a successful classification model.More details about the ImageNet dataset will be discussed in Sec. III-B.AlexNet has five convolution layers, three pooling layers, and two fully-connected layers with approximately 60 million free parameters.AlexNet is our default CNN architecture for evaluation and analysis in the remainder of the paper.", "rationale": "The paper uses AlexNet, CifarNet, and GoogLeNet with various numbers of parameters."}, {"context": "In this section, we evaluate and compare the performances of nine CNN model configurations (CifarNet, AlexNet-ImNet,AlexNet-RI-H, AlexNet-TL-H, AlexNet-RI-L, GoogLeNet-RI-H, GoogLeNet-TL-H, GoogLeNet-RI-L and combined) on two important CADe problems using publicly available datasets [22, 41, 37].", "rationale": "The paper uses AlexNet and GoogLeNet."}, {"context": "In this paper, we exploit three important, but previously under-studied factors of employing deep convolutional neural networks to computer-aided detection problems.Particularly, we explore and evaluate different CNN architectures varying in width (ranging from 5 thousand to 160 million parameters) and depth (various numbers of layers), describe the effects of varying dataset scale and spatial image context on performance, and discuss when and why transfer learning from pre-trained ImageNet CNN models can be valuable. We further verify our hypothesis by inheriting and adapting rich hierarchical image features [5, 33] from the large-scale ImageNet dataset for computer aided diagnosis (CAD). We also explore CNN architectures of the most studied seven-layered \u201cAlexNet-CNN\u201d [4], a shallower \u201cCifar-CNN\u201d [22], and a much deeper version of \u201cGoogLeNet-CNN\u201d [33] (with our modifications on CNN structures). This study is partially motivated by recent studies [34, 35] in computer vision. The thorough quantitative analysis and evaluation on deep CNN [34] or sparsity image coding methods [35] elucidate the emerging techniques of the time and provide useful suggestions for their future stages of development, respectively.", "rationale": "The paper uses AlexNet, CifarNet, and GoogLeNet with various numbers of parameters."}, {"context": "In this work, we mainly focus on AlexNet and GoogLeNet. AlexNet is the first notably successful CNN architecture on the ImageNet challenge and has rekindled significant research interests on CNN. GoogLeNet is the state-of-the-art deep model, which has outperformed other notable models, such as AlexNet, OverFeat, and VGGNet [67, 68] in various computer vision benchmarks. Likewise, a reasonable assumption is that OverFeat and VGGNet may generate quantitative performance results ranked between AlexNet\u2019s and GoogLeNet\u2019s. For completeness, we include the Overfeat and VGGNet in the following evaluations, to bolster our hypothesis.", "rationale": "The paper uses AlexNet, CifarNet, and GoogLeNet with various numbers of parameters."}], [{"context": "Interstitial Lung Disease Dataset. We utilize the publicly available dataset of [37]. It contains 905 image slices from 120 patients, with six lung tissue types annotations containing at least one of the following: healthy (NM), emphysema (EM), ground glass (GG), fibrosis (FB), micronodules (MN) and consolidation (CD) (Figure 3).At the slice level, the objective is to classify the status of \u201cpresence/absence\u201d of any of the six ILD classes for an input axial CT slice [40].Characterizing an arbitrary CT slice against any possible ILD type, without any manual ROI (in contrast to [38, 39]), can be useful for large-scale patient screening.For slice-level ILD classification, we sampled the slices 12 times with random translations and rotations.After this, we balanced the numbers of CT slice samples for the six classes by randomly sampling several instances at various rates.For patch-based classification, we sampled up to 100 patches of size 64\\times 64 from each ROI.This dataset is divided into five folds with disjoint patient subsets.The average number of CT slices (training instances) per fold is small, as shown in Table I.Slice-level ILD classification is a very challenging task where CNN models need to learn from very small numbers of training examples and predict ILD labels on unseen patients.", "rationale": "The ILD dataset has 905 image slices from 120 patients."}], [{"context": "Interstitial Lung Disease Dataset. We utilize the publicly available dataset of [37]. It contains 905 image slices from 120 patients, with six lung tissue types annotations containing at least one of the following: healthy (NM), emphysema (EM), ground glass (GG), fibrosis (FB), micronodules (MN) and consolidation (CD) (Figure 3). At the slice level, the objective is to classify the status of \u201cpresence/absence\u201d of any of the six ILD classes for an input axial CT slice [40]. Characterizing an arbitrary CT slice against any possible ILD type, without any manual ROI (in contrast to [38], [39]), can be useful for large-scale patient screening. For slice-level ILD classification, we sampled the slices 12 times with random translations and rotations. After this, we balanced the numbers of CT slice samples for the six classes by randomly sampling several instances at various rates. For patch-based classification, we sampled up to 100 patches of size 64\u00d764 from each ROI. This dataset is divided into five folds with disjoint patient subsets. The average number of CT slices (training instances) per fold is small, as shown in Table I. Slice-level ILD classification is a very challenging task where CNN models need to learn from very small numbers of training examples and predict ILD labels on unseen patients.", "rationale": "The six classes are healthy, emphysema, ground glass, fibrosis, micronodules, and consolidation."}], [{"context": "To leverage the CNN architectures designed for color images and to transfer CNN parameters pre-trained on ImageNet, we transform all gray-scale axial CT slice images via three CT window ranges: lung window range [-1400, -200HU], high-attenuation range [-160, 240HU], and low-attenuation range [-1400; -950HU]. We then encode the transformed images into RGB channels (to be aligned with the input channels of CNN models [4], [33] pre-trained from natural image datasets [1]). The low-attenuation CT window is useful for visualizing certain texture patterns of lung diseases (especially emphysema). The usage of different CT attenuation channels improves classification results over the usage of a single CT windowing channel, as demonstrated in [40]. More importantly, these CT windowing processes do not depend on the lung segmentation, which instead is directly defined in the CT HU space. Figure 4 shows a representative example of lung, high-attenuation, and low-attenuation CT windowing for an axis lung CT slice.", "rationale": "The authors transformed every gray-scale axial CT image using the three CT windows of lung window range [-1400, -200HU], high-attenuation range [-160, 240HU], and low-attenuation range [-1400; -950HU], then encoded the transformed images into RGB images."}], [{"context": "As observed in [40], lung segmentation is crucial to holistic slice-level ILD classification. We empirically compare performance in two scenarios with a rough lung segmentation111This can be achieved by segmenting the lung using simple label-fusion methods [48]. In the first case, we overlay the target image slice with the average lung mask among the training folds. In the second, we perform simple morphology operations to obtain the lung boundary. In order to retain information from the inside of the lung, we apply Gaussian smoothing to the regions outside of the lung boundary. There is no significant difference between two setups. Due to the high precision of CNN based image processing, highly accurate lung segmentation is not necessary . The localization of ILD regions within the lung is simultaneously learned through selectively weighted CNN reception fields in the deepest convolutional layers during the classification based CNN training [49, 50].Some areas outside of the lung appear in both healthy or diseased images. CNN training learns to ignore them by setting very small filter weights around the corresponding regions (Figure 13). This observation is validated by [40].", "rationale": "The model learns very small weights in the filters for such areas."}], [{"context": "We use CifarNet [5] as used in [22] as a baseline for the LN detection.AlexNet [4] and GoogLeNet [33] are also modified to evaluate these state-of-the-art CNN architecture from ImageNet classification task [2] to our CADe problems and datasets.A simplified illustration of three CNN architectures exploited is shown in Figure 5.CifarNet always takes 32\\times 32\\times 3 image patches as input while AlexNet and GoogLeNet are originally designed for the fixed image dimension of 256\\times 256\\times 3 pixels.We also reduced the filter size, stride and pooling parameters of AlexNet and GoogLeNet to accommodate a smaller input size of 64\\times 64\\times 3 pixels.We do so to produce and evaluate \u201csimplified\u201d AlexNet and GoogLeNet versions that are better suited to the smaller scale training datasets common in CADe problems.Throughout the paper, we refer to the models as CifarNet (32x32) or CifarNet (dropping 32x32); AlexNet (256x256) or AlexNet-H (high resolution); AlexNet (64x64) or AlexNet-L (low resolution); GoogLeNet (256x256) or GoogLeNet-H and GoogLeNet (64x64) or GoogLeNet-L (dropping 3 since all image inputs are three channels).", "rationale": "The authors reduced the filter size and stride of the two models because the input size used was smaller than what the original models were trained on."}], [{"context": "CifarNet, introduced in [5], was the state-of-the-art model for object recognition on the Cifar10 dataset, which consists of 32\\times 32 images of 10 object classes.The objects are normally centered in the images.Some example images and class categories from the Cifar10 dataset are shown in Figure 7.CifarNet has three convolution layers, three pooling layers, and one fully-connected layer.This CNN architecture, also used in [22] has about 0.15 million free parameters.We adopt it as a baseline model for the LN detection.", "rationale": "CifarNet was a CNN model that was used for the object recognition task using the Cifar10 dataset."}], [{"context": "Results for lymph node detection in the mediastinum and abdomen are reported in Table II.FROC curves are illustrated in Figure 8.The area-under-the-FROC-curve (AUC) and true positive rate (TPR, recall or sensitivity) at three false positives per patient (TPR/3FP) are used as performance metrics.Of the nine investigated CNN models, CifarNet, AlexNet-ImNet and GoogLeNet-RI-H generally yielded the least competitive detection accuracy results.Our LN datasets are significantly more complex (i.e., display much larger within-class appearance variations), especially due to the extracted fields-of-view (FOVs) of (35mm-128mm) compared to (30mm-45mm) in [22], where CifarNet is also employed.In this experiment, CifarNet is under-trained with respect to our enhanced LN datasets, due to its limited input resolution and parameter complexity.The inferior performance of AlexNet-ImNet implies that using the pre-trained ImageNet CNNs alone as \u201coff-the-shelf\u201d deep image feature extractors may not be optimal or adequate for mediastinal and abdominal LN detection tasks.To complement \u201coff-the-shelf\u201d CNN features, [10, 9, 12] all add and integrate various other hand-crafted image features as hybrid inputs for the final CADe classification.", "rationale": "CifarNet, AlexNet-ImNet and GoogLeNet-RI-H were the models that had the worst results."}], [{"context": "GoogLeNet-RI-H performs poorly, as it is susceptible to over-fitting. No sufficient data samples are available to train GoogLeNet-RI-H with random initialization.Indeed, due to GoogLeNet-RI-H\u2019s complexity and 22-layer depth, million-image datasets may be required to properly train this model.However, GoogLeNet-TL-H significantly improves upon GoogLeNet-RI-H (0.81 versus 0.61 TPR/3FP in mediastinum; 0.70 versus 0.48 TPR/3FP in abdomen). This indicates that transfer learning offers a much better initialization of CNN parameters than random initialization. Likewise, AlexNet-TL-H consistently outperforms AlexNet-RI-H, though by smaller margins (0.81 versus 0.79 TPR/3FP in mediastinum; 0.69 versus 0.67 TPR/3FP in abdomen). This is also consistent with the findings reported for ILD detection in Table III and Figure 11.", "rationale": "The model suffers from over-fitting, as it is a very complex model but it does not have enough training data for training."}], [{"context": "To investigate the performance difference between five-fold cross-validation (CV) in Sec. IV-B and leave-one-patient-out (LOO) validation, this experiment is performed under the LOO protocol. By comparing results in Table III (CV-5) to those in Table VI (LOO), one can see that LOO\u2019s quantitative performances are remarkably better than CV-5\u2019s. For example, in ILD slice-level classification, the accuracy level drastically increases from 0.46 to 0.867 using AlexNet-TL, and from 0.57 to 0.902 for GoogLeNet-TL.", "rationale": "LOO performs better than five-fold cross validation."}], [{"context": "The last pooling layer (pool-5) activation maps of the ImageNet pre-trained AlexNet [4] (analogical to AlexNet-ImNet) and AlexNet-TL, obtained by processing two input images of Figure 2 (b,c), are shown in Figure 13 (a,b). The last pooling layer activation map summarizes the entire input image by highlighting which relative locations or neural reception fields relative to the image are activated. There are a total of 256 (6x6) reception fields in AlexNet [4]. Pooling units where the relative image location of the disease region is present in the image are highlighted with green boxes. Next, we reconstruct the original ILD images using the process of de-convolution, back-propagating with convolution and un-pooling from the activation maps of the chosen pooling units [72]. From the reconstructed images (Figure 13 bottom), we observe that with fine-tuning, AlexNet-TL detects and localizes objects of interest (ILD disease regions depicted in in Figure 2 (b) and (c)) better than AlexNet-ImNet. The filters shown in Figure 13 that better localize regions on the input images (Figure 2 (b) and (c)) respectively, produce relatively higher activations (in the top 5%) among all 512 reception field responses in the fine-tuned AlexNet-TL model. As observed in [73], the final CNN classification score can not be driven solely by a single strong activation in the receptions fields, but often by a sparse set of high activations (i.e., varying selective or sparse activations per input image).", "rationale": "A process consisting of deconvolution, back-propagation with convolution, and un-pooling from the activation maps of the pooling units was used to reconstruct the original ILD images."}], [{"context": "While it is a more practical CADe scheme, slice-level CNN learning [40] is very challenging, as it is restricted to only 905 CT image slices with tagged ILD labels. We only benchmark the slice-level ILD classification results in this section. Even with the help of data augmentation (described in Sec. II), the classification accuracy of GoogLeNet-TL from Table III is only 0.57. However, transfer learning from ImageNet pre-trained model is consistently beneficial, as evidenced by AlexNet-TL (0.46) versus AlexNet-RI (0.44), and GoogLeNet-TL (0.57) versus GoogLeNet-RI (0.41). It especially prevents GoogLeNet from over-fitting on the limited CADe datasets. Finally, when the cross-validation is conducted by randomly splitting the set of all 905 CT axial slices into five folds, markedly higher F-scores are obtained (Slice-Random in Table IV). This further validates the claim that the dataset poorly generalizes ILDs for different patients. Figure 10 shows examples of misclassified ILD patches (in axial view), with their ground truth labels and inaccurately classified labels.", "rationale": "Transfer learning was shown to be beneficial in the paper's experiments."}, {"context": "\u2022Deep CNN architectures with 8, even 22 layers [4, 33], can be useful even for CADe problems where the available training datasets are limited. Previously, CNN models used in medical image analysis applications have often been 2\\sim 5 orders of magnitude smaller.\u2022The trade-off between using better learning models and using more training data [51] should be carefully considered when searching for an optimal solution to any CADe problem (e.g., mediastinal and abdominal LN detection).\u2022Limited datasets can be a bottleneck to further advancement of CADe. Building progressively growing (in scale), well annotated datasets is at least as crucial as developing new algorithms. This has been accomplished, for instance, in the field of computer vision. The well-known scene recognition problem has made tremendous progress, thanks to the steady and continuous development of Scene-15, MIT Indoor-67, SUN-397 and Place datasets [58].\u2022Transfer learning from the large scale annotated natural image datasets (ImageNet) to CADe problems has been consistently beneficial in our experiments. This sheds some light on cross-dataset CNN learning in the medical image domain, e.g., the union of the ILD [37] and LTRC datasets [64], as suggested in this paper.\u2022Finally, applications of off-the-shelf deep CNN image features to CADe problems can be improved by either exploring the performance-complementary properties of hand-crafted features [10, 9, 12], or by training CNNs from scratch and better fine-tuning CNNs on the target medical image dataset, as evaluated in this paper.", "rationale": "Transfer learning was shown to be beneficial in the paper's experiments, as seen by the differences in performance between AlexNet-TL/GoogLeNet-TL and their non-transfer learning counterparts."}]], "composition": ["ImageNet has more than 1.2 million images and about 1000 classes.", "The paper uses AlexNet, CifarNet, and GoogLeNet with various numbers of parameters.", "The ILD dataset has 905 image slices from 120 patients.", "The six classes are healthy, emphysema, ground glass, fibrosis, micronodules, and consolidation.", "The authors transformed every gray-scale axial CT image using the three CT windows of lung window range [-1400, -200HU], high-attenuation range [-160, 240HU], and low-attenuation range [-1400; -950HU], then encoded the transformed images into RGB images.", "The model learns very small weights in the filters for such areas.", "The authors reduced the filter size and stride of the two models because the input size used was smaller than what the original models were trained on.", "CifarNet was a CNN model that was used for the object recognition task using the Cifar10 dataset.", "CifarNet, AlexNet-ImNet and GoogLeNet-RI-H were the models that had the worst results.", "The model suffers from over-fitting, as it is a very complex model but it does not have enough training data for training.", "LOO performs better than five-fold cross validation.", "A process consisting of deconvolution, back-propagation with convolution, and un-pooling from the activation maps of the pooling units was used to reconstruct the original ILD images.", "Transfer learning was shown to be beneficial in the paper's experiments, as seen by the differences in performance between AlexNet-TL/GoogLeNet-TL and their non-transfer learning counterparts."], "Is_figure_in_evidence": [true, true, true, true, true, true, true, true, true, true, false, true, true], "Is_table_in_evidence": [false, true, false, false, false, false, false, false, false, false, true, false, false], "question_key": ["943", "946", "952", "953", "954", "955", "956", "957", "959", "960", "962", "963", "964"], "passages": ["Tremendous progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets (i.e. ImageNet [1, 2]) and the recent revival of deep convolutional neural networks (CNN) [3, 4].For data-driven learning, large-scale well-annotated datasets with representative data distribution characteristics are crucial to learning more accurate or generalizable models [5, 4].Unlike previous image datasets used in computer vision, ImageNet [1] offers a very comprehensive database of more than 1.2 million categorized natural images of 1000+ classes.The CNN models trained upon this database serve as the backbone for significantly improving many object detection and image segmentation problems using other datasets [6, 7], e.g., PASCAL [8] and medical image categorization [9, 10, 11, 12].However, there exists no large-scale annotated medical image dataset comparable to ImageNet, as data acquisition is difficult, and quality annotation is costly.", "There are currently three major techniques that successfully employ CNNs to medical image classification: 1) training the \u201cCNN from scratch\u201d [13, 14, 15, 16, 17]; 2) using \u201coff-the-shelf CNN\u201d features (without retraining the CNN) as complementary information channels to existing hand-crafted image features, for Chest X-rays [10] and CT lung nodule identification [9, 12]; and 3) performing unsupervised pre-training on natural or medical images and fine-tuning on medical target images using CNN or other types of deep learning models [18, 19, 20, 21].A decompositional 2.5D view resampling and an aggregation of random view classification scores are used to eliminate the \u201ccurse-of-dimensionality\u201d issue in [22], in order to acquire a sufficient number of training image samples.", "Previous studies have analyzed three-dimensional patch creation for LN detection [23, 24], atlas creation from chest CT [25] and the extraction of multi-level image features [26, 27].At present, there are several extensions or variations of the decompositional view representation introduced in [22, 28], such as: using a novel vessel-aligned multi-planar image representation for pulmonary embolism detection [29], fusing unregistered multiview for mammogram analysis [16] and classifying pulmonary peri-fissural nodules via an ensemble of 2D views [12].", "Although natural images and medical images differ significantly, conventional image descriptors developed for object recognition in natural images, such as the scale-invariant feature transform (SIFT) [30] and the histogram of oriented gradients (HOG) [31], have been widely used for object detection and segmentation in medical image analysis. Recently, ImageNet pre-trained CNNs have been used for chest pathology identification and detection in X-ray and CT modalities [10, 9, 12].They have yielded the best performance results by integrating low-level image features (e.g., GIST [32], bag of visual words (BoVW) and bag-of-frequency [12]). However, the fine-tuning of an ImageNet pre-trained CNN model on medical image datasets has not yet been exploited.", "In this paper, we exploit three important, but previously under-studied factors of employing deep convolutional neural networks to computer-aided detection problems.Particularly, we explore and evaluate different CNN architectures varying in width (ranging from 5 thousand to 160 million parameters) and depth (various numbers of layers), describe the effects of varying dataset scale and spatial image context on performance, and discuss when and why transfer learning from pre-trained ImageNet CNN models can be valuable. We further verify our hypothesis by inheriting and adapting rich hierarchical image features [5, 33] from the large-scale ImageNet dataset for computer aided diagnosis (CAD). We also explore CNN architectures of the most studied seven-layered \u201cAlexNet-CNN\u201d [4], a shallower \u201cCifar-CNN\u201d [22], and a much deeper version of \u201cGoogLeNet-CNN\u201d [33] (with our modifications on CNN structures). This study is partially motivated by recent studies [34, 35] in computer vision. The thorough quantitative analysis and evaluation on deep CNN [34] or sparsity image coding methods [35] elucidate the emerging techniques of the time and provide useful suggestions for their future stages of development, respectively.", "Two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification are studied in this work. On mediastinal LN detection, we surpass all currently reported results.We obtain 86%percent8686\\%86 % sensitivity on 3 false positives (FP) per patient, versus the prior state-of-art sensitivities of 78%percent7878\\%78 % [36] (stacked shallow learning) and 70%percent7070\\%70 % [22] (CNN), as prior state-of-the-art. For the first time, ILD classification results under the patient-level five-fold cross-validation protocol (CV5) are investigated and reported. The ILD dataset [37] contains 905 annotated image slices with 120 patients and 6 ILD labels. Such sparsely annotated datasets are generally difficult for CNN learning, due to the paucity of labeled instances.", "Evaluation protocols and details are critical to deriving significant empirical findings [34].Our experimental results suggest that different CNN architectures and dataset re-sampling protocols are critical for the LN detection tasks where the amount of labeled training data is sufficient and spatial contexts are local.Since LN images are more flexible than ILD images with respect to resampling and reformatting, LN datasets may be more readily augmented by such image transformations. As a result, LN datasets contain more training and testing data instances (due to data auugmentation) than ILD datasets. They nonetheless remain less comprehensive than natural image datasets, such as ImageNet.Fine-tuning ImageNet-trained models for ILD classification is clearly advantageous and yields early promising results, when the amount of labeled training data is highly insufficient and multi-class categorization is used, as opposed to the LN dataset\u2019s binary class categorization.Another significant finding is that CNNs trained from scratch or fine-tuned from ImageNet models consistently outperform CNNs that merely use off-the-shelf CNN features, in both the LN and ILD classification problems. We further analyze, via CNN activation visualizations, when and why transfer learning from non-medical to medical images in CADe problems can be valuable.", "We employ CNNs (with the characteristics defined above) to thoraco-abdominal lymph node (LN) detection (evaluated separately on the mediastinal and abdominal regions) and interstitial lung disease (ILD) detection. For LN detection, we use randomly sampled 2.5D views in CT [22]. We use 2D CT slices [38, 39, 40] for ILD detection.We then evaluate and compare CNN performance results.", "Until the detection aggregation approach [22, 41], thoracoabdominal lymph node (LN) detection via CADe mechanisms has yielded poor performance results. In [22], each 3D LN candidate produces up to 100 random 2.5D orthogonally sampled images or views which are then used to train an effective CNN model. The best performance on abdominal LN detection is achieved at 83%percent8383\\%83 % recall on 3FP per patient [22], using a \u201cCifar-10\u201d CNN. Using the thoracoabdominal LN detection datasets [22], we aim to surpass this CADe performance level, by testing different CNN architectures, exploring various dataset re-sampling protocols, and applying transfer learning from ImageNet pre-trained CNN models.", "Interstitial lung disease (ILD) comprises more than 150 lung diseases affecting the interstitium, which can severely impair the patient\u2019s ability to breathe. Gao et al. [40] investigate the ILD classification problem in two scenarios: 1) slice-level classification: assigning a holistic two-dimensional axial CT slice image with its occurring ILD disease label(s); and 2) patch-level classification: a/ sampling patches within the 2D ROIs (Regions of Interest provided by [37]), then b/ classifying patches into seven category labels ( six disease labels and one \u201chealthy\u201d label). Song et al. [38, 39] only address the second sub-task of patch-level classification under the \u201cleave-one-patient-out\u201d (LOO) criterion. By training on the moderate-to-small scale ILD dataset [37], our main objective is to exploit and benchmark CNN based ILD classification performances under the CV5 metric (which is more realistic and unbiased than LOO [38, 39] and hard-split [40]), with and without transfer learning.", "Thoracoabdominal Lymph Node Datasets.We use the publicly available dataset from [22, 41].There are 388 mediastinal LNs labeled by radiologists in 90 patient CT scans, and 595 abdominal LNs in 86 patient CT scans.To facilitate comparison, we adopt the data preparation protocol of [22], where positive and negative LN candidates are sampled with the fields-of-view (FOVs) of 30mm to 45mm, surrounding the annotated and detected LN centers (obtained by a candidate generation process).More precisely, [22, 41, 36] follow a coarse-to-fine CADe scheme, partially inspired by [42], which operates with \u223c100%similar-toabsentpercent100\\sim 100\\%\u223c 100 % detection recalls at the cost of approximately 40 false or negative LN candidates per patient scan.In this work, positive and negative LN candidate are first sampled up to 200 times with translations and rotations.Afterwards, negative LN samples are randomly re-selected at a lower rate close to the total number of positives.LN candidates are randomly extracted from fields-of-view (FOVs) spanning 35mm to 128mm in soft-tissue window [-100, 200HU]. This allows us to capture multiple spatial scales of image context [43, 44]).The samples are then rescaled to a 64\u00d764646464\\times 6464 \u00d7 64 pixel resolution via B-spline interpolation. A few examples of LNs with axial, coronal, and sagittal views encoded in RGB color images [22] are shown in Figure 1.", "Unlike the heart or the liver, lymph nodes have no pre-determined anatomic orientation. Hence, the purely random image resampling (with respect to scale, displacement and orientation) and reformatting (the axial, coronal, and sagittal views are in any system randomly resampled coordinates) is a natural choice, which also happens to yield high CNN performance. Although we integrate three channels of information from three orthogonal views for LN detection, the pixel-wise spatial correlations between or among channels are not necessary. The convolutional kernels in the lower level CNN architectures can learn the optimal weights to linearly combine the observations from the axial, coronal, and sagittal channels by computing their dot-products.Transforming axial, coronal, and sagittal representations to RGB also facilitates transfer learning from CNN models trained on ImageNet.", "This learning representation (i.e., \u201cbuilt-in CNN\u201d) is flexible, in that it naturally combines multiple sources or channels of information. In the recent literature [45], even heterogeneous class-conditional probability maps can be combined with raw images to improve performance.This set-up is similar to that of other works in computer vision, such as [46], where heterogeneous image information channels are jointly fed into the CNN convolutional layers for high-accuracy human parsing and segmentation. Finally, if there are correlations among CNN input channels, one may observe the corresponding correlated patterns in the learned filters.", "In summary, the assumption that there are or must be pixel-wise spatial correlations among input channels does not apply to the CNN model representation. For other medical imaging problems, such as pulmonary embolism detection [29], in which orientation can be constrained along the attached vessel axis, vessel-aligned multi-planar image representation (MPR) is more effective than randomly aligned MPR.", "Interstitial Lung Disease Dataset. We utilize the publicly available dataset of [37]. It contains 905 image slices from 120 patients, with six lung tissue types annotations containing at least one of the following: healthy (NM), emphysema (EM), ground glass (GG), fibrosis (FB), micronodules (MN) and consolidation (CD) (Figure 3).At the slice level, the objective is to classify the status of \u201cpresence/absence\u201d of any of the six ILD classes for an input axial CT slice [40].Characterizing an arbitrary CT slice against any possible ILD type, without any manual ROI (in contrast to [38, 39]), can be useful for large-scale patient screening.For slice-level ILD classification, we sampled the slices 12 times with random translations and rotations.After this, we balanced the numbers of CT slice samples for the six classes by randomly sampling several instances at various rates.For patch-based classification, we sampled up to 100 patches of size 64\u00d764646464\\times 6464 \u00d7 64 from each ROI.This dataset is divided into five folds with disjoint patient subsets.The average number of CT slices (training instances) per fold is small, as shown in Table I.Slice-level ILD classification is a very challenging task where CNN models need to learn from very small numbers of training examples and predict ILD labels on unseen patients.", "In the publicly available ILD dataset, very few CT slices are labeled as normal or healthy. The remaining CT slices cannot be simply classified as normal, because many ILD disease regions or slices have not yet been labeled. ILD [37] is a partially labeled database; this is one of its main limitations. Research is being conducted to address this issue. In particular,[47] has proposed to fully label the ILD dataset pixel-wise via proposed segmentation label propagation.", "To leverage the CNN architectures designed for color images and to transfer CNN parameters pre-trained on ImageNet, we transform all gray-scale axial CT slice images via three CT window ranges: lung window range [-1400, -200HU], high-attenuation range [-160, 240HU], and low-attenuation range [-1400; -950HU]. We then encode the transformed images into RGB channels (to be aligned with the input channels of CNN models [4, 33] pre-trained from natural image datasets [1]). The low-attenuation CT window is useful for visualizing certain texture patterns of lung diseases (especially emphysema). The usage of different CT attenuation channels improves classification results over the usage of a single CT windowing channel, as demonstrated in [40]. More importantly, these CT windowing processes do not depend on the lung segmentation, which instead is directly defined in the CT HU space. Figure 4 shows a representative example of lung, high-attenuation, and low-attenuation CT windowing for an axis lung CT slice.", "As observed in [40], lung segmentation is crucial to holistic slice-level ILD classification. We empirically compare performance in two scenarios with a rough lung segmentation111This can be achieved by segmenting the lung using simple label-fusion methods [48]. In the first case, we overlay the target image slice with the average lung mask among the training folds. In the second, we perform simple morphology operations to obtain the lung boundary. In order to retain information from the inside of the lung, we apply Gaussian smoothing to the regions outside of the lung boundary. There is no significant difference between two setups. Due to the high precision of CNN based image processing, highly accurate lung segmentation is not necessary . The localization of ILD regions within the lung is simultaneously learned through selectively weighted CNN reception fields in the deepest convolutional layers during the classification based CNN training [49, 50].Some areas outside of the lung appear in both healthy or diseased images. CNN training learns to ignore them by setting very small filter weights around the corresponding regions (Figure 13). This observation is validated by [40].", "In this study, we explore, evaluate and analyze the influence of various CNN Architectures, dataset characteristics (when we need more training data or better models for object detection [51]) and CNN transfer learning from non-medical to medical image domains.These three key elements of building effective deep CNN models for CADe problems are described below.", "We mainly explore three convolutional neural network architectures (CifarNet [5, 22], AlexNet [4] and GoogLeNet [33]) with different model training parameter values.The current deep learning models [22, 52, 53] in medical image tasks are at least 2\u223c5similar-to252\\sim 52 \u223c 5 orders of magnitude smaller than even AlexNet [4].More complex CNN models [22, 52] have only about 150K or 15K parameters.Roth et al. [22] adopt the CNN architecture tailored to the Cifar-10 dataset [5] and operate on image windows of 32\u00d732\u00d733232332\\times 32\\times 332 \u00d7 32 \u00d7 3 pixels for lymph node detection, while the simplest CNN in [54] has only one convolutional, pooling, and FC layer, respectively.", "We use CifarNet [5] as used in [22] as a baseline for the LN detection.AlexNet [4] and GoogLeNet [33] are also modified to evaluate these state-of-the-art CNN architecture from ImageNet classification task [2] to our CADe problems and datasets.A simplified illustration of three CNN architectures exploited is shown in Figure 5.CifarNet always takes 32\u00d732\u00d733232332\\times 32\\times 332 \u00d7 32 \u00d7 3 image patches as input while AlexNet and GoogLeNet are originally designed for the fixed image dimension of 256\u00d7256\u00d732562563256\\times 256\\times 3256 \u00d7 256 \u00d7 3 pixels.We also reduced the filter size, stride and pooling parameters of AlexNet and GoogLeNet to accommodate a smaller input size of 64\u00d764\u00d736464364\\times 64\\times 364 \u00d7 64 \u00d7 3 pixels.We do so to produce and evaluate \u201csimplified\u201d AlexNet and GoogLeNet versions that are better suited to the smaller scale training datasets common in CADe problems.Throughout the paper, we refer to the models as CifarNet (32x32) or CifarNet (dropping 32x32); AlexNet (256x256) or AlexNet-H (high resolution); AlexNet (64x64) or AlexNet-L (low resolution); GoogLeNet (256x256) or GoogLeNet-H and GoogLeNet (64x64) or GoogLeNet-L (dropping 3 since all image inputs are three channels).", "CifarNet, introduced in [5], was the state-of-the-art model for object recognition on the Cifar10 dataset, which consists of 32\u00d732323232\\times 3232 \u00d7 32 images of 10 object classes.The objects are normally centered in the images.Some example images and class categories from the Cifar10 dataset are shown in Figure 7.CifarNet has three convolution layers, three pooling layers, and one fully-connected layer.This CNN architecture, also used in [22] has about 0.15 million free parameters.We adopt it as a baseline model for the LN detection.", "The AlexNet architecture was published in [4], achieved significantly improved performance over the other non-deep learning methods for ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012.This success has revived the interest in CNNs [3] in computer vision.ImageNet consists of 1.2 million 256\u00d7256256256256\\times 256256 \u00d7 256 images belonging to 1000 categories.At times, the objects in the image are small and obscure, and thus pose more challenges for learning a successful classification model.More details about the ImageNet dataset will be discussed in Sec. III-B.AlexNet has five convolution layers, three pooling layers, and two fully-connected layers with approximately 60 million free parameters.AlexNet is our default CNN architecture for evaluation and analysis in the remainder of the paper.", "The GoogLeNet model proposed in [33], is significantly more complex and deep than all previous CNN architectures.More importantly, it also introduces a new module called \u201cInception\u201d, which concatenates filters of different sizes and dimensions into a single new filter (refer to Figure 6).Overall, GoogLeNet has two convolution layers, two pooling layers, and nine \u201cInception\u201d layers.Each \u201cInception\u201d layer consists of six convolution layers and one pooling layer.An illustration of an \u201cInception\u201d layer (inception3a) from GoogLeNet is shown in Figure 6.GoogLeNet is the current state-of-the-art CNN architecture for the ILSVRC challenge, where it achieved 5.5% top-5 classification error on the ImageNet challenge, compared to AlexNet\u2019s 15.3% top-5 classification error.", "ImageNet [1] has more than 1.2 million 256\u00d7256256256256\\times 256256 \u00d7 256 images categorized under 1000 object class categories.There are more than 1000 training images per class.The database is organized according to the WordNet [55] hierarchy, which currently contains only nouns in 1000 object categories.The image-object labels are obtained largely through crowd-sourcing, e.g., Amazon Mechanical Turk, and human inspection.Some examples of object categories in ImageNet are \u201csea snake\u201d, \u201csandwich\u201d, \u201cvase\u201d, \u201cleopard\u201d, etc.ImageNet is currently the largest image dataset among other standard datasets for visual recognition.Indeed, the Caltech101, Caltech256 and Cifar10 dataset merely contain 60000 32\u00d732323232\\times 3232 \u00d7 32 images and 10 object classes.Furthermore, due to the large number (1000+) of object classes, the objects belonging to each ImageNet class category can be occluded, partial and small, relative to those in the previous public image datasets.This significant intra-class variation poses greater challenges to any data-driven learning system that builds a classifier to fit given data and generalize to unseen data.For comparison, some example images of Cifar10 dataset and ImageNet images in the \u201ctennis ball\u201d class category are shown in Figure 7.The ImageNet dataset is publicly available, and the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) has become the standard benchmark for large-scale object recognition.", "When learned from scratch, all the parameters of CNN models are initialized with random Gaussian distributions and trained for 30 epochs with the mini-batch size of 50 image instances.Training convergence can be observed within 30 epochs. The other hyperparameters are momentum: 0.9; weight decay: 0.0005; (base) learning rate: 0.01, decreased by a factor of 10 at every 10 epochs. We use the Caffe framework [56] and NVidia K40 GPUs to train the CNNs.", "AlexNet and GoogLeNet CNN models can be either learned from scratch or fine-tuned from pre-trained models. Girshick et al. [6] find that, by applying ImageNet pre-trained ALexNet to PASCAL dataset [8], performances of semantic 20-class object detection and segmentation tasks significantly improve over previous methods that use no deep CNNs. AlexNet can be fine-tuned on the PASCAL dataset to surpass the performance of the ImageNet pre-trained AlexNet, although the difference is not as significant as that between the CNN and non-CNN methods. Similarly, [57, 58] also demonstrate that better performing deep models are learned via CNN transfer learning from ImageNet to other datasets of limited scales.", "Our hypothesis on CNN parameter transfer learning is the following: despite the disparity between natural images and natural images, CNNs comprehensively trained on the large scale well-annotated ImageNet may still be transferred to make medical image recognition tasks more effective. Collecting and annotating large numbers of medical images still poses significant challenges. On the other hand, the mainstream deep CNN architectures (e.g., AlexNet and GoogLeNet) contain tens of millions of free parameters to train, and thus require sufficiently large numbers of labeled medical images.", "For transfer learning, we follow the approach of [57, 6] where all CNN layers except the last are fine-tuned at a learning rate 10 times smaller than the default learning rate. The last fully-connected layer is random initialized and freshly trained, in order to accommodate the new object categories in our CADe applications. Its learning rate is kept at the original 0.01. We denote the models with random initialization or transfer learning as AlexNet-RI and AlexNet-TL, and GoogLeNet-RI and GoogLeNet-TL. We found that the transfer learning strategy yields the best performance results. Determining the optimal learning rate for different layers is challenging, especially for very deep networks such as GoogLeNet.", "We also perform experiments using \u201coff-the-shelf\u201d CNN features of AlexNet pre-trained on ImageNet and training only the final classifier layer to complete the new CADe classification tasks. Parameters in the convolutional and fully connected layers are fixed and are used as deep image extractors, as in [10, 9, 12]. We refer to this model as AlexNet-ImNet in the remainder of the paper. Note that [10, 9, 12] train support vector machines and random forest classifiers using ImageNet pre-trained CNN features.Our simplified implementation is intended to determine whether fine-tuning the \u201cend-to-end\u201d CNN network is necessary to improve performance, as opposed to merely training the final classification layer. This is a slight modification from the method described in [10, 9, 12].", "Finally, transfer learning in CNN representation, as empirically verified in previous literature [59, 60, 61, 11, 62], can be effective in various cross-modality imaging settings (RGB images to depth images [59, 60], natural images to general CT and MRI images [11], and natural images to neuroimaging [61] or ultrasound [62] data). More thorough theoretical studies on cross-modality imaging statistics and transferability will be needed for future studies.", "In this section, we evaluate and compare the performances of nine CNN model configurations (CifarNet, AlexNet-ImNet,AlexNet-RI-H, AlexNet-TL-H, AlexNet-RI-L, GoogLeNet-RI-H, GoogLeNet-TL-H, GoogLeNet-RI-L and combined) on two important CADe problems using publicly available datasets [22, 41, 37].", "We train and evaluate CNNs using three-fold cross-validation (folds are split into disjoint sets of patients), with the different CNN architectures described above.In testing, each LN candidate has multiple random 2.5D views tested by CNN classifiers to generate LN class probability scores.We follow the random view aggregation by averaging probabilities, as in [22].", "We first sample the LN image patches at a 64\u00d764646464\\times 6464 \u00d7 64 pixel resolution. We then up-sample the 64\u00d764646464\\times 6464 \u00d7 64 pixel LN images via bi-linear interpolation to 256\u00d7256256256256\\times 256256 \u00d7 256 pixels, in order to accommodate AlexNet-RI-L, AlexNet-TL-H, GoogLeNet-RI-H and GoogLeNet-TL-H. For the modified AlexNet-RI-L at (64\u00d764646464\\times 6464 \u00d7 64) pixel resolution, we reduce the number of first layer convolution filters from 96 to 64 and reduce the stride from 4 to 2. For the modified GoogLeNet-RI (64\u00d764646464\\times 6464 \u00d7 64), we decrease the number of first layer convolution filters from 64 to 32, the pad size from 3 to 2, the kernel size from 7 to 5, stride from 2 to 1 and the stride of the subsequent pooling layer from 2 to 1. We slightly reduce the number of convolutional filters in order to accommodate the smaller input image sizes of target medical image datasets [22, 37], while preventing over-fitting. This eventually improves performance on patch-based classification. CifarNet is used in [22] to detect LN samples of 32\u00d732\u00d733232332\\times 32\\times 332 \u00d7 32 \u00d7 3 images. For consistency purposes, we down-sample 64\u00d764\u00d736464364\\times 64\\times 364 \u00d7 64 \u00d7 3 resolution LN sample images to the dimension of 32\u00d732\u00d733232332\\times 32\\times 332 \u00d7 32 \u00d7 3.", "Results for lymph node detection in the mediastinum and abdomen are reported in Table II.FROC curves are illustrated in Figure 8.The area-under-the-FROC-curve (AUC) and true positive rate (TPR, recall or sensitivity) at three false positives per patient (TPR/3FP) are used as performance metrics.Of the nine investigated CNN models, CifarNet, AlexNet-ImNet and GoogLeNet-RI-H generally yielded the least competitive detection accuracy results.Our LN datasets are significantly more complex (i.e., display much larger within-class appearance variations), especially due to the extracted fields-of-view (FOVs) of (35mm-128mm) compared to (30mm-45mm) in [22], where CifarNet is also employed.In this experiment, CifarNet is under-trained with respect to our enhanced LN datasets, due to its limited input resolution and parameter complexity.The inferior performance of AlexNet-ImNet implies that using the pre-trained ImageNet CNNs alone as \u201coff-the-shelf\u201d deep image feature extractors may not be optimal or adequate for mediastinal and abdominal LN detection tasks.To complement \u201coff-the-shelf\u201d CNN features, [10, 9, 12] all add and integrate various other hand-crafted image features as hybrid inputs for the final CADe classification.", "GoogLeNet-RI-H performs poorly, as it is susceptible to over-fitting. No sufficient data samples are available to train GoogLeNet-RI-H with random initialization.Indeed, due to GoogLeNet-RI-H\u2019s complexity and 22-layer depth, million-image datasets may be required to properly train this model.However, GoogLeNet-TL-H significantly improves upon GoogLeNet-RI-H (0.81 versus 0.61 TPR/3FP in mediastinum; 0.70 versus 0.48 TPR/3FP in abdomen). This indicates that transfer learning offers a much better initialization of CNN parameters than random initialization. Likewise, AlexNet-TL-H consistently outperforms AlexNet-RI-H, though by smaller margins (0.81 versus 0.79 TPR/3FP in mediastinum; 0.69 versus 0.67 TPR/3FP in abdomen). This is also consistent with the findings reported for ILD detection in Table III and Figure 11.", "GoogLeNet-TL-H yields results similar to AlexNet-TL-H\u2019s for the mediastinal LN detection, and slightly outperforms Alex-Net-H for abdominal LN detection.AlexNet-RI-H exhibits less severe over-fitting than GoogLeNet-RI-H.We also evaluate a simple ensemble by averaging the probability scores from five CNNs: AlexNet-RI-H, AlexNet-TL-H, AlexNet-RI-H, GoogLeNet-TL-H and GoogLeNet-RI-L.This combined ensemble outputs the classification accuracies matching or slightly exceeding the best performing individual CNN models on the mediastinal or abdominal LN detection tasks, respectively.", "Many of our CNN models achieve notably better (FROC-AUC and TPR/3FP) results than the previous state-of-the-art models [36] for mediastinal LN detection: GoogLeNet-RI-L obtains an AUC=0.95 and 0.85 TPR/3FP, versus AUC=0.92 and 0.70 TPR/3FP [22] and 0.78 TPR/3FP [36] which uses stacked shallow learning.This difference lies in the fact that annotated lymph node segmentation masks are required to learn a mid-level semantic boundary detector [36], whereas CNN approaches only need LN locations for training [22]. In abdominal LN detection, [22] obtains the best trade-off between its CNN model complexity and sampled data configuration. Our best performing CNN model is GoogLeNet-TL (256x256) which obtains an AUC=0.92 and 0.70 TPR/3FP.", "The main difference between our dataset preparation protocol and that from [22] is a more aggressive extraction of random views within a much larger range of FOVs.The usage of larger FOVs to capture more image spatial context is inspired by deep zoom-out features [44] that improve semantic segmentation. This image sampling scheme contributes to our best reported performance results in both mediastinal LN detection (in this paper) and automated pancreas segmentation [45]. As shown in Figure 1, abdominal LNs are surrounded by many other similar looking objects.Meanwhile, mediastinal LNs are more easily distinguishable, due to the images\u2019 larger spatial contexts.Finally, from the perspective of the data-model trade-off: \u201cDo We Need More Training Data or Better Models?\u201d [51], more abdomen CT scans from distinct patient populations need to be acquired and annotated, in order to take full advantage of deep CNN models of high capacity.Nevertheless, deeper and wider CNN models (e.g., GoogLeNet-RI-L and GoogLeNet-TL-H versus Cifar-10 [22]) have shown improved results in the mediastinal LN detection.", "Figure 9 provides examples of misclassified lymph nodes (in axial view) (both false negatives (Left) and false positives(Right)), from the Abdomen and Mediastinum datasets. The overall reported LN detection results are clinically significant, as indicated in [63].", "The CNN models evaluated in this experiment are 1) AlexNet-RI (training from scratch on the ILD dataset with random initialization); 2) AlexNet-TL (with transfer learning from [4]); 3) AlexNet-ImNet: pre-trained ImageNet-CNN model [4] with only the last cost function layer retrained from random initialization, according to the six ILD classes (similar to [9] but without using additional hand-crafted non-deep feature descriptors, such as GIST and BoVW); 4) GoogLeNet-RI (random initialization); 5) GoogLeNet-TL (GoogLeNet with transfer learning from [33]). All ILD images (patches of 64\u00d764646464\\times 6464 \u00d7 64 and CT axial slices of 512\u00d7512512512512\\times 512512 \u00d7 512) are re-sampled to a fixed dimension of 256\u00d7256256256256\\times 256256 \u00d7 256 pixels.", "We evaluate the ILD classification task with five-fold CV on patient-level split, as it is more informative for real clinical performance than LOO.The classification accuracy rates for interstitial lung disease detection are shown in Table III.Two sub-tasks on ILD patch and slice classifications are conducted.In general, patch-level ILD classification is less challenging than slice-level classification, as far more data samples can be sampled from the manually annotated ROIs (up to 100 image patches per ROI), available from [37].From Table III, all five deep models evaluated obtain comparable results within the range of classification accuracy rates [0.74,0.76]0.740.76[0.74,0.76][ 0.74 , 0.76 ].Their averaged model achieves a slightly better accuracy of 0.79.", "F1-scores [38, 39, 54] and the confusion matrix (Table V) for patch-level ILD classification using GoogLeNet-TL under five-fold cross-validation (we denote as Patch-CV5) are also computed.F1-scores are reported on patch classification only (32\u00d732323232\\times 3232 \u00d7 32 pixel patches extracted from manual ROIs) [38, 39, 54], as shown in Table IV. Both [38] and [39] use the evaluation protocol of \u201cleave-one-patient-out\u201d (LOO), which is arguably much easier and not directly comparable to 10-fold CV [54] or our Patch-CV5.In this study, we classify six ILD classes by adding a consolidation (CD) class to five classes of healthy (normal - NM), emphysema (EM), ground glass (GG), fibrosis (FB), and micronodules (MN) in [38, 39, 54].Patch-CV10 [54] and Patch-CV5 report similar medium to high F-scores. This implies that the ILD dataset (although one of the mainstream public medical image datasets) may not adequately represent ILD disease CT lung imaging patterns, over a population of only 120 patients.Patch-CV5 yields higher F-scores than [54] and classifies the extra consolidation (CD) class.At present, the most pressing task is to drastically expand the dataset or to explore across-dataset deep learning on the combined ILD and LTRC datasets [64].", "Recently, Gao et al. [40] have argued that a new CADe protocol on holistic classification of ILD diseases directly, using axial CT slice attenuation patterns and CNN, may be more realistic for clinical applications.We refer to this as slice-level classification, as image patch sampling from manual ROIs can be completely avoided (hence, no manual ROI inputs will be provided).The experimental results in [40] are conducted with a patient-level hard split of 100 (training) and 20 (testing).The method\u2019s testing F-scores (i.e., Slice-Test) are given in Table IV.Note that the F-scores in [40] are not directly comparable to our results, due to different evaluation criteria. Only Slice-Test is evaluated and reported in [40], and we find that F-scores can change drastically from different rounds of the five-fold CV.", "While it is a more practical CADe scheme, slice-level CNN learning [40] is very challenging, as it is restricted to only 905 CT image slices with tagged ILD labels. We only benchmark the slice-level ILD classification results in this section. Even with the help of data augmentation (described in Sec. II), the classification accuracy of GoogLeNet-TL from Table III is only 0.57. However, transfer learning from ImageNet pre-trained model is consistently beneficial, as evidenced by AlexNet-TL (0.46) versus AlexNet-RI (0.44), and GoogLeNet-TL (0.57) versus GoogLeNet-RI (0.41). It especially prevents GoogLeNet from over-fitting on the limited CADe datasets. Finally, when the cross-validation is conducted by randomly splitting the set of all 905 CT axial slices into five folds, markedly higher F-scores are obtained (Slice-Random in Table IV). This further validates the claim that the dataset poorly generalizes ILDs for different patients. Figure 10 shows examples of misclassified ILD patches (in axial view), with their ground truth labels and inaccurately classified labels.", "No existing work has reached the performance requirements for a realistic clinical setting [40], in which simple ROI-guided image patch extraction and classification (which requires manual ROI selection by clinicians) is implemented. The main goal of this paper is to investigate the three factors (CNN architectures, dataset characteristics and transfer learning) that affect performance on a specific medical image analysis problem and to ultimately deliver clinically relevant results. For ILD classification, the most critical performance bottlenecks are the challenge of cross-dataset learning and the limited patient population size. We attempt to overcome these obstacles by merging the ILD [37] and LTRC datasets.Although the ILD [37] and LTRC datasets [64] (used in [19]) were generated and annotated separately, they contain many common disease labels. For instance, the ILD disease classes emphysema (EM), ground glass (GG), fibrosis (FB), and micronodules (MN) belong to both datasets, and thus can be jointly trained/tested to form a larger and unified dataset.", "Adapting fully convolutional CNN or FCNN to parse every pixel location in the ILD lung CT images or slices, or adapting other methods from CNN based semantic image segmentation using PASCAL or ImageNet, may improve accuracy and efficiency. However, current FCNN approaches [65, 66] lack adequate spatial resolution in their directly output label space. A segmentation label propagation method was recently proposed [47] to provide full pixel-wise labeling of the ILD data images.In this work, we sample image patches from the slice using the ROIs for the ILD provided in the dataset, in order to be consistent with previous methods in patch-level [38, 39, 54] and slice-level classification [40].", "In this work, we mainly focus on AlexNet and GoogLeNet. AlexNet is the first notably successful CNN architecture on the ImageNet challenge and has rekindled significant research interests on CNN. GoogLeNet is the state-of-the-art deep model, which has outperformed other notable models, such as AlexNet, OverFeat, and VGGNet [67, 68] in various computer vision benchmarks. Likewise, a reasonable assumption is that OverFeat and VGGNet may generate quantitative performance results ranked between AlexNet\u2019s and GoogLeNet\u2019s. For completeness, we include the Overfeat and VGGNet in the following evaluations, to bolster our hypothesis.", "OverFeat is described in [67] as an integrated framework for using CNN for classification, localization and detection.Its architecture is similar to that of AlexNet, but contains far more parameters (e.g., 1024 convolution filters in both \u201cconv4\u201d and \u201cconv5\u201d layers compared to 384 and 256 convolution kernels in the \u201cconv4\u201d and \u201cconv5\u201d layers of AlexNet), and operates more densely (e.g., smaller kernel size of 2 in \u201cpool2\u201d layer \u201cpool5\u201d compared to the kernel size 3 in \u201cpool2\u201d and \u201cpool5\u201d of AlexNet) on the input image. Overfeat is the winning model of the ILSVRC 2013 in detection and classification tasks.", "The VGGNet architecture is introduced in [68], where it is designed to significantly increase the depth of the existing CNN architectures with 16 or 19 layers. Very small 3\u00d73333\\times 33 \u00d7 3 size convolutional filters are used in all convolution layers with a convolutional stride of size 1, in order to reduce the number of parameters in deeper networks. Since VGGNet is substantially deeper than the other CNN models, VGGNet is more susceptible to the vanishing gradient problem [69, 70, 71]. Hence, the network may be more difficult to train. Training the network requires far more memory and computation time than AlexNet. We use the 16 layer variant as our default VGGNet model in our study.", "The classification accuracy results for ILD slice and patch level classification of five CNN architectures (CifarNet, AlexNet, Overfeat, VGGNet and GoogLeNet) are shown in Table VI. Based on the analysis in Sec. IV-B, transfer learning is only used for the slice level classification task. From Table VI, quantitative classification accuracy rates increase as the CNN model becomes more complex (CifarNet, AlexNet, Overfeat, VGGNet and GoogLeNet, in ascending order), for both ILD slice and patch level classification problems. The reported results validate our assumption that OverFeat\u2019s and VGGNet\u2019s performance levels fall between AlexNet\u2019s and GoogLeNet\u2018s (this observation is consistent with the computer vision findings). CifarNet is designed for images with smaller dimensions (32\u00d732323232\\times 3232 \u00d7 32 images), and thus is not catered to classification tasks involving 256\u00d7256256256256\\times 256256 \u00d7 256 images.", "To investigate the performance difference between five-fold cross-validation (CV) in Sec. IV-B and leave-one-patient-out (LOO) validation, this experiment is performed under the LOO protocol. By comparing results in Table III (CV-5) to those in Table VI (LOO), one can see that LOO\u2019s quantitative performances are remarkably better than CV-5\u2019s. For example, in ILD slice-level classification, the accuracy level drastically increases from 0.46 to 0.867 using AlexNet-TL, and from 0.57 to 0.902 for GoogLeNet-TL.", "CNN training is implemented with the Caffe [56] deep learning framework, using a NVidia K40 GPU on Ubuntu 14.04 Linux OS.All models are trained for up to 90 epochs with early stopping criteria, where a model snapshot with low validation loss is taken for the final model. Other hyper-parameters are fixed as follows: momentum: 0.9; weight decay: 0.0005; and a step learning rate schedule with base learning rate of 0.01, decreased by a factor of 10 every 30 epochs. The image batch size is set to 128, except for GoogLeNet\u2019s (64) and VGG-16\u2019s (32), which are the maximum batch sizes that can fit in the NVidia K40 GPU with 12GB of memory capacity. Table VII illustrates the training time and memory requirements of the five CNN architectures on ILD patch-based classification up to 90 epochs.", "Medical datasets are often \u201cbiased\u201d, in that the number of healthy samples is much larger than the number of diseased instances, or that the numbers of images per class are uneven. In ILD dataset, the number of fibrosis samples is about 3.5 times greater than the number of emphysema samples. The number of non-LNs is 3\u223c4similar-to343\\sim 43 \u223c 4 times greater than the number of LNs in lymph node detection. Different sampling or resampling rates are routinely applied to both ILD and LN detection to balance the data sample number or scale per class, as in[22]. We refer this as \u201cEqual Prior\u201d. If we use the same sampling rate, that will lead to a \u201cBiased Prior\u201d across different classes.", "Without loss of generality, after GoogLeNet is trained on the training sets under \u201cEqual\u201d or \u201cBiased\u201d priors, we compare its classification results on the balanced validation sets. Evaluating a classifier on a biased validation set will cause unfair assessment of its performance. For instance, a classifier that predicts every image patch as \u201cnon-LN\u201d will still achieve a 70%percent7070\\%70 % accuracy rate on a biased set with3.53.53.53.5 times as many non-LN samples as LN samples. The classification accuracy results of GoogLeNet trained under two configurations are shown in Table VIII. Overall, it achieves lower accuracy results when trained with a \u201cbiased prior\u201d in both tasks, and the accuracy difference for ILD patch-based classification is small. ", "In this section, we determine and analyze, via CNN visualization, the reasons for which transfer learning is beneficial to achieve better performance on CAD applications.", "Thoracoabdominal LN Detection.In Figure 12, the first layer convolution filters from five different CNN architectures are visualized.We notice that without transfer learning [57, 6], somewhat blurry filters are learned (AlexNet-RI (256x256), AlexNet-RI (64x64), GoogLeNet-RI (256x256) and GoogLeNet-RI (64x64)).However, in AlexNet-TL (256x256), many higher orders of contrast- or edge-preserving patterns (that enable capturing image appearance details) are evidently learned through fine-tuning from ImageNet.With a smaller input resolution, AlexNet-RI (64x64) and GoogLeNet-RI (64x64) can learn image contrast filters to some degree; whereas, GoogLeNet-RI (256x256) and AlexNet-RI (256x256) have over-smooth low-level filters throughout.", "ILD classification. We focus on analyzing visual CNN optimization traces and activations from the ILD dataset, as its slice-level setting is most similar to ImageNet\u2019s. Indeed, both datasets use full-size images. The traces of the training loss, validation loss and validation accuracy of AlexNet-RI and AlexNet-TL, are shown in Figure 11. For AlexNet-RI in Figure 11 (a), the training loss significantly decreases as the number of training epochs increases, while the validation loss notably increases and the validation accuracy does not improve much before reaching a plateau. With transfer learning and fine-tuning, much better and consistent performances of training loss, validation loss and validation accuracy traces are obtained (see Figure 11 (b)). We begin the optimization problem \u2013 that of fine-tuning the ImageNet pre-trained CNN to classify a comprehensive set of images \u2013 by initializing the parameters close to an optimal solution.One could compare this process to making adults learn to classify ILDs, as opposed to babies.During the process, the validation loss, having remained at lower values throughout, achieves higher final accuracy levels than the validation loss on a similar problem with random initialization.Meanwhile, the training losses in both cases decrease to values near zero. This indicates that both AlexNet-RI and AlexNet-TL over-fit on the ILD dataset, due to its small instance size. The quantitative results in Table III indicate that AlexNet-TL and GoogLeNet-TL have consistently better classification accuracies than AlexNet-RI and GoogLeNet-RI, respectively.", "The last pooling layer (pool-5) activation maps of the ImageNet pre-trained AlexNet [4] (analogical to AlexNet-ImNet) and AlexNet-TL, obtained by processing two input images of Figure 2 (b,c), are shown in Figure 13 (a,b).The last pooling layer activation map summarizes the entire input image by highlighting which relative locations or neural reception fields relative to the image are activated. There are a total of 256 (6x6) reception fields in AlexNet [4].Pooling units where the relative image location of the disease region is present in the image are highlighted with green boxes.Next, we reconstruct the original ILD images using the process of de-convolution, back-propagating with convolution and un-pooling from the activation maps of the chosen pooling units [72]. From the reconstructed images (Figure 13 bottom), we observe that with fine-tuning, AlexNet-TL detects and localizes objects of interest (ILD disease regions depicted in in Figure 2 (b) and (c)) better than AlexNet-ImNet. The filters shown in Figure 13 that better localize regions on the input images (Figure 2 (b) and (c)) respectively, produce relatively higher activations (in the top 5%) among all 512 reception field responses in the fine-tuned AlexNet-TL model. As observed in [73], the final CNN classification score can not be driven solely by a single strong activation in the receptions fields, but often by a sparse set of high activations (i.e., varying selective or sparse activations per input image).", "We summarize our findings as follows.", "\u2022Deep CNN architectures with 8, even 22 layers [4, 33], can be useful even for CADe problems where the available training datasets are limited. Previously, CNN models used in medical image analysis applications have often been 2\u223c5similar-to252\\sim 52 \u223c 5 orders of magnitude smaller.\u2022The trade-off between using better learning models and using more training data [51] should be carefully considered when searching for an optimal solution to any CADe problem (e.g., mediastinal and abdominal LN detection).\u2022Limited datasets can be a bottleneck to further advancement of CADe. Building progressively growing (in scale), well annotated datasets is at least as crucial as developing new algorithms. This has been accomplished, for instance, in the field of computer vision. The well-known scene recognition problem has made tremendous progress, thanks to the steady and continuous development of Scene-15, MIT Indoor-67, SUN-397 and Place datasets [58].\u2022Transfer learning from the large scale annotated natural image datasets (ImageNet) to CADe problems has been consistently beneficial in our experiments. This sheds some light on cross-dataset CNN learning in the medical image domain, e.g., the union of the ILD [37] and LTRC datasets [64], as suggested in this paper.\u2022Finally, applications of off-the-shelf deep CNN image features to CADe problems can be improved by either exploring the performance-complementary properties of hand-crafted features [10, 9, 12], or by training CNNs from scratch and better fine-tuning CNNs on the target medical image dataset, as evaluated in this paper.", "In this paper, we exploit and extensively evaluate three important, previously under-studied factors on deep convolutional neural networks (CNN) architecture, dataset characteristics, and transfer learning.We evaluate CNN performance on two different computer-aided diagnosis applications: thoraco-abdominal lymph node detection and interstitial lung disease classification.The empirical evaluation, CNN model visualization, CNN performance analysis, and conclusive insights can be generalized to the design of high performance CAD systems for other medical imaging tasks."], "figure_types": {"a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/10-Figure10-1.png": "photograph(s)", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/10-TableVI-1.png": "table", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/10-TableVII-1.png": "table", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/11-TableVIII-1.png": "table", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/12-Figure11-1.png": "plot", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/12-Figure12-1.png": "other", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/13-Figure13-1.png": "photograph(s)", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/3-Figure1-1.png": "photograph(s)", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/3-Figure2-1.png": "photograph(s)", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/4-Figure3-1.png": "photograph(s)", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/4-Figure4-1.png": "photograph(s)", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/5-Figure5-1.png": "schematic", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/5-Figure6-1.png": "schematic", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/5-Figure7-1.png": "photograph(s)", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/7-Figure8-1.png": "plot", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/7-TableII-1.png": "table", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/8-Figure9-1.png": "photograph(s)", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/8-TableIV-1.png": "table", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/8-TableV-1.png": "table", "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f/9-TableIII-1.png": "table"}}, "1511.04587": {"paper_id": "paper_44", "title": "Accurate Image Super-Resolution Using Very Deep Convolutional Networks", "arxiv_url": "https://arxiv.org/abs/1511.04587", "s2orc_url": "https://www.semanticscholar.org/paper/b5f3e5d2912bedbcd9458952d664b08db6aed962", "all_figures_tables": {"b5f3e5d2912bedbcd9458952d664b08db6aed962/1-Figure1-1.png": "Figure 1: Our VDSR improves PSNR for scale factor \u00d72 on dataset Set5 in comparison to the state-of-the-art methods (SRCNN uses the public slower implementation using CPU). VDSR outperforms SRCNN by a large margin (0.87 dB).", "b5f3e5d2912bedbcd9458952d664b08db6aed962/3-Figure2-1.png": "Figure 2: Our Network Structure. We cascade a pair of layers (convolutional and nonlinear) repeatedly. An interpolated low-resolution (ILR) image goes through layers and transforms into a high-resolution (HR) image. The network predicts a residual image and the addition of ILR and the residual gives the desired output. We use 64 filters for each convolutional layer and some sample feature maps are drawn for visualization. Most features after applying rectified linear units (ReLu) are zero.", "b5f3e5d2912bedbcd9458952d664b08db6aed962/4-Table1-1.png": "Table 1: Performance table (PSNR) for residual and non-residual networks (\u2018Set5\u2019 dataset, \u00d7 2). Residual networks rapidly approach their convergence within 10 epochs.", "b5f3e5d2912bedbcd9458952d664b08db6aed962/5-Figure3-1.png": "Figure 3: Depth vs Performance", "b5f3e5d2912bedbcd9458952d664b08db6aed962/6-Figure5-1.png": "Figure 5: (Top) Our results using a single network for all scale factors. Super-resolved images over all scales are clean and sharp. (Bottom) Results of Dong et al. [5] (\u00d73 model used for all scales). Result images are not visually pleasing. To handle multiple scales, existing methods require multiple networks.", "b5f3e5d2912bedbcd9458952d664b08db6aed962/6-Table2-1.png": "Table 2: Scale Factor Experiment. Several models are trained with different scale sets. Quantitative evaluation (PSNR) on dataset \u2018Set5\u2019 is provided for scale factors 2,3 and 4. Red color indicates that test scale is included during training. Models trained with multiple scales perform well on the trained scales.", "b5f3e5d2912bedbcd9458952d664b08db6aed962/7-Figure6-1.png": "Figure 6: Super-resolution results of \u201c148026\u201d (B100) with scale factor \u00d73. VDSR recovers sharp lines.", "b5f3e5d2912bedbcd9458952d664b08db6aed962/7-Figure7-1.png": "Figure 7: Super-resolution results of \u201c38092\u201d (B100) with scale factor \u00d73. The horn in the image is sharp in the result of VDSR."}, "referred_figures_tables": [["b5f3e5d2912bedbcd9458952d664b08db6aed962/5-Figure3-1.png"], ["b5f3e5d2912bedbcd9458952d664b08db6aed962/1-Figure1-1.png"]], "question_id": [1, 8], "question": ["Did increasing the network depth improved the results?", "What is the goal behind using a single model SR approach?"], "question_section": ["Abstract", "Introduction "], "question_trigger_sentence": ["We find increasing our network depth", "We propose a single-model SR approach"], "question_type": ["Shallow", "Deep/complex question"], "evidential_info": [[{"context": "However, we argue that increasing depth significantly boosts performance. We successfully use 20 weight layers (3\\times 3 for each layer). Our network is very deep (20 vs. 3 [6]) and information used for reconstruction (receptive field) is much larger (41\\times 41 vs. 13\\times 13).", "rationale": "The authors successfully used increased depth to boost performance."}, {"context": "In this section, we study three properties of our proposed method. First, we show that large depth is necessary for the task of SR. A very deep network utilizes more contextual information in an image and models complex functions with many nonlinear layers. We experimentally verify that deeper networks give better performances than shallow ones.", "rationale": "Performance increases as depth increases."}, {"context": "We now experimentally show that very deep networks significantly improve SR performance. We train and test networks of depth ranging from 5 to 20 (only counting weight layers excluding nonlinearity layers). In Figure 3, we show the results. In most cases, performance increases as depth increases. As depth increases, performance improves rapidly.", "rationale": "Deeper networks performed better than shallow networks."}, {"context": "In this work, we have presented a super-resolution method using very deep networks. Training a very deep network is hard due to a slow convergence rate. We use residual-learning and extremely high learning rates to optimize a very deep network fast. Convergence speed is maximized and we use gradient clipping to ensure the training stability. We have demonstrated that our method outperforms the existing method by a large margin on benchmarked images. We believe our approach is readily applicable to other image restoration problems such as denoising and compression artifact removal.", "rationale": "The authors successfully used a very deep network to outperform state-of-the-art methods."}], [{"context": "Scale As in most existing SR methods, SRCNN is trained for a single scale factor and is supposed to work only with the specified scale. Thus, if a new scale is on demand, a new model has to be trained. To cope with multiple scale SR (possibly including fractional factors), we need to construct individual single scale SR system for each scale of interest.", "rationale": "The single model approach should accept multiple scales for the input."}, {"context": "However, preparing many individual machines for all possible scenarios to cope with multiple scales is inefficient and impractical.In this work, we design and train a single network to handle multiple scale SR problem efficiently. This turns out to work very well. Our single machine is compared favorably to a single-scale expert for the given sub-task. For three scales factors (\u00d72,3,4\\times 2,3,4\u00d7 2 , 3 , 4), we can reduce the number of parameters by three-fold.", "rationale": "Having multiple models for each scale requires too much training and storing."}, {"context": "Scale Factor We propose a single-model SR approach. Scales are typically user-specified and can be arbitrary including fractions. For example, one might need smooth zoom-in in an image viewer or resizing to a specific dimension. Training and storing many scale-dependent models in preparation for all possible scenarios is impractical. We find a single convolutional network is sufficient for multi-scale-factor super-resolution.", "rationale": "Existing methods are only trained for a single scale, and adapting the method to another scale requires retraining."}, {"context": "Contribution In summary, in this work, we propose a highly accurate SR method based on a very deep convolutional network. Very deep networks converge too slowly if small learning rates are used. Boosting convergence rate with high learning rates lead to exploding gradients and we resolve the issue with residual-learning and gradient clipping. In addition, we extend our work to cope with multi-scale SR problem in a single network. Our method is relatively accurate and fast in comparison to state-of-the-art methods as illustrated in Figure 1.", "rationale": "The single model approach fixes the multi-scale problem."}]], "composition": ["Yes, it did boost performance.", "Existing methods are only trained for a single scale, so adapting them to other scales requires retraining. However, this would be impractical, so having a single model that accepts multiple scales would fix the problem."], "Is_figure_in_evidence": [true, true], "Is_table_in_evidence": [false, false], "question_key": ["967", "974"], "passages": ["We address the problem of generating a high-resolution (HR) image given a low-resolution (LR) image, commonly referred as single image super-resolution (SISR) [12], [8], [9]. SISR is widely used in computer vision applications ranging from security and surveillance imaging to medical imaging where more image details are required on demand.", "Many SISR methods have been studied in the computer vision community. Early methods include interpolation such as bicubic interpolation and Lanczos resampling [7] more powerful methods utilizing statistical image priors [20, 13] or internal patch recurrence [9].", "Currently, learning methods are widely used to model a mapping from LR to HR patches. Neighbor embedding [4, 15] methods interpolate the patch subspace. Sparse coding [25, 26, 21, 22] methods use a learned compact dictionary based on sparse signal representation. Lately, random forest [18] and convolutional neural network (CNN) [6] have also been used with large improvements in accuracy.", "Among them, Dong et al. [6] has demonstrated that a CNN can be used to learn a mapping from LR to HR in an end-to-end manner. Their method, termed SRCNN, does not require any engineered features that are typically necessary in other methods [25, 26, 21, 22] and shows the state-of-the-art performance.", "While SRCNN successfully introduced a deep learning technique into the super-resolution (SR) problem, we find its limitations in three aspects: first, it relies on the context of small image regions; second, training converges too slowly; third, the network only works for a single scale.", "In this work, we propose a new method to practically resolve the issues.", "Context We utilize contextual information spread over very large image regions. For a large scale factor, it is often the case that information contained in a small patch is not sufficient for detail recovery (ill-posed). Our very deep network using large receptive field takes a large image context into account.", "Convergence We suggest a way to speed-up the training: residual-learning CNN and extremely high learning rates. As LR image and HR image share the same information to a large extent, explicitly modelling the residual image, which is the difference between HR and LR images, is advantageous. We propose a network structure for efficient learning when input and output are highly correlated. Moreover, our initial learning rate is 104superscript10410^{4}10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT times higher than that of SRCNN [6]. This is enabled by residual-learning and gradient clipping.", "Scale Factor We propose a single-model SR approach. Scales are typically user-specified and can be arbitrary including fractions. For example, one might need smooth zoom-in in an image viewer or resizing to a specific dimension. Training and storing many scale-dependent models in preparation for all possible scenarios is impractical. We find a single convolutional network is sufficient for multi-scale-factor super-resolution.", "Contribution In summary, in this work, we propose a highly accurate SR method based on a very deep convolutional network. Very deep networks converge too slowly if small learning rates are used. Boosting convergence rate with high learning rates lead to exploding gradients and we resolve the issue with residual-learning and gradient clipping. In addition, we extend our work to cope with multi-scale SR problem in a single network. Our method is relatively accurate and fast in comparison to state-of-the-art methods as illustrated in Figure 1.", "SRCNN is a representative state-of-art method for deep learning-based SR approach. So, let us analyze and compare it with our proposed method.", "ModelSRCNN consists of three layers: patch extraction/representation, non-linear mapping and reconstruction. Filters of spatial sizes 9\u00d79999\\times 99 \u00d7 9, 1\u00d71111\\times 11 \u00d7 1, and 5\u00d75555\\times 55 \u00d7 5 were used respectively.", "In [6], Dong et al. attempted to prepare deeper models, but failed to observe superior performance after a week of training. In some cases, deeper models gave inferior performance. They conclude that deeper networks do not result in better performance (Figure 9).", "However, we argue that increasing depth significantly boosts performance. We successfully use 20 weight layers (3\u00d73333\\times 33 \u00d7 3 for each layer). Our network is very deep (20 vs. 3 [6]) and information used for reconstruction (receptive field) is much larger (41\u00d741414141\\times 4141 \u00d7 41 vs. 13\u00d713131313\\times 1313 \u00d7 13).", "TrainingFor training, SRCNN directly models high-resolution images. A high-resolution image can be decomposed into a low frequency information (corresponding to low-resolution image) and high frequency information (residual image or image details). Input and output images share the same low-frequency information. This indicates that SRCNN serves two purposes: carrying the input to the end layer and reconstructing residuals. Carrying the input to the end is conceptually similar to what an auto-encoder does. Training time might be spent on learning this auto-encoder so that the convergence rate of learning the other part (image details) is significantly decreased. In contrast, since our network models the residual images directly, we can have much faster convergence with even better accuracy.", "Scale As in most existing SR methods, SRCNN is trained for a single scale factor and is supposed to work only with the specified scale. Thus, if a new scale is on demand, a new model has to be trained. To cope with multiple scale SR (possibly including fractional factors), we need to construct individual single scale SR system for each scale of interest.", "However, preparing many individual machines for all possible scenarios to cope with multiple scales is inefficient and impractical.In this work, we design and train a single network to handle multiple scale SR problem efficiently. This turns out to work very well. Our single machine is compared favorably to a single-scale expert for the given sub-task. For three scales factors (\u00d72,3,4\\times 2,3,4\u00d7 2 , 3 , 4), we can reduce the number of parameters by three-fold.", "In addition to the aforementioned issues, there are some minor differences. Our output image has the same size as the input image by padding zeros every layer during training whereas output from SRCNN is smaller than the input. Finally, we simply use the same learning rates for all layers while SRCNN uses different learning rates for different layers in order to achieve stable convergence.", "For SR image reconstruction, we use a very deep convolutional network inspired by Simonyan and Zisserman [19]. The configuration is outlined in Figure 2. We use d\ud835\udc51ditalic_d layers where layers except the first and the last are of the same type: 64 filter of the size 3\u00d73\u00d76433643\\times 3\\times 643 \u00d7 3 \u00d7 64, where a filter operates on 3\u00d73333\\times 33 \u00d7 3 spatial region across 64 channels (feature maps). The first layer operates on the input image. The last layer, used for image reconstruction, consists of a single filter of size 3\u00d73\u00d76433643\\times 3\\times 643 \u00d7 3 \u00d7 64.", "The network takes an interpolated low-resolution image (to the desired size) as input and predicts image details. Modelling image details is often used in super-resolution methods [21, 22, 15, 3] and we find that CNN-based methods can benefit from this domain-specific knowledge.", "In this work, we demonstrate that explicitly modelling image details (residuals) has several advantages. These are further discussed later in Section 4.2.", "One problem with using a very deep network to predict dense outputs is that the size of the feature map gets reduced every time convolution operations are applied. For example, when an input of size (n+1)\u00d7(n+1)\ud835\udc5b1\ud835\udc5b1(n+1)\\times(n+1)( italic_n + 1 ) \u00d7 ( italic_n + 1 ) is applied to a network with receptive field size n\u00d7n\ud835\udc5b\ud835\udc5bn\\times nitalic_n \u00d7 italic_n, the output image is 1\u00d71111\\times 11 \u00d7 1.", "This is in accordance with other super-resolution methods since many require surrounding pixels to infer center pixels correctly. This center-surround relation is useful since the surrounding region provides more constraints to this ill-posed problem (SR). For pixels near the image boundary, this relation cannot be exploited to the full extent and many SR methods crop the result image.", "This methodology, however, is not valid if the required surround region is very big. After cropping, the final image is too small to be visually pleasing.", "To resolve this issue, we pad zeros before convolutions to keep the sizes of all feature maps (including the output image) the same. It turns out that zero-padding works surprisingly well. For this reason, our method differs from most other methods in the sense that pixels near the image boundary are also correctly predicted.", "Once image details are predicted, they are added back to the input ILR image to give the final image (HR). We use this structure for all experiments in our work.", "We now describe the objective to minimize in order to find optimal parameters of our model. Let \ud835\udc31\ud835\udc31{\\bf x}bold_x denote an interpolated low-resolution image and \ud835\udc32\ud835\udc32{\\bf y}bold_y a high-resolution image.Given a training dataset {\ud835\udc31(i),\ud835\udc32(i)}i=1N\\{{\\bf x}^{(i)},{\\bf y}^{(i)}\\}{}_{i=1}^{N}{ bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , bold_y start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT } start_FLOATSUBSCRIPT italic_i = 1 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT, our goal is to learn a model f\ud835\udc53fitalic_f that predicts values \ud835\udc32^=f(\ud835\udc31)^\ud835\udc32\ud835\udc53\ud835\udc31\\mathbf{\\hat{y}}=f(\\mathbf{x})over^ start_ARG bold_y end_ARG = italic_f ( bold_x ), where \ud835\udc32^^\ud835\udc32\\mathbf{\\hat{y}}over^ start_ARG bold_y end_ARG is an estimate of the target HR image. We minimize the mean squared error 12\u2016\ud835\udc32\u2212f(\ud835\udc31)\u2016212superscriptnorm\ud835\udc32\ud835\udc53\ud835\udc312\\frac{1}{2}||\\mathbf{y}-f(\\mathbf{x})||^{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG | | bold_y - italic_f ( bold_x ) | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTaveraged over the training set is minimized.", "Residual-Learning In SRCNN, the exact copy of the input has to go through all layers until it reaches the output layer. With many weight layers, this becomes an end-to-end relation requiring very long-term memory. For this reason, the vanishing/exploding gradients problem [2] can be critical. We can solve this problem simply with residual-learning.", "As the input and output images are largely similar, we define a residual image \ud835\udc2b=\ud835\udc32\u2212\ud835\udc31\ud835\udc2b\ud835\udc32\ud835\udc31{\\bf r}={\\bf y}-{\\bf x}bold_r = bold_y - bold_x, where most values are likely to be zero or small. We want to predict this residual image. The loss function now becomes 12\u2016\ud835\udc2b\u2212f(\ud835\udc31)\u2016212superscriptnorm\ud835\udc2b\ud835\udc53\ud835\udc312\\frac{1}{2}||\\mathbf{r}-f(\\mathbf{x})||^{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG | | bold_r - italic_f ( bold_x ) | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, where f(\ud835\udc31)\ud835\udc53\ud835\udc31f(\\bf{x})italic_f ( bold_x ) is the network prediction.", "In networks, this is reflected in the loss layer as follows.Our loss layer takes three inputs: residual estimate, network input (ILR image) and ground truth HR image. The loss is computed as the Euclidean distance between the reconstructed image (the sum of network input and output) and ground truth.", "Training is carried out by optimizing the regression objective using mini-batch gradient descent based on back-propagation (LeCun et al. [14]). We set the momentum parameter to 0.9. The training is regularized by weight decay (L2subscript\ud835\udc3f2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT penalty multiplied by0.0001).", "High Learning Rates for Very Deep NetworksTraining deep models can fail to converge in realistic limit of time. SRCNN [6] fails to show superior performance with more than three weight layers. While there can be various reasons, one possibility is that they stopped their training procedure before networks converged. Their learning rate 10\u22125superscript10510^{-5}10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT is too small for a network to converge within a week on a common GPU. Looking at Fig. 9 of [6], it is not easy to say their deeper networks have converged and their performances were saturated. While more training will eventually resolve the issue, but increasing depth to 20 does not seems practical with SRCNN.", "It is a basic rule of thumb to make learning rate high to boost training. But simply setting learning rate high can also lead to vanishing/exploding gradients [2]. For the reason, we suggest an adjustable gradient clipping for maximal boost in speed while suppressing exploding gradients.", "Adjustable Gradient ClippingGradient clipping is a technique that is often used in training recurrent neural networks [17]. But, to our knowledge, its usage is limited in training CNNs. While there exist many ways to limit gradients, one of the common strategies is to clip individual gradients to the predefined range[\u2212\u03b8,\u03b8]\ud835\udf03\ud835\udf03[-\\theta,\\theta][ - italic_\u03b8 , italic_\u03b8 ].", "With clipping, gradients are in a certain range. With stochastic gradient descent commonly used for training, learning rate is multiplied to adjust the step size. If high learning rate is used, it is likely that \u03b8\ud835\udf03\\thetaitalic_\u03b8 is tuned to be small to avoid exploding gradients in a high learning rate regime. But as learning rate is annealed to get smaller, the effective gradient (gradient multiplied by learning rate) approaches zero and training can take exponentially many iterations to converge if learning rate is decreased geometrically.", "For maximal speed of convergence, we clip the gradients to [\u2212\u03b8\u03b3,\u03b8\u03b3]\ud835\udf03\ud835\udefe\ud835\udf03\ud835\udefe[-\\frac{\\theta}{\\gamma},\\frac{\\theta}{\\gamma}][ - divide start_ARG italic_\u03b8 end_ARG start_ARG italic_\u03b3 end_ARG , divide start_ARG italic_\u03b8 end_ARG start_ARG italic_\u03b3 end_ARG ], where \u03b3\ud835\udefe\\gammaitalic_\u03b3 denotes the current learning rate. We find the adjustable gradient clipping makes our convergence procedure extremely fast. Our 20-layer network training is done within 4 hours whereas 3-layer SRCNN takes several days to train.", "Multi-Scale While very deep models can boost performance, more parameters are now needed to define a network. Typically, one network is created for each scale factor. Considering that fractional scale factors are often used, we need an economical way to store and retrieve networks.", "For this reason, we also train a multi-scale model. With this approach, parameters are shared across all predefined scale factors. Training a multi-scale model is straightforward. Training datasets for several specified scales are combined into one big dataset.", "Data preparation is similar to SRCNN [5] with some differences. Input patch size is now equal to the size of the receptive field and images are divided into sub-images with no overlap. A mini-batch consists of 64 sub-images, where sub-images from different scales can be in the same batch.", "We implement our model using the MatConvNet111http://www.vlfeat.org/matconvnet/ package [23].", "In this section, we study three properties of our proposed method. First, we show that large depth is necessary for the task of SR. A very deep network utilizes more contextual information in an image and models complex functions with many nonlinear layers. We experimentally verify that deeper networks give better performances than shallow ones.", "Second, we show that our residual-learning network converges much faster than the standard CNN. Moreover, our network gives a significant boost in performance.", "Third, we show that our method with a single network performs as well as a method using multiple networks trained for each scale. We can effectively reduce model capacity (the number of parameters) of multi-network approaches.", "Convolutional neural networks exploit spatially-local correlation by enforcing a local connectivity pattern between neurons of adjacent layers [1]. In other words, hidden units in layer m\ud835\udc5amitalic_m take as input a subset of units in layer m\u22121\ud835\udc5a1m-1italic_m - 1. They form spatially contiguous receptive fields.", "Each hidden unit is unresponsive to variations outside of the receptive field with respect to the input. The architecture thus ensures that the learned filters produce the strongest response to a spatially local input pattern.", "However, stacking many such layers leads to filters that become increasingly \u201cglobal\u201d (i.e. responsive to a larger region of pixel space). In other words, a filter of very large support can be effectively decomposed into a series of small filters.", "In this work, we use filters of the same size, 3\u00d7\\times\u00d73, for all layers. For the first layer, the receptive field is of size 3\u00d7\\times\u00d73. For the next layers, the size of the receptive field increases by 2 in both height and width. For depth D\ud835\udc37Ditalic_D network, the receptive field has size (2D+1)\u00d7(2D+1)2\ud835\udc3712\ud835\udc371(2D+1)\\times(2D+1)( 2 italic_D + 1 ) \u00d7 ( 2 italic_D + 1 ). Its size is proportional to the depth.", "In the task of SR, this corresponds to the amount of contextual information that can be exploited to infer high-frequency components. A large receptive field means the network can use more context to predict image details. As SR is an ill-posed inverse problem, collecting and analyzing more neighbor pixels give more clues. For example, if there are some image patterns entirely contained in a receptive field, it is plausible that this pattern is recognized and used to super-resolve the image.", "In addition, very deep networks can exploit high nonlinearities. We use 19 rectified linear units and our networks can model very complex functions with moderate number of channels (neurons). The advantages of making a thin deep network is well explained in Simonyan and Zisserman [19].", "We now experimentally show that very deep networks significantly improve SR performance. We train and test networks of depth ranging from 5 to 20 (only counting weight layers excluding nonlinearity layers). In Figure 3, we show the results. In most cases, performance increases as depth increases. As depth increases, performance improves rapidly.", "As we already have a low-resolution image as the input, predicting high-frequency components is enough for the purpose of SR. Although the concept of predicting residuals has been used in previous methods [21, 22, 26], it has not been studied in the context of deep-learning-based SR framework.", "In this work, we have proposed a network structure that learns residual images. We now study the effect of this modification to a standard CNN structure in detail.", "First, we find that this residual network converges much faster. Two networks are compared experimentally: the residual network and the standard non-residual network. We use depth 10 (weight layers) and scale factor 2. Performance curves for various learning rates are shown in Figure 4. All use the same learning rate scheduling mechanism that has been mentioned above.", "Second, at convergence, the residual network shows superior performance. In Figure 4, residual networks give higher PSNR when training is done.", "Another remark is that if small learning rates are used, networks do not converge in the given number of epochs. If initial learning rate 0.1 is used, PSNR of a residual-learning network reaches 36.90 within 10 epochs. But if 0.001 is used instead, the network never reaches the same level of performance (its performance is 36.52 after 80 epochs). In a similar manner, residual and non-residual networks show dramatic performance gaps after 10 epochs (36.90 vs. 27.42 for rate 0.1).", "In short, this simple modification to a standard non-residual network structure is very powerful and one can explore the validity of the idea in other image restoration problems where input and output images are highly correlated.", "Scale augmentation during training is a key technique to equip a network with super-resolution machines of multiple scales. Many SR processes for different scales can be executed with our multi-scale machine with much smaller capacity than that of single-scale machines combined.", "We start with an interesting experiment as follows: we train our network with a single scale factor strainsubscript\ud835\udc60trains_{\\text{train}}italic_s start_POSTSUBSCRIPT train end_POSTSUBSCRIPT and it is tested under another scale factor stestsubscript\ud835\udc60tests_{\\text{test}}italic_s start_POSTSUBSCRIPT test end_POSTSUBSCRIPT. Here, factors 2,3 and 4 that are widely used in SR comparisons are considered. Possible pairs (strainsubscript\ud835\udc60trains_{\\text{train}}italic_s start_POSTSUBSCRIPT train end_POSTSUBSCRIPT,stestsubscript\ud835\udc60tests_{\\text{test}}italic_s start_POSTSUBSCRIPT test end_POSTSUBSCRIPT) are tried for the dataset \u2018Set5\u2019 [15]. Experimental results are summarized in Table 2.", "Performance is degraded if strain\u2260stestsubscript\ud835\udc60trainsubscript\ud835\udc60tests_{\\text{train}}\\neq s_{\\text{test}}italic_s start_POSTSUBSCRIPT train end_POSTSUBSCRIPT \u2260 italic_s start_POSTSUBSCRIPT test end_POSTSUBSCRIPT. For scale factor 2, the model trained with factor 2 gives PSNR of 37.10 (in dB), whereas models trained with factor 3 and 4 give 30.05 and 28.13, respectively. A network trained over single-scale data is not capable of handling other scales. In many tests, it is even worse than bicubic interpolation, the method used for generating the input image.", "We now test if a model trained with scale augmentation is capable of performing SR at multiple scale factors. The same network used above is trained with multiple scale factors strain={2,3,4}subscript\ud835\udc60train234s_{\\text{train}}=\\{2,3,4\\}italic_s start_POSTSUBSCRIPT train end_POSTSUBSCRIPT = { 2 , 3 , 4 }. In addition, we experiment with the cases strain={2,3},{2,4},{3,4}subscript\ud835\udc60train232434s_{\\text{train}}=\\{2,3\\},\\{2,4\\},\\{3,4\\}italic_s start_POSTSUBSCRIPT train end_POSTSUBSCRIPT = { 2 , 3 } , { 2 , 4 } , { 3 , 4 } for more comparisons.", "We observe that the network copes with any scale used during training. When strain={2,3,4}subscript\ud835\udc60train234s_{\\text{train}}=\\{2,3,4\\}italic_s start_POSTSUBSCRIPT train end_POSTSUBSCRIPT = { 2 , 3 , 4 } (\u00d72,3,4\\times 2,3,4\u00d7 2 , 3 , 4 in Table 2), its PSNR for each scale is comparable to those achieved from the corresponding result of single-scale network: 37.06 vs. 37.10 (\u00d72absent2\\times 2\u00d7 2), 33.27 vs. 32.89 (\u00d73absent3\\times 3\u00d7 3), 30.95 vs. 30.86 (\u00d74absent4\\times 4\u00d7 4).", "Another pattern is that for large scales (\u00d73,4\\times 3,4\u00d7 3 , 4), our multi-scale network outperforms single-scale network: our model (\u00d72,3\\times 2,3\u00d7 2 , 3), (\u00d73,4\\times 3,4\u00d7 3 , 4) and (\u00d72,3,4\\times 2,3,4\u00d7 2 , 3 , 4) give PSNRs 33.22, 33.24 and 33.27 for test scale 3, respectively, whereas (\u00d73absent3\\times 3\u00d7 3) gives 32.89. Similarly, (\u00d72,4\\times 2,4\u00d7 2 , 4), (\u00d73,4\\times 3,4\u00d7 3 , 4) and (\u00d72,3,4\\times 2,3,4\u00d7 2 , 3 , 4) give 30.86, 30.94 and 30.95 (vs. 30.84 by \u00d74absent4\\times 4\u00d7 4 model), respectively. From this, we observe that training multiple scales boosts the performance for large scales.", "In this section, we evaluate the performance of our method on several datasets. We first describe datasets used for training and testing our method. Next, parameters necessary for training are given.", "After outlining our experimental setup, we compare our method with several state-of-the-art SISR methods.", "Training dataset Different learning-based methods use different training images. For example, RFL [18] has two methods, where the first one uses 91 images from Yang et al. [25] and the second one uses 291 images with the addition of 200 images from Berkeley Segmentation Dataset [16]. SRCNN [6] uses a very large ImageNet dataset.", "We use 291 images as in [18] for benchmark with other methods in this section. In addition, data augmentation (rotation or flip) is used. For results in previous sections, we used 91 images to train network fast, so performances can be slightly different.", "Test dataset For benchmark, we use four datasets. Datasets \u2018Set5\u2019 [15] and \u2018Set14\u2019 [26] are often used for benchmark in other works [22, 21, 5]. Dataset \u2018Urban100\u2019, a dataset of urban images recently provided by Huang et al. [11], is very interesting as it contains many challenging images failed by many of the existing methods. Finally, dataset \u2018B100\u2019, natural images in the Berkeley Segmentation Dataset used in Timofte et al. [22] and Yang and Yang [24] for benchmark, is also employed.", "We provide parameters used to train our final model. We use a network of depth 20. Training uses batches of size 64. Momentum and weight decay parameters are set to 0.9 and 0.00010.00010.00010.0001, respectively.", "For weight initialization, we use the method described in He et al. [10]. This is a theoretically sound procedure for networks utilizing rectified linear units (ReLu).", "We train all experiments over 80 epochs (9960 iterations with batch size 64). Learning rate was initially set to 0.1 and then decreased by a factor of 10 every 20 epochs. In total, the learning rate was decreased 3 times, and the learningis stopped after 80 epochs. Training takes roughly 4 hours on GPU Titan Z.", "For benchmark, we follow the publicly available framework of Huang et al. [21]. It enables the comparison of many state-of-the-art results with the same evaluation procedure.", "The framework applies bicubic interpolation to color components of an image and sophisticated models to luminance components as in other methods [4], [9], [26]. This is because human vision is more sensitive to details in intensity than in color.", "This framework crops pixels near image boundary. For our method, this procedure is unnecessary as our network outputs the full-sized image. For fair comparison, however, we also crop pixels to the same amount.", "We provide quantitative and qualitative comparisons. Compared methods are A+ [22], RFL[18], SelfEx [11] and SRCNN [5]. In Table 3, we provide a summary of quantitative evaluation on several datasets. Our methods outperform all previous methods in these datasets. Moreover, our methods are relatively fast. The public code of SRCNN based on a CPU implementation is slower than the code used by Dong et. al [6] in their paper based on a GPU implementation.", "In Figures 6 and 7, we compare our method with top-performing methods. In Figure 6, only our method perfectly reconstructs the line in the middle. Similarly, in Figure 7, contours are clean and vivid in our method whereas they are severely blurred or distorted in other methods.", "In this work, we have presented a super-resolution method using very deep networks. Training a very deep network is hard due to a slow convergence rate. We use residual-learning and extremely high learning rates to optimize a very deep network fast. Convergence speed is maximized and we use gradient clipping to ensure the training stability. We have demonstrated that our method outperforms the existing method by a large margin on benchmarked images. We believe our approach is readily applicable to other image restoration problems such as denoising and compression artifact removal."], "figure_types": {"b5f3e5d2912bedbcd9458952d664b08db6aed962/1-Figure1-1.png": "plot", "b5f3e5d2912bedbcd9458952d664b08db6aed962/3-Figure2-1.png": "schematic", "b5f3e5d2912bedbcd9458952d664b08db6aed962/4-Table1-1.png": "table", "b5f3e5d2912bedbcd9458952d664b08db6aed962/5-Figure3-1.png": "plot", "b5f3e5d2912bedbcd9458952d664b08db6aed962/6-Figure5-1.png": "photograph(s)", "b5f3e5d2912bedbcd9458952d664b08db6aed962/6-Table2-1.png": "table", "b5f3e5d2912bedbcd9458952d664b08db6aed962/7-Figure6-1.png": "photograph(s)", "b5f3e5d2912bedbcd9458952d664b08db6aed962/7-Figure7-1.png": "photograph(s)"}}, "1801.06146": {"paper_id": "paper_49", "title": "Universal Language Model Fine-tuning for Text Classification", "arxiv_url": "https://arxiv.org/abs/1801.06146", "s2orc_url": "https://www.semanticscholar.org/paper/ad76c236fe641aa52d1d6c28bf362ae9ffac91e7", "all_figures_tables": {"ad76c236fe641aa52d1d6c28bf362ae9ffac91e7/5-Table1-1.png": "Table 1: Text classification datasets and tasks.", "ad76c236fe641aa52d1d6c28bf362ae9ffac91e7/6-Table2-1.png": "Table 2: Test accuracy scores on two text classification datasets used by McCann et al. (2017).", "ad76c236fe641aa52d1d6c28bf362ae9ffac91e7/6-Table3-1.png": "Table 3: Test error rates (%) on three text classification datasets used by Johnson and Zhang (2017)."}, "referred_figures_tables": [["ad76c236fe641aa52d1d6c28bf362ae9ffac91e7/6-Table2-1.png"], ["ad76c236fe641aa52d1d6c28bf362ae9ffac91e7/5-Table1-1.png", "ad76c236fe641aa52d1d6c28bf362ae9ffac91e7/6-Table2-1.png"], ["ad76c236fe641aa52d1d6c28bf362ae9ffac91e7/5-Table1-1.png"], ["ad76c236fe641aa52d1d6c28bf362ae9ffac91e7/6-Table2-1.png"]], "question_id": [4, 6, 12, 14], "question": ["What metrics are used to compare the performance of ULMFiT against existing approaches?", "How well do the proposed model in this paper and the model in Dai and Lee (2015) generalize to documents of varying lengths?", "Out of all the classification datasets used in the experiments of this paper, what is the ratio of number of samples in the largest to the smallest dataset?", "By how much does the proposed approach outperform CoVE?"], "question_section": ["1 Introduction", "1 Introduction", "3.3 Target task classifier fine-tuning", "4.2 Results"], "question_trigger_sentence": ["We significantly outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets.", "Dai and Le (2015) also fine-tune a language model, but overfit with 10k labeled examples and require millions of in-domain documents for good performance. In contrast, ULMFiT leverages general-domain pretraining and novel finetuning techniques to prevent overfitting even with only 100 labeled examples and achieves state-oft he-art results also on small datasets.", "Table 1: Text classification datasets and tasks with number of classes and training examples.", "Our method outperforms both CoVe, a state-of-the-art transfer learning method based on hypercolumns, as well as the state-of-the-art on both datasets."], "question_type": ["Shallow question", "Deep/complex question", "Testing question", "Testing question"], "evidential_info": [[{"context": "In order to assess the impact of each contribution, we perform a series of analyses and ablations. We run experiments on three corpora, IMDb, TREC-6, and AG that are representative of different tasks, genres, and sizes. For all experiments, we split off 10\\% of the training set and report error rates on this validation set with unidirectional LMs. We fine-tune the classifier for 50 epochs and train all methods but ULMFiT with early stopping.", "rationale": "In a series of analyses and ablations, the authors ran experiments on three corpora, IMDb, TRE-6, and AG, and for each split off 10% of the training set and reported error rates on this validation set."}, {"context": "Our contributions are the following: 1) We propose Universal Language Model Fine-tuning (ULMFiT), a method that can be used to achieve CV-like transfer learning for any task for NLP. 2) We propose discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning. 3) We significantly outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets. 4) We show that our method enables extremely sample-efficient transfer learning and perform an extensive ablation analysis. 5) We make the pretrained models and our code available to enable wider adoption.", "rationale": "ULMFiT significantly outperformed the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets."}, {"context": "For consistency, we report all results as error rates (lower is better). We show the test error rates on the IMDb and TREC-6 datasets used by McCann et al. (2017) in Table 2. Our method outperforms both CoVe, a state-of-the-art transfer learning method based on hypercolumns, as well as the state-of-the-art on both datasets. On IMDb, we reduce the error dramatically by 43.9% and 22% with regard to CoVe and the state-of-the-art respectively. This is promising as the existing state-of-the-art requires complex architectures (Peters et al., 2018), multiple forms of attention (McCann et al., 2017) and sophisticated embedding schemes (Johnson and Zhang, 2016), while our method employs a regular LSTM with dropout. We note that the language model fine-tuning approach of Dai and Le (2015) only achieves an error of 7.64 vs. 4.6 for our method on IMDb, demonstrating the benefit of transferring knowledge from a large ImageNet-like corpus using our fine-tuning techniques. IMDb in particular is reflective of real world datasets: Its documents are generally a few paragraphs long\u2014similar to emails (e.g for legal discovery) and online comments (e.g for community management); and sentiment analysis is similar to many commercial applications, e.g. product response tracking and support email routing.", "rationale": "For consistency, the authors reported all results as error rates where lower is better."}], [{"context": "We propose Universal Language Model Fine-tuning (ULMFiT), which pretrains a language model (LM) on a large general-domain corpus and fine-tunes it on the target task using novel techniques. The method is universal in the sense that it meets these practical criteria: 1) It works across tasks varying in document size, number, and label type; 2) it uses a single architecture and training process; 3) it requires no custom feature engineering or preprocessing; and 4) it does not require additional in-domain documents or labels.", "rationale": "This work evaluated their method on six widely-studied datasets with varying number of documents and varying document length."}, {"context": "We evaluate our method on six widely-studied datasets, with varying numbers of documents and varying document length, used by state-of-the-art text classification and transfer learning approaches Johnson and Zhang (2017); McCann et\u00a0al. (2017) as instances of three common text classification tasks: sentiment analysis, question classification, and topic classification. We show the statistics for each dataset and task in Table 1.", "rationale": "The results showed that this work\u2019s method outperformed various state-of-the-art in both IMDb and TREC-6 datasets. Also, compared to Dai and Le, it achieved a lower error rate on IMDb which reflects document lengths in the real-world."}, {"context": "For consistency, we report all results as error rates (lower is better). We show the test error rates on the IMDb and TREC-6 datasets used by McCann et\u00a0al. (2017) in Table 2. Our method outperforms both CoVe, a state-of-the-art transfer learning method based on hypercolumns, as well as the state-of-the-art on both datasets. On IMDb, we reduce the error dramatically by 43.9% and 22% with regard to CoVe and the state-of-the-art respectively. This is promising as the existing state-of-the-art requires complex architectures Peters et\u00a0al. (2018), multiple forms of attention McCann et\u00a0al. (2017) and sophisticated embedding schemes Johnson and Zhang (2016), while our method employs a regular LSTM with dropout. We note that the language model fine-tuning approach of Dai and Le (2015) only achieves an error of 7.64 vs. 4.6 for our method on IMDb, demonstrating the benefit of transferring knowledge from a large ImageNet-like corpus using our fine-tuning techniques. IMDb in particular is reflective of real-world datasets: Its documents are generally a few paragraphs long\u2014similar to emails (e.g for legal discovery) and online comments (e.g for community management); and sentiment analysis is similar to many commercial applications, e.g. product response tracking and support email routing.", "rationale": "ULMFiT outperformed highly engineered models and transfer learning approaches on six widely studied text classification tasks."}, {"context": "We propose a new method, Universal Language Model Fine-tuning (ULMFiT) that addresses these issues and enables robust inductive transfer learning for any NLP task, akin to fine-tuning ImageNet models: The same 3-layer LSTM architecture\u2014with the same hyperparameters and no additions other than tuned dropout hyperparameters\u2014outperforms highly engineered models and transfer learning approaches on six widely studied text classification tasks. On IMDb, with 100 labeled examples, ULMFiT matches the performance of training from scratch with 10\u00d710\\times10 \u00d7 and\u2014given 50k unlabeled examples\u2014with 100\u00d7100\\times100 \u00d7 more data.", "rationale": "ULMFiT is a universal method as it works across tasks varying in document size, number, and label type."}], [{"context": "We evaluate our method on six widely-studied datasets, with varying numbers of documents and varying document length, used by state-of-the-art text classification and transfer learning approaches Johnson and Zhang (2017); McCann et\u00a0al. (2017) as instances of three common text classification tasks: sentiment analysis, question classification, and topic classification. We show the statistics for each dataset and task in Table 1.", "rationale": "The statistics for each dataset and task are found in Table  1."}], [{"context": "For consistency, we report all results as error rates (lower is better). We show the test error rates on the IMDb and TREC-6 datasets used by McCann et\u00a0al. (2017) in Table 2. Our method outperforms both CoVe, a state-of-the-art transfer learning method based on hypercolumns, as well as the state-of-the-art on both datasets. On IMDb, we reduce the error dramatically by 43.9% and 22% with regard to CoVe and the state-of-the-art respectively. This is promising as the existing state-of-the-art requires complex architectures Peters et\u00a0al. (2018), multiple forms of attention McCann et\u00a0al. (2017) and sophisticated embedding schemes Johnson and Zhang (2016), while our method employs a regular LSTM with dropout. We note that the language model fine-tuning approach of Dai and Le (2015) only achieves an error of 7.64 vs. 4.6 for our method on IMDb, demonstrating the benefit of transferring knowledge from a large ImageNet-like corpus using our fine-tuning techniques. IMDb in particular is reflective of real-world datasets: Its documents are generally a few paragraphs long\u2014similar to emails (e.g for legal discovery) and online comments (e.g for community management); and sentiment analysis is similar to many commercial applications, e.g. product response tracking and support email routing.", "rationale": "On IMDb, the proposed approach reduced the error by 43.9% when compared to CoVe. The results are shown in Table 2."}, {"context": "On TREC-6, our improvement\u2014similar as the improvements of state-of-the-art approaches\u2014is not statistically significant, due to the small size of the 500-examples test set.Nevertheless, the competitive performance on TREC-6 demonstrates that our model performs well across different dataset sizes and can deal with examples that range from single sentences\u2014in the case of TREC-6\u2014to several paragraphs for IMDb. Note that despite pretraining on more than two orders of magnitude less data than the 7 million sentence pairs used by McCann et\u00a0al. (2017), we consistently outperform their approach on both datasets.", "rationale": "On TREC-6, the proposed approach did not improve performance significantly compared to state-of-the-art approaches."}]], "composition": ["For consistency, the authors reported all results as error rates where lower is better.", "This work\u2019s method was evaluated in six widely-studied datasets which varied in document length and experimental results demonstrated that the method outperformed existing approaches in all of these datasets. This indicates that this work\u2019s method generalizes to documents of varying lengths. Also compared to Dai and Le, this method achieved a lower error rate on IMDb showing that it also generalizes better to document lengths reflective of the real world.", "The statistics for each dataset and task are found in Table  1. The ratio of the number of samples in the largest to smallest dataset could be calculated from these statistics.", "On IMDb, the proposed approach reduced the error by 43.9% when compared to CoVe but, on TREC-6, the approach did not improve performance significantly. The results are shown in Table 2."], "Is_figure_in_evidence": [false, false, false, false], "Is_table_in_evidence": [true, true, true, true], "question_key": ["990", "992", "995", "998"], "passages": ["Inductive transfer learning has had a large impact on computer vision (CV). Applied CV models (including object detection, classification, and segmentation) are rarely trained from scratch, but instead are fine-tuned from models that have been pretrained on ImageNet, MS-COCO, and other datasets Sharif\u00a0Razavian et\u00a0al. (2014); Long et\u00a0al. (2015a); He et\u00a0al. (2016); Huang et\u00a0al. (2017).", "Text classification is a category of Natural Language Processing (NLP) tasks with real-world applications such as spam, fraud, and bot detection Jindal and Liu (2007); Ngai et\u00a0al. (2011); Chu et\u00a0al. (2012), emergency response Caragea et\u00a0al. (2011), and commercial document classification, such as for legal discovery Roitblat et\u00a0al. (2010).", "While Deep Learning models have achieved state-of-the-art on many NLP tasks, these models are trained from scratch, requiring large datasets, and days to converge. Research in NLP focused mostly on transductive transfer Blitzer et\u00a0al. (2007). For inductive transfer, fine-tuning pretrained word embeddings Mikolov et\u00a0al. (2013), a simple transfer technique that only targets a model\u2019s first layer, has had a large impact in practice and is used in most state-of-the-art models. Recent approaches that concatenate embeddings derived from other tasks with the input at different layers Peters et\u00a0al. (2017); McCann et\u00a0al. (2017); Peters et\u00a0al. (2018) still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness.", "In light of the benefits of pretraining Erhan et\u00a0al. (2010), we should be able to do better than randomly initializing the remaining parameters of our models. However, inductive transfer via fine-tuning has been unsuccessful for NLP Mou et\u00a0al. (2016). Dai and Le (2015) first proposed fine-tuning a language model (LM) but require millions of in-domain documents to achieve good performance, which severely limits its applicability.", "We show that not the idea of LM fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption. LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods.", "We propose a new method, Universal Language Model Fine-tuning (ULMFiT) that addresses these issues and enables robust inductive transfer learning for any NLP task, akin to fine-tuning ImageNet models: The same 3-layer LSTM architecture\u2014with the same hyperparameters and no additions other than tuned dropout hyperparameters\u2014outperforms highly engineered models and transfer learning approaches on six widely studied text classification tasks. On IMDb, with 100100100100 labeled examples, ULMFiT matches the performance of training from scratch with 10\u00d710\\times10 \u00d7 and\u2014given 50505050k unlabeled examples\u2014with 100\u00d7100\\times100 \u00d7 more data.", "Our contributions are the following: 1) We propose Universal Language Model Fine-tuning (ULMFiT), a method that can be used to achieve CV-like transfer learning for any task for NLP. 2) We propose discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning. 3) We significantly outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets. 4) We show that our method enables extremely sample-efficient transfer learning and perform an extensive ablation analysis. 5) We make the pretrained models and our code available to enable wider adoption.", "Features in deep neural networks in CV have been observed to transition from general to task-specific from the first to the last layer Yosinski et\u00a0al. (2014). For this reason, most work in CV focuses on transferring the first layers of the model Long et\u00a0al. (2015b). Sharif\u00a0Razavian et\u00a0al. (2014) achieve state-of-the-art results using features of an ImageNet model as input to a simple classifier. In recent years, this approach has been superseded by fine-tuning either the last Donahue et\u00a0al. (2014) or several of the last layers of a pretrained model and leaving the remaining layers frozen Long et\u00a0al. (2015a).", "In NLP, only recently have methods been proposed that go beyond transferring word embeddings. The prevailing approach is to pretrain embeddings that capture additional context via other tasks. Embeddings at different levels are then used as features, concatenated either with the word embeddings or with the inputs at intermediate layers. This method is known as hypercolumns Hariharan et\u00a0al. (2015) in CV333A hypercolumn at a pixel in CV is the vector of activations of all CNN units above that pixel. In analogy, a hypercolumn for a word or sentence in NLP is the concatenation of embeddings at different layers in a pretrained model. and is used by Peters et\u00a0al. (2017), Peters et\u00a0al. (2018), Wieting and Gimpel (2017), Conneau et\u00a0al. (2017), and McCann et\u00a0al. (2017) who use language modeling, paraphrasing, entailment, and Machine Translation (MT) respectively for pretraining. Specifically, Peters et\u00a0al. (2018) require engineered custom architectures, while we show state-of-the-art performance with the same basic architecture across a range of tasks. In CV, hypercolumns have been nearly entirely superseded by end-to-end fine-tuning Long et\u00a0al. (2015a).", "A related direction is multi-task learning (MTL) Caruana (1993). This is the approach taken by Rei (2017) and Liu et\u00a0al. (2018) who add a language modeling objective to the model that is trained jointly with the main task model.MTL requires the tasks to be trained from scratch every time, which makes it inefficient and often requires careful weighting of the task-specific objective functions Chen et\u00a0al. (2017).", "Fine-tuning has been used successfully to transfer between similar tasks, e.g. in QA Min et\u00a0al. (2017), for distantly supervised sentiment analysis Severyn and Moschitti (2015), or MT domains Sennrich et\u00a0al. (2015) but has been shown to fail between unrelated ones Mou et\u00a0al. (2016). Dai and Le (2015) also fine-tune a language model, but overfit with 10101010k labeled examples and require millions of in-domain documents for good performance. In contrast, ULMFiT leverages general-domain pretraining and novel fine-tuning techniques to prevent overfitting even with only 100100100100 labeled examples and achieves state-of-the-art results also on small datasets.", "We are interested in the most general inductive transfer learning setting for NLP Pan and Yang (2010): Given a static source task \ud835\udcafSsubscript\ud835\udcaf\ud835\udc46\\mathcal{T}_{S}caligraphic_T start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT and any target task \ud835\udcafTsubscript\ud835\udcaf\ud835\udc47\\mathcal{T}_{T}caligraphic_T start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT with \ud835\udcafS\u2260\ud835\udcafTsubscript\ud835\udcaf\ud835\udc46subscript\ud835\udcaf\ud835\udc47\\mathcal{T}_{S}\\neq\\mathcal{T}_{T}caligraphic_T start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT \u2260 caligraphic_T start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, we would like to improve performance on \ud835\udcafTsubscript\ud835\udcaf\ud835\udc47\\mathcal{T}_{T}caligraphic_T start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT. Language modeling can be seen as the ideal source task and a counterpart of ImageNet for NLP: It captures many facets of language relevant for downstream tasks, such as long-term dependencies Linzen et\u00a0al. (2016), hierarchical relations Gulordava et\u00a0al. (2018), and sentiment Radford et\u00a0al. (2017). In contrast to tasks like MT McCann et\u00a0al. (2017) and entailment Conneau et\u00a0al. (2017), it provides data in near-unlimited quantities for most domains and languages. Additionally, a pretrained LM can be easily adapted to the idiosyncrasies of a target task, which we show significantly improves performance (see Section 5). Moreover, language modeling already is a key component of existing tasks such as MT and dialogue modeling. Formally, language modeling induces a hypothesis space \u210b\u210b\\mathcal{H}caligraphic_H that should be useful for many other NLP tasks Vapnik and Kotz (1982); Baxter (2000).", "We propose Universal Language Model Fine-tuning (ULMFiT), which pretrains a language model (LM) on a large general-domain corpus and fine-tunes it on the target task using novel techniques. The method is universal in the sense that it meets these practical criteria: 1) It works across tasks varying in document size, number, and label type; 2) it uses a single architecture and training process; 3) it requires no custom feature engineering or preprocessing; and 4) it does not require additional in-domain documents or labels.", "In our experiments, we use the state-of-the-art language model AWD-LSTM Merity et\u00a0al. (2017a), a regular LSTM (with no attention, short-cut connections, or other sophisticated additions) with various tuned dropout hyperparameters. Analogous to CV, we expect that downstream performance can be improved by using higher-performance language models in the future.", "ULMFiT consists of the following steps, which we show in Figure 1: a) General-domain LM pretraining (\u00a73.1); b) target task LM fine-tuning (\u00a73.2); and c) target task classifier fine-tuning (\u00a73.3). We discuss these in the following sections.", "An ImageNet-like corpus for language should be large and capture general properties of language. We pretrain the language model on Wikitext-103 Merity et\u00a0al. (2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words. Pretraining is most beneficial for tasks with small datasets and enables generalization even with 100100100100 labeled examples. We leave the exploration of more diverse pretraining corpora to future work, but expect that they would boost performance. While this stage is the most expensive, it only needs to be performed once and improves performance and convergence of downstream models.", "No matter how diverse the general-domain data used for pretraining is, the data of the target task will likely come from a different distribution. We thus fine-tune the LM on data of the target task. Given a pretrained general-domain LM, this stage converges faster as it only needs to adapt to the idiosyncrasies of the target data, and it allows us to train a robust LM even for small datasets. We propose discriminative fine-tuning  and slanted triangular learning rates for fine-tuning the LM, which we introduce in the following.", "As different layers capture different types of information Yosinski et\u00a0al. (2014), they should be fine-tuned to different extents.To this end, we propose a novel fine-tuning method, discriminative fine-tuning444An unrelated method of the same name exists for deep Boltzmann machines Salakhutdinov and Hinton (2009)..", "Instead of using the same learning rate for all layers of the model, discriminative fine-tuning allows us to tune each layer with different learning rates. For context, the regular stochastic gradient descent (SGD) update of a model\u2019s parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 at time step t\ud835\udc61titalic_t looks like the following Ruder (2016):\u03b8t=\u03b8t\u22121\u2212\u03b7\u22c5\u2207\u03b8J(\u03b8)subscript\ud835\udf03\ud835\udc61subscript\ud835\udf03\ud835\udc611\u22c5\ud835\udf02subscript\u2207\ud835\udf03\ud835\udc3d\ud835\udf03\\theta_{t}=\\theta_{t-1}-\\eta\\cdot\\nabla_{\\theta}J(\\theta)italic_\u03b8 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_\u03b8 start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT - italic_\u03b7 \u22c5 \u2207 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT italic_J ( italic_\u03b8 )(1)where \u03b7\ud835\udf02\\etaitalic_\u03b7 is the learning rate and \u2207\u03b8J(\u03b8)subscript\u2207\ud835\udf03\ud835\udc3d\ud835\udf03\\nabla_{\\theta}J(\\theta)\u2207 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT italic_J ( italic_\u03b8 ) is the gradient with regard to the model\u2019s objective function. For discriminative fine-tuning, we split the parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 into {\u03b81,\u2026,\u03b8L}superscript\ud835\udf031\u2026superscript\ud835\udf03\ud835\udc3f\\{\\theta^{1},\\ldots,\\theta^{L}\\}{ italic_\u03b8 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , \u2026 , italic_\u03b8 start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT } where \u03b8lsuperscript\ud835\udf03\ud835\udc59\\theta^{l}italic_\u03b8 start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT contains the parameters of the model at the l\ud835\udc59litalic_l-th layer and L\ud835\udc3fLitalic_L is the number of layers of the model. Similarly, we obtain {\u03b71,\u2026,\u03b7L}superscript\ud835\udf021\u2026superscript\ud835\udf02\ud835\udc3f\\{\\eta^{1},\\ldots,\\eta^{L}\\}{ italic_\u03b7 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , \u2026 , italic_\u03b7 start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT } where \u03b7lsuperscript\ud835\udf02\ud835\udc59\\eta^{l}italic_\u03b7 start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT is the learning rate of the l\ud835\udc59litalic_l-th layer.", "The SGD update with discriminative fine-tuning is then the following:\u03b8tl=\u03b8t\u22121l\u2212\u03b7l\u22c5\u2207\u03b8lJ(\u03b8)superscriptsubscript\ud835\udf03\ud835\udc61\ud835\udc59superscriptsubscript\ud835\udf03\ud835\udc611\ud835\udc59\u22c5superscript\ud835\udf02\ud835\udc59subscript\u2207superscript\ud835\udf03\ud835\udc59\ud835\udc3d\ud835\udf03\\theta_{t}^{l}=\\theta_{t-1}^{l}-\\eta^{l}\\cdot\\nabla_{\\theta^{l}}J(\\theta)italic_\u03b8 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = italic_\u03b8 start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT - italic_\u03b7 start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT \u22c5 \u2207 start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_J ( italic_\u03b8 )(2)We empirically found it to work well to first choose the learning rate \u03b7Lsuperscript\ud835\udf02\ud835\udc3f\\eta^{L}italic_\u03b7 start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT of the last layer by fine-tuning only the last layer and using \u03b7l\u22121=\u03b7l/2.6superscript\ud835\udf02\ud835\udc591superscript\ud835\udf02\ud835\udc592.6\\eta^{l-1}=\\eta^{l}/2.6italic_\u03b7 start_POSTSUPERSCRIPT italic_l - 1 end_POSTSUPERSCRIPT = italic_\u03b7 start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT / 2.6 as the learning rate for lower layers.", "For adapting its parameters to task-specific features, we would like the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters. Using the same learning rate (LR) or an annealed learning rate throughout training is not the best way to achieve this behaviour.Instead, we propose slanted triangular learning rates (STLR), which first linearly increases the learning rate and then linearly decays it according to the following update schedule, which can be seen in Figure 2:cut=\u230aT\u22c5cut_frac\u230bp={t/cut,ift<cut1\u2212t\u2212cutcut\u22c5(1/cut_frac\u22121),otherwise\u03b7t=\u03b7max\u22c51+p\u22c5(ratio\u22121)ratio\ud835\udc50\ud835\udc62\ud835\udc61\u22c5\ud835\udc47\ud835\udc50\ud835\udc62\ud835\udc61_\ud835\udc53\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc5dcases\ud835\udc61\ud835\udc50\ud835\udc62\ud835\udc61if\ud835\udc61\ud835\udc50\ud835\udc62\ud835\udc611\ud835\udc61\ud835\udc50\ud835\udc62\ud835\udc61\u22c5\ud835\udc50\ud835\udc62\ud835\udc611\ud835\udc50\ud835\udc62\ud835\udc61_\ud835\udc53\ud835\udc5f\ud835\udc4e\ud835\udc501otherwisesubscript\ud835\udf02\ud835\udc61\u22c5subscript\ud835\udf02\ud835\udc5a\ud835\udc4e\ud835\udc651\u22c5\ud835\udc5d\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c1\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\\begin{split}cut&=\\lfloor T\\cdot cut\\_frac\\rfloor\\\\p&=\\begin{cases}t/cut,&\\text{if}\\ t<cut\\\\1-\\frac{t-cut}{cut\\cdot(1/cut\\_frac-1)},&\\text{otherwise}\\end{cases}\\\\\\eta_{t}&=\\eta_{max}\\cdot\\frac{1+p\\cdot(ratio-1)}{ratio}\\end{split}start_ROW start_CELL italic_c italic_u italic_t end_CELL start_CELL = \u230a italic_T \u22c5 italic_c italic_u italic_t _ italic_f italic_r italic_a italic_c \u230b end_CELL end_ROW start_ROW start_CELL italic_p end_CELL start_CELL = { start_ROW start_CELL italic_t / italic_c italic_u italic_t , end_CELL start_CELL if italic_t < italic_c italic_u italic_t end_CELL end_ROW start_ROW start_CELL 1 - divide start_ARG italic_t - italic_c italic_u italic_t end_ARG start_ARG italic_c italic_u italic_t \u22c5 ( 1 / italic_c italic_u italic_t _ italic_f italic_r italic_a italic_c - 1 ) end_ARG , end_CELL start_CELL otherwise end_CELL end_ROW end_CELL end_ROW start_ROW start_CELL italic_\u03b7 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = italic_\u03b7 start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT \u22c5 divide start_ARG 1 + italic_p \u22c5 ( italic_r italic_a italic_t italic_i italic_o - 1 ) end_ARG start_ARG italic_r italic_a italic_t italic_i italic_o end_ARG end_CELL end_ROW(3)where T\ud835\udc47Titalic_T is the number of training iterations555In other words, the number of epochs times the number of updates per epoch., cut_frac\ud835\udc50\ud835\udc62\ud835\udc61_\ud835\udc53\ud835\udc5f\ud835\udc4e\ud835\udc50cut\\_fracitalic_c italic_u italic_t _ italic_f italic_r italic_a italic_c is the fraction of iterations we increase the LR, cut\ud835\udc50\ud835\udc62\ud835\udc61cutitalic_c italic_u italic_t is the iteration when we switch from increasing to decreasing the LR, p\ud835\udc5dpitalic_p is the fraction of the number of iterations we have increased or will decrease the LR respectively, ratio\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5cratioitalic_r italic_a italic_t italic_i italic_o specifies how much smaller the lowest LR is from the maximum LR \u03b7maxsubscript\ud835\udf02\ud835\udc5a\ud835\udc4e\ud835\udc65\\eta_{max}italic_\u03b7 start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT, and \u03b7tsubscript\ud835\udf02\ud835\udc61\\eta_{t}italic_\u03b7 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the learning rate at iteration t\ud835\udc61titalic_t. We generally use cut_frac=0.1\ud835\udc50\ud835\udc62\ud835\udc61_\ud835\udc53\ud835\udc5f\ud835\udc4e\ud835\udc500.1cut\\_frac=0.1italic_c italic_u italic_t _ italic_f italic_r italic_a italic_c = 0.1, ratio=32\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c32ratio=32italic_r italic_a italic_t italic_i italic_o = 32 and \u03b7max=0.01subscript\ud835\udf02\ud835\udc5a\ud835\udc4e\ud835\udc650.01\\eta_{max}=0.01italic_\u03b7 start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT = 0.01.", "STLR modifies triangular learning rates Smith (2017) with a short increase and a long decay period, which we found key for good performance.666We also credit personal communication with the author. In Section 5, we compare against aggressive cosine annealing, a similar schedule that has recently been used to achieve state-of-the-art performance in CV Loshchilov and Hutter (2017).777While Loshchilov and Hutter (2017) use multiple annealing cycles, we generally found one cycle to work best.", "Finally, for fine-tuning the classifier, we augment the pretrained language model with two additional linear blocks.Following standard practice for CV classifiers, each block uses batch normalization Ioffe and Szegedy (2015) and dropout, with ReLU activations for the intermediate layer and a softmax activation that outputs a probability distribution over target classes at the last layer. Note that the parameters in these task-specific classifier layers are the only ones that are learned from scratch. The first linear layer takes as the input the pooled last hidden layer states.", "The signal in text classification tasks is often contained in a few words, which may occur anywhere in the document. As input documents can consist of hundreds of words, information may get lost if we only consider the last hidden state of the model. For this reason, we concatenate the hidden state at the last time step \ud835\udc21Tsubscript\ud835\udc21\ud835\udc47\\mathbf{h}_{T}bold_h start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT of the document with both the max-pooled and the mean-pooled representation of the hidden states over as many time steps as fit in GPU memory \ud835\udc07={\ud835\udc211,\u2026,\ud835\udc21T}\ud835\udc07subscript\ud835\udc211\u2026subscript\ud835\udc21\ud835\udc47\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{T}\\}bold_H = { bold_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , bold_h start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT }:\ud835\udc21c=[\ud835\udc21T,\ud835\ude96\ud835\ude8a\ud835\udea1\ud835\ude99\ud835\ude98\ud835\ude98\ud835\ude95(\ud835\udc07),\ud835\ude96\ud835\ude8e\ud835\ude8a\ud835\ude97\ud835\ude99\ud835\ude98\ud835\ude98\ud835\ude95(\ud835\udc07)]subscript\ud835\udc21\ud835\udc50subscript\ud835\udc21\ud835\udc47\ud835\ude96\ud835\ude8a\ud835\udea1\ud835\ude99\ud835\ude98\ud835\ude98\ud835\ude95\ud835\udc07\ud835\ude96\ud835\ude8e\ud835\ude8a\ud835\ude97\ud835\ude99\ud835\ude98\ud835\ude98\ud835\ude95\ud835\udc07\\mathbf{h}_{c}=[\\mathbf{h}_{T},\\mathtt{maxpool}(\\mathbf{H}),\\mathtt{meanpool}(\\mathbf{H})]bold_h start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = [ bold_h start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , typewriter_maxpool ( bold_H ) , typewriter_meanpool ( bold_H ) ](4)where [][][ ] is concatenation.", "Fine-tuning the target classifier is the most critical part of the transfer learning method. Overly aggressive fine-tuning will cause catastrophic forgetting, eliminating the benefit of the information captured through language modeling; too cautious fine-tuning will lead to slow convergence (and resultant overfitting). Besides discriminative fine-tuning and triangular learning rates, we propose gradual unfreezing for fine-tuning the classifier.", "Rather than fine-tuning all layers at once, which risks catastrophic forgetting, we propose to gradually unfreeze the model starting from the last layer as this contains the least general knowledge Yosinski et\u00a0al. (2014): We first unfreeze the last layer and fine-tune all unfrozen layers for one epoch. We then unfreeze the next lower frozen layer and repeat, until we fine-tune all layers until convergence at the last iteration. This is similar to \u2018chain-thaw\u2019 Felbo et\u00a0al. (2017), except that we add a layer at a time to the set of \u2018thawed\u2019 layers, rather than only training a single layer at a time.", "While discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing all are beneficial on their own, we show in Section 5 that they complement each other and enable our method to perform well across diverse datasets.", "Language models are trained with backpropagation through time (BPTT) to enable gradient propagation for large input sequences. In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixed-length batches of size b\ud835\udc4fbitalic_b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences Merity et\u00a0al. (2017a).", "Similar to existing work Peters et\u00a0al. (2017, 2018), we are not limited to fine-tuning a unidirectional language model. For all our experiments, we pretrain both a forward and a backward LM. We fine-tune a classifier for each LM independently using BPT3C and average the classifier predictions.", "While our approach is equally applicable to sequence labeling tasks, we focus on text classification tasks in this work due to their important real-world applications.", "We evaluate our method on six widely-studied datasets, with varying numbers of documents and varying document length, used by state-of-the-art text classification and transfer learning approaches Johnson and Zhang (2017); McCann et\u00a0al. (2017) as instances of three common text classification tasks: sentiment analysis, question classification, and topic classification. We show the statistics for each dataset and task in Table 1.", "For sentiment analysis, we evaluate our approach on the binary movie review IMDb dataset\u00a0Maas et\u00a0al. (2011) and on the binary and five-class version of the Yelp review dataset compiled by Zhang et\u00a0al. (2015).", "We use the six-class version of the small TREC dataset\u00a0Voorhees and Tice (1999) dataset of open-domain, fact-based questions divided into broad semantic categories.", "For topic classification, we evaluate on the large-scale AG news and DBpedia ontology datasets created by Zhang et\u00a0al. (2015).", "We use the same pre-processing as in earlier work Johnson and Zhang (2017); McCann et\u00a0al. (2017). In addition, to allow the language model to capture aspects that might be relevant for classification, we add special tokens for upper-case words, elongation, and repetition.", "We are interested in a model that performs robustly across a diverse set of tasks. To this end, if not mentioned otherwise, we use the same set of hyperparameters across tasks, which we tune on the IMDb validation set. We use the AWD-LSTM language model Merity et\u00a0al. (2017a) with an embedding size of 400400400400, 3333 layers, 1150115011501150 hidden activations per layer, and a BPTT batch size of 70707070. We apply dropout of 0.40.40.40.4 to layers, 0.30.30.30.3 to RNN layers, 0.40.40.40.4 to input embedding layers, 0.050.050.050.05 to embedding layers, and weight dropout of 0.50.50.50.5 to the RNN hidden-to-hidden matrix. The classifier has a hidden layer of size 50505050. We use Adam with \u03b21=0.7subscript\ud835\udefd10.7\\beta_{1}=0.7italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.7 instead of the default \u03b21=0.9subscript\ud835\udefd10.9\\beta_{1}=0.9italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9 and \u03b22=0.99subscript\ud835\udefd20.99\\beta_{2}=0.99italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.99, similar to Dozat and Manning (2017). We use a batch size of 64646464, a base learning rate of 0.0040.0040.0040.004 and 0.010.010.010.01 for fine-tuning the LM and the classifier respectively, and tune the number of epochs on the validation set of each task888On small datasets such as TREC-6, we fine-tune the LM only for 15151515 epochs without overfitting, while we can fine-tune longer on larger datasets. We found 50505050 epochs to be a good default for fine-tuning the classifier.. We otherwise use the same practices used in Merity et\u00a0al. (2017a).", "For each task, we compare against the current state-of-the-art. For the IMDb and TREC-6 datasets, we compare against CoVe McCann et\u00a0al. (2017), a state-of-the-art transfer learning method for NLP. For the AG, Yelp, and DBpedia datasets, we compare against the state-of-the-art text categorization method by Johnson and Zhang (2017).", "For consistency, we report all results as error rates (lower is better). We show the test error rates on the IMDb and TREC-6 datasets used by McCann et\u00a0al. (2017) in Table 2. Our method outperforms both CoVe, a state-of-the-art transfer learning method based on hypercolumns, as well as the state-of-the-art on both datasets. On IMDb, we reduce the error dramatically by 43.9% and 22% with regard to CoVe and the state-of-the-art respectively. This is promising as the existing state-of-the-art requires complex architectures Peters et\u00a0al. (2018), multiple forms of attention McCann et\u00a0al. (2017) and sophisticated embedding schemes Johnson and Zhang (2016), while our method employs a regular LSTM with dropout. We note that the language model fine-tuning approach of Dai and Le (2015) only achieves an error of 7.64 vs. 4.6 for our method on IMDb, demonstrating the benefit of transferring knowledge from a large ImageNet-like corpus using our fine-tuning techniques. IMDb in particular is reflective of real-world datasets: Its documents are generally a few paragraphs long\u2014similar to emails (e.g for legal discovery) and online comments (e.g for community management); and sentiment analysis is similar to many commercial applications, e.g. product response tracking and support email routing.", "On TREC-6, our improvement\u2014similar as the improvements of state-of-the-art approaches\u2014is not statistically significant, due to the small size of the 500-examples test set.Nevertheless, the competitive performance on TREC-6 demonstrates that our model performs well across different dataset sizes and can deal with examples that range from single sentences\u2014in the case of TREC-6\u2014to several paragraphs for IMDb. Note that despite pretraining on more than two orders of magnitude less data than the 7 million sentence pairs used by McCann et\u00a0al. (2017), we consistently outperform their approach on both datasets.", "We show the test error rates on the larger AG, DBpedia, Yelp-bi, and Yelp-full datasets in Table 3. Our method again outperforms the state-of-the-art significantly. On AG, we observe a similarly dramatic error reduction by 23.7% compared to the state-of-the-art. On DBpedia, Yelp-bi, and Yelp-full, we reduce the error by 4.8%, 18.2%, 2.0% respectively.", "In order to assess the impact of each contribution, we perform a series of analyses and ablations. We run experiments on three corpora, IMDb, TREC-6, and AG that are representative of different tasks, genres, and sizes. For all experiments, we split off 10%percent1010\\%10 % of the training set and report error rates on this validation set with unidirectional LMs. We fine-tune the classifier for 50505050 epochs and train all methods but ULMFiT with early stopping.", "One of the main benefits of transfer learning is being able to train a model for a task with a small number of labels. We evaluate ULMFiT on different numbers of labeled examples in two settings: only labeled examples are used for LM fine-tuning (\u2018supervised\u2019); and all task data is available and can be used to fine-tune the LM (\u2018semi-supervised\u2019). We compare ULMFiT to training from scratch\u2014which is necessary for hypercolumn-based approaches. We split off balanced fractions of the training data, keep the validation set fixed, and use the same hyperparameters as before. We show the results in Figure 3.", "On IMDb and AG, supervised ULMFiT with only 100100100100 labeled examples matches the performance of training from scratch with 10\u00d710\\times10 \u00d7 and 20\u00d720\\times20 \u00d7 more data respectively, clearly demonstrating the benefit of general-domain LM pretraining. If we allow ULMFiT to also utilize unlabeled examples (50505050k for IMDb, 100100100100k for AG), at 100100100100 labeled examples, we match the performance of training from scratch with 50\u00d750\\times50 \u00d7 and 100\u00d7100\\times100 \u00d7 more data on AG and IMDb respectively. On TREC-6, ULMFiT significantly improves upon training from scratch; as examples are shorter and fewer, supervised and semi-supervised ULMFiT achieve similar results.", "We compare using no pretraining with pretraining on WikiText-103 Merity et\u00a0al. (2017b) in Table 4. Pretraining is most useful for small and medium-sized datasets, which are most common in commercial applications. However, even for large datasets, pretraining improves performance.", "In order to gauge the importance of choosing an appropriate LM, we compare a vanilla LM with the same hyperparameters without any dropout999To avoid overfitting, we only train the vanilla LM classifier for 5555 epochs and keep dropout of 0.40.40.40.4 in the classifier. with the AWD-LSTM LM with tuned dropout parameters in Table 5. Using our fine-tuning techniques, even a regular LM reaches surprisingly good performance on the larger datasets. On the smaller TREC-6, a vanilla LM without dropout runs the risk of overfitting, which decreases performance.", "We compare no fine-tuning against fine-tuning the full model Erhan et\u00a0al. (2010) (\u2018Full\u2019), the most commonly used fine-tuning method, with and without discriminative fine-tuning (\u2018Discr\u2019) and slanted triangular learning rates (\u2018Stlr\u2019) in Table 6. Fine-tuning the LM is most beneficial for larger datasets. \u2018Discr\u2019 and \u2018Stlr\u2019 improve performance across all three datasets and are necessary on the smaller TREC-6, where regular fine-tuning is not beneficial.", "We compare training from scratch, fine-tuning the full model (\u2018Full\u2019), only fine-tuning the last layer (\u2018Last\u2019) Donahue et\u00a0al. (2014), \u2018Chain-thaw\u2019 Felbo et\u00a0al. (2017), and gradual unfreezing (\u2018Freez\u2019). We furthermore assess the importance of discriminative fine-tuning (\u2018Discr\u2019) and slanted triangular learning rates (\u2018Stlr\u2019). We compare the latter to an alternative, aggressive cosine annealing schedule (\u2018Cos\u2019) Loshchilov and Hutter (2017). We use a learning rate \u03b7L=0.01superscript\ud835\udf02\ud835\udc3f0.01\\eta^{L}=0.01italic_\u03b7 start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT = 0.01 for \u2018Discr\u2019, learning rates of 0.0010.0010.0010.001 and 0.00010.00010.00010.0001 for the last and all other layers respectively for \u2018Chain-thaw\u2019 as in Felbo et\u00a0al. (2017), and a learning rate of 0.0010.0010.0010.001 otherwise. We show the results in Table 7.", "Fine-tuning the classifier significantly improves over training from scratch, particularly on the small TREC-6. \u2018Last\u2019, the standard fine-tuning method in CV, severely underfits and is never able to lower the training error to 00. \u2018Chain-thaw\u2019 achieves competitive performance on the smaller datasets, but is outperformed significantly on the large AG. \u2018Freez\u2019 provides similar performance as \u2018Full\u2019. \u2018Discr\u2019 consistently boosts the performance of \u2018Full\u2019 and \u2018Freez\u2019, except for the large AG. Cosine annealing is competitive with slanted triangular learning rates on large data, but under-performs on smaller datasets. Finally, full ULMFiT classifier fine-tuning (bottom row) achieves the best performance on IMDB and TREC-6 and competitive performance on AG. Importantly, ULMFiT is the only method that shows excellent performance across the board\u2014and is therefore the only universal method.", "While our results demonstrate that how we fine-tune the classifier makes a significant difference, fine-tuning for inductive transfer is currently under-explored in NLP as it mostly has been thought to be unhelpful Mou et\u00a0al. (2016). To better understand the fine-tuning behavior of our model, we compare the validation error of the classifier fine-tuned with ULMFiT and \u2018Full\u2019 during training in Figure 4.", "On all datasets, fine-tuning the full model leads to the lowest error comparatively early in training, e.g. already after the first epoch on IMDb. The error then increases as the model starts to overfit and knowledge captured through pretraining is lost. In contrast, ULMFiT is more stable and suffers from no such catastrophic forgetting; performance remains similar or improves until late epochs, which shows the positive effect of the learning rate schedule.", "At the cost of training a second model, ensembling the predictions of a forward and backwards LM-classifier brings a performance boost of around 0.50.50.50.5\u20130.70.70.70.7. On IMDb we lower the test error from 5.305.305.305.30 of a single model to 4.584.584.584.58 for the bidirectional model.", "While we have shown that ULMFiT can achieve state-of-the-art performance on widely used text classification tasks, we believe that language model fine-tuning will be particularly useful in the following settings compared to existing transfer learning approaches Conneau et\u00a0al. (2017); McCann et\u00a0al. (2017); Peters et\u00a0al. (2018): a) NLP for non-English languages, where training data for supervised pretraining tasks is scarce; b) new NLP tasks where no state-of-the-art architecture exists; and c) tasks with limited amounts of labeled data (and some amounts of unlabeled data).", "Given that transfer learning and particularly fine-tuning for NLP is under-explored, many future directions are possible. One possible direction is to improve language model pretraining and fine-tuning and make them more scalable: for ImageNet, predicting far fewer classes only incurs a small performance drop Huh et\u00a0al. (2016), while recent work shows that an alignment between source and target task label sets is important Mahajan et\u00a0al. (2018)\u2014focusing on predicting a subset of words such as the most frequent ones might retain most of the performance while speeding up training. Language modeling can also be augmented with additional tasks in a multi-task learning fashion Caruana (1993) or enriched with additional supervision, e.g. syntax-sensitive dependencies Linzen et\u00a0al. (2016) to create a model that is more general or better suited for certain downstream tasks, ideally in a weakly-supervised manner to retain its universal properties.", "Another direction is to apply the method to novel tasks and models. While an extension to sequence labeling is straightforward, other tasks with more complex interactions such as entailment or question answering may require novel ways to pretrain and fine-tune. Finally, while we have provided a series of analyses and ablations, more studies are required to better understand what knowledge a pretrained language model captures, how this changes during fine-tuning, and what information different tasks require.", "We have proposed ULMFiT, an effective and extremely sample-efficient transfer learning method that can be applied to any NLP task. We have also proposed several novel fine-tuning techniques that in conjunction prevent catastrophic forgetting and enable robust learning across a diverse range of tasks. Our method significantly outperformed existing transfer learning techniques and the state-of-the-art on six representative text classification tasks. We hope that our results will catalyze new developments in transfer learning for NLP."], "figure_types": {"ad76c236fe641aa52d1d6c28bf362ae9ffac91e7/5-Table1-1.png": "table", "ad76c236fe641aa52d1d6c28bf362ae9ffac91e7/6-Table2-1.png": "table", "ad76c236fe641aa52d1d6c28bf362ae9ffac91e7/6-Table3-1.png": "table"}}, "2201.06009": {"paper_id": "paper_5", "title": "MemPrompt: Memory-assisted Prompt Editing with User Feedback", "arxiv_url": "https://arxiv.org/abs/2201.06009", "s2orc_url": "https://www.semanticscholar.org/paper/41f44979cf1cd3f4cbd615dc130bc33721f5281b", "all_figures_tables": {"41f44979cf1cd3f4cbd615dc130bc33721f5281b/15-Figure10-1.png": "Figure 10: MemPrompt: retrieving feedback from memory. User enters a question which GPT-3 has incorrectly answered in the past, and has received feedback from a user (step 1). The feedback is retrieved from memory (step 2), and both question and feedback are added to the prompt. The prompt contains examples that allow GPT-3 to react to user feedback and generate correct understanding and answer.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/15-Figure9-1.png": "Figure 9: MemPrompt: adding to memory. User enters a question for which no feedback is available (steps 1, 2). Directly prompting GPT-3 with the question leads to incorrect answer and understanding (step 3). User-provides feedback on the incorrect understanding (step 4), which is added to memory (step 5).", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/16-Figure11-1.png": "Figure 11: Overview of GUD-IR. To retrieve a relevant feedback that applies to x, GUD-IR first generates a feedback f\u0302b using a generative model. This is then aligned with a corpus of feedbacks fb1, fb2, . . . , fb|tr| (e.g., sourced from the train split). The best matching feedback \u02c6fb\u2217 is then used for x. Thus, GUD-IR decomposes the retrieval problem x \u2192 fb into two sub-problems: (i) generate a rough feedback (x \u2192 f\u0302b) and (ii) search for the closest feedback in a large store \u02c6fb\u2217 = argminj\u2208[1,|tr|] |f\u0302b\u2212 fbj |.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/17-Figure12-1.png": "Figure 12: Distribution of similarity scores between expected fb \u2217 and u\u0302 for retrieval (left) and GUD-IR (right). The similarity scores are higher using GUD-IR.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/19-Figure13-1.png": "Figure 13: The prompt used for our tasks. During inference, an input question xi, and optionally a feedback fbi is appended after this prompt, and the model is expected to generate the answer yi and its understanding of the question intent ui as a continuation. The prompt contains examples of the form (x \u2192 u,y), expressed \"x # u y END #\", and (x, fb\u2192 u,y), expressed \"x | clarification: fb # u y END #\". (u and y are expressed together as a single sentence, e.g., \"[The synonym for &lt;word&gt; is] [&lt;word&gt;].\")", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/2-Figure2-1.png": "Figure 2: Proposed architecture: (left) GPT-3 does not account for user feedback. (right) MemPrompt maintains a memoryM of corrective feedback, and searches for feedback from prior queries with a similar intent as x using a retrieval functionM(x). x is then concatenated to the retrieved feedback and appended to the prompt for querying GPT-3. Users can also give new feedback on the model\u2019s task understanding u, then added toM.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/20-Figure14-1.png": "Figure 14: The prompt used for our word scrambling tasks derived from Brown et al. (2020)", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/24-Figure17-1.png": "Figure 17: Instruction accuracy vs. time for WEBQA.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/24-Table6-1.png": "Table 6: Relevant examples fetched with time: as time proceeds, the examples fetched from the memory become increasingly relevant to the input question, leading to increasingly accurate predictions.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/24-Table7-1.png": "Table 7: Relevant examples retrieved at increasing timesteps: as time proceeds, the examples fetched from the memory become relevant to the input question, leading to accurate predictions.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/25-Table8-1.png": "Table 8: Relevant examples retrieved for WEBQA QA task (Section \u00a74.3). The retrieved examples get increasingly relevant as time proceeds.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/26-Figure18-1.png": "Figure 18: Finding 2 Large gains on queries asked in English and Punjabi by MemPrompt.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/26-Table9-1.png": "Table 9: Relevant examples retrieved for WEBQA QA task (Section \u00a74.3). The retrieved examples get increasingly relevant as time proceeds.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/28-Table10-1.png": "Table 10: Sample x-y pairs in English. The same type of question can be asked in multiple ways. Our method makes no assumptions as to how a question might be asked.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/30-Table13-1.png": "Table 13: A subset of random samples where GPT-3-175B without memory was incorrect.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/4-Table1-1.png": "Table 1: Feedback types and demonstration of understanding: our system leverages user feedback to prevent failures caused due to a misunderstanding of the task (INS) or semantics of the input (CAT and NL). We achieve this by having the model articulate an understanding u, on which a user can provide feedback using fb.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/6-Figure5-1.png": "Figure 5: Sample snapshot of memory for lexical QA.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/7-Figure6-1.png": "Figure 6: ERT-CAT: Label accuracy increases with time for all values of clarification probabilities Pr(fbi).", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/7-Figure7-1.png": "Figure 7: ERT-CAT: Instruction accuracy sharply increases with a larger clarification probability, showing that MemPrompt responds to feedback. With time, lower values of Pr(fbi) catch up as memory is gradually filled with error cases and feedback.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/7-Table2-1.png": "Table 2: MemPrompt outperforms NO-MEM for both the categorical and the more challenging ERT-NL setup having longer, ambiguous inputs.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/8-Table3-1.png": "Table 3: ERT NL task error categories", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/9-Figure8-1.png": "Figure 8: Avg. performance on lexical (top) and word scramble (bottom) tasks with time (x-axis). Accuracy increases with time as memory is filled up with feedback from past errors.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/9-Table4-1.png": "Table 4: Results on lexical qa: MemPrompt has the best performance across all lexical QA tasks.", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/9-Table5-1.png": "Table 5: GROW-PROMPT and MemPrompt outperform NO-MEM on all word scramble QA tasks."}, "referred_figures_tables": [["41f44979cf1cd3f4cbd615dc130bc33721f5281b/9-Figure8-1.png"]], "question_id": [8], "question": ["Why does the approach not simply add all feedback examples in memory to the prompt if they will be adding examples anyways?"], "question_section": ["Approach"], "question_trigger_sentence": ["Although the model has not changed, adding fb corrects its erroneous behavior because we provide a few positive \u201ctraining\u201d examples containing feedback (x, fb \u2192 u, y) in the prompt (Appendix B)."], "question_type": ["Deep/complex question"], "evidential_info": [[{"context": "Figure 8 reports the overall performance on the word reasoning tasks.The accuracy improves substantially within 300 examples when using memory (in yellow) vs. no memory (in blue).Note that we are operating in a few-shot prompting regime (i.e., there is no training data over which we train).The performance of grow-prompt (red) lies in between, showing that non-selective memory is partially helpful, although not as effective as failure-driven retrieval (our model).However, grow-prompt is \\sim 3x more expensive\u00a0(larger prompts) and cannot scale beyond the 2048 tokens limit.We also found that the retrieved feedback from memory was effective 97% of the time; only in \\approx 3% of cases feedback had no positive effect.", "rationale": "This paragraph mentions how the performance of Grow-Prompt is limited by the fact that the maximum context size allowed is 2048 tokens. Also, they mention how the GrowPrompt baseline is 3x more expensive than the proposed approach because of increased context size/prompt length."}]], "composition": ["The proposed approach (MemPrompt) probably does not add all feedback examples in memory to the prompt since the size of the prompt is limited to 2048 tokens. Additionally, increasing the size of the prompt leads to higher cost (in terms of compute resources required to process the query)."], "Is_figure_in_evidence": [true], "Is_table_in_evidence": [false], "question_key": ["1019"], "passages": ["Language models are now better than ever before at generating realistic content, but still lack commonsense Bender and Koller (2020); Marcus (2021). One failure mode due to a lack of commonsense is in misunderstanding a user\u2019s intent. The typical remedy of retraining with more data is prohibitive due to the cost and infrastructure requirements. In such cases, even if users repeatedly observe the model making a mistake, there are no avenues to provide feedback to the model to make it more accurate and personalized over time.", "Our goal is to allow users to correct such errors directly through interaction, and without retraining by injecting the knowledge required to correct the model\u2019s misunderstanding.Building upon the recent success of injecting commonsense in the input (Lewis et\u00a0al., 2020; Talmor et\u00a0al., 2020), we propose a novel approach of injecting knowledge in the input via interactive feedback from an end-user.", "Our approach, \\ours, pairs gpt-3 with a growing memory of cases where the model misunderstood user\u2019s intent and was provided with corrective feedback.This feedback is question dependent, and thus the prompt for each sample is edited to adapt to the input.In this sense, our work can be seen as an instance of prompt engineering\u00a0Liu et\u00a0al. (2021b) which involves editing the prompts. Our work adds interactivity to prompt engineering as it involves dynamically updating the prompt for every instance.", "Figure 1 presents a sample interaction between a user and gpt-3 that our setup enables.The model was asked for a similar word. However, the model\u2019s (incorrect) task understanding \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u was \u201cThe homophone of good is\u201d.The user can detect such discrepancy between the intended and interpreted task instruction, and can provide feedback \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb as \"similar to means with a similar meaning\", clarifying that they actually wanted a synonym.Crucially, note that such instructional correction is feasible even if the user does not knowthe correct answer to their question, as they are critiquing the model\u2019s understanding of theirintent, rather than the answers themselves.Thus, our setup does not require the users to be experts at tasks being solved, another advantage of our approach.", "Further, it is desirable to have a system that can leverage past feedback on new, unseen examples for prompt-editing. We maintain a memory \u2133\u2133\\mathcal{M}caligraphic_M of such feedback as a set of key-value pairs, where thekey is a misunderstood question, and the value is the user\u2019s feedback to correct that misunderstanding. Given a new question, we check if the model has made a mistakeon a similar question earlier, by querying the memory for a similar question. If found,append the corresponding feedback to the question prompt. This mechanism aims toprevent the model from making the same type of mistake twice. This failure-driven remindingmechanism draws inspiration from the theory of recursive reminding in psychology Jacoby and Wahlheim (2013),which suggests humans index error corrections in the context in which those errors occurred.", "This paper sets out the general architecture and a simple implementation of its components.We then demonstrate the system on four tasks, using simulated user feedback:(1) lexical relations (e.g., antonyms, Figure 1),(2) word scrambling (e.g., anagrams), (3) ethics (with user feedback being the appropriate class of ethicalconsideration, e.g., \u201cit is about cheating\u201d, using a small set of categories), and (4) ethics (with user feedback beingnatural language). We find that in all cases, gpt-3\u2019s accuracy significantly increases with time, without retraining,as our approach enables it to use corrective feedback from earlier examples to avoid similar misunderstandings on future examples. In summary, our contributions are:\u2219\u2219\\bullet\u2219We show that a large model like gpt-3 can be improved after deployment, without retraining, through a memory-assisted architecture.\u2219\u2219\\bullet\u2219Our implementation, \\ours, is the first demonstration that this is possible - this is an important step forward for real use of LMs, and the paper sets out a general architecture that others can build on, a specific implementation, and detailed evaluation on multiple tasks.", "In Tandon et\u00a0al. (2022), we show that using a memory of user feedback can be used to repair erroneous model in a supervised setting.In this work, we build upon the recent advances in few-shot prompting to modify gpt-3\u2019s behavior by adding user feedback to the query (prompt).Like others, we use gpt-3 with few-shot prompting, where the prompt consistsof a prefix prefix\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc53\ud835\udc56\ud835\udc65prefixitalic_p italic_r italic_e italic_f italic_i italic_x containing a few input-output \u201ctraining\u201d examples of the task, followed by the input x\ud835\udc65xitalic_x, e.g., a question,to operate on. However, while prior work has focused on constructing better prefixes, e.g., dynamically selecting good \u201ctraining\u201d examplesbased on the question Le\u00a0Scao and Rush (2021); Liu et\u00a0al. (2021a), or even representing the prefix latently Li and Liang (2021),our work elaborates the input x\ud835\udc65xitalic_x itself to clarify the intended task, by adding user feedback fb\ud835\udc53\ud835\udc4ffbitalic_f italic_b from previous misunderstandings.", "Similarly, our work can be seen as a form of retrieval-augmented QA. Extensive prior work has used retrievals from a text corpus to aid QA, e.g., Pan et\u00a0al. (2019); Guu et\u00a0al. (2020), or retrievals of prior QA pairs for nearest-neighbor QA (Khandelwal et\u00a0al., 2020). In contrast, we retrieve from a dynamic memory of user feedback.", "The idea of failure-driven reminding and dynamic memory date back severaldecades, e.g., Schank (1983); Riesbeck (1981).Our work resurrects these ideas in a modern context.", "Learning from instruction has become important for large LMs that can perform a task based on direct instruction ratherthan examples Wei et\u00a0al. (2021); Mishra et\u00a0al. (2021). Our work extends this by adding an adaptive component when those instructions are misinterpreted.While it may not be possible for a user to provide meaningful feedback on the output itself, giving feedback on the understanding of the instruction is more feasible.", "Our approach aims to modify the model\u2019s behavior through prompting, given a wrong answer.An alternative, recently explored approach is \u201cmodel editing\u201d - updating the modelitself by modifying its parameters to fix incorrect answers (Mitchell et\u00a0al., 2021; De\u00a0Cao et\u00a0al., 2021; Hase et\u00a0al., 2021).Model editing approaches have to date been limited due to uncontrollable out-of-scope changes Mitchell et\u00a0al. (2021). In contrast, our goal is not just to correct a prediction, but to generalize that correctionfor new problems by collecting feedback to clarify the misunderstanding without damaging the model\u2019s basic problem-solving acumen.", "Finally, our work is a simple example of debugging and learning via dialog. While system debugging through dialogue has been explored in many contexts\u00a0(Hixon et\u00a0al., 2015; Wang et\u00a0al., 2016; Davis, 1977), our contribution is a dialogue about the model\u2019s understanding of the user\u2019s intent.", "In our setup, given an input \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, a model generates an output \ud835\udc32\ud835\udc32\\mathbf{y}bold_y and a sentence \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u expressing its understanding of the task, a skill learned through few-shot examples in theprompt (Appendix\u00a0D).The user can then critique \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u by providing natural language feedback \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb. This is feasible even if the user does not know the correctness of \ud835\udc32\ud835\udc32\\mathbf{y}bold_y because they are critiquing the model\u2019s understanding of their intent rather the answers themselves. ", "Given a new query, \\oursuses \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb from similar, prior queries to enrich the (few-shot) prompt \ud835\udc29\ud835\udc29\\mathbf{p}bold_p.We use the principle that if two inputs xisubscript\ud835\udc65\ud835\udc56{x}_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and xjsubscript\ud835\udc65\ud835\udc57{x}_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT are similar (i.e., xi\u223cxjsimilar-tosubscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57{x}_{i}\\sim{x}_{j}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u223c italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT), then their feedback \ud835\udc1f\ud835\udc1bisubscript\ud835\udc1f\ud835\udc1b\ud835\udc56\\mathbf{fb}_{i}bold_fb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and \ud835\udc1f\ud835\udc1bjsubscript\ud835\udc1f\ud835\udc1b\ud835\udc57\\mathbf{fb}_{j}bold_fb start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT should be exchangeable (xi\u223cxj\u21d4fbi\u223cfbj)\u21d4similar-tosubscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57similar-to\ud835\udc53subscript\ud835\udc4f\ud835\udc56\ud835\udc53subscript\ud835\udc4f\ud835\udc57(x_{i}\\sim x_{j}\\Leftrightarrow fb_{i}\\sim fb_{j})( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u223c italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u21d4 italic_f italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u223c italic_f italic_b start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ).The underlying assumption here is that for a fixed model, similar inputs will incur similar errors, and thus can use the same feedback for correction.Fig. 2 gives an overview of \\ours, with the following components:", ": \u2133\u2133\\mathcal{M}caligraphic_M is a growing table of key\u00a0(\ud835\udc31isubscript\ud835\udc31\ud835\udc56\\mathbf{x}_{i}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) - value\u00a0(\ud835\udc1f\ud835\udc1bisubscript\ud835\udc1f\ud835\udc1b\ud835\udc56\\mathbf{fb}_{i}bold_fb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) pairs that supports read, write, and lookup operations.The write operation is used whenever a user gives new feedback.", ":The memory allows lookup operations, denoted as \u2133(\ud835\udc31)\u2133\ud835\udc31\\mathcal{M}(\\mathbf{x})caligraphic_M ( bold_x ), that matches the query=\ud835\udc31\ud835\udc31\\mathbf{x}bold_x against all the keys of \u2133\u2133\\mathcal{M}caligraphic_M.", ": A gating function allowing irrelevant, retrieved feedback to be ignored.", "Let us briefly recap few-shot prompting with gpt-3. Consider a general setup where given an input \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, a model is expected to generate an output \ud835\udc32\ud835\udc32\\mathbf{y}bold_y. In a few-shot prompting mode\u00a0(Brown et\u00a0al., 2020), a prompt \ud835\udc29\ud835\udc29\\mathbf{p}bold_p consists of k\ud835\udc58kitalic_k (\ud835\udc31,\ud835\udc32)\ud835\udc31\ud835\udc32(\\mathbf{x},\\mathbf{y})( bold_x , bold_y ) \u201cin-context\u201d examples, i.e., \ud835\udc29=\ud835\udc311.\ud835\udc321#\ud835\udc312.\ud835\udc322\u2026#\ud835\udc31k.\ud835\udc32kformulae-sequence\ud835\udc29subscript\ud835\udc311subscript\ud835\udc321#subscript\ud835\udc312subscript\ud835\udc322\u2026#subscript\ud835\udc31\ud835\udc58subscript\ud835\udc32\ud835\udc58\\mathbf{p}=\\mathbf{x}_{1}.\\mathbf{y}_{1}\\#\\mathbf{x}_{2}.\\mathbf{y}_{2}\\ldots\\#\\mathbf{x}_{k}.\\mathbf{y}_{k}bold_p = bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . bold_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT # bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . bold_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2026 # bold_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . bold_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT,where ##\\## is a token separating examples and . indicates concatenation.During inference, the user inputs a question \ud835\udc31isubscript\ud835\udc31\ud835\udc56\\mathbf{x}_{i}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and the model is fed \ud835\udc29#\ud835\udc31i\ud835\udc29#subscript\ud835\udc31\ud835\udc56\\mathbf{p}\\ \\#\\ \\mathbf{x}_{i}bold_p # bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (i.e., the question suffixed to the prompt) and is expected to generate the answer \ud835\udc32isubscript\ud835\udc32\ud835\udc56\\mathbf{y}_{i}bold_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as a continuation.", "As mentioned, given an input \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, we prompt the model to generate an output \ud835\udc32\ud835\udc32\\mathbf{y}bold_y and a sentence \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u expressing its understanding of the task.Thus, the in-context examples for \\oursare of the form \ud835\udc31\u2192\ud835\udc2e,\ud835\udc32\u2192\ud835\udc31\ud835\udc2e\ud835\udc32\\mathbf{x}\\rightarrow\\mathbf{u},\\mathbf{y}bold_x \u2192 bold_u , bold_y.In addition to the input \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, \\oursretrieves a \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb if a question similar to \ud835\udc31\ud835\udc31\\mathbf{x}bold_x has been asked before.To enable the model to react to such feedback, we also include examples of the form (\ud835\udc31,\ud835\udc1f\ud835\udc1b\u2192\ud835\udc2e,\ud835\udc32)formulae-sequence\u2192\ud835\udc31\ud835\udc1f\ud835\udc1b\ud835\udc2e\ud835\udc32(\\mathbf{x},\\mathbf{fb}\\rightarrow\\mathbf{u},\\mathbf{y})( bold_x , bold_fb \u2192 bold_u , bold_y ) in the prompt, which are aimed to teach the model to react to \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb\u00a0(Appendix\u00a0D).", "Existing methods for receiving user feedback typically assume the user knows the correct answer \ud835\udc32\ud835\udc32\\mathbf{y}bold_y Elgohary et\u00a0al. (2021).This assumption is paradoxical: if the user knew the answer, why would they be using the model? Further, allowing only \u201coracle\u201d users (who know correct \ud835\udc32\ud835\udc32\\mathbf{y}bold_y) might lead to sampling biases.On the contrary, in real settings, the user may not know the correct \ud835\udc32\ud835\udc32\\mathbf{y}bold_y, but they know what they are looking for.Thus, we propose eliciting a verbalization of task understanding \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u from the model in addition to the answer. End users can thus critique \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u.", "We operationalize this idea by including task verbalization in the prompt (Fig.\u00a03).Given a question What sounds like < sighted > ?, a vanilla prompting approach will generate the answer cited.In contrast, we include a \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u the homophone for in the prompt.gpt-3 is adept at reasoning with just a handful of examples, and thus can be expected to mimic the prompt to generate task understanding and answer.gpt-3 generates such additional information in all our tasks.Given a test question What sounds similar to < sighted > ?, if the model generates the word that has the same meaning as \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u, the user has a reason to believe that the answer is wrong.", "Our approach is not foolproof\u2014 the model may spell out a wrong \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u while giving out the correct answer, misleading the user into believing that there is an error (or vice-versa). Hallucinating remains a critical limitation of generative models Cao et\u00a0al. (2022), therefore additional heuristics and model calibration might be necessary to make our approach foolproof. In practice, however, we found such cases to be rare for the tasks in this paper.", "Once the feedback is received from the user, can the model successfully utilize it? By adding a few examples of the form \ud835\udc31,\ud835\udc1f\ud835\udc1b\u2192\ud835\udc2e,\ud835\udc32formulae-sequence\u2192\ud835\udc31\ud835\udc1f\ud835\udc1b\ud835\udc2e\ud835\udc32\\mathbf{x},\\mathbf{fb}\\rightarrow\\mathbf{u},\\mathbf{y}bold_x , bold_fb \u2192 bold_u , bold_y in the prompt and setting \ud835\udc1f\ud835\udc1b=\ud835\udc2e\ud835\udc1f\ud835\udc1b\ud835\udc2e\\mathbf{fb}=\\mathbf{u}bold_fb = bold_u, we force the model to use the task understanding present in the input when generating the output\u00a0(Figure\u00a04).Recently, it has been shown that such repetition plays a crucial role in the success of few-shot prompting models\u00a0(Madaan and Yazdanbakhsh, 2022).", "Within the setup \ud835\udc31\u2192\ud835\udc2e,\ud835\udc32\u2192\ud835\udc31\ud835\udc2e\ud835\udc32\\mathbf{x}\\rightarrow\\mathbf{u},\\mathbf{y}bold_x \u2192 bold_u , bold_y, we focus on following two modes of failure:\u2219\u2219\\bullet\u2219Task instruction understanding: this is especially concerning in a multi-tasking setup, where the model may consider the question to be about a different task than the one user intended.\u2219\u2219\\bullet\u2219Task nuanced understanding: when the model understands the task type, but misunderstands the subtle intent in a question. ", "While feedback on the model output is our primary goal, we also experiment with settings where an Oracle is available to provide feedback on the labels\u00a0(Section\u00a0\u00a74.3).We note again that the model reacts to the feedback because some in-context samples are of the form: (\ud835\udc31,\ud835\udc1f\ud835\udc1b\u2192\ud835\udc2e,\ud835\udc32)formulae-sequence\u2192\ud835\udc31\ud835\udc1f\ud835\udc1b\ud835\udc2e\ud835\udc32(\\mathbf{x},\\mathbf{fb}\\rightarrow\\mathbf{u},\\mathbf{y})( bold_x , bold_fb \u2192 bold_u , bold_y ) and (\ud835\udc31\u2192\ud835\udc2e,\ud835\udc32\u2192\ud835\udc31\ud835\udc2e\ud835\udc32\\mathbf{x}\\rightarrow\\mathbf{u},\\mathbf{y}bold_x \u2192 bold_u , bold_y). We consider a diverse set of tasks (\ud835\udc31\u2192\ud835\udc32\u2192\ud835\udc31\ud835\udc32\\mathbf{x}\\rightarrow\\mathbf{y}bold_x \u2192 bold_y), \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb and \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u, as summarized in Table 1.", "We apply our approach to four tasks: (1) lexical relations (e.g., antonyms, Figure 1),(2) word scrambling (e.g., anagrams), (3) ethics (with user feedback being the appropriate class of ethicalconsideration), and (4) ethics (with user feedback being natural language).For all five tasks, the dataset consists of (\ud835\udc31,\ud835\udc1f\ud835\udc1b\u2192\ud835\udc2e,\ud835\udc32)formulae-sequence\u2192\ud835\udc31\ud835\udc1f\ud835\udc1b\ud835\udc2e\ud835\udc32(\\mathbf{x},\\mathbf{fb}\\rightarrow\\mathbf{u},\\mathbf{y})( bold_x , bold_fb \u2192 bold_u , bold_y ) tuples, where \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb clarifies the task in \ud835\udc31\ud835\udc31\\mathbf{x}bold_x.We have a simulated conversational setting, in which a user can ask the model \ud835\udc31\ud835\udc31\\mathbf{x}bold_x (covering any of these five tasks). If the model gives a wrong answer to query \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, then \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb is used as the simulated corrective feedback.The sources for these datasets are listed in Appendix \u00a0\u00a7E.", "The lexical relation task is to predict a word with a given lexical relationship to an input word.We use five relationships: synonym (syn), antonym (ant), homophone\u00a0(hom), definition (defn), and sentence usage generation (sent).", "For this task, given a word with its characters transformed, the model is expected to recover the original characters.There are four transformation operations the user can request: reversal of words (rev, yppup \u2192\u2192\\rightarrow\u2192 puppy), cycle letters in word (cyc, atc \u2192\u2192\\rightarrow\u2192 cat), random insertions (rand, c!r ic/ke!t\u2192\u2192\\rightarrow\u2192 cricket), and anagrams by changing all but the first and last (anag1, eelhpnat \u2192\u2192\\rightarrow\u2192 elephant) or all but the first and last 2 characters (anag2, elapehnt \u2192\u2192\\rightarrow\u2192 elephant).We use the original dataset by Brown et\u00a0al. (2020).222word scrambling dataset https://github.com/openai/gpt-3/tree/master/data", "For both these tasks, each question can be asked in multiple ways\u00a0(e.g., for synonym generation, the users might ask questions of the form what is like, what has a similar sense, what is akin to, what is something like, etc.)Similarly for the lexical relations task, we specify the task description x\ud835\udc65xitalic_x using different phrasings, e.g., \u201crearrange the letters\u201d (which the system sometimes misunderstands), and the (simulated) user feedback fb\ud835\udc53\ud835\udc4ffbitalic_f italic_b is a clearer task description, e.g., \u201cThe anagram is\u201d. The system thus accumulates a set of (x\ud835\udc65xitalic_x, fb\ud835\udc53\ud835\udc4ffbitalic_f italic_b) pairs in memory after each failure, helping it avoid future misunderstandings of x\ud835\udc65xitalic_x through feedback retrieval.", "For ethical reasoning, we consider a setup where given a situation\u00a0(e.g., cheating on your partner), the model is expected to provide a judgment on whether the situation is ethical or not\u00a0(e.g., it\u2019s not okay).In addition to providing a judgment on the ethics of the situation, the model also elucidates its understanding of what the question is about\u00a0(e.g., being loyal).While the user may not know the answer, we posit that they would be able to provide feedback on the broader context.For example, if the model generates being financially savvy instead of being loyal for the situation cheating on your partner, a user can still point out this problem and provide feedback.", "We use a subset 333social norms dataset (social-chemistry-101, Forbes et\u00a0al. (2020)) https://github.com/mbforbes/social-chemistry-101 of the dataset provided by\u00a0delphi\u00a0(Jiang et\u00a0al., 2021). We simulate two different kinds of user feedback, using two of theannotations attached to each example in the Delphi dataset:", "\u2219\u2219\\bullet\u2219Categorical feedback\u00a0(ert-cat): In this setting, the model generates its understanding u\ud835\udc62uitalic_u of the situation by selecting one of 10 different possible categories of morality to which the situation might belong: care, loyalty, authority, fairness, sanctity, degradation, cheating, subversion, betrayal, and harm.These categories are explicitly provided for each example in the Delphi dataset.\u2219\u2219\\bullet\u2219Natural language feedback\u00a0(ert-nl): For this, we use the associated \u201crule of thumb\u201d (RoT) annotation \u2014a general moral principle \u2014 attached to each example in the Delphi dataset.To compile a challenging subset of the data for ert-nl, we sample by input length, preferring long \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, with a short feedback \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb. Specifically, we use the top 1% of the inputs by length to create a challenging set of input situations\u00a0(\ud835\udc31\ud835\udc31\\mathbf{x}bold_x).User feedback \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb is a natural language feedback on the understanding \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u.", "In both the cases, the model is \u201ctaught\u201d to generate a category \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u (as well as the okay/not-okay answer \ud835\udc32\ud835\udc32\\mathbf{y}bold_y to the ethical question) by being given a few examples in the prompt prefix, thus articulating which moral category (for ert-cat) or rule-of-thumb\u00a0(for ert-nl) it thinks is applicable. The simulated feedback \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb is the gold category associated with the example in the question, if gpt-3 gets the answer wrong.", "We selected these tasks because situations that involve reasoning about similar ethical principles can utilize similar past feedback. For example, sharing an extra umbrella with your friend if they don\u2019t have one, and donating surplus food to the homeless both involve compassion.", "\u2133\u2133\\mathcal{M}caligraphic_M uses the user input \ud835\udc31\ud835\udc31\\mathbf{x}bold_x as the key and the corresponding feedback \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb as value.Given a question \ud835\udc31isubscript\ud835\udc31\ud835\udc56\\mathbf{x}_{i}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, if the user detects that the model has misunderstood the question, they may provide a \ud835\udc1f\ud835\udc1bisubscript\ud835\udc1f\ud835\udc1b\ud835\udc56\\mathbf{fb}_{i}bold_fb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT with clarification probability Pr(\ud835\udc1f\ud835\udc1bi)\ud835\udc43\ud835\udc5fsubscript\ud835\udc1f\ud835\udc1b\ud835\udc56Pr(\\mathbf{fb}_{i})italic_P italic_r ( bold_fb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ).The feedback is stored in a memory \u2133\u2133\\mathcal{M}caligraphic_M, with \ud835\udc31isubscript\ud835\udc31\ud835\udc56\\mathbf{x}_{i}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as the key and \ud835\udc1f\ud835\udc1bisubscript\ud835\udc1f\ud835\udc1b\ud835\udc56\\mathbf{fb}_{i}bold_fb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as the value.For a subsequent question \ud835\udc31jsubscript\ud835\udc31\ud835\udc57\\mathbf{x}_{j}bold_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, the retriever \u2133(\ud835\udc31)\u2133\ud835\udc31\\mathcal{M}(\\mathbf{x})caligraphic_M ( bold_x ) checks if a similar question appears in memory. If yes, then the corresponding feedback is attached with the question and fed to the model for generation.", "For example, the model might misunderstand a question asking for synonym, e.g., what is akin to fast ? as one that requires antonyms.As mentioned, in our setup, the model generates its understanding of the task \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u, and not just the answer to the question.The user, by inspecting \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u = The opposite of fast is: might determine that the model has misunderstood them, and give feedback i wanted a synonym, which gets stored in \u2133\u2133\\mathcal{M}caligraphic_M.If a similar question\u00a0(e.g., what is akin to pretty ?) is asked later by the same or a different user, the corresponding feedback\u00a0(i wanted a synonym) is attached with the question to generate the answer. Figure 5 illustrates a sample memory for this task.", "A retrieved past feedback that is incorrect might cause the model to make a mistake, thus necessitating a good retrieval function. We propose a two-stage method for effective retrieval involving: transforming \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, followed by a similarity lookup of the transformed \ud835\udc31\ud835\udc31\\mathbf{x}bold_x in \u2133\u2133\\mathcal{M}caligraphic_M. When the task involves high surface-level similarity among past feedback, such as in lexical word tasks, then a simple heuristic-based transformation is sufficient.However, such simple transformations are insufficient for tasks that involves more complex retrieval e.g., when two lexically dissimilar situations can share the same understanding.For example, consider two situations from ert-nl: Filling a false time sheet at work and Being at a party, and telling parents I am studying.These situations look lexically dissimilar but correspond to the same underlying social principle lying to authority.In our experiments, off-the-shelf methods failed to address these challenges\u00a0(see \u00a74 later).", "To address these challenges with transformation in complex tasks, we have designed a novel seq2seq based transformation called gud-ir. Given \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, gud-ir generates a transformed feedback \ud835\udc1f\ud835\udc1b^^\ud835\udc1f\ud835\udc1b\\hat{\\mathbf{fb}}over^ start_ARG bold_fb end_ARG for \ud835\udc31\ud835\udc31\\mathbf{x}bold_x using a generative seq2seq model. Our approach is inspired and supported by the recent success of generate and retrieve Mao et\u00a0al. (2021) methods.However, despite the similarity, the methods have different goals: Mao et\u00a0al. (2021) leverage generative models for query expansion, whereas our goal is explainable input understanding.See Appendix\u00a0B for more details on gud-ir.", "After the transformation stage, the closest matching entry is then used as the corresponding \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb. Transformation reduces \u2133(\ud835\udc31)\u2133\ud835\udc31\\mathcal{M}(\\mathbf{x})caligraphic_M ( bold_x ) to a search over \ud835\udc1f\ud835\udc1b1,\ud835\udc1f\ud835\udc1b2,\u2026,\ud835\udc1f\ud835\udc1b|\u2133|subscript\ud835\udc1f\ud835\udc1b1subscript\ud835\udc1f\ud835\udc1b2\u2026subscript\ud835\udc1f\ud835\udc1b\u2133\\mathbf{fb}_{1},\\mathbf{fb}_{2},\\ldots,\\mathbf{fb}_{|\\mathcal{M}|}bold_fb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_fb start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , bold_fb start_POSTSUBSCRIPT | caligraphic_M | end_POSTSUBSCRIPT with \ud835\udc1f\ud835\udc1b^^\ud835\udc1f\ud835\udc1b\\hat{\\mathbf{fb}}over^ start_ARG bold_fb end_ARG as the search query. We compute similarity based on a fine-tuned Sentence transformers\u00a0(Reimers and Gurevych, 2019).", "\ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C concatenates \ud835\udc31\ud835\udc31\\mathbf{x}bold_x with relevant \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb retrieved by \u2133(\ud835\udc31)\u2133\ud835\udc31\\mathcal{M}(\\mathbf{x})caligraphic_M ( bold_x ). To ensure that the \ud835\udc31\ud835\udc31\\mathbf{x}bold_x is appended with \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb only if it is relevant, our current implementation of combiner uses a threshold on the similarity score between the \ud835\udc31\ud835\udc31\\mathbf{x}bold_x and the closest feedback \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb retrieved by \u2133(\ud835\udc31)\u2133\ud835\udc31\\mathcal{M}(\\mathbf{x})caligraphic_M ( bold_x ).We rely on the model (gpt-3) to pay attention to the relevant parts of the input. Exploring more complex gating mechanisms remains an important future work.", "We compare \\ours(memory-assisted prompt editing) with two baselines:\u2219\u2219\\bullet\u2219no-mem This is the standard gpt-3 444We use gpt-3-175b\u00a0(davinci) for all experiments. in few-shot prompting mode\u00a0(hyper-parameters listed in Appendix\u00a0\u00a7C). Input is \ud835\udc29#\ud835\udc31i\ud835\udc29#subscript\ud835\udc31\ud835\udc56\\mathbf{p}\\ \\#\\ \\mathbf{x}_{i}bold_p # bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (i.e., question \ud835\udc31isubscript\ud835\udc31\ud835\udc56\\mathbf{x}_{i}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT appended to prompt \ud835\udc29\ud835\udc29\\mathbf{p}bold_p).It generates answer \ud835\udc32isubscript\ud835\udc32\ud835\udc56\\mathbf{y}_{i}bold_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and its understanding of the user\u2019s intent \ud835\udc2eisubscript\ud835\udc2e\ud835\udc56\\mathbf{u}_{i}bold_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.\u2219\u2219\\bullet\u2219grow-prompt: Similar to no-mem, but the \ud835\udc29\ud835\udc29\\mathbf{p}bold_p is continuously grown with a subset of memory \u2133\u2133\\mathcal{M}caligraphic_M that can fit within the prompt (max. 2048 tokens).The most recent subset of \u2133\u2133\\mathcal{M}caligraphic_M of memory inserted is inserted in the prompt.The ethical reasoning tasks\u00a0(ert) involve long examples, and the initial prompt itself takes close to the max allowed tokens.Thus, the grow-prompt setup is only provided for the lexical relations and word scrambling tasks.", "We use two different metrics:", "\u2219\u2219\\bullet\u2219Acc(\ud835\udc32)\ud835\udc34\ud835\udc50\ud835\udc50\ud835\udc32Acc(\\mathbf{y})italic_A italic_c italic_c ( bold_y ): % of cases where answer matched the ground truth.\u2219\u2219\\bullet\u2219Acc(\ud835\udc2e)\ud835\udc34\ud835\udc50\ud835\udc50\ud835\udc2eAcc(\\mathbf{u})italic_A italic_c italic_c ( bold_u ): % of cases where the model\u2019s understanding of user\u2019s intent is correct. Acc(\ud835\udc2e)\ud835\udc34\ud835\udc50\ud835\udc50\ud835\udc2eAcc(\\mathbf{u})italic_A italic_c italic_c ( bold_u ) is also referred to as instruction accuracy.As discussed in \u00a0\u00a73.4, depending on the task, the model generates its understanding on either the instruction or semantics of the question.", "In real-world cases, we cannot expect a user to provide feedback for all the examples (e.g., the user might not know that the understanding of the model is wrong).To simulate this realistic setting, we experiment with various values of clarification probabilities Pr\ud835\udc43\ud835\udc5fPritalic_P italic_r.", "Does pairing gpt-3 with \\ourshelp? \u00a74.1.1 empirically validates this on ethical reasoning tasks and \u00a74.1.2 on word reasoning tasks.", "Table 2 presents results on the delphi dataset (1,000 points in the test set). Recall from \u00a73.5 that there are two kinds of feedback on delphi questions: cat and nl feedback. \\oursgets over 25% relative improvement for both ert-nl and ert-cat.We found that having an efficient retriever was critical for ert-nl: sentence transformer based retriever scored 38.5, vs. 45.2 using gud-ir, a 17% improvement.", "Figure 7 demonstrates that the instruction accuracy increases over time for different values of clarification probability.", "Fig. 6 shows that label accuracy improves over time. Baseline (no-mem) saturates after 200 time steps; \\ourscontinues to improve.Continuous improvement is one of our key advantages.These charts show that instruction accuracy and label accuracy are correlated\u00a0(corr. coeff = 0.36).", "We observe that using a higher clarification probability leads to a sharp increase in instruction and label accuracy early on in the training for both ert-cat and ert-nl. This is because with a large clarification probability, memory fills up with feedback sooner, leading to higher accuracy.", "In ert nl and cat tasks, a primary source of label errors is confusion between labels such as okay and good due to the nuanced differences e.g., input = teaching your child a musical instrument. \\ourspredicts good, but the expected answer is okay. Jiang et\u00a0al. (2021) make similar observations.", "We randomly sampled examples from the ert-nl dev set where the model generates an incorrect understanding\u00a0(i.e., Acc(\ud835\udc2e)=0\ud835\udc34\ud835\udc50\ud835\udc50\ud835\udc2e0Acc(\\mathbf{u})=0italic_A italic_c italic_c ( bold_u ) = 0 based on exact match).Our goal is to understand the typical errors made by the model and use the analysis to calibrate the findings in Table\u00a02.We select ert-nl for the analysis because it involves free-form natural language which is difficult to study quantitatively.", "\u2219\u2219\\bullet\u2219Correct, lexically variant understanding (30%):Exact match underestimates model performance (as the task involves generation). \u223csimilar-to\\sim\u223c 30% \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u is a lexical variation of the reference gold understanding. E.g., telling a spouse your true feeling vs. loving your partner. The generated label in these 30% cases is still correct.(Table\u00a03, row 1)\u2219\u2219\\bullet\u2219Distracted understanding (50%): A major source of instruction and label errors is the model getting distracted by an unimportant context.Bad retrieval accounts for 30% errors within this category, e.g., matching a situation in the memory where the expected understanding is only partially applicable to the query. (Table\u00a03, row 2)\u2219\u2219\\bullet\u2219Retrieval failures (18%): These errors are caused by an irrelevant retrieved understanding from the memory , when using a state-of-the-art retrieval method (Table\u00a03, row 3).gud-ir helps to reduce these retrieval failures.See Appendix\u00a0\u00a7B.Table 3 presents canonical examples of these error categories. We also find that over time, more relevant past examples are fetched (see Table 7).", "For these tasks, we compare gold \ud835\udc2e*superscript\ud835\udc2e\\mathbf{u}^{*}bold_u start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT and generated \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u based on hard-coded linguistic variations (e.g., the antonym is matches the opposite is).While we do not explicitly evaluate task accuracy, there is a near-perfect correlation between the accuracy of \ud835\udc32\ud835\udc32\\mathbf{y}bold_y and \ud835\udc2e\ud835\udc2e\\mathbf{u}bold_u\u00a0(i.e., if the gpt-3 understands the task correctly, the output was almost always correct).This shows improving model\u2019s understanding of a task might lead to an improved performance.", "Figure 8 reports the overall performance on the word reasoning tasks.The accuracy improves substantially within 300 examples when using memory (in yellow) vs. no memory (in blue).Note that we are operating in a few-shot prompting regime (i.e., there is no training data over which we train).The performance of grow-prompt (red) lies in between, showing that non-selective memory is partially helpful, although not as effective as failure-driven retrieval (our model).However, grow-prompt is \u223csimilar-to\\sim\u223c 3x more expensive\u00a0(larger prompts) and cannot scale beyond the 2048 tokens limit.We also found that the retrieved feedback from memory was effective 97% of the time; only in \u2248\\approx\u2248 3% of cases feedback had no positive effect.", "When the memory is used for every example (green line, Fig 8, top), the performance improves quickly vs. the yellow line\u00a0(Pr(\ud835\udc1f\ud835\udc1bi)\ud835\udc43\ud835\udc5fsubscript\ud835\udc1f\ud835\udc1b\ud835\udc56Pr(\\mathbf{fb}_{i})italic_P italic_r ( bold_fb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = 0.5).", "Recent work such as Liu et\u00a0al. (2021a) investigate using dynamic prompts for better generation. For a given input \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, their method(\u00a0kate) relies on retrieving examples from the training set that are similar to \ud835\udc31\ud835\udc31\\mathbf{x}bold_x for dynamically creating the prompt \ud835\udc29\ud835\udc29\\mathbf{p}bold_p. Note that our method edits \ud835\udc31\ud835\udc31\\mathbf{x}bold_x with a feedback \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb, and is thus complementary to kate.We verify this by experiments on ert-cat and ert-nl. Specifically, we create dynamic prompts using kate, whereas \\oursis used like before to attach a \ud835\udc1f\ud835\udc1b\ud835\udc1f\ud835\udc1b\\mathbf{fb}bold_fb to the question. We observe a consistent 10% improvement by using kate across all baselines, showing that the improvements are complementary.", "requires the model to verbalize its understanding of the question, on which a user provides feedback.To investigate the efficacy of \\oursin settings where generating an understanding is not easy, we experiment with factual question answering on the webqa dataset\u00a0(Berant et\u00a0al., 2013), and find that \\oursis effective even with label feedback (Appendix\u00a0\u00a7F).", "We demonstrate an application of \\oursfor personalization with a use-case where user language preferences can be folded in the memory. We simulate a user who does not speak fluent English and uses code-mixed language. The queries posed by the user contain words from two Indian languages: Hindi and Punjabi. gpt-3 predictably misunderstands the task. The user clarifies the meanings of their dialect/language phrases. While initial queries fail, subsequent queries that reuse similar words succeed because their clarifications are present in the memory (details in Appendix\u00a0\u00a7G).", "We present \\ours, a novel, memory-enhanced gpt-3 that allows users to interact and improve the model without retraining. A key insight is to have the model articulate not just its answer but also its understanding of the user\u2019s intent, providing an avenue for feedback.We show that deployed systems with fixed large-language models can still be improved by interacting with end-users, potentially improving their performance and broadening their utility.", "We have shown how to improve very large models through interaction. Our memory-based enhancement is a low-cost utility enhancement eventually geared towards personalized, correctable models, which is currently an open question in NLP with unresolved issues. While our method is a step toward a promising open direction, it comes with limitations and opportunities when deploying to the real world.", "In practical deployments of the \\oursmethod, the memory can grow to orders of magnitude, introducing scaling challenges. We anticipate using memory as a buffer between cycles of re-training, and these cycles could range from a week to several months. Between cycles of re-training, \\ourscan serve as a way to avoid repeating mistakes and collect feedback which can be used to fine-tune and improve the next version of the model.", "Currently, we operate with a single user at a time, but a real-world deployment could encounter multiple users. These users could exhibit characteristics of a user community where some feedback could apply to multiple users in a community cluster, while others differ in interpretation and style. In such a multi-user environment, managing the memory effectively when dealing with incompatible entries would be important. Existing initial ideas towards managing a bank of beliefs could be extended to address these problems, e.g., Kassner et\u00a0al. (2021). In addition, when looking up such a rich and potentially noisy feedback collection, rather than retrieving a single feedback item, it would help to have an adapter over the memory that generates feedback by adapting the existing, diverse, and related past feedback to the current scenario. This increases the diversity of the generated knowledge and reduces the impact of erroneous feedback and noise.", "Extending the discussion on noise in feedback, our setting assumes that users will not provide any adversarial feedback. However, in real-world environments, this assumption is unlikely to hold. Additionally, there is a risk in the real-world deployment of our system, wherein an adversarial user might provide harmful feedback, thus maliciously controlling the systems (potentially a home-based robot) where our method is deployed. Thus, robust mechanisms such as gud-ir and memory adapters will be critical for successful real-world deployments.", "Privacy is another ethical concern, as the deployed system collects and records feedback from a user, some of which could contain personal information (when I look for an interesting movie, I mean something that contains romance). Therefore, the system needs to win the trust of the users so they would be encouraged to interact closely, and to win this trust, the system needs to demonstrate smartness, receptivity to user feedback, and the ability to maintain the memory without leaking any personal information safely.", "Finally, large-language models generate text that might be biased and insensitive to a user\u2019s socio-cultural context\u00a0(Bordia and Bowman, 2019; Sharma et\u00a0al., 2021; Hovy and Prabhumoye, 2021).In a multi-user deployment of our system, the memory could contain feedback from user communities of diverse beliefs, gender identities, and cultural backgrounds could lead to conflicts. Thus the system will need checks and balances to ensure that the content produced by the system as a result of the feedback is not harmful."], "figure_types": {"41f44979cf1cd3f4cbd615dc130bc33721f5281b/15-Figure10-1.png": "schematic", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/15-Figure9-1.png": "schematic", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/16-Figure11-1.png": "schematic", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/17-Figure12-1.png": "plot", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/19-Figure13-1.png": "other", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/2-Figure2-1.png": "schematic", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/20-Figure14-1.png": "table", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/24-Figure17-1.png": "plot", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/24-Table6-1.png": "table", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/24-Table7-1.png": "table", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/25-Table8-1.png": "table", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/26-Figure18-1.png": "plot", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/26-Table9-1.png": "table", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/28-Table10-1.png": "table", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/30-Table13-1.png": "table", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/4-Table1-1.png": "table", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/6-Figure5-1.png": "table", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/7-Figure6-1.png": "plot", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/7-Figure7-1.png": "plot", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/7-Table2-1.png": "table", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/8-Table3-1.png": "table", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/9-Figure8-1.png": "plot", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/9-Table4-1.png": "table", "41f44979cf1cd3f4cbd615dc130bc33721f5281b/9-Table5-1.png": "table"}}, "1506.03340": {"paper_id": "paper_50", "title": "Teaching Machines to Read and Comprehend", "arxiv_url": "https://arxiv.org/abs/1506.03340", "s2orc_url": "https://www.semanticscholar.org/paper/d1505c6123c102e53eb19dff312cb25cea840b72", "all_figures_tables": {"d1505c6123c102e53eb19dff312cb25cea840b72/10-Figure5-1.png": "Figure 5: Aggregated precision for documents up to a certain lengths. The points mark the ith decile in document lengths across the corpus.", "d1505c6123c102e53eb19dff312cb25cea840b72/10-Table6-1.png": "Table 6: Model hyperparameters", "d1505c6123c102e53eb19dff312cb25cea840b72/11-Figure6-1.png": "Figure 6: Attention heat maps from the Attentive Reader for two more correctly answered validation set queries. Both examples require significant lexical generalisation and co-reference resolution to find the correct answers ent201 and ent214, respectively.", "d1505c6123c102e53eb19dff312cb25cea840b72/12-Figure7-1.png": "Figure 7: Two more correctly answered validation set queries. The left example (entity ent315) requires correctly attributing the quote, which does not appear trivial with a number of other candidate entities in the vicinity. The right hand side shows our model is not afraid of Chuck Norris (ent164).", "d1505c6123c102e53eb19dff312cb25cea840b72/13-Figure10-1.png": "Figure 10: Attention of the Impatient Reader at time steps 1, 2 and 3.", "d1505c6123c102e53eb19dff312cb25cea840b72/13-Figure11-1.png": "Figure 11: Attention of the Impatient Reader at time steps 4, 5 and 6.", "d1505c6123c102e53eb19dff312cb25cea840b72/14-Figure12-1.png": "Figure 12: Attention of the Impatient Reader at time steps 7, 8 and 9.", "d1505c6123c102e53eb19dff312cb25cea840b72/14-Figure13-1.png": "Figure 13: Attention of the Impatient Reader at time steps 10, 11 and 12.", "d1505c6123c102e53eb19dff312cb25cea840b72/2-Table1-1.png": "Table 1: Corpus statistics. Articles were collected starting in April 2007 for CNN and June 2010 for the Daily Mail, both until the end of April 2015. Validation data is from March, test data from April 2015. Articles of over 2000 tokens and queries whose answer entity did not appear in the context were filtered out.", "d1505c6123c102e53eb19dff312cb25cea840b72/3-Table3-1.png": "Table 3: Original and anonymised version of a data point from the Daily Mail validation set. The anonymised entity markers are constantly permuted during training and testing.", "d1505c6123c102e53eb19dff312cb25cea840b72/4-Table4-1.png": "Table 4: Resolution strategies using PropBank triples. x denotes the entity proposed as answer, V is a fully qualified PropBank frame (e.g. give.01.V). Strategies are ordered by precedence and answers determined accordingly. This heuristic algorithm was iteratively tuned on the validation data set.", "d1505c6123c102e53eb19dff312cb25cea840b72/5-Figure1-1.png": "Figure 1: Document and query embedding models.", "d1505c6123c102e53eb19dff312cb25cea840b72/7-Figure2-1.png": "Figure 2: Precision@Recall for the attention models on the CNN validation data.", "d1505c6123c102e53eb19dff312cb25cea840b72/7-Table5-1.png": "Table 5: Accuracy of all the models and benchmarks on the CNN and Daily Mail datasets. The Uniform Reader baseline sets all of them(t) parameters to be equal.", "d1505c6123c102e53eb19dff312cb25cea840b72/8-Figure3-1.png": "Figure 3: Attention heat maps from the Attentive Reader for two correctly answered validation set queries (the correct answers are ent23 and ent63, respectively). Both examples require significant lexical generalisation and co-reference resolution in order to be answered correctly by a given model."}, "referred_figures_tables": [["d1505c6123c102e53eb19dff312cb25cea840b72/2-Table1-1.png"], ["d1505c6123c102e53eb19dff312cb25cea840b72/2-Table1-1.png"], ["d1505c6123c102e53eb19dff312cb25cea840b72/2-Table1-1.png"], ["d1505c6123c102e53eb19dff312cb25cea840b72/2-Table1-1.png"], ["d1505c6123c102e53eb19dff312cb25cea840b72/8-Figure3-1.png"], ["d1505c6123c102e53eb19dff312cb25cea840b72/7-Table5-1.png"], ["d1505c6123c102e53eb19dff312cb25cea840b72/7-Table5-1.png"], ["d1505c6123c102e53eb19dff312cb25cea840b72/7-Table5-1.png"], ["d1505c6123c102e53eb19dff312cb25cea840b72/2-Table1-1.png"], ["d1505c6123c102e53eb19dff312cb25cea840b72/2-Table1-1.png"]], "question_id": [0, 1, 4, 5, 11, 12, 13, 14, 20, 16], "question": ["Where do the authors source their labelled dataset from? ", "What is the ratio of the total number of articles collected from CNN and Daily News?", "The paper mentions using Daily News and CNN bullet-point summaries to generate queries. Would the authors' approach towards building this supervised dataset work effectively if these news sources created the summaries by merely extracting sentences from the whole article, instead of rephrasing and condensing text?", "How are the bullet-point summaries converted to queries?", "What is the main mathematical difference between the attentive LSTM reader and the vanilla Deep LSTM?", "Assuming the authors performed a brute force hyperparameter search on all permutations of the five hyperparameters - hidden layer sizes, depths, LR, batch size and dropout - how many total experiments would they have had to perform?", "Is hyperparameter optimization performed independently for the two dataset corpora?", "The deepest model that the authors experimented with had 8 layers in it. True or False? ", "The Daily Mail part of the dataset is approximately 2x larger than the CNN section of the dataset. True or false?  ", "Do the authors claim that bigger datasets would improve the performance and expressiveness of reading comprehension models?"], "question_section": ["Introduction", "Supervised training data for reading comprehension", "2 Supervised training data for reading comprehension", "Supervised training data for reading comprehension", "3.2 Neural Network Models", "4 Empirical Evaluation", "4 Empirical Evaluation", "4 Empirical Evaluation", "1 Introduction", "5 Conclusion"], "question_trigger_sentence": ["Using this approach we have collected two new corpora of roughly a million news stories with associated queries from the CNN and Daily Mail websites", "We have collected 93k articles from the CNN and 220k articles from the Daily Mail websites. ", "Of key importance is that these summary points are abstractive and do not simply copy sentences from the documents", "We construct a corpus of document\u2013query\u2013answer triples by turning these bullet points into Cloze [12] style questions by replacing one entity at a time with a placeholder.", "The Attentive Reader The Deep LSTM Reader must propagate dependencies over long distances in order to connect queries to their answers. The fixed width hidden vector forms a bottleneck for this information flow that we propose to circumvent using an attention mechanism inspired by recent results in translation and image recognition [6, 7]. This attention model first encodes the document and the query using separate bidirectional single layer LSTMs [19].", "For the Deep LSTM Reader, we consider hidden layer sizes [64, 128, 256], depths [1, 2, 4], initial learning rates [1E\u22123, 5E\u22124, 1E\u22124, 5E\u22125], batch sizes [16, 32] and dropout [0.0, 0.1, 0.2]. We evaluate two types of feeds. In the cqa setup we feed first the context document and subsequently the question into the encoder, while the qca model starts by feeding in the question followed by the context document", "All model hyperparameters were tuned on the respective validation sets of the two corpora.", "For the Deep LSTM Reader, we consider hidden layer sizes [64, 128, 256], depths [1, 2, 4]\n\nMemory constraints prevented us from experimenting with deeper Attentive Readers.", "Table 1: Corpus statistics.", "As such our data provides a scalable challenge that should support NLP research into the future. Further, significantly bigger training data sets can be acquired using the techniques we have described, undoubtedly allowing us to train more expressive and accurate models."], "question_type": ["Shallow question", "Shallow question", "Deep/complex question", "Shallow question", "Testing question", "Testing question", "Shallow question", "Shallow question", "Shallow question", "Shallow question"], "evidential_info": [[{"context": "In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context\u2013query\u2013answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites.", "rationale": "Authors mention that they create their dataset using articles from CNN and Daily Mail, two news websites. Additionally, they explain that they collected approximately 93,000 articles from CNN and 220,000 articles from Daily Mail."}, {"context": "Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document\u2013query\u2013answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets\u2014and to apply this method to other sources\u2014isavailable online333http://www.github.com/deepmind/rc-data/.", "rationale": "Mentions they create a dataset of one million data points from CNN and Daily Mail articles."}], [{"context": "Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document\u2013query\u2013answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets\u2014and to apply this method to other sources\u2014isavailable online333http://www.github.com/deepmind/rc-data/.", "rationale": "Mentions that approximately 93k articles, 220k articles were collected from CNN and Daily Mail respectively."}], [{"context": "In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context\u2013query\u2013answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites.", "rationale": "Mentions that both Daily Mail and CNN\u2019s bullet point summaries are abstractive (i.e. not just excerpts copied from the text). They mention that each bullet point is turned into a Cloze style question which can then be answered by the context (the news article in question)."}, {"context": "Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document\u2013query\u2013answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets\u2014and to apply this method to other sources\u2014isavailable online333http://www.github.com/deepmind/rc-data/.", "rationale": "They explain that they convert the \u201cparaphrased sentences\u201d of the bullet points to context-query-answer tuples while creating the dataset."}], [{"context": "In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context\u2013query\u2013answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites.", "rationale": "Explains how a summary and the corresponding article is converted into a context-query-answer tuple using entity detection and anonymisation algorithms."}, {"context": "Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document\u2013query\u2013answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets\u2014and to apply this method to other sources\u2014isavailable online333http://www.github.com/deepmind/rc-data/.", "rationale": "Explains that each bullet point contains a highlight from the news article. They convert each highlight/bullet point into a Cloze style question."}], [{"context": "The Attentive Reader can be viewed as a generalisation of the application ofMemory Networks to question answering [3]. That model employsan attention mechanism at the sentence level where each sentence is representedby a bag of embeddings. The Attentive Reader employs a finer grained tokenlevel attention mechanism where the tokens are embedded given their entirefuture and past context in the input document.", "rationale": "Qualitatively explains how the attention mechanism worked through a heatmap visualisation showing which words the attention based model \u201cfocussed\u201d on."}, {"context": "We can visualise the attention mechanism as a heatmap over a context document togain further insight into the models\u2019 performance. The highlighted words showwhich tokens in the document were attended to by the model. In addition we mustalso take into account that the vectors at each token integrate long rangecontextual information via the bidirectional LSTM encoders.Figure 3 depicts heat maps for two queries that were correctlyanswered by the Attentive Reader.777Note that these examples were chosenas they were short, the average CNN validation document contained 763 tokens and27 entities, thus most instances were significantly harder to answer than theseexamples. In both cases confidently arriving at the correct answer requires themodel to perform both significant lexical generalsiation, e.g.\u00a0\u2018killed\u2019\\rightarrow \u2018deceased\u2019, and co-reference or anaphora resolution, e.g.\u00a0\u2018ent119 was killed\u2019 \\rightarrow \u2018he was identified.\u2019 However it is alsoclear that the model is able to integrate these signals with rough heuristicindicators such as the proximity of query words to the candidate answer.", "rationale": "Explains how the attentive reader model uses a token-level attention mechanism where token embeddings contain information on their surrounding context in the document."}], [{"context": "All model hyperparameters were tuned on the respective validation sets of thetwo corpora.555For the Deep LSTM Reader, we consider hidden layer sizes{[64,128,\\underline{256}]}, depths {[1,\\underline{2},4]}, initiallearning rates {[1\\text{\\sc{e}}{-}3,5\\text{\\sc{e}}{-}4,\\underline{1\\text{\\sc{e}}{-}4},5\\text{\\sc{e}}{-}5]}, batchsizes {[16,\\underline{32}]} and dropout [0.0,\\underline{0.1},0.2]. Weevaluate two types of feeds. In the cqa setup we feed first thecontext document and subsequently the question into the encoder, while theqca model starts by feeding in the question followed by the contextdocument. We report results on the best model (underlined hyperparameters,qca setup). For the attention models we consider hidden layer sizes[64,128,256], single layer, initial learning rates[1\\text{\\sc{e}}{-}4,5\\text{\\sc{e}}{-}5,2.5\\text{\\sc{e}}{-}5,1\\text{\\sc{e}}{-}5], batch sizes [8,16,32] and dropout[0,0.1,0.2,0.5]. For all models we used asynchronous RmsProp[20] with a momentum of 0.9 and a decay of 0.95.See Appendix A for more details of the experimental setup.Our experimental results are in Table 5, with theAttentive and Impatient Readers performing best across both datasets.", "rationale": "For the Deep LSTM Reader experiments, the authors performed a hyperparameter search on the parameters hidden layer sizes (64, 128, 256), depths (1, 2, 4), learning rates (1e-3, 5e-4, 5e-5), batch sizes (16, 32) and dropout (0, 0.1, 0.2).  For attention models they performed a search on hidden layer size (64, 128, 256), learning rates (1E\u22124, 5E\u22125, 2.5E\u22125, 1E\u22125), batch sizes (8, 16, 32) and dropout (0, 0.1, 0.2, 0.5)."}], [{"context": "All model hyperparameters were tuned on the respective validation sets of thetwo corpora.555For the Deep LSTM Reader, we consider hidden layer sizes{[64,128,\\underline{256}]}, depths {[1,\\underline{2},4]}, initiallearning rates {[1\\text{\\sc{e}}{-}3,5\\text{\\sc{e}}{-}4,\\underline{1\\text{\\sc{e}}{-}4},5\\text{\\sc{e}}{-}5]}, batchsizes {[16,\\underline{32}]} and dropout [0.0,\\underline{0.1},0.2]. Weevaluate two types of feeds. In the cqa setup we feed first thecontext document and subsequently the question into the encoder, while theqca model starts by feeding in the question followed by the contextdocument. We report results on the best model (underlined hyperparameters,qca setup). For the attention models we consider hidden layer sizes[64,128,256], single layer, initial learning rates[1\\text{\\sc{e}}{-}4,5\\text{\\sc{e}}{-}5,2.5\\text{\\sc{e}}{-}5,1\\text{\\sc{e}}{-}5], batch sizes [8,16,32] and dropout[0,0.1,0.2,0.5]. For all models we used asynchronous RmsProp[20] with a momentum of 0.9 and a decay of 0.95.See Appendix A for more details of the experimental setup.Our experimental results are in Table 5, with theAttentive and Impatient Readers performing best across both datasets.", "rationale": "Indicates that the hyperparameters were tuned on the \u201crespective\u201d validation corpora."}], [{"context": "All model hyperparameters were tuned on the respective validation sets of thetwo corpora.555For the Deep LSTM Reader, we consider hidden layer sizes{[64,128,\\underline{256}]}, depths {[1,\\underline{2},4]}, initiallearning rates {[1\\text{\\sc{e}}{-}3,5\\text{\\sc{e}}{-}4,\\underline{1\\text{\\sc{e}}{-}4},5\\text{\\sc{e}}{-}5]}, batchsizes {[16,\\underline{32}]} and dropout [0.0,\\underline{0.1},0.2]. Weevaluate two types of feeds. In the cqa setup we feed first thecontext document and subsequently the question into the encoder, while theqca model starts by feeding in the question followed by the contextdocument. We report results on the best model (underlined hyperparameters,qca setup). For the attention models we consider hidden layer sizes[64,128,256], single layer, initial learning rates[1\\text{\\sc{e}}{-}4,5\\text{\\sc{e}}{-}5,2.5\\text{\\sc{e}}{-}5,1\\text{\\sc{e}}{-}5], batch sizes [8,16,32] and dropout[0,0.1,0.2,0.5]. For all models we used asynchronous RmsProp[20] with a momentum of 0.9 and a decay of 0.95.See Appendix A for more details of the experimental setup.Our experimental results are in Table 5, with theAttentive and Impatient Readers performing best across both datasets.", "rationale": "The authors experiment with depths of (1, 2, 4) during the hyperparameter search for the Deep LSTM Reader, and attained best performance when the depth was 2 layers. For attention based models, the authors only experimented with single layer models."}], [{"context": "Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document\u2013query\u2013answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets\u2014and to apply this method to other sources\u2014isavailable online333http://www.github.com/deepmind/rc-data/.", "rationale": "Explains that the dataset was made using 93k CNN and 220k Daily Mail articles. Additionally Table 1 shows that the number of queries in train set created from CNN is 380,298 while that corresponding number is 879,450 for DailyMail."}], [{"context": "While obtaining supervised natural language reading comprehension data hasproved difficult, some researchers have explored generating synthetic narrativesand queries [3, 4]. Such approaches allowthe generation of almost unlimited amounts of supervised data and enableresearchers to isolate the performance of their algorithms on individualsimulated phenomena. Work on such data has shown that neural network basedmodels hold promise for modelling reading comprehension, something that wewill build upon here. Historically, however, many similar approaches inComputational Linguistics have failed to manage the transition from syntheticdata to real environments, as such closed worlds inevitably fail tocapture the complexity, richness, and noise of natural language[5].", "rationale": "Mentions that previous approaches in the literature that attempt to use synthetic dataset approaches (which can quickly be used to generated datasets of large sizes) have shown promise for reading comprehension tasks"}, {"context": "The supervised paradigm for training machine reading and comprehension modelsprovides a promising avenue for making progress on the path to building fullnatural language understanding systems. We have demonstrated a methodology forobtaining a large number of document-query-answer triples and shown thatrecurrent and attention based neural networks provide an effective modellingframework for this task.Our analysis indicates that the Attentive and Impatient Readers are able topropagate and integrate semantic information over long distances. In particularwe believe that the incorporation of an attention mechanism is the keycontributor to these results.", "rationale": "Authors mention that creating a large-scale labelled dataset as one of their contributions."}, {"context": "Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document\u2013query\u2013answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets\u2014and to apply this method to other sources\u2014isavailable online333http://www.github.com/deepmind/rc-data/.", "rationale": "Indicates that attention based models and LSTM models are effective at reading comprehension tasks."}, {"context": "Note that the focus of this paper is to provide a corpus for evaluating a model\u2019sability to read and comprehend a single document, not world knowledge orco-occurrence. To understand that distinction consider for instance thefollowing Cloze form queries (created from headlines in the Daily Mailvalidation set):a) The hi-tech bra that helps you beat breast X;b) Could Saccharin help beat X ?;c) Can fish oils help fight prostate X ?An ngram language model trained on the Daily Mail would easily correctly predictthat (X = cancer), regardless of the contents of the contextdocument, simply because this is a very frequently cured entity in the Daily Mailcorpus.", "rationale": "Explains that they (the authors) intend to provide a dataset that could be used to benchmark performance on reading comprehension tasks"}]], "composition": ["The source of the labelled dataset in the paper is two news websites, namely, CNN and Daily News. The authors created the dataset of approximately one million data points from ~93k CNN and ~220k Daily Mail online news articles.", "Assuming \u201cDaily News\u201d here refers to \u201cDaily Mail\u201d, one of the websites the authors sourced the data from, the ratio of CNN:(Daily Mail) articles is approximately 93:220 or 1:2.36.", "The authors, in multiple places, emphasize that their approach relies on the fact that DailyMail and CNN both use abstractive summaries for their bullet points. This fact probably implies that the authors approach would not work on news sources that merely use excerpts or extracts for summaries.", "Each article in the news websites they used (CNN, DailyMail) has a couple of bullet points containing an abstractive summary of the article. They convert each bullet point into a Cloze style question and answer using entity detection algorithms. More details on what Cloze-style questions are is not available in this paper.", "The main difference between the attention-based LSTM and the vanilla one is that the former addresses the limitation of vanilla LSTM\u2019s fixed and limited context size by taking into account the entire context of every token via a token-level attention mechanism.", "For Deep LSTM readers 3 values of hidden layer sizes, 3 values of depths, 3 starting LRs, 2 batch sizes and 3 dropout fractions are considered. This is a total of (3 * 3 * 3 * 2 * 3) = 162 configurations. Similarly for attention models, they experimented with 3 hidden layer sizes, 4 possible learning rates, 3 batch sizes and 4 values of dropout (3*4*3*4) = 144 possible configurations. Thus, if the authors performed a brute force hyperparameter search, they would have had to done 162 experiments for the vanilla Deep LSTM model and 144 experiments for the attention based model, or a total of 162 + 144 = 306 experiments.", "Yes, it does appear that hyperparameter optimization for each dataset is performed independently.", "False. The deepest model the authors experimented with appears to be a four layer DeepLSTM Reader model. For attention models, the authors exclusively used only a single layer model.", "True. The ratio of number of articles from CNN and DailyMail is 1:2.36. Similarly, the ratio of queries from these datasets is given by 380,298:879,450 = 1:2.31. Since 2.31 and 2.36 both round down to 2, the statement is true, approximately.", "Based on the information in this paper alone, it is unclear if a bigger dataset would improve the performance of reading comprehension models. While authors explain that a key contribution they make is the creation of a real-world, massive labelled reading comprehension dataset, it is unclear if such a dataset is essential to improve the performance of reading comprehension models - the authors pitch their dataset-building approach also as a way of evaluating performance of these models, which is different from the dataset itself leading to better performance."], "Is_figure_in_evidence": [false, false, false, false, true, false, false, false, false, false], "Is_table_in_evidence": [true, true, true, true, false, true, true, true, true, true], "question_key": ["1024", "1025", "1028", "1029", "1033", "1034", "1035", "1036", "1037", "1038"], "passages": ["Progress on the path from shallow bag-of-words information retrievalalgorithms to machines capable of reading and understanding documents has beenslow. Traditional approaches to machine reading and comprehension have beenbased on either hand engineered grammars [1], or informationextraction methods of detecting predicate argument triples that can later bequeried as a relational database [2].Supervised machine learning approaches have largely been absent from this spacedue to both the lack of large scale training datasets, and the difficulty instructuring statistical models flexible enough to learn to exploit documentstructure.", "While obtaining supervised natural language reading comprehension data hasproved difficult, some researchers have explored generating synthetic narrativesand queries [3, 4]. Such approaches allowthe generation of almost unlimited amounts of supervised data and enableresearchers to isolate the performance of their algorithms on individualsimulated phenomena. Work on such data has shown that neural network basedmodels hold promise for modelling reading comprehension, something that wewill build upon here. Historically, however, many similar approaches inComputational Linguistics have failed to manage the transition from syntheticdata to real environments, as such closed worlds inevitably fail tocapture the complexity, richness, and noise of natural language[5].", "In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context\u2013query\u2013answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites.", "We demonstrate the efficacy of our new corpora by building novel deep learningmodels for reading comprehension. These models draw on recent developmentsfor incorporating attention mechanisms into recurrent neural network architectures[6, 7, 8, 4]. This allows a model tofocus on the aspects of a document that it believes will help it answer aquestion, and also allows us to visualises its inference process.We compare these neural models to a range of baselines and heuristic benchmarksbased upon a traditional frame semantic analysis provided by a state-of-the-artnatural language processing (NLP) pipeline. Our results indicate that the neuralmodels achieve a higher accuracy, and do so without any specific encoding of thedocument or query structure.", "The reading comprehension task naturally lends itself to a formulation as asupervised learning problem. Specifically we seek to estimate the conditionalprobability p(a|c,q)\ud835\udc5dconditional\ud835\udc4e\ud835\udc50\ud835\udc5ep(a|c,q)italic_p ( italic_a | italic_c , italic_q ), where c\ud835\udc50citalic_c is a context document, q\ud835\udc5eqitalic_q a query relating tothat document, and a\ud835\udc4eaitalic_a the answer to that query.For a focused evaluation we wish to be able to exclude additional information,such as world knowledge gained from co-occurrence statistics, in order to test amodel\u2019s core capability to detect and understand the linguistic relationshipsbetween entities in the context document.", "Such an approach requires a large training corpus of document\u2013query\u2013answertriples and until now such corpora have been limited to hundreds of examples andthus mostly of use only for testing [9]. This limitationhas meant that most work in this area has taken the form of unsupervisedapproaches which use templates or syntactic/semantic analysers to extractrelation tuples from the document to form a knowledge graph that can be queried.", "Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document\u2013query\u2013answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets\u2014and to apply this method to other sources\u2014isavailable online333http://www.github.com/deepmind/rc-data/.", "Note that the focus of this paper is to provide a corpus for evaluating a model\u2019sability to read and comprehend a single document, not world knowledge orco-occurrence. To understand that distinction consider for instance thefollowing Cloze form queries (created from headlines in the Daily Mailvalidation set):a) The hi-tech bra that helps you beat breast X;b) Could Saccharin help beat X ?;c) Can fish oils help fight prostate X ?An ngram language model trained on the Daily Mail would easily correctly predictthat (X = cancer), regardless of the contents of the contextdocument, simply because this is a very frequently cured entity in the Daily Mailcorpus.", "To prevent such degenerate solutions and create a focused task we anonymise andrandomise our corpora with the following procedure,a) use a coreference system to establish coreferents in eachdata point;b) replace all entities with abstract entity markers according tocoreference;c) randomly permute these entity markers whenever a data point is loaded.", "Compare the original and anonymised version of the example in Table3. Clearly a human reader can answer both queries correctly.However in the anonymised setup the context document is required for answeringthe query, whereas the original version could also be answered by someone withthe requisite background knowledge.Therefore, following this procedure, the only remaining strategy for answeringquestions is to do so by exploiting the context presented with each question.Thus performance on our two corpora truly measures reading comprehensioncapability. Naturally a production system would benefit from using allavailable information sources, such as clues through language and co-occurrencestatistics.", "Table 2 gives an indication of the difficulty of thetask, showing how frequent the correct answer is contained in the top N\ud835\udc41Nitalic_N entitymarkers in a given document. Note that our models don\u2019t distinguish betweenentity markers and regular words. This makes the task harder and the models moregeneral.", "So far we have motivated the need for better datasets and tasks to evaluate thecapabilities of machine reading models. We proceed by describing a number ofbaselines, benchmarks and new models to evaluate against this paradigm. Wedefine two simple baselines, the majority baseline (maximum frequency)picks the entity most frequently observed in the context document, whereas theexclusive majority (exclusive frequency) chooses the entity mostfrequently observed in the context but not observed in the query. The ideabehind this exclusion is that the placeholder is unlikely to be mentioned twicein a single Cloze form query.", "Traditionally, a pipeline of NLP models has been used for attempting questionanswering, that is models that make heavy use of linguistic annotation,structured world knowledge and semantic parsing and similar NLP pipelineoutputs.Building on these approaches, we define a number of NLP-centric models for ourmachine reading task.", "Frame-semantic parsing attempts to identify predicates and their arguments,allowing models access to information about \u201cwho did what to whom\u201d. Naturallythis kind of annotation lends itself to being exploited for question answering.We develop a benchmark that makes use of frame-semantic annotationswhich we obtained by parsing our model with a state-of-the-art frame-semanticparser [13, 14]. As the parser makes extensive useof linguistic information we run these benchmarks on the unanonymised version ofour corpora. There is no significant advantage in this as the frame-semanticapproach used here does not possess the capability to generalise through alanguage model beyond exploiting one during the parsing phase.Thus, the key objective of evaluating machine comprehension abilities ismaintained. Extracting entity-predicate triples\u2014denoted as(e1,V,e2)subscript\ud835\udc521\ud835\udc49subscript\ud835\udc522(e_{1},V,e_{2})( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_V , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )\u2014from both the query q\ud835\udc5eqitalic_q and context document d\ud835\udc51ditalic_d, we attempt toresolve queries using a number of rules with an increasing recall/precisiontrade-off as follows (Table 4).", "For reasons of clarity, we pretend that all PropBank triples are of the form(e1,V,e2)subscript\ud835\udc521\ud835\udc49subscript\ud835\udc522(e_{1},V,e_{2})( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_V , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ). In practice, we take the argument numberings of the parser intoaccount and only compare like with like, except in cases such as the permutedframe rule, where ordering is relaxed. In the case of multiple possible answersfrom a single rule, we randomly choose one.", "We consider another baseline that relies on word distance measurements. Here, wealign the placeholder of the Cloze form question with each possible entity inthe context document and calculate a distance measure between the question and thecontext around the aligned entity.This score is calculated by summing the distances of every word in q\ud835\udc5eqitalic_qto their nearest aligned word in d\ud835\udc51ditalic_d, where alignment is defined by matchingwords either directly or as aligned by the coreference system. We tune themaximum penalty per word (m=8\ud835\udc5a8m=8italic_m = 8) on the validation data.", "Neural networks have successfully been applied to a range of tasks in NLP.This includes classification tasks such as sentiment analysis[15] or POS tagging [16], as wellas generative problems such as language modelling or machine translation[17].We propose three neural models for estimating the probability of word type a\ud835\udc4eaitalic_afrom document d\ud835\udc51ditalic_d answering query q\ud835\udc5eqitalic_q:p(a|d,q)\ud835\udc5dconditional\ud835\udc4e\ud835\udc51\ud835\udc5e\\displaystyle p(a|d,q)italic_p ( italic_a | italic_d , italic_q )\u221dexp\u2061(W(a)g(d,q)),s.t.\u00a0a\u2208V,formulae-sequenceproportional-toabsent\ud835\udc4a\ud835\udc4e\ud835\udc54\ud835\udc51\ud835\udc5es.t.\u00a0\ud835\udc4e\ud835\udc49\\displaystyle\\propto\\exp\\left(W(a)g(d,q)\\right),\\quad\\text{s.t. }a\\in V,\u221d roman_exp ( italic_W ( italic_a ) italic_g ( italic_d , italic_q ) ) , s.t. italic_a \u2208 italic_V ,where V\ud835\udc49Vitalic_V is the vocabulary444The vocabulary includes all the word typesin the documents, questions, the entity maskers, and the question unknownentity marker.,and W(a)\ud835\udc4a\ud835\udc4eW(a)italic_W ( italic_a ) indexes row a\ud835\udc4eaitalic_a of weight matrix W\ud835\udc4aWitalic_W and through a slight abuse ofnotation word types double as indexes. Note that we do not privilege entities orvariables, the model must learn to differentiate these in the input sequence.The function g(d,q)\ud835\udc54\ud835\udc51\ud835\udc5eg(d,q)italic_g ( italic_d , italic_q ) returns a vector embedding of a document and query pair.", "Long short-term memory (LSTM, [18]) networks haverecently seen considerable success in tasks such as machine translation andlanguage modelling [17]. When used for translation, DeepLSTMs [19] have shown a remarkable ability to embed longsequences into a vector representation which contains enough information togenerate a full translation in another language. Our first neural model forreading comprehension tests the ability of Deep LSTM encoders to handlesignificantly longer sequences. We feed our documents one word at a time intoa Deep LSTM encoder, after a delimiter we then also feed the query into theencoder. Alternatively we also experiment with processing the query then thedocument. The result is that this model processes each document query pair as asingle long sequence. Given the embedded document and query the networkpredicts which token in the document answers the query.", "We employ a Deep LSTM cell with skip connections from each input x(t)\ud835\udc65\ud835\udc61x(t)italic_x ( italic_t ) to everyhidden layer, and from every hidden layer to the output y(t)\ud835\udc66\ud835\udc61y(t)italic_y ( italic_t ):x\u2032(t,k)superscript\ud835\udc65\u2032\ud835\udc61\ud835\udc58\\displaystyle x^{\\prime}(t,k)italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_t , italic_k )=x(t)||y\u2032(t,k\u22121),y(t)=y\u2032(t,1)||\u2026||y\u2032(t,K)\\displaystyle=x(t)||y^{\\prime}(t,k-1),\\quad\\quad y(t)=y^{\\prime}(t,1)||\\ldots||y^{\\prime}(t,K)= italic_x ( italic_t ) | | italic_y start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_t , italic_k - 1 ) , italic_y ( italic_t ) = italic_y start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_t , 1 ) | | \u2026 | | italic_y start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_t , italic_K )i(t,k)\ud835\udc56\ud835\udc61\ud835\udc58\\displaystyle i(t,k)italic_i ( italic_t , italic_k )=\u03c3(Wkxix\u2032(t,k)+Wkhih(t\u22121,k)+Wkcic(t\u22121,k)+bki)absent\ud835\udf0esubscript\ud835\udc4a\ud835\udc58\ud835\udc65\ud835\udc56superscript\ud835\udc65\u2032\ud835\udc61\ud835\udc58subscript\ud835\udc4a\ud835\udc58\u210e\ud835\udc56\u210e\ud835\udc611\ud835\udc58subscript\ud835\udc4a\ud835\udc58\ud835\udc50\ud835\udc56\ud835\udc50\ud835\udc611\ud835\udc58subscript\ud835\udc4f\ud835\udc58\ud835\udc56\\displaystyle=\\sigma\\left(W_{kxi}x^{\\prime}(t,k)+W_{khi}h(t-1,k)+W_{kci}c(t-1,k)+b_{ki}\\right)= italic_\u03c3 ( italic_W start_POSTSUBSCRIPT italic_k italic_x italic_i end_POSTSUBSCRIPT italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_t , italic_k ) + italic_W start_POSTSUBSCRIPT italic_k italic_h italic_i end_POSTSUBSCRIPT italic_h ( italic_t - 1 , italic_k ) + italic_W start_POSTSUBSCRIPT italic_k italic_c italic_i end_POSTSUBSCRIPT italic_c ( italic_t - 1 , italic_k ) + italic_b start_POSTSUBSCRIPT italic_k italic_i end_POSTSUBSCRIPT )f(t,k)\ud835\udc53\ud835\udc61\ud835\udc58\\displaystyle f(t,k)italic_f ( italic_t , italic_k )=\u03c3(Wkxfx(t)+Wkhfh(t\u22121,k)+Wkcfc(t\u22121,k)+bkf)absent\ud835\udf0esubscript\ud835\udc4a\ud835\udc58\ud835\udc65\ud835\udc53\ud835\udc65\ud835\udc61subscript\ud835\udc4a\ud835\udc58\u210e\ud835\udc53\u210e\ud835\udc611\ud835\udc58subscript\ud835\udc4a\ud835\udc58\ud835\udc50\ud835\udc53\ud835\udc50\ud835\udc611\ud835\udc58subscript\ud835\udc4f\ud835\udc58\ud835\udc53\\displaystyle=\\sigma\\left(W_{kxf}x(t)+W_{khf}h(t-1,k)+W_{kcf}c(t-1,k)+b_{kf}\\right)= italic_\u03c3 ( italic_W start_POSTSUBSCRIPT italic_k italic_x italic_f end_POSTSUBSCRIPT italic_x ( italic_t ) + italic_W start_POSTSUBSCRIPT italic_k italic_h italic_f end_POSTSUBSCRIPT italic_h ( italic_t - 1 , italic_k ) + italic_W start_POSTSUBSCRIPT italic_k italic_c italic_f end_POSTSUBSCRIPT italic_c ( italic_t - 1 , italic_k ) + italic_b start_POSTSUBSCRIPT italic_k italic_f end_POSTSUBSCRIPT )c(t,k)\ud835\udc50\ud835\udc61\ud835\udc58\\displaystyle c(t,k)italic_c ( italic_t , italic_k )=f(t,k)c(t\u22121,k)+i(t,k)tanh\u2061(Wkxcx\u2032(t,k)+Wkhch(t\u22121,k)+bkc)absent\ud835\udc53\ud835\udc61\ud835\udc58\ud835\udc50\ud835\udc611\ud835\udc58\ud835\udc56\ud835\udc61\ud835\udc58subscript\ud835\udc4a\ud835\udc58\ud835\udc65\ud835\udc50superscript\ud835\udc65\u2032\ud835\udc61\ud835\udc58subscript\ud835\udc4a\ud835\udc58\u210e\ud835\udc50\u210e\ud835\udc611\ud835\udc58subscript\ud835\udc4f\ud835\udc58\ud835\udc50\\displaystyle=f(t,k)c(t-1,k)+i(t,k)\\tanh\\left(W_{kxc}x^{\\prime}(t,k)+W_{khc}h(t-1,k)+b_{kc}\\right)= italic_f ( italic_t , italic_k ) italic_c ( italic_t - 1 , italic_k ) + italic_i ( italic_t , italic_k ) roman_tanh ( italic_W start_POSTSUBSCRIPT italic_k italic_x italic_c end_POSTSUBSCRIPT italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_t , italic_k ) + italic_W start_POSTSUBSCRIPT italic_k italic_h italic_c end_POSTSUBSCRIPT italic_h ( italic_t - 1 , italic_k ) + italic_b start_POSTSUBSCRIPT italic_k italic_c end_POSTSUBSCRIPT )o(t,k)\ud835\udc5c\ud835\udc61\ud835\udc58\\displaystyle o(t,k)italic_o ( italic_t , italic_k )=\u03c3(Wkxox\u2032(t,k)+Wkhoh(t\u22121,k)+Wkcoc(t,k)+bko)absent\ud835\udf0esubscript\ud835\udc4a\ud835\udc58\ud835\udc65\ud835\udc5csuperscript\ud835\udc65\u2032\ud835\udc61\ud835\udc58subscript\ud835\udc4a\ud835\udc58\u210e\ud835\udc5c\u210e\ud835\udc611\ud835\udc58subscript\ud835\udc4a\ud835\udc58\ud835\udc50\ud835\udc5c\ud835\udc50\ud835\udc61\ud835\udc58subscript\ud835\udc4f\ud835\udc58\ud835\udc5c\\displaystyle=\\sigma\\left(W_{kxo}x^{\\prime}(t,k)+W_{kho}h(t-1,k)+W_{kco}c(t,k)+b_{ko}\\right)= italic_\u03c3 ( italic_W start_POSTSUBSCRIPT italic_k italic_x italic_o end_POSTSUBSCRIPT italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_t , italic_k ) + italic_W start_POSTSUBSCRIPT italic_k italic_h italic_o end_POSTSUBSCRIPT italic_h ( italic_t - 1 , italic_k ) + italic_W start_POSTSUBSCRIPT italic_k italic_c italic_o end_POSTSUBSCRIPT italic_c ( italic_t , italic_k ) + italic_b start_POSTSUBSCRIPT italic_k italic_o end_POSTSUBSCRIPT )h(t,k)\u210e\ud835\udc61\ud835\udc58\\displaystyle h(t,k)italic_h ( italic_t , italic_k )=o(t,k)tanh\u2061(c(t,k))absent\ud835\udc5c\ud835\udc61\ud835\udc58\ud835\udc50\ud835\udc61\ud835\udc58\\displaystyle=o(t,k)\\tanh\\left(c(t,k)\\right)= italic_o ( italic_t , italic_k ) roman_tanh ( italic_c ( italic_t , italic_k ) )y\u2032(t,k)superscript\ud835\udc66\u2032\ud835\udc61\ud835\udc58\\displaystyle y^{\\prime}(t,k)italic_y start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_t , italic_k )=Wkyh(t,k)+bkyabsentsubscript\ud835\udc4a\ud835\udc58\ud835\udc66\u210e\ud835\udc61\ud835\udc58subscript\ud835\udc4f\ud835\udc58\ud835\udc66\\displaystyle=W_{ky}h(t,k)+b_{ky}= italic_W start_POSTSUBSCRIPT italic_k italic_y end_POSTSUBSCRIPT italic_h ( italic_t , italic_k ) + italic_b start_POSTSUBSCRIPT italic_k italic_y end_POSTSUBSCRIPTwhere ||||| | indicates vector concatenation h(t,k)\u210e\ud835\udc61\ud835\udc58h(t,k)italic_h ( italic_t , italic_k ) is the hidden state for layerk\ud835\udc58kitalic_k at time t\ud835\udc61titalic_t, and i\ud835\udc56iitalic_i, f\ud835\udc53fitalic_f, o\ud835\udc5coitalic_o are the input, forget, andoutput gates respectively.Thus our Deep LSTM Reader is defined by gLSTM(d,q)=y(|d|+|q|)superscript\ud835\udc54LSTM\ud835\udc51\ud835\udc5e\ud835\udc66\ud835\udc51\ud835\udc5eg^{\\text{LSTM}}(d,q)=y(|d|+|q|)italic_g start_POSTSUPERSCRIPT LSTM end_POSTSUPERSCRIPT ( italic_d , italic_q ) = italic_y ( | italic_d | + | italic_q | ) with input x(t)\ud835\udc65\ud835\udc61x(t)italic_x ( italic_t ) the concatenation of d\ud835\udc51ditalic_d and q\ud835\udc5eqitalic_q separated by the delimiter ||||||| | |.", "The Deep LSTM Reader must propagate dependencies over long distances in order toconnect queries to their answers. The fixed width hidden vector forms abottleneck for this information flow that we propose to circumvent using anattention mechanism inspired by recent results in translation and imagerecognition [6, 7].This attention model first encodes the document and the query using separatebidirectional single layer LSTMs [19].", "We denote theoutputs of the forward and backward LSTMs as y\u2192(t)\u2192\ud835\udc66\ud835\udc61\\overrightarrow{y}(t)over\u2192 start_ARG italic_y end_ARG ( italic_t ) and y\u2190(t)\u2190\ud835\udc66\ud835\udc61\\overleftarrow{y}(t)over\u2190 start_ARG italic_y end_ARG ( italic_t )respectively. The encoding u\ud835\udc62uitalic_u of a query of length |q|\ud835\udc5e|q|| italic_q | is formed by theconcatenation of the final forward and backward outputs,u=yq\u2192(|q|)||yq\u2190(1).u=\\overrightarrow{y_{q}}(|q|)\\,\\,||\\,\\,\\overleftarrow{y_{q}}(1).italic_u = over\u2192 start_ARG italic_y start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG ( | italic_q | ) | | over\u2190 start_ARG italic_y start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG ( 1 ) .", "For the document the composite output for each token at position t\ud835\udc61titalic_t is,yd(t)=yd\u2192(t)||yd\u2190(t).y_{d}(t)=\\overrightarrow{y_{d}}(t)\\,\\,||\\,\\,\\overleftarrow{y_{d}}(t).italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_t ) = over\u2192 start_ARG italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG ( italic_t ) | | over\u2190 start_ARG italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG ( italic_t ) .The representation r\ud835\udc5fritalic_r of the document d\ud835\udc51ditalic_d is formed by a weighted sum of theseoutput vectors. These weights are interpreted as the degree to which the networkattends to a particular token in the document when answering the query:m(t)\ud835\udc5a\ud835\udc61\\displaystyle m(t)italic_m ( italic_t )=tanh\u2061(Wymyd(t)+Wumu),absentsubscript\ud835\udc4a\ud835\udc66\ud835\udc5asubscript\ud835\udc66\ud835\udc51\ud835\udc61subscript\ud835\udc4a\ud835\udc62\ud835\udc5a\ud835\udc62\\displaystyle=\\tanh\\left(W_{ym}y_{d}(t)+W_{um}u\\right),= roman_tanh ( italic_W start_POSTSUBSCRIPT italic_y italic_m end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_t ) + italic_W start_POSTSUBSCRIPT italic_u italic_m end_POSTSUBSCRIPT italic_u ) ,s(t)\ud835\udc60\ud835\udc61\\displaystyle s(t)italic_s ( italic_t )\u221dexp\u2061(wms\u22bam(t)),proportional-toabsentsuperscriptsubscriptw\ud835\udc5a\ud835\udc60\u22ba\ud835\udc5a\ud835\udc61\\displaystyle\\propto\\exp\\left(\\mathrm{w}_{ms}^{\\intercal}m(t)\\right),\u221d roman_exp ( roman_w start_POSTSUBSCRIPT italic_m italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22ba end_POSTSUPERSCRIPT italic_m ( italic_t ) ) ,r\ud835\udc5f\\displaystyle ritalic_r=yds,absentsubscript\ud835\udc66\ud835\udc51\ud835\udc60\\displaystyle=y_{d}s,= italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_s ,where we are interpreting ydsubscript\ud835\udc66\ud835\udc51y_{d}italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT as a matrix with each column being the compositerepresentation yd(t)subscript\ud835\udc66\ud835\udc51\ud835\udc61y_{d}(t)italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_t ) of document token t\ud835\udc61titalic_t.The variable s(t)\ud835\udc60\ud835\udc61s(t)italic_s ( italic_t ) is the normalised attention at token t\ud835\udc61titalic_t. Giventhis attention score the embedding of the document r\ud835\udc5fritalic_r is computed as theweighted sum of the token embeddings.The model is completed with the definition of the joint document and queryembedding via a non-linear combination:gAR(d,q)=tanh\u2061(Wrgr+Wugu).superscript\ud835\udc54AR\ud835\udc51\ud835\udc5esubscript\ud835\udc4a\ud835\udc5f\ud835\udc54\ud835\udc5fsubscript\ud835\udc4a\ud835\udc62\ud835\udc54\ud835\udc62\\displaystyle g^{\\text{AR}}(d,q)=\\tanh\\left(W_{rg}r+W_{ug}u\\right).italic_g start_POSTSUPERSCRIPT AR end_POSTSUPERSCRIPT ( italic_d , italic_q ) = roman_tanh ( italic_W start_POSTSUBSCRIPT italic_r italic_g end_POSTSUBSCRIPT italic_r + italic_W start_POSTSUBSCRIPT italic_u italic_g end_POSTSUBSCRIPT italic_u ) .", "The Attentive Reader can be viewed as a generalisation of the application ofMemory Networks to question answering [3]. That model employsan attention mechanism at the sentence level where each sentence is representedby a bag of embeddings. The Attentive Reader employs a finer grained tokenlevel attention mechanism where the tokens are embedded given their entirefuture and past context in the input document.", "The Attentive Reader is able to focus on the passages of a context documentthat are most likely to inform the answer to the query. We can go further byequipping the model with the ability to reread from the document as each querytoken is read. At each token i\ud835\udc56iitalic_i of the query q\ud835\udc5eqitalic_q the model computes a documentrepresentation vector r(i)\ud835\udc5f\ud835\udc56r(i)italic_r ( italic_i ) using the bidirectional embedding yq(i)=yq\u2192(i)||yq\u2190(i)y_{q}(i)=\\overrightarrow{y_{q}}(i)\\,\\,||\\,\\,\\overleftarrow{y_{q}}(i)italic_y start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_i ) = over\u2192 start_ARG italic_y start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG ( italic_i ) | | over\u2190 start_ARG italic_y start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG ( italic_i ):m(i,t)\ud835\udc5a\ud835\udc56\ud835\udc61\\displaystyle m(i,t)italic_m ( italic_i , italic_t )=tanh\u2061(Wdmyd(t)+Wrmr(i\u22121)+Wqmyq(i)),1\u2264i\u2264|q|,formulae-sequenceabsentsubscript\ud835\udc4a\ud835\udc51\ud835\udc5asubscript\ud835\udc66\ud835\udc51\ud835\udc61subscript\ud835\udc4a\ud835\udc5f\ud835\udc5a\ud835\udc5f\ud835\udc561subscript\ud835\udc4a\ud835\udc5e\ud835\udc5asubscript\ud835\udc66\ud835\udc5e\ud835\udc561\ud835\udc56\ud835\udc5e\\displaystyle=\\tanh\\left(W_{dm}y_{d}(t)+W_{rm}r(i-1)+W_{qm}y_{q}(i)\\right),\\quad 1\\leq i\\leq|q|,= roman_tanh ( italic_W start_POSTSUBSCRIPT italic_d italic_m end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_t ) + italic_W start_POSTSUBSCRIPT italic_r italic_m end_POSTSUBSCRIPT italic_r ( italic_i - 1 ) + italic_W start_POSTSUBSCRIPT italic_q italic_m end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_i ) ) , 1 \u2264 italic_i \u2264 | italic_q | ,s(i,t)\ud835\udc60\ud835\udc56\ud835\udc61\\displaystyle s(i,t)italic_s ( italic_i , italic_t )\u221dexp\u2061(wms\u22bam(i,t)),proportional-toabsentsuperscriptsubscriptw\ud835\udc5a\ud835\udc60\u22ba\ud835\udc5a\ud835\udc56\ud835\udc61\\displaystyle\\propto\\exp\\left(\\mathrm{w}_{ms}^{\\intercal}m(i,t)\\right),\u221d roman_exp ( roman_w start_POSTSUBSCRIPT italic_m italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22ba end_POSTSUPERSCRIPT italic_m ( italic_i , italic_t ) ) ,r(0)\ud835\udc5f0\\displaystyle r(0)italic_r ( 0 )=\ud835\udc2b\ud835\udfce,r(i)=yd\u22bas(i)+tanh\u2061(Wrrr(i\u22121))1\u2264i\u2264|q|.formulae-sequenceabsentsubscript\ud835\udc2b0formulae-sequence\ud835\udc5f\ud835\udc56superscriptsubscript\ud835\udc66\ud835\udc51\u22ba\ud835\udc60\ud835\udc56subscript\ud835\udc4a\ud835\udc5f\ud835\udc5f\ud835\udc5f\ud835\udc5611\ud835\udc56\ud835\udc5e\\displaystyle=\\mathbf{r_{0}},\\quad r(i)=y_{d}^{\\intercal}s(i)+\\tanh\\left(W_{rr}r(i-1)\\right)\\quad 1\\leq i\\leq|q|.= bold_r start_POSTSUBSCRIPT bold_0 end_POSTSUBSCRIPT , italic_r ( italic_i ) = italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22ba end_POSTSUPERSCRIPT italic_s ( italic_i ) + roman_tanh ( italic_W start_POSTSUBSCRIPT italic_r italic_r end_POSTSUBSCRIPT italic_r ( italic_i - 1 ) ) 1 \u2264 italic_i \u2264 | italic_q | .The result is an attention mechanism that allows the model to recurrentlyaccumulate information from the document as it sees each query token, ultimatelyoutputting a final joint document query representation for the answer prediction,gIR(d,q)=tanh\u2061(Wrgr(|q|)+Wqgu).superscript\ud835\udc54IR\ud835\udc51\ud835\udc5esubscript\ud835\udc4a\ud835\udc5f\ud835\udc54\ud835\udc5f\ud835\udc5esubscript\ud835\udc4a\ud835\udc5e\ud835\udc54\ud835\udc62\\displaystyle g^{\\text{IR}}(d,q)=\\tanh\\left(W_{rg}r(|q|)+W_{qg}u\\right).italic_g start_POSTSUPERSCRIPT IR end_POSTSUPERSCRIPT ( italic_d , italic_q ) = roman_tanh ( italic_W start_POSTSUBSCRIPT italic_r italic_g end_POSTSUBSCRIPT italic_r ( | italic_q | ) + italic_W start_POSTSUBSCRIPT italic_q italic_g end_POSTSUBSCRIPT italic_u ) .", "Having described a number of models in the previous section, we next evaluatethese models on our reading comprehension corpora. Our hypothesis is that neural models should in principle be well suited for thistask. However, we argued that simple recurrent models such as the LSTMprobably have insufficient expressive power for solving tasks that requirecomplex inference. We expect that the attention-based models would thereforeoutperform the pure LSTM-based approaches.", "Considering the second dimension of our investigation, the comparison oftraditional versus neural approaches to NLP, we do not have a strong priorfavouring one approach over the other. While numerous publications in the pastfew years have demonstrated neural models outperforming classical methods, itremains unclear how much of that is a side-effect of the language modellingcapabilities intrinsic to any neural model for NLP. The entity anonymisation andpermutation aspect of the task presented here may end up levelling the playingfield in that regard, favouring models capable of dealing with syntax ratherthan just semantics.", "With these considerations in mind, the experimental part of this paper isdesigned with a three-fold aim. First, we want to establish the difficulty of ourmachine reading task by applying a wide range of models to it. Second, we comparethe performance of parse-based methods versus that of neural models. Third,within the group of neural models examined, we want to determine what eachcomponent contributes to the end performance; that is, we want to analyse theextent to which an LSTM can solve this task, and to what extent variousattention mechanisms impact performance.", "All model hyperparameters were tuned on the respective validation sets of thetwo corpora.555For the Deep LSTM Reader, we consider hidden layer sizes[64,128,256\u00af]64128\u00af256{[64,128,\\underline{256}]}[ 64 , 128 , under\u00af start_ARG 256 end_ARG ], depths [1,2\u00af,4]1\u00af24{[1,\\underline{2},4]}[ 1 , under\u00af start_ARG 2 end_ARG , 4 ], initiallearning rates [1e\u22123,5e\u22124,1e\u22124\u00af,5e\u22125]1e35e4\u00af1e45e5{[1\\text{\\sc{e}}{-}3,5\\text{\\sc{e}}{-}4,\\underline{1\\text{\\sc{e}}{-}4},5\\text{\\sc{e}}{-}5]}[ 1 e - 3 , 5 e - 4 , under\u00af start_ARG 1 e - 4 end_ARG , 5 e - 5 ], batchsizes [16,32\u00af]16\u00af32{[16,\\underline{32}]}[ 16 , under\u00af start_ARG 32 end_ARG ] and dropout [0.0,0.1\u00af,0.2]0.0\u00af0.10.2[0.0,\\underline{0.1},0.2][ 0.0 , under\u00af start_ARG 0.1 end_ARG , 0.2 ]. Weevaluate two types of feeds. In the cqa setup we feed first thecontext document and subsequently the question into the encoder, while theqca model starts by feeding in the question followed by the contextdocument. We report results on the best model (underlined hyperparameters,qca setup). For the attention models we consider hidden layer sizes[64,128,256]64128256[64,128,256][ 64 , 128 , 256 ], single layer, initial learning rates[1e\u22124,5e\u22125,2.5e\u22125,1e\u22125]1e45e52.5e51e5[1\\text{\\sc{e}}{-}4,5\\text{\\sc{e}}{-}5,2.5\\text{\\sc{e}}{-}5,1\\text{\\sc{e}}{-}5][ 1 e - 4 , 5 e - 5 , 2.5 e - 5 , 1 e - 5 ], batch sizes [8,16,32]81632[8,16,32][ 8 , 16 , 32 ] and dropout[0,0.1,0.2,0.5]00.10.20.5[0,0.1,0.2,0.5][ 0 , 0.1 , 0.2 , 0.5 ]. For all models we used asynchronous RmsProp[20] with a momentum of 0.90.90.90.9 and a decay of 0.950.950.950.95.See Appendix A for more details of the experimental setup.Our experimental results are in Table 5, with theAttentive and Impatient Readers performing best across both datasets.", "While the one frame-semantic model proposed in this paper is clearly asimplification of what could be achieved with annotations from an NLP pipeline,it does highlight the difficulty of the task when approached from a symbolic NLPperspective.", "Two issues stand out when analysing the results in detail. First, theframe-semantic pipeline has a poor degree of coverage with many relations notbeing picked up by our PropBank parser as they do not adhere to the defaultpredicate-argument structure. This effect is exacerbated by the type of languageused in the highlights that form the basis of our datasets.The second issue is that the frame-semantic approach does not trivially scale tosituations where several sentences, and thus frames, are required to answer aquery. This was true for the majority of queries in the dataset.", "More surprising perhaps is the relatively strong performance of the worddistance benchmark, particularly relative to the frame-semantic benchmark, whichwe had expected to perform better. Here, again, the nature of the datasets usedcan explain aspects of this result. Where the frame-semantic model suffered dueto the language used in the highlights, the word distance model benefited.Particularly in the case of the Daily Mail dataset, highlights frequently havesignificant lexical overlap with passages in the accompanying article,which makes it easy for the word distance benchmark.For instance the query \u201cTom Hanks is friends with X\u2019smanager, Scooter Brown\u201d has the phrase \u201c\u2026 turns out he is goodfriends with Scooter Brown, manager for Carly Rae Jepson\u201d in the context. Theword distance benchmark correctly aligns these two while the frame-semanticapproach fails to pickup the friendship or management relations when parsingthe query.We expect that on other types of machine reading data where questions ratherthan Cloze queries are used this particular model would perform significantlyworse.", "Within the group of neural models explored here, the results paint a clearpicture with the Impatient and the Attentive Readers outperforming all othermodels. This is consistent with our hypothesis that attention is a keyingredient for machine reading and question answering due to the need topropagate information over long distances. The Deep LSTM Readerperforms surprisingly well, once again demonstrating that this simple sequentialarchitecture can do a reasonable job of learning to abstract long sequences,even when they are up to two thousand tokens in length. However this model doesfail to match the performance of the attention based models, even though theseonly use single layer LSTMs.666Memory constraints prevented us fromexperimenting with deeper Attentive Readers.", "The poor results of the Uniform Reader support our hypothesis ofthe significance of the attention mechanism in the Attentive model\u2019sperformance as the only difference between these models is that the attentionvariables are ignored in the Uniform Reader. The precision@recall statistics inFigure\u00a02 again highlight the strength of the attentive approach.", "We can visualise the attention mechanism as a heatmap over a context document togain further insight into the models\u2019 performance. The highlighted words showwhich tokens in the document were attended to by the model. In addition we mustalso take into account that the vectors at each token integrate long rangecontextual information via the bidirectional LSTM encoders.Figure 3 depicts heat maps for two queries that were correctlyanswered by the Attentive Reader.777Note that these examples were chosenas they were short, the average CNN validation document contained 763 tokens and27 entities, thus most instances were significantly harder to answer than theseexamples. In both cases confidently arriving at the correct answer requires themodel to perform both significant lexical generalsiation, e.g.\u00a0\u2018killed\u2019\u2192\u2192\\rightarrow\u2192 \u2018deceased\u2019, and co-reference or anaphora resolution, e.g.\u00a0\u2018ent119 was killed\u2019 \u2192\u2192\\rightarrow\u2192 \u2018he was identified.\u2019 However it is alsoclear that the model is able to integrate these signals with rough heuristicindicators such as the proximity of query words to the candidate answer.", "The supervised paradigm for training machine reading and comprehension modelsprovides a promising avenue for making progress on the path to building fullnatural language understanding systems. We have demonstrated a methodology forobtaining a large number of document-query-answer triples and shown thatrecurrent and attention based neural networks provide an effective modellingframework for this task.Our analysis indicates that the Attentive and Impatient Readers are able topropagate and integrate semantic information over long distances. In particularwe believe that the incorporation of an attention mechanism is the keycontributor to these results.", "The attention mechanismthat we have employed is just one instantiation of a very general idea whichcan be further exploited. However, the incorporation of world knowledge andmulti-document queries will also require the development of attention andembedding mechanisms whose complexity to query does not scale linearly with thedata set size.There are still many queries requiring complex inference and long range reference resolution that our models are not yet able to answer. As such our data provides a scalable challenge that should support NLP research into the future. Further, significantly bigger training data sets can be acquired using the techniques we have described, undoubtedly allowing us to train more expressive and accurate models."], "figure_types": {"d1505c6123c102e53eb19dff312cb25cea840b72/10-Figure5-1.png": "plot", "d1505c6123c102e53eb19dff312cb25cea840b72/10-Table6-1.png": "table", "d1505c6123c102e53eb19dff312cb25cea840b72/11-Figure6-1.png": "other", "d1505c6123c102e53eb19dff312cb25cea840b72/12-Figure7-1.png": "other", "d1505c6123c102e53eb19dff312cb25cea840b72/13-Figure10-1.png": "other", "d1505c6123c102e53eb19dff312cb25cea840b72/13-Figure11-1.png": "other", "d1505c6123c102e53eb19dff312cb25cea840b72/14-Figure12-1.png": "other", "d1505c6123c102e53eb19dff312cb25cea840b72/14-Figure13-1.png": "other", "d1505c6123c102e53eb19dff312cb25cea840b72/2-Table1-1.png": "table", "d1505c6123c102e53eb19dff312cb25cea840b72/3-Table3-1.png": "table", "d1505c6123c102e53eb19dff312cb25cea840b72/4-Table4-1.png": "table", "d1505c6123c102e53eb19dff312cb25cea840b72/5-Figure1-1.png": "schematic", "d1505c6123c102e53eb19dff312cb25cea840b72/7-Figure2-1.png": "plot", "d1505c6123c102e53eb19dff312cb25cea840b72/7-Table5-1.png": "table", "d1505c6123c102e53eb19dff312cb25cea840b72/8-Figure3-1.png": "other"}}, "1801.04381": {"paper_id": "paper_53", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "arxiv_url": "https://arxiv.org/abs/1801.04381", "s2orc_url": "https://www.semanticscholar.org/paper/dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "all_figures_tables": {"dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/13-Figure7-1.png": "Figure 7: Distribution of activation patterns. The x-axis is the layer index, and we show minimum/maximum/average number of positive channels after each convolution with ReLU. y-axis is either absolute or relative number of channels. The \u201cthreshold\u201d line indicates the ReLU invertibility threshold - that is the number of positive dimensions is higher than the input space. In our case this is 1/6 fraction of the channels. Note how at the beginning of the training on Figure 7a the distribution is much more tightly concentrated around the mean. After the training has finished (Figure 7b), the average hasn\u2019t changed but the standard deviation grew dramatically. Best viewed in color.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/14-Figure8-1.png": "Figure 8: MobileNetv2 semantic segmentation visualization results on PASCAL VOC 2012 val set. OS: output stride. S: single scale input. MS+F: Multi-scale inputs with scales = {0.5, 0.75, 1, 1.25, 1.5, 1.75} and left-right flipped inputs. Employing output stride = 16 and single input scale = 1 attains a good trade-off between FLOPS and accuracy.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/3-Figure1-1.png": "Figure 1: Examples of ReLU transformations of low-dimensional manifolds embedded in higher-dimensional spaces. In these examples the initial spiral is embedded into an n-dimensional space using random matrix T followed by ReLU, and then projected back to the 2D space using T\u22121. In examples above n = 2, 3 result in information loss where certain points of the manifold collapse into each other, while for n = 15 to 30 the transformation is highly non-convex.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/3-Figure2-1.png": "Figure 2: Evolution of separable convolution blocks. The diagonally hatched texture indicates layers that do not contain non-linearities. The last (lightly colored) layer indicates the beginning of the next block. Note: 2d and 2c are equivalent blocks when stacked. Best viewed in color.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/3-Figure3-1.png": "Figure 3: The difference between residual block [1, 28] and inverted residual. Diagonally hatched layers do not use non-linearities. We use thickness of each block to indicate its relative number of channels. Note how classical residuals connects the layers with high number of channels, whereas the inverted residuals connect the bottlenecks. Best viewed in color.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/4-Table1-1.png": "Table 1: Bottleneck residual block transforming from k to k\u2032 channels, with stride s, and expansion factor t.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/5-Figure4-1.png": "Figure 4: Comparison of convolutional blocks for different architectures. ShuffleNet uses Group Convolutions [19] and shuffling, it also uses conventional residual approach where inner blocks are narrower than output. ShuffleNet and NasNet illustrations are from respective papers.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/5-Table2-1.png": "Table 2: MobileNetV2 : Each line describes a sequence of 1 or more identical (modulo stride) layers, repeated n times. All layers in the same sequence have the same number c of output channels. The first layer of each sequence has a stride s and all others use stride 1. All spatial convolutions use 3 \u00d7 3 kernels. The expansion factor t is always applied to the input size as described in Table 1.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/5-Table3-1.png": "Table 3: The max number of channels/memory (in Kb) that needs to be materialized at each spatial resolution for different architectures. We assume 16-bit floats for activations. For ShuffleNet, we use 2x, g = 3 that matches the performance of MobileNetV1 and MobileNetV2. For the first layer of MobileNetV2 and ShuffleNet we can employ the trick described in Section 5 to reduce memory requirement. Even though ShuffleNet employs bottlenecks elsewhere, the nonbottleneck tensors still need to be materialized due to the presence of shortcuts between non-bottleneck tensors.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/6-Figure5-1.png": "Figure 5: Performance curve of MobileNetV2 vs MobileNetV1, ShuffleNet, NAS. For our networks we use multipliers 0.35, 0.5, 0.75, 1.0 for all resolutions, and additional 1.4 for for 224. Best viewed in color.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/6-Figure6-1.png": "Figure 6: The impact of non-linearities and various types of shortcut (residual) connections.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/7-Table4-1.png": "Table 4: Performance on ImageNet, comparison for different networks. As is common practice for ops, we count the total number of Multiply-Adds. In the last column we report running time in milliseconds (ms) for a single large core of the Google Pixel 1 phone (using TF-Lite). We do not report ShuffleNet numbers as the framework does not yet support efficient group convolutions.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/7-Table5-1.png": "Table 5: Comparison of the size and the computational cost between SSD and SSDLite configured with MobileNetV2 and making predictions for 80 classes.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/7-Table6-1.png": "Table 6: Performance comparison of MobileNetV2 + SSDLite and other realtime detectors on the COCO dataset object detection task. MobileNetV2 + SSDLite achieves competitive accuracy with significantly fewer parameters and smaller computational complexity. All models are trained on trainval35k and evaluated on test-dev. SSD/YOLOv2 numbers are from [34]. The running time is reported for the large core of the Google Pixel 1 phone, using an internal version of the TF-Lite engine.", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/8-Table7-1.png": "Table 7: MobileNet + DeepLabv3 inference strategy on the PASCAL VOC 2012 validation set. MNet V2*: Second last feature map is used for DeepLabv3 heads, which includes (1) Atrous Spatial Pyramid Pooling (ASPP) module, and (2) 1 \u00d7 1 convolution as well as image-pooling feature. OS: output stride that controls the output resolution of the segmentation map. MF: Multi-scale and left-right flipped inputs during test. All of the models have been pretrained on COCO. The potential candidate for on-device applications is shown in bold face. PASCAL images have dimension 512 \u00d7 512 and atrous convolution allows us to control output feature resolution without increasing the number of parameters."}, "referred_figures_tables": [["dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/7-Table4-1.png", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/7-Table6-1.png"], ["dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/5-Figure4-1.png"], ["dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/6-Figure6-1.png", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/6-Figure6-1.png"], ["dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/5-Table2-1.png"]], "question_id": [2, 6, 10, 11], "question": ["Do the authors evaluate their architecture on non-mobile/cellphone type of edge devices such as FPGAs?", "What is the key difference in model structure between Mobilenet style models and Shufflenet? ", "Do the authors measure the quantify the impact on their model's performance when using RELU6 instead of RELU?", "What are the likely problems authors would have encountered if they did not use batch normalization and dropout during training?"], "question_section": ["1. Introduction", "3.3. Inverted residuals", "4. Model Architecture", "4. Model Architecture"], "question_trigger_sentence": ["This paper introduces a new neural network architecture that is specifically tailored for mobile and resource constrained environments.", "In Table 3 we compare the needed sizes for each resolution between MobileNetV1, MobileNetV2 and ShuffleNet.", "We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27].", "We always use kernel size 3 \u00d7 3 as is standard for modern networks, and utilize dropout and batch normalization during training."], "question_type": ["Shallow question", "Testing question", "Shallow question", "Deep/complex question"], "evidential_info": [[{"context": "Table 4: Performance on ImageNet, comparison for different networks. As is common practice for ops, we count the total number of Multiply-Adds. In the last column we report running time in milliseconds (ms) for a single large core of the Google Pixel 1 phone (using TF-Lite). We do not report ShuffleNet numbers as efficient group convolutions and shuffling are not yet supported.", "rationale": "Table 4 reports the running time for a single large core of the Google Pixel 1 phone in milliseconds."}, {"context": "Table 6: Performance comparison of MobileNetV2 + SSDLite and other realtime detectors on the COCO dataset object detection task. MobileNetV2 + SSDLite achieves competitive accuracy with significantly fewer parameters and smaller computational complexity. All models are trained on trainval35k and evaluated on test-dev. SSD/YOLOv2 numbers are from [35]. The running time is reported for the large core of the Google Pixel 1 phone, using an internal version of the TF-Lite engine.", "rationale": "Table 6 reports the running time for a single large core of the Google Pixel 1 phone in milliseconds."}], [{"context": "Figure 4: Comparison of convolutional blocks for different architectures. ShuffleNet uses Group Convolutions [20] and shuffling, it also uses conventional residual approach where inner blocks are narrower than output. ShuffleNet and NasNet illustrations are from respective papers.", "rationale": "ShuffleNet uses group convolutions and shuffling, utilizing conventional residual approach."}], [{"context": "The importance of residual connection has been studied extensively [8, 30, 46]. The new result reported in this paper is that the shortcut connecting bottleneck perform better than shortcuts connecting the expanded layers (see Figure 6b for comparison).", "rationale": "Ablation study shows that the shortcut connecting bottleneck (inverted residual connections) performs better than connecting the expanded layers."}, {"context": "The linear bottleneck models are strictly less powerful than models with non-linearities, because the activations can always operate in linear regime with appropriate changes to biases and scaling. However our experiments shown in Figure 6a indicate that linear bottlenecks improve performance, providing support that non-linearity destroys information in low-dimensional space.", "rationale": "Ablation study shows that the linear bottlenecks improves the performance compared to non-linearity ones."}], [{"context": "The architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers described in the Table 2. We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size 3 \u00d7 3 as is standard for modern networks, and utilize dropout and batch normalization during training.", "rationale": "The authors used 3 \u00d7 3 kernel size follwoing the standard for modern networks, and utilized batch normalization and dropout during training."}]], "composition": ["The authors only evaluated their architecture on mobile devices (Google Pixel 1) and did not evaluated on non-mobile type of devices.", "ShuffleNet introduces group convolutions and shuffling, while existing mobilenet style models do not have.", "While the authors showed the effect of inverted residual connections and linear bottlenecks, they did not measure the impact of using RELU6 instead of RELU in the ablation study.", "Since there is no evidential information about the effect of batch normalization and dropout, this question cannot be answered and requires external knowledges."], "Is_figure_in_evidence": [false, true, true, false], "Is_table_in_evidence": [true, false, false, true], "question_key": ["1059", "1062", "1066", "1067"], "passages": ["Neural networks have revolutionized many areas of machine intelligence, enabling superhuman accuracy for challenging image recognition tasks. However, the drive to improve accuracy often comes at a cost: modern state of the art networks require high computational resources beyond the capabilities of many mobile and embedded applications.", "This paper introduces a new neural network architecture that is specifically tailored for mobile and resource constrained environments. Our network pushes the state of the art for mobile tailored computer vision models, by significantly decreasing the number of operations and memory needed while retaining the same accuracy.", "Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution. The official implementation is available as part of TensorFlow-Slim model library in\u00a0[4].", "This module can be efficiently implemented using standard operations in any modern framework and allowsour models to beat state of the art along multiple performance points using standard benchmarks.Furthermore, this convolutional module is particularly suitable for mobile designs, because it allows to significantly reduce the memory footprint needed during inference by never fully materializing large intermediate tensors. This reduces the need for main memory access in many embedded hardware designs, that provide small amounts of very fast software controlled cache memory.", "Tuning deep neural architectures to strike an optimal balance between accuracy and performance has been an area of active research for the last several years.Both manual architecture search and improvements in training algorithms, carried out by numerous teams has lead to dramatic improvements over early designs such as AlexNet\u00a0[5], VGGNet\u00a0[6], GoogLeNet\u00a0[7]. , and ResNet\u00a0[8].Recently there has been lots of progress in algorithmic architecture exploration included hyper-parameter optimization [9, 10, 11] as well as various methods of network pruning [12, 13, 14, 15, 16, 17] and connectivity learning [18, 19].A substantial amount of work has also been dedicated to changing the connectivity structure of the internal convolutional blocks such as in ShuffleNet [20] or introducing sparsity [21] and others [22].", "Recently, [23, 24, 25, 26], opened up a new direction of bringing optimization methods including genetic algorithms and reinforcement learning to architectural search.However one drawback is that the resulting networks end up very complex.In this paper, we pursue the goal of developing better intuition about how neural networks operate and use that to guide the simplest possible network design.Our approach should be seen as complimentary to the one described in [23] and related work.In this vein our approach is similar to those taken by [20, 22] and allows to further improve the performance, while providing a glimpse on its internal operation.Our network design is based on MobileNetV1\u00a0[27]. It retains its simplicity and does not require any special operators while significantly improves its accuracy, achieving state of the art on multiple image classification and detection tasks for mobile applications.", "Depthwise Separable Convolutions are a key building block for many efficient neural network architectures [27, 28, 20] and we use them in the present work as well.The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers.The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel.The second layer is a 1\u00d71111\\times 11 \u00d7 1 convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the input channels.", "Standard convolution takes an hi\u00d7wi\u00d7disubscript\u210e\ud835\udc56subscript\ud835\udc64\ud835\udc56subscript\ud835\udc51\ud835\udc56h_{i}\\times w_{i}\\times d_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u00d7 italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT input tensor Lisubscript\ud835\udc3f\ud835\udc56L_{i}italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and applies convolutional kernel K\u2208\u211bk\u00d7k\u00d7di\u00d7dj\ud835\udc3esuperscript\u211b\ud835\udc58\ud835\udc58subscript\ud835\udc51\ud835\udc56subscript\ud835\udc51\ud835\udc57K\\in{\\cal R}^{k\\times k\\times d_{i}\\times d_{j}}italic_K \u2208 caligraphic_R start_POSTSUPERSCRIPT italic_k \u00d7 italic_k \u00d7 italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUPERSCRIPT to produce an hi\u00d7wi\u00d7djsubscript\u210e\ud835\udc56subscript\ud835\udc64\ud835\udc56subscript\ud835\udc51\ud835\udc57h_{i}\\times w_{i}\\times d_{j}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u00d7 italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT output tensor Ljsubscript\ud835\udc3f\ud835\udc57L_{j}italic_L start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT.Standard convolutional layers have the computational cost of hi\u22c5wi\u22c5di\u22c5dj\u22c5k\u22c5k\u22c5subscript\u210e\ud835\udc56subscript\ud835\udc64\ud835\udc56subscript\ud835\udc51\ud835\udc56subscript\ud835\udc51\ud835\udc57\ud835\udc58\ud835\udc58h_{i}\\cdot w_{i}\\cdot d_{i}\\cdot d_{j}\\cdot k\\cdot kitalic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u22c5 italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u22c5 italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u22c5 italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u22c5 italic_k \u22c5 italic_k.", "Depthwise separable convolutions are a drop-in replacement for standard convolutional layers.Empirically they work almost as well as regular convolutions but only cost:hi\u22c5wi\u22c5di(k2+dj)\u22c5subscript\u210e\ud835\udc56subscript\ud835\udc64\ud835\udc56subscript\ud835\udc51\ud835\udc56superscript\ud835\udc582subscript\ud835\udc51\ud835\udc57h_{i}\\cdot w_{i}\\cdot d_{i}(k^{2}+d_{j})italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u22c5 italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u22c5 italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )(1)which is the sum of the depthwise and 1\u00d71111\\times 11 \u00d7 1 pointwise convolutions.Effectively depthwise separable convolution reduces computation compared to traditional layers by almost a factor of k2superscript\ud835\udc582k^{2}italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT111more precisely, by a factor k2dj/(k2+dj)superscript\ud835\udc582subscript\ud835\udc51\ud835\udc57superscript\ud835\udc582subscript\ud835\udc51\ud835\udc57k^{2}d_{j}/(k^{2}+d_{j})italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ).MobileNetV2 uses k=3\ud835\udc583k=3italic_k = 3 (3\u00d73333\\times 33 \u00d7 3 depthwise separable convolutions) so the computational cost is 8888 to 9999 times smaller than that of standard convolutions at only a small reduction in accuracy [27].", "Consider a deep neural network consisting of n\ud835\udc5bnitalic_n layers Lisubscript\ud835\udc3f\ud835\udc56L_{i}italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT each of which has an activation tensor of dimensions hi\u00d7wi\u00d7disubscript\u210e\ud835\udc56subscript\ud835\udc64\ud835\udc56subscript\ud835\udc51\ud835\udc56h_{i}\\times w_{i}\\times d_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u00d7 italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u00d7 italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.Throughout this section we will be discussing the basic properties of theseactivation tensors, which we will treat as containers of hi\u00d7wisubscript\u210e\ud835\udc56subscript\ud835\udc64\ud835\udc56h_{i}\\times w_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u00d7 italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u201cpixels\u201d with disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT dimensions.Informally, for an input set of real images, we say that the set of layer activations (for any layer Lisubscript\ud835\udc3f\ud835\udc56L_{i}italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) forms a \u201cmanifold of interest\u201d. It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces.In other words, when we look at all individual d\ud835\udc51ditalic_d-channel pixels of a deepconvolutional layer, the information encoded in those values actually lie in some manifold, which in turn is embeddable into a low-dimensional subspace222Note that dimensionality of the manifold differs from the dimensionality of a subspace that could be embedded via a linear transformation..", "At a first glance, such a fact could then be captured and exploited by simply reducing the dimensionality of a layer thus reducing the dimensionality of the operating space.This has been successfully exploited by MobileNetV1\u00a0[27] to effectively trade off between computation and accuracy via a width multiplier parameter, and has been incorporated into efficient model designs of other networks as well [20].Following that intuition, the width multiplier approach allows one to reduce the dimensionality of the activation space until the manifold of interest spansthis entire space.However, this intuition breaks down when we recall that deep convolutional neural networks actually have non-linear per coordinate transformations, such as ReLUReLU\\operatorname{ReLU}roman_ReLU.For example, ReLUReLU\\operatorname{ReLU}roman_ReLU applied to a line in 1D space produces a \u2019ray\u2019, where as in \u211bnsuperscript\u211b\ud835\udc5b{\\cal R}^{n}caligraphic_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT space, it generally results in a piece-wise linear curve with n\ud835\udc5bnitalic_n-joints.", "It is easy to see that in general if a result of a layer transformation ReLU\u2061(Bx)ReLU\ud835\udc35\ud835\udc65\\operatorname{ReLU}(Bx)roman_ReLU ( italic_B italic_x ) has a non-zero volume S\ud835\udc46Sitalic_S, the points mapped to interior\u2061Sinterior\ud835\udc46\\operatorname{interior}{S}roman_interior italic_S are obtained via a linear transformation B\ud835\udc35Bitalic_B of the input, thus indicating that the part of the input space corresponding to the full dimensional output, is limited to a linear transformation.In other words, deep networks only have the power of a linear classifier on the non-zero volume part of the output domain.We refer to supplemental material for a more formal statement.", "On the other hand, when ReLUReLU\\operatorname{ReLU}roman_ReLU collapses the channel, it inevitably loses information in that channel. However if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels.In supplemental materials, we show that if the input manifold can be embedded into a significantly lower-dimensional subspace of the activation space then the ReLUReLU\\operatorname{ReLU}roman_ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions.", "To summarize, we have highlighted two properties that are indicative ofthe requirement that the manifold of interest should lie in a low-dimensionalsubspace of the higher-dimensional activation space:", "1.If the manifold of interest remains non-zero volume after ReLUReLU\\operatorname{ReLU}roman_ReLU transformation, it corresponds to a linear transformation.2.ReLUReLU\\operatorname{ReLU}roman_ReLU is capable of preserving complete information about the input manifold, but only if the input manifold lies in a low-dimensional subspace of the input space.", "These two insights provide us with an empirical hint for optimizing existing neural architectures: assuming the manifold of interest is low-dimensional we can capture this by inserting linear bottleneck layers into the convolutional blocks.Experimental evidence suggests that using linear layers is crucial as it prevents non-linearities from destroying too much information.In Section\u00a06, we show empirically that using non-linear layers in bottlenecks indeed hurts the performance by several percent, further validating our hypothesis333We note that in the presence of shortcuts the information loss is actually less strong.. We note that similar reports where non-linearity was helped were reported in [29] where non-linearity was removed from the input of the traditional residual block and that lead to improved performance on CIFAR dataset.", "For the remainder of this paper we will be utilizing bottleneck convolutions.We will refer to the ratio between the size of the input bottleneck and the inner size as the expansion ratio.", "The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8].However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks.Figure\u00a03 provides a schematic visualization of the difference in the designs.The motivation for inserting shortcuts is similar to that of classical residual connections: we want to improve the ability of a gradient to propagate across multiplier layers.However, the inverted design is considerably more memory efficient (see Section\u00a04 for details), as well as works slightly better in our experiments.", "The basic implementation structure is illustrated in Table\u00a01.For a block of size h\u00d7w\u210e\ud835\udc64h\\times witalic_h \u00d7 italic_w, expansion factor t\ud835\udc61titalic_t and kernel size k\ud835\udc58kitalic_k with d\u2032superscript\ud835\udc51\u2032d^{\\prime}italic_d start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT input channels and d\u2032\u2032superscript\ud835\udc51\u2032\u2032d^{\\prime\\prime}italic_d start_POSTSUPERSCRIPT \u2032 \u2032 end_POSTSUPERSCRIPT output channels, the total number of multiply add required ish\u22c5w\u22c5d\u2032\u22c5t(d\u2032+k2+d\u2032\u2032)\u22c5\u210e\ud835\udc64superscript\ud835\udc51\u2032\ud835\udc61superscript\ud835\udc51\u2032superscript\ud835\udc582superscript\ud835\udc51\u2032\u2032h\\cdot w\\cdot d^{\\prime}\\cdot t(d^{\\prime}+k^{2}+d^{\\prime\\prime})italic_h \u22c5 italic_w \u22c5 italic_d start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u22c5 italic_t ( italic_d start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT + italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_d start_POSTSUPERSCRIPT \u2032 \u2032 end_POSTSUPERSCRIPT ).Compared with (1) this expression has an extra term, as indeed we have an extra 1\u00d71111\\times 11 \u00d7 1 convolution, however the nature of our networks allows us to utilize much smaller input and output dimensions.In Table\u00a03 we compare the needed sizes for each resolution between MobileNetV1, MobileNetV2 and ShuffleNet.", "One interesting property of our architecture is that it provides a natural separation between the input/output domains of the building blocks (bottleneck layers), and the layer transformation \u2013 that is a non-linear function that converts input to the output.The former can be seen as the capacity of the network at each layer, whereas the latter as the expressiveness.This is in contrast with traditional convolutional blocks, both regular and separable, where both expressiveness and capacity are tangled together and are functions of the output layer depth.", "In particular, in our case, when inner layer depth is 00 the underlying convolution is the identity function thanks to the shortcut connection.When the expansion ratio is smaller than 1111, this is a classical residual convolutional block [8, 30].However, for our purposes we show that expansion ratio greater than 1111 is the most useful.", "This interpretation allows us to study the expressiveness of the network separately from its capacity and we believe that further exploration of this separation is warranted to provide a better understanding of the network properties.", "Now we describe our architecture in detail. As discussed in the previous section thebasic building block is a bottleneck depth-separable convolution with residuals.The detailed structureof this block is shown in Table\u00a01. The architecture of MobileNetV2 contains the initial fully convolution layer with 32323232 filters, followed by 19191919 residual bottleneck layers described in the Table\u00a02. We use ReLU6ReLU6{\\operatorname{\\mathop{ReLU6}\\,}}ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size 3\u00d73333\\times 33 \u00d7 3 as is standard for modern networks, and utilize dropout and batch normalization during training.", "With the exception of the first layer, we use constant expansion rate throughout the network. In our experiments we find that expansion rates between 5555 and 10101010 result in nearly identical performance curves, with smaller networks being better off with slightly smaller expansion rates and larger networks having slightly better performance with larger expansion rates.", "For all our main experiments we use expansion factor of 6666 applied to the size of the input tensor. For example, for a bottleneck layer that takes 64646464-channel input tensor and produces a tensor with 128128128128 channels, the intermediate expansion layer is then 64\u22c56=384\u22c564638464\\cdot 6=38464 \u22c5 6 = 384 channels.", "As in [27] we tailor our architecture to different performance points, by using the input image resolution and width multiplier as tunable hyper parameters, that can be adjusted depending on desired accuracy/performance trade-offs. Our primary network (width multiplier 1111, 224\u00d7224224224224\\times 224224 \u00d7 224), has a computational cost of 300 million multiply-adds and uses 3.4 million parameters. We explore the performance trade offs, for input resolutions from 96969696 to 224224224224, and width multipliers of 0.350.350.350.35 to 1.41.41.41.4.The network computational cost ranges from 7777 multiply adds to 585M MAdds, while the model size vary between 1.7M and 6.9M parameters.", "One minor implementation difference, with [27] is that for multipliers less than one, we apply width multiplier to all layers except the very last convolutional layer.This improves performance for smaller models.", "The inverted residual bottleneck layers allow a particularly memory efficient implementation which is very important for mobile applications. A standard efficient implementation of inference that uses for instanceTensorFlow[31] or Caffe\u00a0[32], builds a directed acyclic compute hypergraph G\ud835\udc3aGitalic_G, consisting of edges representing the operations and nodes representing tensors of intermediate computation. The computation is scheduled in order to minimize the total number of tensors that needs to be stored in memory.In the most general case, it searches over all plausible computation orders \u03a3(G)\u03a3\ud835\udc3a\\Sigma(G)roman_\u03a3 ( italic_G ) and picks the one that minimizesM(G)=min\u03c0\u2208\u03a3(G)\u2061maxi\u22081..n\u2061[\u2211A\u2208R(i,\u03c0,G)|A|]+size(\u03c0i).M(G)=\\min_{\\pi\\in\\Sigma(G)}\\max_{i\\in 1..n}\\left[\\sum_{A\\in R(i,\\pi,G)}|A|\\right]+\\text{size}(\\pi_{i}).italic_M ( italic_G ) = roman_min start_POSTSUBSCRIPT italic_\u03c0 \u2208 roman_\u03a3 ( italic_G ) end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_i \u2208 1 . . italic_n end_POSTSUBSCRIPT [ \u2211 start_POSTSUBSCRIPT italic_A \u2208 italic_R ( italic_i , italic_\u03c0 , italic_G ) end_POSTSUBSCRIPT | italic_A | ] + size ( italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) .where R(i,\u03c0,G)\ud835\udc45\ud835\udc56\ud835\udf0b\ud835\udc3aR(i,\\pi,G)italic_R ( italic_i , italic_\u03c0 , italic_G ) is the list of intermediate tensors that are connected to any of \u03c0i\u2026\u03c0nsubscript\ud835\udf0b\ud835\udc56\u2026subscript\ud835\udf0b\ud835\udc5b\\pi_{i}\\dots\\pi_{n}italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2026 italic_\u03c0 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT nodes, |A|\ud835\udc34|A|| italic_A | represents the size of the tensor A\ud835\udc34Aitalic_A andsize(i)\ud835\udc60\ud835\udc56\ud835\udc67\ud835\udc52\ud835\udc56size(i)italic_s italic_i italic_z italic_e ( italic_i ) is the total amount of memory needed for internal storage during operation i\ud835\udc56iitalic_i.", "For graphs that have only trivial parallel structure (such as residual connection), there is only one non-trivial feasible computation order, and thus the total amount and a bound on the memory needed for inference on compute graph G\ud835\udc3aGitalic_G can be simplified:M(G)=maxop\u2208G\u2061[\u2211A\u2208opinp|A|+\u2211B\u2208opout|B|+|op|]\ud835\udc40\ud835\udc3asubscript\ud835\udc5c\ud835\udc5d\ud835\udc3asubscript\ud835\udc34subscriptop\ud835\udc56\ud835\udc5b\ud835\udc5d\ud835\udc34subscript\ud835\udc35subscriptop\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc35\ud835\udc5c\ud835\udc5dM(G)=\\max_{op\\in G}\\left[\\sum_{A\\in\\text{op}_{inp}}|A|+\\sum_{B\\in\\text{op}_{out}}|B|+|op|\\right]italic_M ( italic_G ) = roman_max start_POSTSUBSCRIPT italic_o italic_p \u2208 italic_G end_POSTSUBSCRIPT [ \u2211 start_POSTSUBSCRIPT italic_A \u2208 op start_POSTSUBSCRIPT italic_i italic_n italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT | italic_A | + \u2211 start_POSTSUBSCRIPT italic_B \u2208 op start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT | italic_B | + | italic_o italic_p | ](2)Or to restate, the amount of memory is simply the maximum total size of combined inputs and outputs across all operations. In what follows we show that if we treat a bottleneck residual block as a single operation (and treat inner convolutionas a disposable tensor), the total amount of memory would be dominated by the size of bottleneck tensors, rather than the size of tensors that are internal to bottleneck (and much larger).", "A bottleneck block operator \u2131(x)\u2131\ud835\udc65{\\cal F}(x)caligraphic_F ( italic_x ) shown in Figure\u00a02(b) can be expressed as a composition of threeoperators \u2131(x)=[A\u2218\ud835\udca9\u2218B]x\u2131\ud835\udc65delimited-[]\ud835\udc34\ud835\udca9\ud835\udc35\ud835\udc65{\\cal F}(x)=[A\\circ{\\cal N}\\circ B]xcaligraphic_F ( italic_x ) = [ italic_A \u2218 caligraphic_N \u2218 italic_B ] italic_x, where A\ud835\udc34Aitalic_A is a lineartransformation A:\u211bs\u00d7s\u00d7k\u2192\u211bs\u00d7s\u00d7n:\ud835\udc34\u2192superscript\u211b\ud835\udc60\ud835\udc60\ud835\udc58superscript\u211b\ud835\udc60\ud835\udc60\ud835\udc5bA:{\\cal R}^{s\\times s\\times k}\\rightarrow{\\cal R}^{s\\times s\\times n}italic_A : caligraphic_R start_POSTSUPERSCRIPT italic_s \u00d7 italic_s \u00d7 italic_k end_POSTSUPERSCRIPT \u2192 caligraphic_R start_POSTSUPERSCRIPT italic_s \u00d7 italic_s \u00d7 italic_n end_POSTSUPERSCRIPT, \ud835\udca9\ud835\udca9{\\cal N}caligraphic_N is a non-linear per-channel transformation: \ud835\udca9:\u211bs\u00d7s\u00d7n\u2192\u211bs\u2032\u00d7s\u2032\u00d7n:\ud835\udca9\u2192superscript\u211b\ud835\udc60\ud835\udc60\ud835\udc5bsuperscript\u211bsuperscript\ud835\udc60\u2032superscript\ud835\udc60\u2032\ud835\udc5b{\\cal N}:{\\cal R}^{s\\times s\\times n}\\rightarrow{\\cal R}^{s^{\\prime}\\times s^{\\prime}\\times n}caligraphic_N : caligraphic_R start_POSTSUPERSCRIPT italic_s \u00d7 italic_s \u00d7 italic_n end_POSTSUPERSCRIPT \u2192 caligraphic_R start_POSTSUPERSCRIPT italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u00d7 italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u00d7 italic_n end_POSTSUPERSCRIPT, and B\ud835\udc35Bitalic_B is again a linear transformation to the output domain: B:\u211bs\u2032\u00d7s\u2032\u00d7n\u2192\u211bs\u2032\u00d7s\u2032\u00d7k\u2032:\ud835\udc35\u2192superscript\u211bsuperscript\ud835\udc60\u2032superscript\ud835\udc60\u2032\ud835\udc5bsuperscript\u211bsuperscript\ud835\udc60\u2032superscript\ud835\udc60\u2032superscript\ud835\udc58\u2032B:{\\cal R}^{s^{\\prime}\\times s^{\\prime}\\times n}\\rightarrow{\\cal R}^{s^{\\prime}\\times s^{\\prime}\\times k^{\\prime}}italic_B : caligraphic_R start_POSTSUPERSCRIPT italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u00d7 italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u00d7 italic_n end_POSTSUPERSCRIPT \u2192 caligraphic_R start_POSTSUPERSCRIPT italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u00d7 italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u00d7 italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT.", "For our networks \ud835\udca9=ReLU6\u2218dwise\u2218ReLU6\ud835\udca9ReLU6dwiseReLU6{\\cal N}={\\operatorname{\\mathop{ReLU6}\\,}}\\circ{\\operatorname{\\mathop{dwise}\\,}}\\circ{\\operatorname{\\mathop{ReLU6}\\,}}caligraphic_N = start_OPFUNCTION ReLU6 end_OPFUNCTION \u2218 start_OPFUNCTION roman_dwise end_OPFUNCTION \u2218 start_OPFUNCTION ReLU6 end_OPFUNCTION, but the results apply to any per-channel transformation. Suppose the size of the input domain is |x|\ud835\udc65|x|| italic_x | and the size of the output domain is |y|\ud835\udc66|y|| italic_y |, then the memory required to compute F(X)\ud835\udc39\ud835\udc4bF(X)italic_F ( italic_X ) can be as low as |s2k|+|s\u2032\u20632k\u2032|+O(max\u2061(s2,s\u2032\u20632))superscript\ud835\udc602\ud835\udc58superscript\ud835\udc60\u20322superscript\ud835\udc58\u2032\ud835\udc42superscript\ud835\udc602superscript\ud835\udc60\u20322|s^{2}k|+|s^{\\prime 2}k^{\\prime}|+O(\\max(s^{2},s^{\\prime 2}))| italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_k | + | italic_s start_POSTSUPERSCRIPT \u2032 2 end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT | + italic_O ( roman_max ( italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , italic_s start_POSTSUPERSCRIPT \u2032 2 end_POSTSUPERSCRIPT ) ).", "The algorithm is based on the fact that the inner tensor \u2110\u2110\\cal Icaligraphic_I can be represented as concatenation of t\ud835\udc61titalic_t tensors, of size n/t\ud835\udc5b\ud835\udc61n/titalic_n / italic_t eachand our function can then be represented as\u2131(x)=\u2211i=1t(Ai\u2218N\u2218Bi)(x)\u2131\ud835\udc65superscriptsubscript\ud835\udc561\ud835\udc61subscript\ud835\udc34\ud835\udc56\ud835\udc41subscript\ud835\udc35\ud835\udc56\ud835\udc65{\\cal F}(x)=\\sum_{i=1}^{t}(A_{i}\\circ N\\circ B_{i})(x)caligraphic_F ( italic_x ) = \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2218 italic_N \u2218 italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ( italic_x )by accumulating the sum, we only require one intermediate block of size n/t\ud835\udc5b\ud835\udc61n/titalic_n / italic_t to be kept in memory at all times. Using n=t\ud835\udc5b\ud835\udc61n=titalic_n = italic_t we end up having to keep only a single channel of the intermediate representation at all times. The two constraints that enabled us to use this trick is (a) the fact that the inner transformation (which includes non-linearity and depthwise) is per-channel, and (b) the consecutive non-per-channel operators have significant ratio of the input size to the output. For most of the traditional neural networks, such trick would not produce a significant improvement.", "We note that, the number of multiply-adds operators needed to compute F(X)\ud835\udc39\ud835\udc4bF(X)italic_F ( italic_X ) using t\ud835\udc61titalic_t-way split is independent of t\ud835\udc61titalic_t, however in existing implementations we find that replacing one matrix multiplication with several smaller ones hurts runtime performance due to increased cache misses. We find that this approach is the most helpful to be used with t\ud835\udc61titalic_t being a small constant between 2222 and 5555. It significantly reduces the memory requirement, but still allows one to utilize most of the efficiencies gained by using highly optimized matrix multiplication and convolution operators provided by deep learning frameworks. It remains to be seen if special framework level optimization may lead to further runtime improvements.", "We train our models using TensorFlow[31].We use the standard RMSPropOptimizer with both decay and momentum set to 0.90.90.90.9.We use batch normalization after every layer, and the standard weight decay is set to 0.000040.000040.000040.00004.Following MobileNetV1[27] setup we use initial learning rate of 0.0450.0450.0450.045, and learning rate decay rate of 0.980.980.980.98 per epoch.We use 16 GPU asynchronous workers, and a batch size of 96969696.", "We compare our networks against MobileNetV1, ShuffleNet and NASNet-A models.The statistics of a few selected models is shown in Table\u00a04 with the full performance graph shown in Figure\u00a05.", "We evaluate and compare the performance of MobileNetV2 and MobileNetV1 as feature extractors [33] for object detection with a modified version of the Single Shot Detector (SSD)\u00a0[34] on COCO dataset [2].We also compare to YOLOv2\u00a0[35] and original SSD (with VGG-16\u00a0[6] as base network) as baselines.We do not compare performance with other architectures such as Faster-RCNN [36] and RFCN [37] since our focus is on mobile/real-time models.", "SSDLite: In this paper, we introduce a mobile friendly variant of regular SSD. We replace all the regular convolutions with separable convolutions (depthwise followed by 1\u00d71111\\times 11 \u00d7 1 projection) in SSD prediction layers.This design is in line with the overall design of MobileNets and is seen to be much more computationally efficient.We call this modified version SSDLite.Compared to regular SSD, SSDLite dramatically reduces both parameter count and computational cost as shown in Table\u00a05.", "For MobileNetV1, we follow the setup in [33].For MobileNetV2, the first layer of SSDLite is attached to the expansion of layer 15 (with output stride of 16). The second and the rest of SSDLite layers are attached on top of the last layer (with output stride of 32323232). This setup is consistent with MobileNetV1 as all layers are attached to the feature map of the same output strides.", "Both MobileNet models are trained and evaluated with Open Source TensorFlow Object Detection API\u00a0[38].The input resolution of both models is 320\u00d7320320320320\\times 320320 \u00d7 320.We benchmark and compare both mAP (COCO challenge metrics), number of parameters and number of Multiply-Adds.The results are shown in Table\u00a06.MobileNetV2 SSDLite is not only the most efficient model, but also the most accurate of the three.Notably, MobileNetV2 SSDLite is 20\u00d720\\times20 \u00d7 more efficient and 10\u00d710\\times10 \u00d7 smaller while still outperforms YOLOv2 on COCO dataset.", "In this section, we compare MobileNetV1 and MobileNetV2 models used as feature extractors with DeepLabv3\u00a0[39] for the task of mobile semantic segmentation.DeepLabv3 adopts atrous convolution\u00a0[40, 41, 42], a powerful tool to explicitly control the resolution of computed feature maps, and builds five parallel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) [43] containing three 3\u00d73333\\times 33 \u00d7 3 convolutions with different atrous rates, (b) 1\u00d71111\\times 11 \u00d7 1 convolution head, and (c) Image-level features [44].We denote by output_stride the ratio of input image spatial resolution to final output resolution, which is controlled by applying the atrous convolution properly.For semantic segmentation, we usually employ output_stride=16output_stride16\\emph{output\\_stride}=16output_stride = 16 or 8888 for denser feature maps.We conduct the experiments on the PASCAL VOC 2012 dataset [3], with extra annotated images from [45] and evaluation metric mIOU.", "To build a mobile model, we experimented with three design variations: (1) different feature extractors, (2) simplifying the DeepLabv3 heads for faster computation, and (3) different inference strategies for boosting the performance.Our results are summarized in Table\u00a07.We have observed that: (a) the inference strategies, including multi-scale inputs and adding left-right flipped images, significantly increase the MAdds and thus are not suitable for on-device applications, (b) using output_stride=16output_stride16\\emph{output\\_stride}=16output_stride = 16 is more efficient than output_stride=8output_stride8\\emph{output\\_stride}=8output_stride = 8, (c) MobileNetV1 is already a powerful feature extractor and only requires about 4.9\u22125.74.95.74.9-5.74.9 - 5.7 times fewer MAdds than ResNet-101 [8] (e.g., mIOU: 78.56 vs 82.70, and MAdds: 941.9B vs 4870.6B), (d) it is more efficient to build DeepLabv3 heads on top of the second last feature map of MobileNetV2 than on the original last-layer feature map, since the second to last feature map contains 320320320320 channels instead of 1280128012801280, and by doing so, we attain similar performance, but require about 2.52.52.52.5 times fewer operations than the MobileNetV1 counterparts, and (e) DeepLabv3 heads are computationally expensive and removing the ASPP module significantly reduces the MAdds with only a slight performance degradation.In the end of the Table\u00a07, we identify a potential candidate for on-device applications (in bold face), which attains 75.32%percent75.3275.32\\%75.32 % mIOU and only requires 2.752.752.752.75B MAdds.", "Inverted residual connections.The importance of residual connection has been studied extensively\u00a0[8, 30, 46].The new result reported in this paper is that the shortcut connecting bottleneck perform better than shortcuts connecting the expanded layers (see Figure\u00a05(b) for comparison).", "Importance of linear bottlenecks. The linearbottleneck models are strictly less powerful than models withnon-linearities, because the activations can always operate in linearregime with appropriate changes to biases and scaling. Howeverour experiments shown in Figure\u00a05(a) indicate thatlinear bottlenecks improve performance, providing support that non-linearity destroys information in low-dimensional space.", "We described a very simple network architecture that allowed us to build a family of highly efficient mobile models. Our basic building unit, has several properties that make it particularly suitable for mobile applications. It allows very memory-efficient inference and relies utilize standard operations present in all neural frameworks.", "For the ImageNet dataset, our architecture improves the state of the art for wide range of performance points.", "For object detection task, our network outperforms state-of-art realtime detectors on COCO dataset both in terms of accuracy and model complexity. Notably, our architecture combined with the SSDLite detection module is 20\u00d720\\times20 \u00d7 less computation and 10\u00d710\\times10 \u00d7 less parameters than YOLOv2.", "On the theoretical side: the proposed convolutional block has a unique property that allows to separate the network expressiveness (encoded by expansion layers) from its capacity (encoded by bottleneck inputs). Exploring this is an important direction for future research.", "We would like to thank Matt Streeter and Sergey Ioffe for their helpful feedback and discussion."], "figure_types": {"dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/13-Figure7-1.png": "plot", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/14-Figure8-1.png": "photograph(s)", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/3-Figure1-1.png": "plot", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/3-Figure2-1.png": "schematic", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/3-Figure3-1.png": "schematic", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/4-Table1-1.png": "table", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/5-Figure4-1.png": "schematic", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/5-Table2-1.png": "table", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/5-Table3-1.png": "table", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/6-Figure5-1.png": "plot", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/6-Figure6-1.png": "plot", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/7-Table4-1.png": "table", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/7-Table5-1.png": "table", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/7-Table6-1.png": "table", "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4/8-Table7-1.png": "table"}}, "1503.04069": {"paper_id": "paper_54", "title": "LSTM: A Search Space Odyssey", "arxiv_url": "https://arxiv.org/abs/1503.04069", "s2orc_url": "https://www.semanticscholar.org/paper/a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "all_figures_tables": {"a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/2-Figure1-1.png": "Figure 1. Detailed schematic of the Simple Recurrent Network (SRN) unit (left) and a Long Short-Term Memory block (right) as used in the hidden layers of a recurrent neural network.", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/4-Figure2-1.png": "Figure 2. (a) Example board (a08-551z, training set) from the IAM-OnDB dataset and (b) its transcription into character label sequences.", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/6-Figure3-1.png": "Figure 3. Test set performance for all 200 trials (top) and for the best 10% (bottom) trials (according to the validation set) for each dataset and variant. Boxes show the range between the 25th and the 75th percentile of the data, while the whiskers indicate the whole range. The red dot represents the mean and the red line the median of the data. The boxes of variants that differ significantly from the vanilla LSTM are shown in blue with thick lines. The grey histogram in the background presents the average number of parameters for the top 10% performers of every variant.", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/8-Figure4-1.png": "Figure 4. Predicted marginal error (blue) and marginal time for different values of the learning rate, hidden size, and the input noise (columns) for the test set of all three datasets (rows). The shaded area indicates the standard deviation between the tree-predicted marginals and thus the reliability of the predicted mean performance. Note that each plot is for the vanilla LSTM but curves for all variants that are not significantly worse look very similar.", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/8-Figure5-1.png": "Figure 5. Pie charts showing which fraction of variance of the test set performance can be attributed to each of the hyperparameters. The percentage of variance that is due to interactions between multiple parameters is indicated as \u201chigher order.\u201d", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/9-Figure6-1.png": "Figure 6. Total marginal predicted performance for all pairs of hyperparameters (left) and the variation only due to their interaction (right). The plot is divided vertically into three subplots, one for every dataset (TIMIT, IAM Online, and JSB Chorales). The subplots itself are divided horizontally into two parts, each containing a lower triangular matrix of heat maps. The rows and columns of these matrices represent the different hyperparameters (learning rate, momentum, hidden size, and input noise) and there is one heat map for every combination. The color encodes the performance as measured by the Classification Error for TIMIT, Character Error Rate for IAM Online and Negative Log-Likelihood for the JSB Chorales Dataset. For all datasets low (blue) is better than high (red)."}, "referred_figures_tables": [["a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/6-Figure3-1.png"], ["a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/4-Figure2-1.png"]], "question_id": [1, 5], "question": ["What are the eight different LSTM variants that the authors experimented with?", "Do the authors use different ratios of test-train-validation split for each dataset? "], "question_section": ["I. INTRODUCTION", "IV. EVALUATION SETUP"], "question_trigger_sentence": ["We evaluate the most popular LSTM architecture (vanilla LSTM; Section II) and eight different variants thereof on three benchmark problems: acoustic modeling, handwriting recognition, and polyphonic music modeling", "Each dataset is split into three parts: a training set, a validation set used for early stopping and for optimizing the hyperparameters, and a test set for the final evaluation."], "question_type": ["Shallow question", "Shallow question"], "evidential_info": [[{"context": "The focus of our study is to empirically compare different LSTM variants, and not to achieve state-of-the-art results.Therefore, our experiments are designed to keep the setup simple and the comparisons fair.The vanilla LSTM is used as a baseline and evaluated together with eight of its variants.Each variant adds, removes, or modifies the baseline in exactly one aspect, which allows to isolate their effect.They are evaluated on three different datasets from different domains to account for cross-domain variations.", "rationale": "The eight variants of vanilla LSTM are used to empirically compare different LSTM variants. Each variant adds, removes, or modifies the baseline in exactly one aspect, which allows to isolate their effect."}, {"context": "This paper reports the results of a large scale study on variants of the LSTM architecture. We conclude that the most commonly used LSTM architecture (vanilla LSTM) performs reasonably well on various datasets.None of the eight investigated modifications significantly improves performance.However, certain modifications such as coupling the input and forget gates (CIFG) or removing peephole connections (NP) simplified LSTMs in our experiments without significantly decreasing performance.These two variants are also attractive because they reduce the number of parameters and the computational cost of the LSTM.", "rationale": "Some variants like coupling the input and forget gates (CIFG) and removing peephole connections (NP) had little effect on performance degradation while simplifies existing LSTM structure."}, {"context": "The first important observation based on Figure 3 is that removing the output activation function (NOAF) or the forget gate (NFG) significantly hurt performance on all three datasets. Apart from the CEC, the ability to forget old information and the squashing of the cell state appear to be critical for the LSTM architecture. Indeed, without the output activation function, the block output can in principle grow unbounded. Coupling the input and the forget gate avoids this problem and might render the use of an output non-linearity less important, which could explain why GRU performs well without it.", "rationale": "No output activation function (NOAF) and no forget gate (NFG) variants significantly hurt performance on all datasets, which shows that the ability to forget old information and the squashing of the cell state is essential for the functionality of LSTM architecture."}, {"context": "Adding full gate recurrence (FGR) did not significantly change performance on TIMIT or IAM Online, but led to worse results on the JSB Chorales dataset. Given that this variant greatly increases the number of parameters, we generally advise against using it. Note that this feature was present in the original proposal of LSTM [14, 15], but has been absent in all following studies.", "rationale": "Adding full gate recurrence (FGR) did not change performance of TIMIT or IAM online dataset while made performance drop on the JSB Chorales dataset."}, {"context": "Removing the input gate (NIG), the output gate (NOG), and the input activation function (NIAF) led to a significant reduc\u0002tion in performance on speech and handwriting recognition. However, there was no significant effect on music modeling performance. A small (but statistically insignificant) average performance improvement was observed for the NIG and NIAF architectures on music modeling. We hypothesize that these behaviors will generalize to similar problems such as language modeling. For supervised learning on continuous real-valued data (such as speech and handwriting recognition), the input gate, output gate, and input activation function are all crucial for obtaining good performance.", "rationale": "Three variants of removing components of LSTM (No Input Gate (NIG), No Output Gate (NOG), No Input Activation Function (NIAF)) hurt the performance of speech and handwriting recognition but not on music modeling performance, which shows they are necessary for obtaining good performance for supervised learning on continuous real-valued data."}], [{"context": "The performance is measured as classification error percentage. The training, testing, and validation sets are split in line with Halberstadt [37] into 3696, 400, and 192 sequences, having 304 frames on average.", "rationale": "The training, testing, and validation sets are split into 3696, 400, and 192 with Halberstadt [37], having 304 frames on average."}, {"context": "The TIMIT Speech corpus [26] is large enough to be a reasonable acoustic modeling benchmark for speech recognition, yet it is small enough to keep a large study such as ours manageable. Our experiments focus on the frame-wise classification task for this dataset, where the objective is to classify each audio-frame as one of 61 phones.2 From the raw audio we extract 12 Mel Frequency Cepstrum Coefficients (MFCCs) [35] + energy over 25ms hamming-windows with stride of 10ms and a pre-emphasis coefficient of 0.97. This preprocessing is standard in speech recognition and was chosen in order to stay comparable with earlier LSTM-based results (e.g. [20, 36]). The 13 coefficients along with their first and second derivatives comprise the 39 inputs to the network and were normalized to have zero mean and unit variance.", "rationale": "The TIMIT Speech corpus is a large acoustic modeling benchmark for speech recognition. The frame-wise classification task is used for the experiments, in which the objective is to classify each audio frame as one of 61 phones."}, {"context": "The IAM Online Handwriting Database [38] consists of English sentences as time series of pen movements that have to be mapped to characters. The IAM-OnDB dataset splits into one training set, two validation sets, and one test set, having 775, 192, 216, and 544 boards each. Each board, see Figure 2(a), contains multiple hand-written lines, which in turn consist of several strokes. We use one line per sequence, and joined the two validation sets together, so the final training, validation, and testing sets contain 5355, 2956 and 3859 sequences respectively.", "rationale": "For the IAM Online Handwriting Database, the training, testing, and validation sets are split into 5355, 3859, and 2956. This dataset consists of English sentences as time series of pen movements."}, {"context": "JSB Chorales: JSB Chorales is a collection of 382 four part harmonized chorales by J. S. Bach [40], consisting of 202 chorales in major keys and 180 chorals in minor keys. We used the preprocessed piano-rolls provided by Boulanger, Lewandowski et al. [41]. 5 These piano-rolls were generated by transposing each MIDI sequence in C major or C minor and sampling frames every quarter note. The networks where trained to do next-step prediction by minimizing the negative log-likelihood. The complete dataset consists of 229, 76, and 77 sequences (training, validation, and test sets respectively) with an average length of 61.", "rationale": "For the JSB Chorales dataset, the training, testing, and validation sets are split into 229, 77, and 76 with an average length of 61. This dataset used the preprocessed piano rolls provided by Boulanger, Lewandowski, et al.,"}]], "composition": ["The authors conducted the experiment with these LSTM variants of the vanilla architecture to empirically compare different LSTM variants: No Input Gate (NIG), No Forget Gate (NFG), No Output Gate (NOG), No Input Activation Function (NIAF), No Output Activation Function (NOAF), Coupled Input and Forget Gate (CIFG), No Peepholes (NP), Full Gate Recurrence (FGR).", "The authors use different ratios of test-train-validation split for each dataset. Speficially, the authors did not use the predefined ratio value when splitting the data into train-validation-test sets for the three datasets (TIMIT Speech corpus, IAM Online Handwriting Database, and JSB Chorales dataset) used in the experiment. Instead, they used the predefined data split for IAM Online Handwriting Database and JSB Chorales dataset. (5355:3859:2956 and 229:77:76) They also followed Halberstadt [37] in splitting the TIMIT dataset (3696:400:192)."], "Is_figure_in_evidence": [true, true], "Is_table_in_evidence": [false, false], "question_key": ["1075", "1076"], "passages": ["Recurrent neural networks with Long Short-Term Memory (which we will concisely refer to as LSTMs) have emerged as an effective and scalable model for several learning problems related to sequential data.Earlier methods for attacking these problems have either been tailored towards a specific problem or did not scale to long time dependencies.LSTMs on the other hand are both general and effective at capturing long-term temporal dependencies.They do not suffer from the optimization hurdles that plague simple recurrent networks (SRNs) [1, 2] and have been used to advance the state-of-the-art for many difficult problems.This includes handwriting recognition [3, 4, 5] and generation [6], language modeling [7] and translation [8], acoustic modeling of speech [9], speech synthesis [10], protein secondary structure prediction [11], analysis of audio [12], and video data [13] among others.", "The central idea behind the LSTM architecture is a memory cell which can maintain its state over time, and non-linear gating units which regulate the information flow into and out of the cell.Most modern studies incorporate many improvements that have been made to the LSTM architecture since its original formulation [14, 15].However, LSTMs are now applied to many learning problems which differ significantly in scale and nature from the problems that these improvements were initially tested on.A systematic study of the utility of various computational components which comprise LSTMs (see Figure\u00a01) was missing. This paper fills that gap and systematically addresses the open question of improving the LSTM architecture.", "We evaluate the most popular LSTM architecture (vanilla LSTM; section\u00a0II) and eight different variants thereof on three benchmark problems: acoustic modeling, handwriting recognition, and polyphonic music modeling.Each variant differs from the vanilla LSTM by a single change.This allows us to isolate the effect of each of these changes on the performance of the architecture.Random search [16, 17, 18] is used to find the best-performing hyperparameters for each variant on each problem, enabling a reliable comparison of the performance of different variants.We also provide insights gained about hyperparameters and their interaction using fANOVA [19].", "The LSTM setup most commonly used in literature was originally described by Graves and Schmidhuber [20]. We refer to it as vanilla LSTM and use it as a reference for comparison of all the variants.The vanilla LSTM incorporates changes by Gers et\u00a0al. [21] and Gers and Schmidhuber [22] into the original LSTM\u00a0[15] and uses full gradient training. section\u00a0III provides descriptions of these major LSTM changes.", "A schematic of the vanilla LSTM block can be seen in Figure\u00a01.It features three gates (input, forget, output), block input, a single cell (the Constant Error Carousel), an output activation function, and peephole connections111Some studies omit peephole connections, described in Section III-B..The output of the block is recurrently connected back to the block input and all of the gates.", "Let \ud835\udc31tsuperscript\ud835\udc31\ud835\udc61\\mathbf{x}^{t}bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT be the input vector at time t\ud835\udc61titalic_t, N\ud835\udc41Nitalic_N be the number of LSTM blocks and M\ud835\udc40Mitalic_M the number of inputs. Then we get the following weights for an LSTM layer:\u2022Input weights: \ud835\udc16zsubscript\ud835\udc16\ud835\udc67\\mathbf{W}_{z}bold_W start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT, \ud835\udc16isubscript\ud835\udc16\ud835\udc56\\mathbf{W}_{i}bold_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, \ud835\udc16fsubscript\ud835\udc16\ud835\udc53\\mathbf{W}_{f}bold_W start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, \ud835\udc16o\u2208\u211dN\u00d7Msubscript\ud835\udc16\ud835\udc5csuperscript\u211d\ud835\udc41\ud835\udc40\\mathbf{W}_{o}\\in\\mathbb{R}^{N\\times M}bold_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_N \u00d7 italic_M end_POSTSUPERSCRIPT\u2022Recurrent weights: \ud835\udc11zsubscript\ud835\udc11\ud835\udc67\\mathbf{R}_{z}bold_R start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT, \ud835\udc11isubscript\ud835\udc11\ud835\udc56\\mathbf{R}_{i}bold_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, \ud835\udc11fsubscript\ud835\udc11\ud835\udc53\\mathbf{R}_{f}bold_R start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, \ud835\udc11osubscript\ud835\udc11\ud835\udc5c\\mathbf{R}_{o}bold_R start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT \u2208\u211dN\u00d7Nabsentsuperscript\u211d\ud835\udc41\ud835\udc41\\in\\mathbb{R}^{N\\times N}\u2208 blackboard_R start_POSTSUPERSCRIPT italic_N \u00d7 italic_N end_POSTSUPERSCRIPT\u2022Peephole weights: \ud835\udc29isubscript\ud835\udc29\ud835\udc56\\mathbf{p}_{i}bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, \ud835\udc29fsubscript\ud835\udc29\ud835\udc53\\mathbf{p}_{f}bold_p start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, \ud835\udc29osubscript\ud835\udc29\ud835\udc5c\\mathbf{p}_{o}bold_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT \u2208\u211dNabsentsuperscript\u211d\ud835\udc41\\in\\mathbb{R}^{N}\u2208 blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT\u2022Bias weights: \ud835\udc1bzsubscript\ud835\udc1b\ud835\udc67\\mathbf{b}_{z}bold_b start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT, \ud835\udc1bisubscript\ud835\udc1b\ud835\udc56\\mathbf{b}_{i}bold_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, \ud835\udc1bfsubscript\ud835\udc1b\ud835\udc53\\mathbf{b}_{f}bold_b start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT, \ud835\udc1bosubscript\ud835\udc1b\ud835\udc5c\\mathbf{b}_{o}bold_b start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT \u2208\u211dNabsentsuperscript\u211d\ud835\udc41\\in\\mathbb{R}^{N}\u2208 blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT", "Then the vector formulas for a vanilla LSTM layer forward pass can be written as:\ud835\udc33\u00aftsuperscript\u00af\ud835\udc33\ud835\udc61\\displaystyle\\bar{\\mathbf{z}}^{t}over\u00af start_ARG bold_z end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\ud835\udc16z\ud835\udc31t+\ud835\udc11z\ud835\udc32t\u22121+\ud835\udc1bzabsentsubscript\ud835\udc16\ud835\udc67superscript\ud835\udc31\ud835\udc61subscript\ud835\udc11\ud835\udc67superscript\ud835\udc32\ud835\udc611subscript\ud835\udc1b\ud835\udc67\\displaystyle=\\mathbf{W}_{z}\\mathbf{x}^{t}+\\mathbf{R}_{z}\\mathbf{y}^{t-1}+\\mathbf{b}_{z}= bold_W start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT bold_y start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_b start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT\ud835\udc33tsuperscript\ud835\udc33\ud835\udc61\\displaystyle\\mathbf{z}^{t}bold_z start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=g(\ud835\udc33\u00aft)absent\ud835\udc54superscript\u00af\ud835\udc33\ud835\udc61\\displaystyle=g(\\bar{\\mathbf{z}}^{t})= italic_g ( over\u00af start_ARG bold_z end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )block input\ud835\udc22\u00aftsuperscript\u00af\ud835\udc22\ud835\udc61\\displaystyle\\bar{\\mathbf{i}}^{t}over\u00af start_ARG bold_i end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\ud835\udc16i\ud835\udc31t+\ud835\udc11i\ud835\udc32t\u22121+\ud835\udc29i\u2299\ud835\udc1ct\u22121+\ud835\udc1biabsentsubscript\ud835\udc16\ud835\udc56superscript\ud835\udc31\ud835\udc61subscript\ud835\udc11\ud835\udc56superscript\ud835\udc32\ud835\udc611direct-productsubscript\ud835\udc29\ud835\udc56superscript\ud835\udc1c\ud835\udc611subscript\ud835\udc1b\ud835\udc56\\displaystyle=\\mathbf{W}_{i}\\mathbf{x}^{t}+\\mathbf{R}_{i}\\mathbf{y}^{t-1}+\\mathbf{p}_{i}\\odot\\mathbf{c}^{t-1}+\\mathbf{b}_{i}= bold_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_y start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2299 bold_c start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\ud835\udc22tsuperscript\ud835\udc22\ud835\udc61\\displaystyle\\mathbf{i}^{t}bold_i start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\u03c3(\ud835\udc22\u00aft)absent\ud835\udf0esuperscript\u00af\ud835\udc22\ud835\udc61\\displaystyle=\\sigma(\\bar{\\mathbf{i}}^{t})= italic_\u03c3 ( over\u00af start_ARG bold_i end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )input gate\ud835\udc1f\u00aftsuperscript\u00af\ud835\udc1f\ud835\udc61\\displaystyle\\bar{\\mathbf{f}}^{t}over\u00af start_ARG bold_f end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\ud835\udc16f\ud835\udc31t+\ud835\udc11f\ud835\udc32t\u22121+\ud835\udc29f\u2299\ud835\udc1ct\u22121+\ud835\udc1bfabsentsubscript\ud835\udc16\ud835\udc53superscript\ud835\udc31\ud835\udc61subscript\ud835\udc11\ud835\udc53superscript\ud835\udc32\ud835\udc611direct-productsubscript\ud835\udc29\ud835\udc53superscript\ud835\udc1c\ud835\udc611subscript\ud835\udc1b\ud835\udc53\\displaystyle=\\mathbf{W}_{f}\\mathbf{x}^{t}+\\mathbf{R}_{f}\\mathbf{y}^{t-1}+\\mathbf{p}_{f}\\odot\\mathbf{c}^{t-1}+\\mathbf{b}_{f}= bold_W start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT bold_y start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_p start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT \u2299 bold_c start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_b start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT\ud835\udc1ftsuperscript\ud835\udc1f\ud835\udc61\\displaystyle\\mathbf{f}^{t}bold_f start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\u03c3(\ud835\udc1f\u00aft)absent\ud835\udf0esuperscript\u00af\ud835\udc1f\ud835\udc61\\displaystyle=\\sigma(\\bar{\\mathbf{f}}^{t})= italic_\u03c3 ( over\u00af start_ARG bold_f end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )forget gate\ud835\udc1ctsuperscript\ud835\udc1c\ud835\udc61\\displaystyle\\mathbf{c}^{t}bold_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\ud835\udc33t\u2299\ud835\udc22t+\ud835\udc1ct\u22121\u2299\ud835\udc1ftabsentdirect-productsuperscript\ud835\udc33\ud835\udc61superscript\ud835\udc22\ud835\udc61direct-productsuperscript\ud835\udc1c\ud835\udc611superscript\ud835\udc1f\ud835\udc61\\displaystyle=\\mathbf{z}^{t}\\odot\\mathbf{i}^{t}+\\mathbf{c}^{t-1}\\odot\\mathbf{f}^{t}= bold_z start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2299 bold_i start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_c start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT \u2299 bold_f start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPTcell\ud835\udc28\u00aftsuperscript\u00af\ud835\udc28\ud835\udc61\\displaystyle\\bar{\\mathbf{o}}^{t}over\u00af start_ARG bold_o end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\ud835\udc16o\ud835\udc31t+\ud835\udc11o\ud835\udc32t\u22121+\ud835\udc29o\u2299\ud835\udc1ct+\ud835\udc1boabsentsubscript\ud835\udc16\ud835\udc5csuperscript\ud835\udc31\ud835\udc61subscript\ud835\udc11\ud835\udc5csuperscript\ud835\udc32\ud835\udc611direct-productsubscript\ud835\udc29\ud835\udc5csuperscript\ud835\udc1c\ud835\udc61subscript\ud835\udc1b\ud835\udc5c\\displaystyle=\\mathbf{W}_{o}\\mathbf{x}^{t}+\\mathbf{R}_{o}\\mathbf{y}^{t-1}+\\mathbf{p}_{o}\\odot\\mathbf{c}^{t}+\\mathbf{b}_{o}= bold_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT bold_y start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT \u2299 bold_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_b start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT\ud835\udc28tsuperscript\ud835\udc28\ud835\udc61\\displaystyle\\mathbf{o}^{t}bold_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\u03c3(\ud835\udc28\u00aft)absent\ud835\udf0esuperscript\u00af\ud835\udc28\ud835\udc61\\displaystyle=\\sigma(\\bar{\\mathbf{o}}^{t})= italic_\u03c3 ( over\u00af start_ARG bold_o end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )output gate\ud835\udc32tsuperscript\ud835\udc32\ud835\udc61\\displaystyle\\mathbf{y}^{t}bold_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=h(\ud835\udc1ct)\u2299\ud835\udc28tabsentdirect-product\u210esuperscript\ud835\udc1c\ud835\udc61superscript\ud835\udc28\ud835\udc61\\displaystyle=h(\\mathbf{c}^{t})\\odot\\mathbf{o}^{t}= italic_h ( bold_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) \u2299 bold_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPTblock outputWhere \u03c3\ud835\udf0e\\sigmaitalic_\u03c3, g\ud835\udc54gitalic_g and h\u210ehitalic_h are point-wise non-linear activation functions.The logistic sigmoid (\u03c3(x)=11+e\u2212x\ud835\udf0e\ud835\udc6511superscript\ud835\udc52\ud835\udc65\\sigma(x)=\\frac{1}{1+e^{-x}}italic_\u03c3 ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT end_ARG) is used as gate activation function and the hyperbolic tangent (g(x)=h(x)=tanh\u2061(x)\ud835\udc54\ud835\udc65\u210e\ud835\udc65\ud835\udc65g(x)=h(x)=\\tanh(x)italic_g ( italic_x ) = italic_h ( italic_x ) = roman_tanh ( italic_x )) is usually used as the block input and output activation function.Point-wise multiplication of two vectors is denoted by \u2299direct-product\\odot\u2299.", "The deltas inside the LSTM block are then calculated as:\u03b4\ud835\udc32t\ud835\udeffsuperscript\ud835\udc32\ud835\udc61\\displaystyle\\mathbf{\\delta y}^{t}italic_\u03b4 bold_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\u0394t+\ud835\udc11zT\u03b4\ud835\udc33t+1+\ud835\udc11iT\u03b4\ud835\udc22t+1+\ud835\udc11fT\u03b4\ud835\udc1ft+1+\ud835\udc11oT\u03b4\ud835\udc28t+1absentsuperscript\u0394\ud835\udc61superscriptsubscript\ud835\udc11\ud835\udc67\ud835\udc47\ud835\udeffsuperscript\ud835\udc33\ud835\udc611superscriptsubscript\ud835\udc11\ud835\udc56\ud835\udc47\ud835\udeffsuperscript\ud835\udc22\ud835\udc611superscriptsubscript\ud835\udc11\ud835\udc53\ud835\udc47\ud835\udeffsuperscript\ud835\udc1f\ud835\udc611superscriptsubscript\ud835\udc11\ud835\udc5c\ud835\udc47\ud835\udeffsuperscript\ud835\udc28\ud835\udc611\\displaystyle=\\Delta^{t}+\\mathbf{R}_{z}^{T}\\mathbf{\\delta z}^{t+1}+\\mathbf{R}_{i}^{T}\\mathbf{\\delta i}^{t+1}+\\mathbf{R}_{f}^{T}\\mathbf{\\delta f}^{t+1}+\\mathbf{R}_{o}^{T}\\mathbf{\\delta o}^{t+1}= roman_\u0394 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_\u03b4 bold_z start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_\u03b4 bold_i start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_\u03b4 bold_f start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_\u03b4 bold_o start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT\u03b4\ud835\udc28\u00aft\ud835\udeffsuperscript\u00af\ud835\udc28\ud835\udc61\\displaystyle\\delta\\bar{\\mathbf{o}}^{t}italic_\u03b4 over\u00af start_ARG bold_o end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\u03b4\ud835\udc32t\u2299h(\ud835\udc1ct)\u2299\u03c3\u2032(\ud835\udc28\u00aft)absentdirect-productdirect-product\ud835\udeffsuperscript\ud835\udc32\ud835\udc61\u210esuperscript\ud835\udc1c\ud835\udc61superscript\ud835\udf0e\u2032superscript\u00af\ud835\udc28\ud835\udc61\\displaystyle=\\mathbf{\\delta y}^{t}\\odot h(\\mathbf{c}^{t})\\odot\\sigma^{\\prime}(\\bar{\\mathbf{o}}^{t})= italic_\u03b4 bold_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2299 italic_h ( bold_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) \u2299 italic_\u03c3 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( over\u00af start_ARG bold_o end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )\u03b4\ud835\udc1ct\ud835\udeffsuperscript\ud835\udc1c\ud835\udc61\\displaystyle\\mathbf{\\delta c}^{t}italic_\u03b4 bold_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\u03b4\ud835\udc32t\u2299\ud835\udc28t\u2299h\u2032(\ud835\udc1ct)+\ud835\udc29o\u2299\u03b4\ud835\udc28\u00aft+\ud835\udc29i\u2299\u03b4\ud835\udc22\u00aft+1absentdirect-product\ud835\udeffsuperscript\ud835\udc32\ud835\udc61superscript\ud835\udc28\ud835\udc61superscript\u210e\u2032superscript\ud835\udc1c\ud835\udc61direct-productsubscript\ud835\udc29\ud835\udc5c\ud835\udeffsuperscript\u00af\ud835\udc28\ud835\udc61direct-productsubscript\ud835\udc29\ud835\udc56\ud835\udeffsuperscript\u00af\ud835\udc22\ud835\udc611\\displaystyle=\\mathbf{\\delta y}^{t}\\odot\\mathbf{o}^{t}\\odot h^{\\prime}(\\mathbf{c}^{t})+\\mathbf{p}_{o}\\odot\\delta\\bar{\\mathbf{o}}^{t}+\\mathbf{p}_{i}\\odot\\delta\\bar{\\mathbf{i}}^{t+1}= italic_\u03b4 bold_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2299 bold_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2299 italic_h start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( bold_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) + bold_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT \u2299 italic_\u03b4 over\u00af start_ARG bold_o end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2299 italic_\u03b4 over\u00af start_ARG bold_i end_ARG start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT+\ud835\udc29f\u2299\u03b4\ud835\udc1f\u00aft+1+\u03b4\ud835\udc1ct+1\u2299\ud835\udc1ft+1direct-productsubscript\ud835\udc29\ud835\udc53\ud835\udeffsuperscript\u00af\ud835\udc1f\ud835\udc611direct-product\ud835\udeffsuperscript\ud835\udc1c\ud835\udc611superscript\ud835\udc1f\ud835\udc611\\displaystyle\\quad+\\mathbf{p}_{f}\\odot\\delta\\bar{\\mathbf{f}}^{t+1}+\\mathbf{\\delta c}^{t+1}\\odot\\mathbf{f}^{t+1}+ bold_p start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT \u2299 italic_\u03b4 over\u00af start_ARG bold_f end_ARG start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT + italic_\u03b4 bold_c start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT \u2299 bold_f start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT\u03b4\ud835\udc1f\u00aft\ud835\udeffsuperscript\u00af\ud835\udc1f\ud835\udc61\\displaystyle\\delta\\bar{\\mathbf{f}}^{t}italic_\u03b4 over\u00af start_ARG bold_f end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\u03b4\ud835\udc1ct\u2299\ud835\udc1ct\u22121\u2299\u03c3\u2032(\ud835\udc1f\u00aft)absentdirect-product\ud835\udeffsuperscript\ud835\udc1c\ud835\udc61superscript\ud835\udc1c\ud835\udc611superscript\ud835\udf0e\u2032superscript\u00af\ud835\udc1f\ud835\udc61\\displaystyle=\\mathbf{\\delta c}^{t}\\odot\\mathbf{c}^{t-1}\\odot\\sigma^{\\prime}(\\bar{\\mathbf{f}}^{t})= italic_\u03b4 bold_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2299 bold_c start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT \u2299 italic_\u03c3 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( over\u00af start_ARG bold_f end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )\u03b4\ud835\udc22\u00aft\ud835\udeffsuperscript\u00af\ud835\udc22\ud835\udc61\\displaystyle\\delta\\bar{\\mathbf{i}}^{t}italic_\u03b4 over\u00af start_ARG bold_i end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\u03b4\ud835\udc1ct\u2299\ud835\udc33t\u2299\u03c3\u2032(\ud835\udc22\u00aft)absentdirect-product\ud835\udeffsuperscript\ud835\udc1c\ud835\udc61superscript\ud835\udc33\ud835\udc61superscript\ud835\udf0e\u2032superscript\u00af\ud835\udc22\ud835\udc61\\displaystyle=\\mathbf{\\delta c}^{t}\\odot\\mathbf{z}^{t}\\odot\\sigma^{\\prime}(\\bar{\\mathbf{i}}^{t})= italic_\u03b4 bold_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2299 bold_z start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2299 italic_\u03c3 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( over\u00af start_ARG bold_i end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )\u03b4\ud835\udc33\u00aft\ud835\udeffsuperscript\u00af\ud835\udc33\ud835\udc61\\displaystyle\\delta\\bar{\\mathbf{z}}^{t}italic_\u03b4 over\u00af start_ARG bold_z end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\u03b4\ud835\udc1ct\u2299\ud835\udc22t\u2299g\u2032(\ud835\udc33\u00aft)absentdirect-product\ud835\udeffsuperscript\ud835\udc1c\ud835\udc61superscript\ud835\udc22\ud835\udc61superscript\ud835\udc54\u2032superscript\u00af\ud835\udc33\ud835\udc61\\displaystyle=\\mathbf{\\delta c}^{t}\\odot\\mathbf{i}^{t}\\odot g^{\\prime}(\\bar{\\mathbf{z}}^{t})= italic_\u03b4 bold_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2299 bold_i start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2299 italic_g start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( over\u00af start_ARG bold_z end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )", "Here \u0394tsuperscript\u0394\ud835\udc61\\Delta^{t}roman_\u0394 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT is the vector of deltas passed down from the layerabove.If E\ud835\udc38Eitalic_E is the loss function it formally corresponds to \u2202E\u2202\ud835\udc32t\ud835\udc38superscript\ud835\udc32\ud835\udc61\\frac{\\partial E}{\\partial\\mathbf{y}^{t}}divide start_ARG \u2202 italic_E end_ARG start_ARG \u2202 bold_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT end_ARG, but not including the recurrent dependencies.The deltas for the inputs are only needed if there is a layer below that needs training, and can be computed as follows:", "\u03b4\ud835\udc31t=\ud835\udc16zT\u03b4\ud835\udc33\u00aft+\ud835\udc16iT\u03b4\ud835\udc22\u00aft+\ud835\udc16fT\u03b4\ud835\udc1f\u00aft+\ud835\udc16oT\u03b4\ud835\udc28\u00aft\ud835\udeffsuperscript\ud835\udc31\ud835\udc61superscriptsubscript\ud835\udc16\ud835\udc67\ud835\udc47\ud835\udeffsuperscript\u00af\ud835\udc33\ud835\udc61superscriptsubscript\ud835\udc16\ud835\udc56\ud835\udc47\ud835\udeffsuperscript\u00af\ud835\udc22\ud835\udc61superscriptsubscript\ud835\udc16\ud835\udc53\ud835\udc47\ud835\udeffsuperscript\u00af\ud835\udc1f\ud835\udc61superscriptsubscript\ud835\udc16\ud835\udc5c\ud835\udc47\ud835\udeffsuperscript\u00af\ud835\udc28\ud835\udc61\\mathbf{\\delta x}^{t}=\\mathbf{W}_{z}^{T}\\delta\\bar{\\mathbf{z}}^{t}+\\mathbf{W}_{i}^{T}\\delta\\bar{\\mathbf{i}}^{t}+\\mathbf{W}_{f}^{T}\\delta\\bar{\\mathbf{f}}^{t}+\\mathbf{W}_{o}^{T}\\delta\\bar{\\mathbf{o}}^{t}italic_\u03b4 bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = bold_W start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_\u03b4 over\u00af start_ARG bold_z end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_\u03b4 over\u00af start_ARG bold_i end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_W start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_\u03b4 over\u00af start_ARG bold_f end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_\u03b4 over\u00af start_ARG bold_o end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT", "Finally, the gradients for the weights are calculated as follows, where\u22c6\u22c6\\mathbf{\\star}\u22c6 can be any of {\ud835\udc33\u00af,\ud835\udc22\u00af,\ud835\udc1f\u00af,\ud835\udc28\u00af}\u00af\ud835\udc33\u00af\ud835\udc22\u00af\ud835\udc1f\u00af\ud835\udc28\\{\\bar{\\mathbf{z}},\\bar{\\mathbf{i}},\\bar{\\mathbf{f}},\\bar{\\mathbf{o}}\\}{ over\u00af start_ARG bold_z end_ARG , over\u00af start_ARG bold_i end_ARG , over\u00af start_ARG bold_f end_ARG , over\u00af start_ARG bold_o end_ARG }, and \u27e8\u22c61,\u22c62\u27e9subscript\u22c61subscript\u22c62\\langle\\star_{1},\\star_{2}\\rangle\u27e8 \u22c6 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u22c6 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u27e9 denotes the outer product of two vectors:", "\u03b4\ud835\udc16\u22c6\ud835\udeffsubscript\ud835\udc16\u22c6\\displaystyle\\delta\\mathbf{W}_{\\star}italic_\u03b4 bold_W start_POSTSUBSCRIPT \u22c6 end_POSTSUBSCRIPT=\u2211t=0T\u27e8\u03b4\u22c6t,\ud835\udc31t\u27e9\\displaystyle=\\sum^{T}_{t=0}\\langle\\mathbf{\\delta\\star}^{t},\\mathbf{x}^{t}\\rangle= \u2211 start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT \u27e8 italic_\u03b4 \u22c6 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u27e9\u03b4\ud835\udc29i\ud835\udeffsubscript\ud835\udc29\ud835\udc56\\displaystyle\\delta\\mathbf{p}_{i}italic_\u03b4 bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT=\u2211t=0T\u22121\ud835\udc1ct\u2299\u03b4\ud835\udc22\u00aft+1absentsubscriptsuperscript\ud835\udc471\ud835\udc610direct-productsuperscript\ud835\udc1c\ud835\udc61\ud835\udeffsuperscript\u00af\ud835\udc22\ud835\udc611\\displaystyle=\\sum^{T-1}_{t=0}\\mathbf{c}^{t}\\odot\\delta\\bar{\\mathbf{i}}^{t+1}= \u2211 start_POSTSUPERSCRIPT italic_T - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT bold_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2299 italic_\u03b4 over\u00af start_ARG bold_i end_ARG start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT\u03b4\ud835\udc11\u22c6\ud835\udeffsubscript\ud835\udc11\u22c6\\displaystyle\\delta\\mathbf{R}_{\\star}italic_\u03b4 bold_R start_POSTSUBSCRIPT \u22c6 end_POSTSUBSCRIPT=\u2211t=0T\u22121\u27e8\u03b4\u22c6t+1,\ud835\udc32t\u27e9\\displaystyle=\\sum^{T-1}_{t=0}\\langle\\mathbf{\\delta\\star}^{t+1},\\mathbf{y}^{t}\\rangle= \u2211 start_POSTSUPERSCRIPT italic_T - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT \u27e8 italic_\u03b4 \u22c6 start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT , bold_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u27e9\u03b4\ud835\udc29f\ud835\udeffsubscript\ud835\udc29\ud835\udc53\\displaystyle\\delta\\mathbf{p}_{f}italic_\u03b4 bold_p start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT=\u2211t=0T\u22121\ud835\udc1ct\u2299\u03b4\ud835\udc1f\u00aft+1absentsubscriptsuperscript\ud835\udc471\ud835\udc610direct-productsuperscript\ud835\udc1c\ud835\udc61\ud835\udeffsuperscript\u00af\ud835\udc1f\ud835\udc611\\displaystyle=\\sum^{T-1}_{t=0}\\mathbf{c}^{t}\\odot\\delta\\bar{\\mathbf{f}}^{t+1}= \u2211 start_POSTSUPERSCRIPT italic_T - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT bold_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2299 italic_\u03b4 over\u00af start_ARG bold_f end_ARG start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT\u03b4\ud835\udc1b\u22c6\ud835\udeffsubscript\ud835\udc1b\u22c6\\displaystyle\\delta\\mathbf{b}_{\\star}italic_\u03b4 bold_b start_POSTSUBSCRIPT \u22c6 end_POSTSUBSCRIPT=\u2211t=0T\u03b4\u22c6t\\displaystyle=\\sum^{T}_{t=0}\\mathbf{\\delta\\star}^{t}= \u2211 start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT italic_\u03b4 \u22c6 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT\u03b4\ud835\udc29o\ud835\udeffsubscript\ud835\udc29\ud835\udc5c\\displaystyle\\delta\\mathbf{p}_{o}italic_\u03b4 bold_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT=\u2211t=0T\ud835\udc1ct\u2299\u03b4\ud835\udc28\u00aftabsentsubscriptsuperscript\ud835\udc47\ud835\udc610direct-productsuperscript\ud835\udc1c\ud835\udc61\ud835\udeffsuperscript\u00af\ud835\udc28\ud835\udc61\\displaystyle=\\sum^{T}_{t=0}\\mathbf{c}^{t}\\odot\\delta\\bar{\\mathbf{o}}^{t}= \u2211 start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT bold_c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2299 italic_\u03b4 over\u00af start_ARG bold_o end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT", "The initial version of the LSTM block [14, 15] included (possibly multiple) cells, input and output gates, but no forget gate and no peephole connections.The output gate, unit biases, or input activation function were omitted for certain experiments.Training was done using a mixture of Real Time Recurrent Learning (RTRL) [23, 24] and Backpropagation Through Time (BPTT) [25, 24]. Only the gradient of the cell was propagated back through time, and the gradient for the other recurrent connections was truncated.Thus, that study did not use the exact gradient for training.Another feature of that version was the use of full gate recurrence, which means that all the gates received recurrent inputs from all gates at the previous time-step in addition to the recurrent inputs from the block outputs.This feature did not appear in any of the later papers.", "The first paper to suggest a modification of the LSTM architecture introduced the forget gate [21], enabling the LSTM to reset its own state.This allowed learning of continual tasks such as embedded Reber grammar.", "Gers and Schmidhuber [22] argued that in order to learn precise timings, the cell needs to control the gates.So far this was only possible through an open output gate. Peephole connections (connections from the cell to the gates, blue in Figure\u00a01) were added to the architecture in order to make precise timings easier to learn.Additionally, the output activation function was omitted, as there was no evidence that it was essential for solving the problems that LSTM had been tested on so far.", "The final modification towards the vanilla LSTM was done by Graves and Schmidhuber [20].This study presented the full backpropagation through time (BPTT) training for LSTM networks with the architecture described in section\u00a0II, and presented results on the TIMIT [26] benchmark. Using full BPTT had the added advantage that LSTM gradients could be checked using finite differences, making practical implementations more reliable.", "Since its introduction the vanilla LSTM has been the most commonly used architecture, but other variants have been suggested too.Before the introduction of full BPTT training, Gers et\u00a0al. [27] utilized a training method based on Extended Kalman Filtering which enabled the LSTM to be trained on some pathological cases at the cost of high computational complexity.Schmidhuber et\u00a0al. [28] proposed using a hybrid evolution-based method instead of BPTT for training but retained the vanilla LSTM architecture.", "Bayer et\u00a0al. [29] evolved different LSTM block architectures that maximize fitness on context-sensitive grammars.A larger study of this kind was later done by Jozefowicz et\u00a0al. [30].Sak et\u00a0al. [9] introduced a linear projection layer that projects the output of the LSTM layer down before recurrent and forward connections in order to reduce the amount of parameters for LSTM networks with many blocks.By introducing a trainable scaling parameter for the slope of the gate activation functions, Doetsch et\u00a0al. [5] were able to improve the performance of LSTM on an offline handwriting recognition dataset.In what they call Dynamic Cortex Memory, Otte et\u00a0al. [31] improved convergence speed of LSTM by adding recurrent connections between the gates of a single block (but not between the blocks).", "Cho et\u00a0al. [32] proposed a simplified variant of the LSTM architecture called Gated Recurrent Unit (GRU).They used neither peephole connections nor output activation functions, and coupled the input and the forget gate into an update gate.Finally, their output gate (called reset gate) only gates the recurrent connections to the block input (\ud835\udc16zsubscript\ud835\udc16\ud835\udc67\\mathbf{W}_{z}bold_W start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT). Chung et\u00a0al. [33] performed an initial comparison between GRU and Vanilla LSTM and reported mixed results.", "The focus of our study is to empirically compare different LSTM variants, and not to achieve state-of-the-art results.Therefore, our experiments are designed to keep the setup simple and the comparisons fair.The vanilla LSTM is used as a baseline and evaluated together with eight of its variants.Each variant adds, removes, or modifies the baseline in exactly one aspect, which allows to isolate their effect.They are evaluated on three different datasets from different domains to account for cross-domain variations.", "For fair comparison, the setup needs to be similar for each variant.Different variants might require different settings of hyperparameters to give good performance, and we are interested in the best performance that can be achieved with each variant.For this reason we chose to tune the hyperparameters like learning rate or amount of input noise individually for each variant.Since hyperparameter space is large and impossible to traverse completely, random search was used in order to obtain good-performing hyperparameters [18] for every combination of variant and dataset.Random search was also chosen for the added benefit of providing enough data for analyzing the general effect of various hyperparameters on the performance of each LSTM variant (Section\u00a0V-B).", "Each dataset is split into three parts: a training set, a validation set used for early stopping and for optimizing the hyperparameters, and a test set for the final evaluation.", "The TIMIT Speech corpus [26] is large enough to be a reasonable acoustic modeling benchmark for speech recognition, yet it is small enough to keep a large study such as ours manageable.Our experiments focus on the frame-wise classification task for this dataset, where the objective is to classify each audio-frame as one of 61 phones.222Note that in linguistics a phone represents a distinct speech sound independent of the language.In contrast, a phoneme refers to a sound that distinguishes two words in a given language [34].These terms are often confused in the machine learning literature.From the raw audio we extract 12 Mel Frequency Cepstrum Coefficients (MFCCs) [35] +energy over 25ms hamming-windows with stride of 10ms and a pre-emphasiscoefficient of 0.97. This preprocessing is standard in speech recognition and was chosen in order to stay comparable with earlier LSTM-based results (e.g. [20, 36]). The 13 coefficients along with their first and second derivatives comprise the 39 inputs to the network and were normalized to have zero mean and unit variance.", "The performance is measured as classification error percentage. The training, testing, and validation sets are split in line with Halberstadt [37] into 3696369636963696, 400400400400, and 192192192192 sequences, having 304304304304 frames on average.", "We restrict our study to the core test set, which is an established subset of the full TIMIT corpus, and use the splits intotraining, testing, and validation sets as detailed by Halberstadt [37].In short, that means we only use the core test set and drop the SA samples333The dialect sentences (the SA samples) were meant to expose the dialectal variants of the speakers and were read by all 630 speakers. We follow [37] and remove them because they bias the distribution of phones. from the training set.The validation set is built from some of the discarded samples from the full test set.", "The IAM Online Handwriting Database [38]444The IAM-OnDB was obtained fromhttp://www.iam.unibe.ch/fki/databases/iam-on-line-handwriting-database consists of English sentences as time series of pen movements that have to be mapped to characters.The IAM-OnDB dataset splits into one training set, two validation sets, and one test set, having 775775775775,192192192192, 216216216216, and 544544544544 boards each.Each board, see Figure\u00a02, contains multiple hand-written lines, which in turn consist of several strokes. We use one line per sequence, and joined the two validation sets together, so the final training, validation, and testing sets contain 5\u200935553555\\,3555 355, 2\u200995629562\\,9562 956 and 3\u200985938593\\,8593 859 sequences respectively.", "Each handwriting line is accompanied with a target character sequence, see Figure\u00a02, assembled from the following 81818181\u00a0ASCII characters:abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!\"#&\\\u2019()*+,-./[]:;?The board labeled as a08-551z (in the training set) contains a sequence of eleven percent (%) characters that does not have an image in the strokes, and the percent character does not occur in any other board.That board was removed from the experiments.", "We subsampled each sequence to half its length, which speeds up the training and does not harm performance.Each frame of the sequence is a 4-dimensional vector containing \u0394x\u0394\ud835\udc65\\Delta xroman_\u0394 italic_x, \u0394y\u0394\ud835\udc66\\Delta yroman_\u0394 italic_y (the change in pen position), t\ud835\udc61titalic_t (time since the beginning of the stroke), and a fourth dimension that contains value of one at the time of the pen lifting (a transition to the next stroke) and zeroes at all other time steps.Possible starts and ends of characters within each stroke are not explicitly marked.No additional preprocessing (like base-line straightening, cursive correction, etc.) was used.", "The networks were trained using the Connectionist Temporal Classification (CTC) error function by Graves et\u00a0al. [39] with 82 outputs (81 characters plus the special empty label). We measure performance in terms of the Character Error Rate (CER) after decoding using best-path decoding [39].", "JSB Chorales is a collection of 382 four-part harmonized chorales by J. S. Bach [40], consisting of 202 chorales in major keys and 180 chorals in minor keys.We used the preprocessed piano-rolls provided by Boulanger-Lewandowski et\u00a0al. [41].555Available at http://www-etud.iro.umontreal.ca/~boulanni/icml2012 at the time of writing.These piano-rolls were generated by transposing each MIDI sequence in C major or C minor and sampling frames every quarter note. The networks where trained to do next-step prediction by minimizing the negative log-likelihood. The complete dataset consists of 229229229229, 76767676, and 77777777 sequences (training, validation, and test sets respectively) with an average length of 61616161.", "A network with a single LSTM hidden layer and a sigmoid output layer was used for the JSB Chorales task.Bidirectional LSTM\u00a0[20] was used for TIMIT and IAM Online tasks, consisting of two hidden layers, one processing the input forwards and the other one backwards in time, both connected to a single softmax output layer.As loss function we employed Cross-Entropy Error for TIMIT and JSB Chorales, while for the IAM Online task the Connectionist Temporal Classification (CTC) loss by Graves et\u00a0al. [39] was used.The initial weights for all networks were drawn from a normal distribution with standard deviation of 0.10.10.10.1.Training was done using Stochastic Gradient Descent with Nesterov-style momentum [42] with updates after each sequence. The learning rate was rescaled by a factor of (1\u2212momentum)1momentum(1-\\text{momentum})( 1 - momentum ). Gradients were computed using full BPTT for LSTMs [20].Training stopped after 150 epochs or once there was no improvement on the validation set for more than fifteen epochs.", "The vanilla LSTM from section\u00a0II is referred as Vanilla\u00a0(V).For activation functions we follow the standard and use the logistic sigmoid for \u03c3\ud835\udf0e\\sigmaitalic_\u03c3, and the hyperbolic tangent for both g\ud835\udc54gitalic_g and h\u210ehitalic_h.The derived eight variants of the V architecture are the following. We only report differences to the forward pass formulas presented in Section\u00a0II-A:", "NIG:No Input Gate: \ud835\udc22t=\ud835\udfcfsuperscript\ud835\udc22\ud835\udc611\\mathbf{i}^{t}=\\mathbf{1}bold_i start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = bold_1NFG:No Forget Gate: \ud835\udc1ft=\ud835\udfcfsuperscript\ud835\udc1f\ud835\udc611\\mathbf{f}^{t}=\\mathbf{1}bold_f start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = bold_1NOG:No Output Gate: \ud835\udc28t=\ud835\udfcfsuperscript\ud835\udc28\ud835\udc611\\mathbf{o}^{t}=\\mathbf{1}bold_o start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = bold_1NIAF:No Input Activation Function: g(\ud835\udc31)=\ud835\udc31\ud835\udc54\ud835\udc31\ud835\udc31g(\\mathbf{x})=\\mathbf{x}italic_g ( bold_x ) = bold_xNOAF:No Output Activation Function: h(\ud835\udc31)=\ud835\udc31\u210e\ud835\udc31\ud835\udc31h(\\mathbf{x})=\\mathbf{x}italic_h ( bold_x ) = bold_xCIFG:Coupled Input and Forget Gate:\ud835\udc1ft=\ud835\udfcf\u2212\ud835\udc22tsuperscript\ud835\udc1f\ud835\udc611superscript\ud835\udc22\ud835\udc61\\mathbf{f}^{t}=\\mathbf{1}-\\mathbf{i}^{t}bold_f start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = bold_1 - bold_i start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPTNP:No Peepholes:\ud835\udc22\u00aftsuperscript\u00af\ud835\udc22\ud835\udc61\\displaystyle\\bar{\\mathbf{i}}^{t}over\u00af start_ARG bold_i end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\ud835\udc16i\ud835\udc31t+\ud835\udc11i\ud835\udc32t\u22121+\ud835\udc1biabsentsubscript\ud835\udc16\ud835\udc56superscript\ud835\udc31\ud835\udc61subscript\ud835\udc11\ud835\udc56superscript\ud835\udc32\ud835\udc611subscript\ud835\udc1b\ud835\udc56\\displaystyle=\\mathbf{W}_{i}\\mathbf{x}^{t}+\\mathbf{R}_{i}\\mathbf{y}^{t-1}+\\mathbf{b}_{i}= bold_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_y start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\ud835\udc1f\u00aftsuperscript\u00af\ud835\udc1f\ud835\udc61\\displaystyle\\bar{\\mathbf{f}}^{t}over\u00af start_ARG bold_f end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\ud835\udc16f\ud835\udc31t+\ud835\udc11f\ud835\udc32t\u22121+\ud835\udc1bfabsentsubscript\ud835\udc16\ud835\udc53superscript\ud835\udc31\ud835\udc61subscript\ud835\udc11\ud835\udc53superscript\ud835\udc32\ud835\udc611subscript\ud835\udc1b\ud835\udc53\\displaystyle=\\mathbf{W}_{f}\\mathbf{x}^{t}+\\mathbf{R}_{f}\\mathbf{y}^{t-1}+\\mathbf{b}_{f}= bold_W start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT bold_y start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_b start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT\ud835\udc28\u00aftsuperscript\u00af\ud835\udc28\ud835\udc61\\displaystyle\\bar{\\mathbf{o}}^{t}over\u00af start_ARG bold_o end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\ud835\udc16o\ud835\udc31t+\ud835\udc11o\ud835\udc32t\u22121+\ud835\udc1boabsentsubscript\ud835\udc16\ud835\udc5csuperscript\ud835\udc31\ud835\udc61subscript\ud835\udc11\ud835\udc5csuperscript\ud835\udc32\ud835\udc611subscript\ud835\udc1b\ud835\udc5c\\displaystyle=\\mathbf{W}_{o}\\mathbf{x}^{t}+\\mathbf{R}_{o}\\mathbf{y}^{t-1}+\\mathbf{b}_{o}= bold_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT bold_y start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_b start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPTFGR:Full Gate Recurrence:\ud835\udc22\u00aftsuperscript\u00af\ud835\udc22\ud835\udc61\\displaystyle\\bar{\\mathbf{i}}^{t}over\u00af start_ARG bold_i end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\ud835\udc16i\ud835\udc31t+\ud835\udc11i\ud835\udc32t\u22121+\ud835\udc29i\u2299\ud835\udc1ct\u22121+\ud835\udc1biabsentsubscript\ud835\udc16\ud835\udc56superscript\ud835\udc31\ud835\udc61subscript\ud835\udc11\ud835\udc56superscript\ud835\udc32\ud835\udc611direct-productsubscript\ud835\udc29\ud835\udc56superscript\ud835\udc1c\ud835\udc611subscript\ud835\udc1b\ud835\udc56\\displaystyle=\\mathbf{W}_{i}\\mathbf{x}^{t}+\\mathbf{R}_{i}\\mathbf{y}^{t-1}+\\mathbf{p}_{i}\\odot\\mathbf{c}^{t-1}+\\mathbf{b}_{i}= bold_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_y start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2299 bold_c start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT+\ud835\udc11ii\ud835\udc22t\u22121+\ud835\udc11fi\ud835\udc1ft\u22121+\ud835\udc11oi\ud835\udc28t\u22121subscript\ud835\udc11\ud835\udc56\ud835\udc56superscript\ud835\udc22\ud835\udc611subscript\ud835\udc11\ud835\udc53\ud835\udc56superscript\ud835\udc1f\ud835\udc611subscript\ud835\udc11\ud835\udc5c\ud835\udc56superscript\ud835\udc28\ud835\udc611\\displaystyle\\quad+\\mathbf{R}_{ii}\\mathbf{i}^{t-1}+\\mathbf{R}_{fi}\\mathbf{f}^{t-1}+\\mathbf{R}_{oi}\\mathbf{o}^{t-1}+ bold_R start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT bold_i start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_f italic_i end_POSTSUBSCRIPT bold_f start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_o italic_i end_POSTSUBSCRIPT bold_o start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT\ud835\udc1f\u00aftsuperscript\u00af\ud835\udc1f\ud835\udc61\\displaystyle\\bar{\\mathbf{f}}^{t}over\u00af start_ARG bold_f end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\ud835\udc16f\ud835\udc31t+\ud835\udc11f\ud835\udc32t\u22121+\ud835\udc29f\u2299\ud835\udc1ct\u22121+\ud835\udc1bfabsentsubscript\ud835\udc16\ud835\udc53superscript\ud835\udc31\ud835\udc61subscript\ud835\udc11\ud835\udc53superscript\ud835\udc32\ud835\udc611direct-productsubscript\ud835\udc29\ud835\udc53superscript\ud835\udc1c\ud835\udc611subscript\ud835\udc1b\ud835\udc53\\displaystyle=\\mathbf{W}_{f}\\mathbf{x}^{t}+\\mathbf{R}_{f}\\mathbf{y}^{t-1}+\\mathbf{p}_{f}\\odot\\mathbf{c}^{t-1}+\\mathbf{b}_{f}= bold_W start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT bold_y start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_p start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT \u2299 bold_c start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_b start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT+\ud835\udc11if\ud835\udc22t\u22121+\ud835\udc11ff\ud835\udc1ft\u22121+\ud835\udc11of\ud835\udc28t\u22121subscript\ud835\udc11\ud835\udc56\ud835\udc53superscript\ud835\udc22\ud835\udc611subscript\ud835\udc11\ud835\udc53\ud835\udc53superscript\ud835\udc1f\ud835\udc611subscript\ud835\udc11\ud835\udc5c\ud835\udc53superscript\ud835\udc28\ud835\udc611\\displaystyle\\quad+\\mathbf{R}_{if}\\mathbf{i}^{t-1}+\\mathbf{R}_{ff}\\mathbf{f}^{t-1}+\\mathbf{R}_{of}\\mathbf{o}^{t-1}+ bold_R start_POSTSUBSCRIPT italic_i italic_f end_POSTSUBSCRIPT bold_i start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT bold_f start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_o italic_f end_POSTSUBSCRIPT bold_o start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT\ud835\udc28\u00aftsuperscript\u00af\ud835\udc28\ud835\udc61\\displaystyle\\bar{\\mathbf{o}}^{t}over\u00af start_ARG bold_o end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT=\ud835\udc16o\ud835\udc31t+\ud835\udc11o\ud835\udc32t\u22121+\ud835\udc29o\u2299\ud835\udc1ct\u22121+\ud835\udc1boabsentsubscript\ud835\udc16\ud835\udc5csuperscript\ud835\udc31\ud835\udc61subscript\ud835\udc11\ud835\udc5csuperscript\ud835\udc32\ud835\udc611direct-productsubscript\ud835\udc29\ud835\udc5csuperscript\ud835\udc1c\ud835\udc611subscript\ud835\udc1b\ud835\udc5c\\displaystyle=\\mathbf{W}_{o}\\mathbf{x}^{t}+\\mathbf{R}_{o}\\mathbf{y}^{t-1}+\\mathbf{p}_{o}\\odot\\mathbf{c}^{t-1}+\\mathbf{b}_{o}= bold_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT bold_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT bold_y start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT \u2299 bold_c start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_b start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT+\ud835\udc11io\ud835\udc22t\u22121+\ud835\udc11fo\ud835\udc1ft\u22121+\ud835\udc11oo\ud835\udc28t\u22121subscript\ud835\udc11\ud835\udc56\ud835\udc5csuperscript\ud835\udc22\ud835\udc611subscript\ud835\udc11\ud835\udc53\ud835\udc5csuperscript\ud835\udc1f\ud835\udc611subscript\ud835\udc11\ud835\udc5c\ud835\udc5csuperscript\ud835\udc28\ud835\udc611\\displaystyle\\quad+\\mathbf{R}_{io}\\mathbf{i}^{t-1}+\\mathbf{R}_{fo}\\mathbf{f}^{t-1}+\\mathbf{R}_{oo}\\mathbf{o}^{t-1}+ bold_R start_POSTSUBSCRIPT italic_i italic_o end_POSTSUBSCRIPT bold_i start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_f italic_o end_POSTSUBSCRIPT bold_f start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT + bold_R start_POSTSUBSCRIPT italic_o italic_o end_POSTSUBSCRIPT bold_o start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT", "The first six variants are self-explanatory. The CIFG variant uses only one gate for gating both the input and the cell recurrent self-connection \u2013 a modification of LSTM referred to as Gated Recurrent Units (GRU) [32].This is equivalent to setting \ud835\udc1ft=\ud835\udfcf\u2212\ud835\udc22tsubscript\ud835\udc1f\ud835\udc611subscript\ud835\udc22\ud835\udc61{\\mathbf{f}_{t}=\\mathbf{1}-\\mathbf{i}_{t}}bold_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_1 - bold_i start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT instead of learning the forget gate weights independently.The FGR variant adds recurrent connections between all the gates as in the original formulation of the LSTM [15].It adds nine additional recurrent weight matrices, thus significantly increasing the number of parameters.", "While there are other methods to efficiently search for good hyperparameters (cf.\u00a0[43, 44]), random search has several advantages for our setting:it is easy to implement, trivial to parallelize, and covers the search space more uniformly, thereby improving the follow-up analysis of hyperparameter importance.", "We performed 27272727 random searches (one for each combination of the nine variants and three datasets).Each random search encompasses 200200200200 trials for a total of 5400540054005400 trials of randomly sampling the following hyperparameters:\u2022number of LSTM blocks per hidden layer:log-uniform samples from [20,200]20200[20,200][ 20 , 200 ];\u2022learning rate:log-uniform samples from [10\u22126,10\u22122]superscript106superscript102[10^{-6},10^{-2}][ 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ];\u2022momentum:1\u2212log-uniform samples from\u00a0[0.01,1.0]1log-uniform samples from\u00a0[0.01,1.0]1-\\text{log-uniform samples from $[0.01,1.0]$}1 - log-uniform samples from [ 0.01 , 1.0 ];\u2022standard deviation of Gaussian input noise:uniform samples from [0,1]01[0,1][ 0 , 1 ].", "In the case of the TIMIT dataset, two additional (boolean) hyperparameters were considered (not tuned for the other two datasets).The first one was the choice between traditional momentum and Nesterov-style momentum [42]. Our analysis showed that this had no measurable effect on performance so the latter was arbitrarily chosen for all further experiments.The second one was whether to clip the gradients to the range [\u22121,1]11[-1,1][ - 1 , 1 ]. This turned out to hurt overall performance,666Although this may very well be the result of the range having been chosen too tightly. therefore the gradients were never clipped in the case of the other two datasets.", "Note that, unlike an earlier small-scale study [33], the number of parameters was not kept fixed for all variants. Since different variants can utilize their parameters differently, fixing this number can bias comparisons.", "Each of the 5400540054005400 experiments was run on one of 128 AMD Opteron CPUs at 2.5\u2009GHz and took 24.3\u2009h on average to complete.This sums up to a total single-CPU computation time of just below 15151515 years.", "For TIMIT the test set performance of the best trial were 29.6% classification error (CIFG) which is close to the best reported result of 26.9% [20].Our best result of -8.38 log-likelihood (NIAF) on the JSB Chorales dataset on the other hand is well below the -5.56 from Boulanger-Lewandowski et\u00a0al. [41].Best LSTM result is 26.9%For the IAM Online dataset our best result was a Character Error Rate of 9.26% (NP) on the test set. The best previously published result is 11.5% CER by Graves et\u00a0al. [45] using a different and much more extensive preprocessing.777Note that these numbers differ from the best test set performances that can be found in Figure\u00a03. This is the case because here we only report the single best performing trial as determined on the validation set. In Figure\u00a03, on the other hand, we show the test set performance of the 20 best trials for each variant.Note though, that the goal of this study is not to provide state-of-the-art results, but to do a fair comparison of different LSTM variants.So these numbers are only meant as a rough orientation for the reader.", "A summary of the random search results is shown in Figure\u00a03.Welch\u2019s t\ud835\udc61titalic_t-test at a significance level of p=0.05\ud835\udc5d0.05p=0.05italic_p = 0.05 was used888We applied the Bonferroni adjustment to correct for performing eight different tests (one for each variant). to determine whether the mean test set performance of each variant was significantly different from that of the baseline.The box for a variant is highlighted in blue if its mean performance differs significantly from the mean performance of the vanilla LSTM.", "The results in the top half of Figure\u00a03 represent the distribution of all 200 test set performances over the whole search space.Any conclusions drawn from them are therefore specific to our choice of search ranges.We have tried to chose reasonable ranges for the hyperparameters that include the best settings for each variant and are still small enough to allow for an effective search.The means and variances tend to be rather similar for the different variants and datasets, but even here some significant differences can be found.", "In order to draw some more interesting conclusions we restrict our further analysis to the top 10% performing trials for each combination of dataset and variant (see bottom half of Figure\u00a03).This way our findings will be less dependent on the chosen search space and will be representative for the case of \u201creasonable hyperparameter tuning efforts.\u201d999How much effort is \u201creasonable\u201d will still depend on the search space. If the ranges are chosen much larger, the search will take much longer to find good hyperparameters.", "The first important observation based on Figure\u00a03 is that removing the output activation function (NOAF) or the forget gate (NFG) significantly hurt performance on all three datasets.Apart from the CEC, the ability to forget old information and the squashing of the cell state appear to be critical for the LSTM architecture.Indeed, without the output activation function, the block output can in principle grow unbounded.Coupling the input and the forget gate avoids this problem and might render the use of an output non-linearity less important, which could explain why GRU performs well without it.", "Input and forget gate coupling (CIFG) did not significantly change mean performance on any of the datasets, although the best performance improved slightly on music modeling. Similarly, removing peephole connections (NP) also did not lead to significant changes, but the best performance improved slightly for handwriting recognition.Both of these variants simplify LSTMs and reduce the computational complexity, so it might be worthwhile to incorporate these changes into the architecture.", "Adding full gate recurrence (FGR) did not significantly change performance on TIMIT or IAM Online, but led to worse results on the JSB Chorales dataset.Given that this variant greatly increases the number of parameters, we generally advise against using it.Note that this feature was present in the original proposal of LSTM [14, 15], but has been absent in all following studies.", "Removing the input gate (NIG), the output gate (NOG), and the input activation function (NIAF) led to a significant reduction in performance on speech and handwriting recognition.However, there was no significant effect on music modeling performance.A small (but statistically insignificant) average performance improvement was observed for the NIG and NIAF architectures on music modeling.We hypothesize that these behaviors will generalize to similar problems such as language modeling.For supervised learning on continuous real-valued data (such as speech and handwriting recognition), the input gate, output gate, and input activation function are all crucial for obtaining good performance.", "The fANOVA framework for assessing hyperparameter importance by Hutter et\u00a0al. [19] is based on the observation that marginalizing over dimensions can be done efficiently in regression trees.This allows predicting the marginal error for one hyperparameter while averaging over all the others.Traditionally this would require a full hyperparameter grid search, whereas here the hyperparameter space can be sampled at random.", "Average performance for any slice of the hyperparameter space is obtained by first training a regression tree and then summing over its predictions along the corresponding subset of dimensions.To be precise, a random regression forest of 100100100100 trees is trained and their prediction performance is averaged.This improves the generalization and allows for an estimation of uncertainty of those predictions.The obtained marginals can then be used to decompose the variance into additive components using the functional ANalysis Of VAriance (fANOVA) method [46] which provides an insight into the overall importance of hyperparameters and their interactions.", "Learning rate is the most important hyperparameter, therefore it is very important to understand how to set it correctly in order to achieve good performance.Figure\u00a04 shows (in blue) how setting the learning rate value affects the predicted average performance on the test set.It is important to note that this is an average over all other hyperparameters and over all the trees in the regression forest.The shaded area around the curve indicates the standard deviation over tree predictions (not over other hyperparameters), thus quantifying the reliability of the average.The same is shown in green with the predicted average training time.", "The plots in Figure\u00a04 show that the optimal value for the learning rate is dependent on the dataset.For each dataset, there is a large basin (up to two orders of magnitude) of good learning rates inside of which the performance does not vary much.A related but unsurprising observation is that there is a sweet-spot for the learning rate at the high end of the basin.101010Note that it is unfortunately outside the investigated range for IAM Online and JSB Chorales. This means that ideally we should have chosen the range of learning rates to include higher values as well.In this region, the performance is good and the training time is small.So while searching for a good learning rate for the LSTM, it is sufficient to do a coarse search by starting with a high value (e.g. 1.01.01.01.0) and dividing it by ten until performance stops increasing.", "Figure\u00a05 also shows that the fraction of variance caused by the learning rate is much bigger than the fraction due to interaction between learning rate and hidden layer size (some part of the \u201chigher order\u201d piece, for more see below at Interaction of Hyperparameters).This suggests that the learning rate can be quickly tuned on a small network and then used to train a large one.", "Not surprisingly the hidden layer size is an important hyperparameter affecting the LSTM network performance.As expected, larger networks perform better, but with diminishing returns.It can also be seen in Figure\u00a04 (middle, green) that the required training time increases with the network size.Note that the scale here is wall-time and thus factors in both the increased computation time for each epoch as well as the convergence speed.", "Additive Gaussian noise on the inputs, a traditional regularizer for neural networks, has been used for LSTM as well.However, we find that not only does it almost always hurt performance, it also slightly increases training times.The only exception is TIMIT, where a small dip in error for the range of [0.2,0.5]0.20.5[0.2,0.5][ 0.2 , 0.5 ] is observed.", "One unexpected result of this study is that momentum affects neither performance nor training time in any significant way.This follows from the observation that for none of the datasets, momentum accounted for more than 1% of the variance of test set performance.It should be noted that for TIMIT the interaction between learning rate and momentum accounts for 2.5% of the total variance, but as with learning rate \u00d7\\times\u00d7 hidden size (cf. Interaction of Hyperparameters below) it does not reveal any interpretable structure.This may be the result of our choice to scale learning rates dependent on momentum (Section\u00a0IV-B).These observations suggest that momentum does not offer substantial benefits when training LSTMs with online stochastic gradient descent.", "Figure\u00a05 shows what fraction of the test set performance variance can be attributed to different hyperparameters. It is obvious that the learning rate is by far the most important hyperparameter, always accounting for more than two thirds of the variance. The next most important hyperparameter is the hidden layer size, followed by the input noise, leaving the momentum with less than one percent of the variance. Higher order interactions play an important role in the case of TIMIT, but are much less important for the other two data sets. ", "Some hyperparameters interact with each other resulting in different performance from what could be expected by looking at them individually.As shown in Figure\u00a05 all these interactions together explain between 5% and 20% of the variance in test set performance.Understanding these interactions might allow us to speed up the search for good combinations of hyperparameters.To that end we visualize the interaction between all pairs of hyperparameters in Figure\u00a06.Each heat map in the left part shows marginal performance for different values of the respective two hyperparameters.This is the average performance predicted by the decision forest when marginalizing over all other hyperparameters.So each one is the 2D version of the performance plots from Figure\u00a04 in the paper.", "The right side employs the idea of ANOVA to better illustrate the interaction between the hyperparameters.This means that variance of performance that can be explained by varying a single hyperparameter has been removed.In case two hyperparameters do not interact at all (are perfectly independent), that residual would thus be all zero (grey).", "For example, looking at the pair hidden size and learning rate on the left side for the TIMIT dataset, we can see that performance varies strongly along the x\ud835\udc65xitalic_x-axis (learning rate), first decreasing and then increasing again.This is what we would expect knowing the valley-shape of the learning rate from Figure\u00a04.Along the y\ud835\udc66yitalic_y-axis (hidden size) performance seems to decrease slightly from top to bottom.Again this is roughly what we would expect from the hidden size plot in Figure\u00a04.", "On the right side of Figure\u00a06 we can see for the same pair of hyperparameters how their interaction differs from the case of them being completely independent. This heat map exhibits less structure, and it may in fact be the case that we would need more samples to properly analyze the interplay between them. However, given our observations so far this might not be worth the effort. In any case, it is clear from the plot on the left that varying the hidden size does not change the region of optimal learning rate.", "One clear interaction pattern can be observed in the IAM\u00a0Online and JSB datasets between learning rate and input noise.Here it can be seen that for high learning rates (\u2a8610\u22124greater-than-or-approximately-equalsabsentsuperscript104\\gtrapprox 10^{-4}\u2a86 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT) lower input noise (\u2a85.5absent.5\\lessapprox.5\u2a85 .5) is better like also observed in the marginals from Figure\u00a04.But this trend reverses for lower learning rates, where higher values of input noise are beneficial.Though interesting this is not of any practical relevance because performance is generally bad in that region of low learning rates.Apart from this, however, it is difficult to discern any regularities in the analyzed hyperparameter interactions.We conclude that there is little practical value in attending to the interplay between hyperparameters.So for practical purposes hyperparameters can be treated as approximately independent and thus optimized separately.", "This paper reports the results of a large scale study on variants of the LSTM architecture. We conclude that the most commonly used LSTM architecture (vanilla LSTM) performs reasonably well on various datasets.None of the eight investigated modifications significantly improves performance.However, certain modifications such as coupling the input and forget gates (CIFG) or removing peephole connections (NP) simplified LSTMs in our experiments without significantly decreasing performance.These two variants are also attractive because they reduce the number of parameters and the computational cost of the LSTM.", "The forget gate and the output activation function are the most critical components of the LSTM block.Removing any of them significantly impairs performance.We hypothesize that the output activation function is needed to prevent the unbounded cell state to propagate through the network and destabilize learning.This would explain why the LSTM variant GRU can perform reasonably well without it: its cell state is bounded because of the coupling of input and forget gate.", "As expected, the learning rate is the most crucial hyperparameter, followed by the network size.Surprisingly though, the use of momentum was found to be unimportant in our setting of online gradient descent.Gaussian noise on the inputs was found to be moderately helpful for TIMIT, but harmful for the other datasets.", "The analysis of hyperparameter interactions revealed no apparent structure.Furthermore, even the highest measured interaction (between learning rate and network size) is quite small.This implies that for practical purposes the hyperparameters can be treated as approximately independent.In particular, the learning rate can be tuned first using a fairly small network, thus saving a lot of experimentation time.", "Neural networks can be tricky to use for many practitioners compared to other methods whose properties are already well understood.This has remained a hurdle for newcomers to the field since a lot of practical choices are based on the intuitions of experts, as well as experiences gained over time. With this study, we have attempted to back some of these intuitions with experimental results.We have also presented new insights, both on architecture selection and hyperparameter tuning for LSTM networks which have emerged as the method of choice for solving complex sequence learning problems. In future work, we plan to explore more complex modifications of the LSTM architecture."], "figure_types": {"a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/2-Figure1-1.png": "schematic", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/4-Figure2-1.png": "other", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/6-Figure3-1.png": "plot", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/8-Figure4-1.png": "plot", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/8-Figure5-1.png": "plot", "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081/9-Figure6-1.png": "plot"}}, "1503.02531": {"paper_id": "paper_55", "title": "Distilling the Knowledge in a Neural Network", "arxiv_url": "https://arxiv.org/abs/1503.02531", "s2orc_url": "https://www.semanticscholar.org/paper/0c908739fbff75f03469d13d4a1a07de3414ee19", "all_figures_tables": {"0c908739fbff75f03469d13d4a1a07de3414ee19/5-Table1-1.png": "Table 1: Frame classification accuracy and WER showing that the distilled single model performs about as well as the averaged predictions of 10 models that were used to create the soft targets.", "0c908739fbff75f03469d13d4a1a07de3414ee19/6-Table2-1.png": "Table 2: Example classes from clusters computed by our covariance matrix clustering algorithm", "0c908739fbff75f03469d13d4a1a07de3414ee19/7-Table3-1.png": "Table 3: Classification accuracy (top 1) on the JFT development set.", "0c908739fbff75f03469d13d4a1a07de3414ee19/7-Table4-1.png": "Table 4: Top 1 accuracy improvement by # of specialist models covering correct class on the JFT test set.", "0c908739fbff75f03469d13d4a1a07de3414ee19/8-Table5-1.png": "Table 5: Soft targets allow a new model to generalize well from only 3% of the training set. The soft targets are obtained by training on the full training set."}, "referred_figures_tables": [["0c908739fbff75f03469d13d4a1a07de3414ee19/6-Table2-1.png"], ["0c908739fbff75f03469d13d4a1a07de3414ee19/7-Table3-1.png"], ["0c908739fbff75f03469d13d4a1a07de3414ee19/6-Table2-1.png"], ["0c908739fbff75f03469d13d4a1a07de3414ee19/7-Table4-1.png"]], "question_id": [1, 8, 9, 12], "question": ["The authors proposed approach only works for classification models, and not for models that have other types of outputs. True or False?", "Would more recent approaches such as DECAF extreme classification (2021) serve as a stronger baseline than the specialized models discussed in the paper?", "Is the KMeans algorithm discussed in the paper require a labelled dataset?", "What factors could the authors have used while deciding the number of specialists to allocate for their task?"], "question_section": ["1. Introduction", "5.2 Specialist Models", "5.3 Assigning classes to specialists", "6.1 Using soft targets to prevent specialists from overfitting"], "question_trigger_sentence": ["For tasks like MNIST in which the cumbersome model almost always produces the correct answer with very high confidence, much of the information about the learned function resides in the ratios of very small probabilities in the soft targets. ", "When the number of classes is very large, it makes sense for the cumbersome model to be an ensemble that contains one generalist model trained on all the data and many \u201cspecialist\u201d models, each\nof which is trained on data that is highly enriched in examples from a very confusable subset of the\nclasses (like different types of mushroom). ", "We applied an on-line version of the K-means algorithm to the columns of the covariance matrix, and obtained reasonable clusters (shown in Table 2).", "If we allow specialists to have a full softmax over all classes, there may be a much better way to prevent them overfitting than using early stopping. A specialist is trained on data that is highly enriched in its special classes. This means that the effective size of its training set is much smaller and it has a strong tendency to overfit on its special classes."], "question_type": ["Shallow question", "Deep/complex question", "Shallow question", "Deep/complex question"], "evidential_info": [[{"context": "In this section we give an example of such a dataset and we show howlearning specialist models that each focus on a different confusablesubset of the classes can reduce the total amount of computationrequired to learn an ensemble. The main problem with specialists thatfocus on making fine-grained distinctions is that they overfit veryeasily and we describe how this overfitting may be prevented by usingsoft targets.", "rationale": "In this work\u2019s approach, specialist models should be learnt such that they can focus on different confusable subset of classes in a dataset."}, {"context": "In order to derive groupings of object categories for the specialists, we decided to focus on categories that our fullnetwork often confuses. Even though we could have computed the confusion matrix and used it as a way to find suchclusters, we opted for a simpler approach that does not require the true labels to construct the clusters.", "rationale": "In this work\u2019s approach, it should be possible to derive groupings from object categories."}, {"context": "In particular, we apply a clustering algorithm to the covariance matrix of the predictions of our generalist model, so that a set of classes S m that are often predicted together will be used as targets for one of our specialist models, m. We applied an on-line version of the K-means algorithm to the columns of the covariance matrix, and obtained reasonable clusters (shown in Table 2). We tried several clustering algorithms which produced similar results.", "rationale": "In this work, the approach assumes that there are classes or sets of classes that the models should be able to predict."}], [{"context": "Starting from the trained baseline full network,the specialists train extremely fast (a few days instead of many weeks for JFT). Also, all the specialistsare trained completely independently. Table \u00a03 shows the absolute test accuracy for thebaseline system and the baseline system combined with the specialistmodels. With 61 specialist models, there is a4.4% relative improvement in test accuracy overall. We also report conditional test accuracy, which is the accuracy by only considering examples belonging to the specialist classes, and restricting our predictions to that subset of classes.", "rationale": "The specialist models were started from the trained baseline full network for JFT."}, {"context": "JFT is an internal Google dataset that has 100 million labeled images with 15,000 labels. When we did this work, Google\u2019s baseline model for JFT was a deep convolutional neural network [7] that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores. This training used two types of parallelism [2]. First, there were many replicas of the neural net running on different sets of cores and processing different mini-batches from the training set. Each replica computes the average gradient on its current mini-batch and sends this gradient to a sharded parameter server which sends back new values for the parameters. These new values reflect all of the gradients received by the parameter server since the last time it sent parameters to the replica. Second, each replica is spread over multiple cores by putting different subsets of the neurons on each core. Ensemble training is yet a third type of parallelism that can be wrapped around the other two types, but only if a lot more cores are available. Waiting for several years to train an ensemble of models was not an option, so we needed a much faster way to improve the baseline model.", "rationale": "The baseline model was Google\u2019s deep convolutional network for JFT."}], [{"context": "JFT is an internal Google dataset that has 100 million labeled imageswith 15,000 labels. When we did this work, Google\u2019s baseline model forJFT was a deep convolutional neural network [7] that had been trained forabout six months using asynchronous stochastic gradient descent on alarge number of cores. This training used two types ofparallelism [2]. First, there were many replicas of the neural net runningon different sets of cores and processing different mini-batches fromthe training set. Each replica computes the average gradient on itscurrent mini-batch and sends this gradient to a sharded parameter server whichsends back new values for the parameters. These new values reflect allof the gradients received by the parameter server since the last timeit sent parameters to the replica. Second, each replica is spread overmultiple cores by putting different subsets of the neurons on eachcore. Ensemble training is yet a third type of parallelism that can bewrapped around the other two types, but only if a lot more cores areavailable. Waiting for several years to train an ensemble of models wasnot an option, so we needed a much faster way to improve the baselinemodel.", "rationale": "The K-means algorithm clusters the set of classes that the models often predict together."}, {"context": "In order to derive groupings of object categories for the specialists, we decided to focus on categories that our fullnetwork often confuses. Even though we could have computed the confusion matrix and used it as a way to find suchclusters, we opted for a simpler approach that does not require the true labels to construct the clusters.", "rationale": "The approach used for grouping in this work did not require true labels to construct clusters that are often confused."}, {"context": "In particular, we apply a clustering algorithm to the covariance matrix of the predictions of our generalist model, so thata set of classes S^{m} that are often predicted together will be used as targets for one of our specialist models, m. We applied an on-lineversion of the K-means algorithm to the columns of the covariance matrix, and obtained reasonable clusters (shown inTable 2). We tried several clustering algorithms which produced similar results.", "rationale": "JFT is a dataset that has 100 million labeled images with 15,000 labels and was used for training of models."}, {"context": "To reduce overfitting and share the work of learning lower level feature detectors, each specialist model is initialized with the weights of the generalist model. These weights are then slightly modified by training the specialist with half its examples coming from its special subset and half sampled at random from the remainder of the training set. After training, we can correct for the biased training set by incrementing the logit of the dustbin class by the log of the proportion by which the specialist class is oversampled.", "rationale": "The specialist models are trained using examples from the training set."}], [{"context": "For our JFT specialist experiments, we trained 61 specialist models, each with 300 classes (plus the dustbin class).Because the sets of classes for the specialists are not disjoint, we often had multiple specialists covering aparticular image class. Table \u00a04 shows the number of test set examples, the change inthe number of examples correct at position 1 when using the specialist(s), and the relative percentage improvement intop1 accuracy for the JFT dataset broken down by the number of specialists covering the class. We are encouraged by thegeneral trend that accuracy improvements are larger when we have more specialists covering a particular class, sincetraining independent specialist models is very easy to parallelize.", "rationale": "Through results shown in Table 4, the authors saw a general trend that accuracy improved when more specialists covered a particular class."}]], "composition": ["In this work, the approach assumes that there are classes that the models should be able to predict. The work focuses on classification models. Thus, whether the approach can work on models with other types of outputs cannot be answered from this paper.", "The specialist models were started from the baseline model which was Google\u2019s deep convolutional network for JFT. The function and performance of DECAF, and how it compares to the JFT baseline model used in this work cannot be answered from this paper.", "The K-means algorithm clusters the set of classes that the models often predict together. In this work, this clustering approach did not require true labels. However, the models themselves were trained using examples from a dataset, JFT, which contains labeled images. Thus, although the K-means algorithm does not require a labeled dataset, the models whose predictions are used in the algorithm required a labeled dataset.", "P0: Through results shown in Table 4, the authors saw a general trend that accuracy improved when more specialists covered a particular class. This could have been a factor that authors considered in deciding on the number of specialists for their task."], "Is_figure_in_evidence": [false, false, false, false], "Is_table_in_evidence": [true, true, true, true], "question_key": ["1079", "1086", "1087", "1090"], "passages": ["Many insects have a larval form that is optimized for extracting energy and nutrients from the environment and acompletely different adult form that is optimized for the very different requirements of traveling and reproduction. Inlarge-scale machine learning, we typically use very similar models for the training stage and the deployment stagedespite their very different requirements: For tasks like speech and object recognition, training must extract structurefrom very large, highly redundant datasets but it does not need to operate in real time and it can use a huge amount ofcomputation. Deployment to a large number of users, however, has much more stringent requirements on latency andcomputational resources. The analogy with insects suggests that we should be willing to train very cumbersome models ifthat makes it easier to extract structure from the data. The cumbersome model could be an ensemble of separately trainedmodels or a single very large model trained with a very strong regularizer such as dropout [9]. Once the cumbersome modelhas been trained, we can then use a different kind of training, which we call \u201cdistillation\u201d to transfer the knowledgefrom the cumbersome model to a small model that is more suitable for deployment. A version of this strategy hasalready been pioneered by Rich Caruana and his collaborators [1]. In their important paper they demonstrateconvincingly that the knowledge acquired by a large ensemble of models can be transferred to a single small model.", "A conceptual block that may have prevented more investigation of this very promising approach is that we tend toidentify the knowledge in a trained model with the learned parameter values and this makes it hard to see how we canchange the form of the model but keep the same knowledge. A more abstract view of the knowledge, that frees it from anyparticular instantiation, is that it is a learned mapping from input vectors to output vectors. For cumbersome modelsthat learn to discriminate between a large number of classes, the normal training objective is to maximize the average logprobability of the correct answer, but a side-effect of the learning is that the trained model assignsprobabilities to all of the incorrect answers and even when these probabilities are very small, some of them are muchlarger than others. The relative probabilities of incorrect answers tell us a lot about how the cumbersome modeltends to generalize. An image of a BMW, for example, may only have a very small chance of being mistaken for a garbagetruck, but that mistake is still many times more probable than mistaking it for a carrot.", "It is generally accepted that the objective function used for training should reflect the true objective of the user asclosely as possible. Despite this, models are usually trained to optimize performance on the training data when the realobjective is to generalize well to new data. It would clearly be better to train models to generalize well, but thisrequires information about the correct way to generalize and this information is not normally available. When we aredistilling the knowledge from a large model into a small one, however, we can train the small model to generalize in thesame way as the large model. If the cumbersome model generalizes well because, for example, it is the average of a largeensemble of different models, a small model trained to generalize in the same way will typically do much better on testdata than a small model that is trained in the normal way on the same training set as was used to train the ensemble.", "An obvious way to transfer the generalization ability of the cumbersome model to a small model is to use the classprobabilities produced by the cumbersome model as \u201csoft targets\u201d for training the small model. For this transferstage, we could use the same training set or a separate \u201ctransfer\u201d set. When thecumbersome model is a large ensemble of simpler models, we can use an arithmetic or geometric mean of their individual predictive distributions as the soft targets. When the softtargets have high entropy, they provide much more information per training case than hard targets and much less variancein the gradient between training cases, so the small model can often be trained on much less data than the originalcumbersome model and using a much higher learning rate.", "For tasks like MNIST in which the cumbersome model almost always produces the correct answer with very high confidence,much of the information about the learned function resides in the ratios of very small probabilities in the softtargets. For example, one version of a 2 may be given a probability of 10\u22126superscript10610^{-6}10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT of being a 3 and 10\u22129superscript10910^{-9}10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPT of being a 7whereas for another version it may be the other way around. This is valuable information that defines a rich similaritystructure over the data (i. e. it says which 2\u2019s look like 3\u2019s and which look like 7\u2019s) but it has very littleinfluence on the cross-entropy cost function during the transfer stage because the probabilities are so close to zero.Caruana and his collaborators circumvent this problem by using the logits (the inputs to the final softmax) rather thanthe probabilities produced by the softmax as the targets for learning the small model and they minimize the squareddifference between the logits produced by the cumbersome model and the logits produced by the small model. Our moregeneral solution, called \u201cdistillation\u201d, is to raise the temperature of the final softmax until the cumbersome modelproduces a suitably soft set of targets. We then use the same high temperature when training the small model to matchthese soft targets. We show later that matching the logits of the cumbersome model is actually a special case ofdistillation.", "The transfer set that is used to train the small model could consist entirely of unlabeled data [1] or we could usethe original training set. We have found that using the original training set works well, especially if we add a smallterm to the objective function that encourages the small model to predict the true targets as well as matching the softtargets provided by the cumbersome model. Typically, the small model cannot exactly match the soft targets and erringin the direction of the correct answer turns out to be helpful.", "Neural networks typically produce class probabilities by using a \u201csoftmax\u201d output layer that converts the logit,zisubscript\ud835\udc67\ud835\udc56z_{i}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, computed for each class into a probability, qisubscript\ud835\udc5e\ud835\udc56q_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, by comparing zisubscript\ud835\udc67\ud835\udc56z_{i}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT with the other logits.qi=exp(zi/T)\u2211jexp(zj/T)subscript\ud835\udc5e\ud835\udc56\ud835\udc52\ud835\udc65\ud835\udc5dsubscript\ud835\udc67\ud835\udc56\ud835\udc47subscript\ud835\udc57\ud835\udc52\ud835\udc65\ud835\udc5dsubscript\ud835\udc67\ud835\udc57\ud835\udc47q_{i}=\\frac{exp(z_{i}/T)}{\\sum_{j}exp(z_{j}/T)}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG italic_e italic_x italic_p ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_T ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_e italic_x italic_p ( italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / italic_T ) end_ARG(1)where T\ud835\udc47Titalic_T is a temperature that is normally set to 1111. Using a higher value for T\ud835\udc47Titalic_T produces a softer probabilitydistribution over classes.", "In the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer setand using a soft target distribution for each case in the transfer set that is produced by using the cumbersome modelwith a high temperature in its softmax. The same high temperature is used when training the distilled model, but afterit has been trained it uses a temperature of 1.", "When the correct labels are known for all or some of the transfer set,this method can be significantly improved by also training thedistilled model to produce the correct labels. One way to do this isto use the correct labels to modify the soft targets, but we foundthat a better way is to simply use a weighted average of two differentobjective functions. The first objective function is the crossentropy with the soft targets and this cross entropy is computed usingthe same high temperature in the softmax of the distilled model as wasused for generating the soft targets from the cumbersome model. Thesecond objective function is the cross entropy with the correctlabels. This is computed using exactly the same logits in softmax ofthe distilled model but at a temperature of 1. We found that the bestresults were generally obtained by using a condiderably lower weighton the second objective function. Since the magnitudes of thegradients produced by the soft targets scale as 1/T21superscript\ud835\udc4721/T^{2}1 / italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT it isimportant to multiply them by T2superscript\ud835\udc472T^{2}italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT when using both hard and softtargets. This ensures that the relative contributions of the hard andsoft targets remain roughly unchanged if the temperature used fordistillation is changed while experimenting with meta-parameters.", "Each case in the transfer set contributes a cross-entropy gradient, dC/dzi\ud835\udc51\ud835\udc36\ud835\udc51subscript\ud835\udc67\ud835\udc56dC/dz_{i}italic_d italic_C / italic_d italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, with respect to each logit, zisubscript\ud835\udc67\ud835\udc56z_{i}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT of thedistilled model. If the cumbersome model has logits visubscript\ud835\udc63\ud835\udc56v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT which produce soft target probabilities pisubscript\ud835\udc5d\ud835\udc56p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and the transfer training is done at a temperature of T\ud835\udc47Titalic_T,this gradient is given by:\u2202C\u2202zi=1T(qi\u2212pi)=1T(ezi/T\u2211jezj/T\u2212evi/T\u2211jevj/T)\ud835\udc36subscript\ud835\udc67\ud835\udc561\ud835\udc47subscript\ud835\udc5e\ud835\udc56subscript\ud835\udc5d\ud835\udc561\ud835\udc47superscript\ud835\udc52subscript\ud835\udc67\ud835\udc56\ud835\udc47subscript\ud835\udc57superscript\ud835\udc52subscript\ud835\udc67\ud835\udc57\ud835\udc47superscript\ud835\udc52subscript\ud835\udc63\ud835\udc56\ud835\udc47subscript\ud835\udc57superscript\ud835\udc52subscript\ud835\udc63\ud835\udc57\ud835\udc47\\frac{\\partial C}{\\partial z_{i}}=\\frac{1}{T}\\left(q_{i}-p_{i}\\right)=\\frac{1}{T}\\left(\\frac{e^{z_{i}/T}}{\\sum_{j}e^{z_{j}/T}}-\\frac{e^{v_{i}/T}}{\\sum_{j}e^{v_{j}/T}}\\right)divide start_ARG \u2202 italic_C end_ARG start_ARG \u2202 italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG = divide start_ARG 1 end_ARG start_ARG italic_T end_ARG ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG italic_T end_ARG ( divide start_ARG italic_e start_POSTSUPERSCRIPT italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_T end_POSTSUPERSCRIPT end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / italic_T end_POSTSUPERSCRIPT end_ARG - divide start_ARG italic_e start_POSTSUPERSCRIPT italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_T end_POSTSUPERSCRIPT end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / italic_T end_POSTSUPERSCRIPT end_ARG )(2)", "If the temperature is high compared with the magnitude of the logits, we can approximate:\u2202C\u2202zi\u22481T(1+zi/TN+\u2211jzj/T\u22121+vi/TN+\u2211jvj/T)\ud835\udc36subscript\ud835\udc67\ud835\udc561\ud835\udc471subscript\ud835\udc67\ud835\udc56\ud835\udc47\ud835\udc41subscript\ud835\udc57subscript\ud835\udc67\ud835\udc57\ud835\udc471subscript\ud835\udc63\ud835\udc56\ud835\udc47\ud835\udc41subscript\ud835\udc57subscript\ud835\udc63\ud835\udc57\ud835\udc47\\frac{\\partial C}{\\partial z_{i}}\\approx\\frac{1}{T}\\left(\\frac{1+z_{i}/T}{N+\\sum_{j}z_{j}/T}-\\frac{1+v_{i}/T}{N+\\sum_{j}v_{j}/T}\\right)divide start_ARG \u2202 italic_C end_ARG start_ARG \u2202 italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG \u2248 divide start_ARG 1 end_ARG start_ARG italic_T end_ARG ( divide start_ARG 1 + italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_T end_ARG start_ARG italic_N + \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / italic_T end_ARG - divide start_ARG 1 + italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_T end_ARG start_ARG italic_N + \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / italic_T end_ARG )(3)", "If we now assume that the logits have been zero-meaned separately for each transfer case so that \u2211jzj=\u2211jvj=0subscript\ud835\udc57subscript\ud835\udc67\ud835\udc57subscript\ud835\udc57subscript\ud835\udc63\ud835\udc570\\sum_{j}z_{j}=\\sum_{j}v_{j}=0\u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = 0 Eq. 3 simplifies to:\u2202C\u2202zi\u22481NT2(zi\u2212vi)\ud835\udc36subscript\ud835\udc67\ud835\udc561\ud835\udc41superscript\ud835\udc472subscript\ud835\udc67\ud835\udc56subscript\ud835\udc63\ud835\udc56\\frac{\\partial C}{\\partial z_{i}}\\approx\\frac{1}{NT^{2}}\\left(z_{i}-v_{i}\\right)divide start_ARG \u2202 italic_C end_ARG start_ARG \u2202 italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG \u2248 divide start_ARG 1 end_ARG start_ARG italic_N italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )(4)So in the high temperature limit, distillation is equivalent tominimizing 1/2(zi\u2212vi)212superscriptsubscript\ud835\udc67\ud835\udc56subscript\ud835\udc63\ud835\udc562{1/2}(z_{i}-v_{i})^{2}1 / 2 ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, provided the logits are zero-meanedseparately for each transfer case. At lower temperatures, distillationpays much less attention to matching logits that are much morenegative than the average. This is potentially advantageous becausethese logits are almost completely unconstrained by the cost functionused for training the cumbersome model so they could be very noisy.On the other hand, the very negative logits may convey usefulinformation about the knowledge acquired by the cumbersomemodel. Which of these effects dominates is an empirical question. Weshow that when the distilled model is much too small to capture all ofthe knowledege in the cumbersome model, intermediate temperatures workbest which strongly suggests that ignoring the large negative logitscan be helpful.", "To see how well distillation works, we trained a single large neuralnet with two hidden layers of 1200 rectified linear hidden units onall 60,000 training cases. The net was strongly regularized usingdropout and weight-constraints as described in [5]. Dropoutcan be viewed as a way of training an exponentially large ensemble ofmodels that share weights. In addition, the input images were jitteredby up to two pixels in any direction. This net achieved 67 testerrors whereas a smaller net with two hidden layers of 800 rectifiedlinear hidden units and no regularization achieved 146 errors. But ifthe smaller net was regularized solely by adding the additional taskof matching the soft targets produced by the large net at atemperature of 20, it achieved 74 test errors. This shows that softtargets can transfer a great deal of knowledge to the distilled model,including the knowledge about how to generalize that is learned fromtranslated training data even though the transfer set does not containany translations.", "When the distilled net had 300 or more units in each of its two hiddenlayers, all temperatures above 8 gave fairly similar results. Butwhen this was radically reduced to 30 units per layer, temperatures in the range2.5 to 4 worked significantly better than higher or lowertemperatures.", "We then tried omitting all examples of the digit 3 from the transfer set. So from the perspective of the distilledmodel, 3 is a mythical digit that it has never seen. Despite this, the distilled model only makes 206 test errors ofwhich 133 are on the 1010 threes in the test set. Most of the errors are caused by the fact that the learned bias forthe 3 class is much too low. If this bias is increased by 3.5 (which optimizes overall performance on the test set), thedistilled model makes 109 errors of which 14 are on 3s. So with the right bias, the distilled model gets 98.6% ofthe test 3s correct despite never having seen a 3 during training. If the transfer set contains only the 7s and 8s fromthe training set, the distilled model makes 47.3% test errors, but when the biases for 7 and 8 are reduced by 7.6 tooptimize test performance, this falls to 13.2% test errors.", "In this section, we investigate the effects of ensembling Deep Neural Network (DNN) acoustic models that are used inAutomatic Speech Recognition (ASR). We show that the distillation strategy that we propose in this paper achieves thedesired effect of distilling an ensemble of models into a single model that works significantly better than a model ofthe same size that is learned directly from the same training data.", "State-of-the-art ASR systems currently use DNNs to map a (short) temporal context of features derivedfrom the waveform to a probability distribution over the discrete states of a Hidden Markov Model (HMM) [4]. Morespecifically, the DNN produces a probability distribution over clusters of tri-phone states at each time and a decoderthen finds a path through the HMM states that is the best compromise between using high probability states and producinga transcription that is probable under the language model.", "Although it is possible (and desirable) to train the DNN in such a way that the decoder (and, thus, the language model)is taken into account by marginalizing over all possible paths, it is common to train the DNN to perform frame-by-frameclassification by (locally) minimizing the cross entropy between the predictions made by the net and the labels givenby a forced alignment with the ground truth sequence of states for each observation:\ud835\udf3d=arg\u2061max\ud835\udf3d\u2032\u2061P(ht|\ud835\udc2ct;\ud835\udf3d\u2032)\ud835\udf3dsubscriptsuperscript\ud835\udf3d\u2032\ud835\udc43conditionalsubscript\u210e\ud835\udc61subscript\ud835\udc2c\ud835\udc61superscript\ud835\udf3d\u2032\\boldsymbol{\\theta}=\\arg\\max_{\\boldsymbol{\\theta}^{\\prime}}P(h_{t}|\\mathbf{s}_{t};\\boldsymbol{\\theta}^{\\prime})bold_italic_\u03b8 = roman_arg roman_max start_POSTSUBSCRIPT bold_italic_\u03b8 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_P ( italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_\u03b8 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT )where \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8 are the parameters of our acoustic model P\ud835\udc43Pitalic_Pwhich maps acoustic observations at time t\ud835\udc61titalic_t, \ud835\udc2ctsubscript\ud835\udc2c\ud835\udc61\\mathbf{s}_{t}bold_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, to a probability, P(ht|\ud835\udc2ct;\ud835\udf3d\u2032)\ud835\udc43conditionalsubscript\u210e\ud835\udc61subscript\ud835\udc2c\ud835\udc61superscript\ud835\udf3d\u2032P(h_{t}|\\mathbf{s}_{t};\\boldsymbol{\\theta}^{\\prime})italic_P ( italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_\u03b8 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) , of the \u201ccorrect\u201dHMM state htsubscript\u210e\ud835\udc61h_{t}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, which is determined by a forced alignment with the correct sequence of words. The model is trained witha distributed stochastic gradient descent approach.", "We use an architecture with 8 hidden layers each containing 2560 rectified linear units and a final softmax layer with14,000 labels (HMM targets htsubscript\u210e\ud835\udc61h_{t}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT). The input is 26 frames of 40 Mel-scaled filterbank coefficients with a 10ms advanceper frame and we predict the HMM state of 21st\ud835\udc60\ud835\udc61{}^{st}start_FLOATSUPERSCRIPT italic_s italic_t end_FLOATSUPERSCRIPT frame. The total number of parameters is about 85M. This is aslightly outdated version of the acoustic model used by Android voice search, and should be considered as a very strongbaseline. To train the DNN acoustic model we use about 2000 hours of spoken English data, which yields about 700Mtraining examples. This system achieves a frame accuracy of 58.9%, and a Word Error Rate (WER) of 10.9% on our development set.", "We trained 10 separate models to predict P(ht|\ud835\udc2ct;\ud835\udf3d)\ud835\udc43conditionalsubscript\u210e\ud835\udc61subscript\ud835\udc2c\ud835\udc61\ud835\udf3dP(h_{t}|\\mathbf{s}_{t};\\boldsymbol{\\theta})italic_P ( italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_\u03b8 ), using exactly the same architecture and trainingprocedure as the baseline. The models are randomly initialized with different initial parameter values and we find thatthis creates sufficient diversity in the trained models to allow the averaged predictions of the ensemble tosignificantly outperform the individual models. We have explored adding diversity to the models by varying the sets ofdata that each model sees, but we found this to not significantly change our results, so we opted for the simplerapproach. For the distillation we tried temperatures of [1,\ud835\udfd0,5,10]12510[1,{\\bf 2},5,10][ 1 , bold_2 , 5 , 10 ] and used a relative weight of 0.5 on thecross-entropy for the hard targets, where bold font indicates the best value that was used fortable\u00a01 .", "Table\u00a01 shows that, indeed, our distillation approach is able to extract more useful informationfrom the training set than simply using the hard labels to train a single model. More than 80% of the improvement inframe classification accuracy achieved by using an ensemble of 10 models is transferred to the distilled model which issimilar to the improvement we observed in our preliminary experiments on MNIST. The ensemble gives a smaller improvementon the ultimate objective of WER (on a 23K-word test set) due to the mismatch in the objective function, but again, theimprovement in WER achieved by the ensemble is transferred to the distilled model.", "We have recently become aware of related work on learning a small acoustic model by matching the class probabilities ofan already trained larger model [8]. However, they do the distillation at a temperature of 1 using alarge unlabeled dataset and their best distilled model only reduces the error rate of the small model by 28% of thegap between the error rates of the large and small models when they are both trained with hard labels.", "Training an ensemble of models is a very simple way to take advantageof parallel computation and the usual objection that an ensemblerequires too much computation at test time can be dealt with by usingdistillation. There is, however, another important objection toensembles: If the individual models are large neural networks and thedataset is very large, the amount of computation required at trainingtime is excessive, even though it is easy to parallelize.", "In this section we give an example of such a dataset and we show howlearning specialist models that each focus on a different confusablesubset of the classes can reduce the total amount of computationrequired to learn an ensemble. The main problem with specialists thatfocus on making fine-grained distinctions is that they overfit veryeasily and we describe how this overfitting may be prevented by usingsoft targets.", "JFT is an internal Google dataset that has 100 million labeled imageswith 15,000 labels. When we did this work, Google\u2019s baseline model forJFT was a deep convolutional neural network [7] that had been trained forabout six months using asynchronous stochastic gradient descent on alarge number of cores. This training used two types ofparallelism [2]. First, there were many replicas of the neural net runningon different sets of cores and processing different mini-batches fromthe training set. Each replica computes the average gradient on itscurrent mini-batch and sends this gradient to a sharded parameter server whichsends back new values for the parameters. These new values reflect allof the gradients received by the parameter server since the last timeit sent parameters to the replica. Second, each replica is spread overmultiple cores by putting different subsets of the neurons on eachcore. Ensemble training is yet a third type of parallelism that can bewrapped around the other two types, but only if a lot more cores areavailable. Waiting for several years to train an ensemble of models wasnot an option, so we needed a much faster way to improve the baselinemodel.", "When the number of classes is very large, it makes sense for thecumbersome model to be an ensemble that contains one generalistmodel trained on all the data and many \u201cspecialist\u201dmodels, each of which is trained on data that is highly enriched inexamples from a very confusable subset of the classes (like differenttypes of mushroom). The softmax of this type of specialist can be mademuch smaller by combining all of the classes it does not care about into asingle dustbin class.", "To reduce overfitting and share the work of learning lower level feature detectors, each specialist model is initializedwith the weights of the generalist model. These weights are then slightly modified by training thespecialist with half its examples coming from its special subset and half sampled at random from the remainder of thetraining set. After training, we can correct for the biased training set by incrementing the logitof the dustbin class by the log of the proportion by which the specialist class is oversampled.", "In order to derive groupings of object categories for the specialists, we decided to focus on categories that our fullnetwork often confuses. Even though we could have computed the confusion matrix and used it as a way to find suchclusters, we opted for a simpler approach that does not require the true labels to construct the clusters.", "In particular, we apply a clustering algorithm to the covariance matrix of the predictions of our generalist model, so thata set of classes Smsuperscript\ud835\udc46\ud835\udc5aS^{m}italic_S start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT that are often predicted together will be used as targets for one of our specialist models, m\ud835\udc5amitalic_m. We applied an on-lineversion of the K-means algorithm to the columns of the covariance matrix, and obtained reasonable clusters (shown inTable 2). We tried several clustering algorithms which produced similar results.", "Before investigating what happens when specialist models aredistilled, we wanted to see how well ensembles containing specialistsperformed. In addition to the specialist models, we always have ageneralist model so that we can deal with classes for which wehave no specialists and so that we can decide which specialists touse. Given an input image \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, we do top-one classification in two steps:", "Step 1: For each test case, we findthe n\ud835\udc5bnitalic_n most probable classes according to the generalist model. Call this set of classes k\ud835\udc58kitalic_k. In ourexperiments, we used n=1\ud835\udc5b1n=1italic_n = 1.", "Step 2: We then take all the specialist models, m\ud835\udc5amitalic_m, whose specialsubset of confusable classes,Smsuperscript\ud835\udc46\ud835\udc5aS^{m}italic_S start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT, has a non-empty intersection with k\ud835\udc58kitalic_k and call this the activeset of specialists Aksubscript\ud835\udc34\ud835\udc58A_{k}italic_A start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (note that this set may be empty). We thenfind the full probability distribution \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q over all the classesthat minimizes:KL(\ud835\udc29g,\ud835\udc2a)+\u2211m\u2208AkKL(\ud835\udc29m,\ud835\udc2a)\ud835\udc3e\ud835\udc3fsuperscript\ud835\udc29\ud835\udc54\ud835\udc2asubscript\ud835\udc5asubscript\ud835\udc34\ud835\udc58\ud835\udc3e\ud835\udc3fsuperscript\ud835\udc29\ud835\udc5a\ud835\udc2aKL(\\mathbf{p}^{g},\\mathbf{q})+\\sum_{m\\in A_{k}}KL(\\mathbf{p}^{m},\\mathbf{q})italic_K italic_L ( bold_p start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT , bold_q ) + \u2211 start_POSTSUBSCRIPT italic_m \u2208 italic_A start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_K italic_L ( bold_p start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , bold_q )(5)where KL\ud835\udc3e\ud835\udc3fKLitalic_K italic_L denotes the KL divergence, and \ud835\udc29msuperscript\ud835\udc29\ud835\udc5a\\mathbf{p}^{m}bold_p start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT \ud835\udc29gsuperscript\ud835\udc29\ud835\udc54\\mathbf{p}^{g}bold_p start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT denote theprobability distribution of a specialist model or the generalist fullmodel. The distribution \ud835\udc29msuperscript\ud835\udc29\ud835\udc5a\\mathbf{p}^{m}bold_p start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT is a distribution over all thespecialist classes of m\ud835\udc5amitalic_m plus a single dustbin class, so whencomputing its KL divergence from the full \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q distribution we sumall of the probabilities that the full \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q distribution assigns toall the classes in m\ud835\udc5amitalic_m\u2019s dustbin.", "Eq.\u00a05 does not have a general closed form solution, though when all the models produce a single probability foreach class the solution is either the arithmetic or geometric mean, depending on whether we use KL(\ud835\udc29,\ud835\udc2a)\ud835\udc3e\ud835\udc3f\ud835\udc29\ud835\udc2aKL(\\mathbf{p},\\mathbf{q})italic_K italic_L ( bold_p , bold_q )or KL(\ud835\udc2a,\ud835\udc29)\ud835\udc3e\ud835\udc3f\ud835\udc2a\ud835\udc29KL(\\mathbf{q},\\mathbf{p})italic_K italic_L ( bold_q , bold_p )). We parameterize \ud835\udc2a=softmax(\ud835\udc33)\ud835\udc2a\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc33\\mathbf{q}=softmax(\\mathbf{z})bold_q = italic_s italic_o italic_f italic_t italic_m italic_a italic_x ( bold_z ) (with T=1\ud835\udc471T=1italic_T = 1) and we use gradient descent to optimize the logits \ud835\udc33\ud835\udc33\\mathbf{z}bold_z w.r.t. eq.\u00a05. Note that thisoptimization must be carried out for each image.", "Starting from the trained baseline full network,the specialists train extremely fast (a few days instead of many weeks for JFT). Also, all the specialistsare trained completely independently. Table \u00a03 shows the absolute test accuracy for thebaseline system and the baseline system combined with the specialistmodels. With 61 specialist models, there is a4.4% relative improvement in test accuracy overall. We also report conditional test accuracy, which is the accuracy by only considering examples belonging to the specialist classes, and restricting our predictions to that subset of classes.", "For our JFT specialist experiments, we trained 61 specialist models, each with 300 classes (plus the dustbin class).Because the sets of classes for the specialists are not disjoint, we often had multiple specialists covering aparticular image class. Table \u00a04 shows the number of test set examples, the change inthe number of examples correct at position 1 when using the specialist(s), and the relative percentage improvement intop1 accuracy for the JFT dataset broken down by the number of specialists covering the class. We are encouraged by thegeneral trend that accuracy improvements are larger when we have more specialists covering a particular class, sincetraining independent specialist models is very easy to parallelize.", "One of our main claims about using soft targets instead of hard targets is that a lot of helpful information can becarried in soft targets that could not possibly be encoded with a single hard target. In this section we demonstratethat this is a very large effect by using far less data to fit the 85Mparameters of the baseline speechmodel described earlier. Table\u00a05 shows that with only 3% of the data (about 20M examples), training the baseline model withhard targets leads to severe overfitting (we did early stopping, as the accuracy drops sharply after reaching 44.5%),whereas the same model trained with soft targets is able to recover almost all the information in the full training set(about 2% shy). It is even more remarkable to note that we did not have to do early stopping: the system with softtargets simply \u201cconverged\u201d to 57%. This shows that soft targets are a very effective way of communicating theregularities discovered by a model trained on all of the data to another model.", "The specialists that we used in our experiments on the JFT datasetcollapsed all of their non-specialist classes into a single dustbinclass. If we allow specialists to have a full softmax over allclasses, there may be a much better way to prevent them overfitting than usingearly stopping. A specialist is trained on data that is highlyenriched in its special classes. This means that the effective sizeof its training set is much smaller and it has a strong tendency tooverfit on its special classes. This problem cannot be solved bymaking the specialist a lot smaller because then we lose the veryhelpful transfer effects we get from modeling all of thenon-specialist classes.", "Our experiment using 3% of the speech data strongly suggests that ifa specialist is initialized with the weights of the generalist, we canmake it retain nearly all of its knowledge about the non-specialclasses by training it with soft targets for the non-special classesin addition to training it with hard targets. The soft targets canbe provided by the generalist. We are currently exploring thisapproach.", "The use of specialists that are trained on subsets of the data hassome resemblance to mixtures of experts [6] which use agating network to compute the probability of assigning each example toeach expert. At the same time as the experts are learning to deal withthe examples assigned to them, the gating network is learning tochoose which experts to assign each example to based on the relativediscriminative performance of the experts for that example. Using the discriminativeperformance of the experts to determine the learned assignments is muchbetter than simply clustering the input vectors and assigning anexpert to each cluster, but it makes the training hard to parallelize: First, theweighted training set for each expert keeps changing in a way thatdepends on all the other experts and second, the gating network needsto compare the performance of different experts on the same example toknow how to revise its assignment probabilities. These difficultieshave meant that mixtures of experts are rarely used in the regimewhere they might be most beneficial: tasks with huge datasets thatcontain distinctly different subsets.", "It is much easier to parallelize the training of multiple specialists.We first train a generalist model and then use the confusion matrix todefine the subsets that the specialists are trained on. Once thesesubsets have been defined the specialists can be trained entirelyindependently. At test time we can use the predictions from thegeneralist model to decide which specialists are relevant and onlythese specialists need to be run.", "We have shown that distilling works very well for transferringknowledge from an ensemble or from a large highly regularized modelinto a smaller, distilled model. On MNIST distillation works remarkablywell even when the transfer set that is used to train the distilledmodel lacks any examples of one or more of the classes. For a deepacoustic model that is version of the one used by Android voice search, we have shownthat nearly all of the improvement that is achieved by training anensemble of deep neural nets can be distilled into a singleneural net of the same size which is far easier to deploy.", "For really big neural networks, it can be infeasible even to train afull ensemble, but we have shown that the performance of a single really bignet that has been trained for a very long time can be significantlyimproved by learning a large number of specialist nets, each of whichlearns to discriminate between the classes in a highly confusablecluster. We have not yet shown that we can distill the knowledge inthe specialists back into the single large net.", "We thank Yangqing Jia for assistance with training models on ImageNet and IlyaSutskever and Yoram Singer for helpful discussions."], "figure_types": {"0c908739fbff75f03469d13d4a1a07de3414ee19/5-Table1-1.png": "table", "0c908739fbff75f03469d13d4a1a07de3414ee19/6-Table2-1.png": "table", "0c908739fbff75f03469d13d4a1a07de3414ee19/7-Table3-1.png": "table", "0c908739fbff75f03469d13d4a1a07de3414ee19/7-Table4-1.png": "table", "0c908739fbff75f03469d13d4a1a07de3414ee19/8-Table5-1.png": "table"}}, "1506.07503": {"paper_id": "paper_56", "title": "Attention-Based Models for Speech Recognition", "arxiv_url": "https://arxiv.org/abs/1506.07503", "s2orc_url": "https://www.semanticscholar.org/paper/b624504240fa52ab76167acfe3156150ca01cf3b", "all_figures_tables": {"b624504240fa52ab76167acfe3156150ca01cf3b/3-Figure1-1.png": "Figure 1: Two steps of the proposed attention-based recurrent sequence generator (ARSG) with a hybrid attention mechanism (computing \u03b1), based on both content (h) and location (previous \u03b1) information. The dotted lines correspond to Eq. (1), thick solid lines to Eq. (2) and dashed lines to Eqs. (3)\u2013(4).", "b624504240fa52ab76167acfe3156150ca01cf3b/5-Figure2-1.png": "Figure 2: Decoding performance w.r.t. the beam size. For rigorous comparison, if decoding failed to generate \u3008eos\u3009, we considered it wrongly recognized without retrying with a larger beams size. The models, especially with smooth focus, perform well even with a beam width as small as 1.", "b624504240fa52ab76167acfe3156150ca01cf3b/6-Figure3-1.png": "Figure 3: Alignments produced by the baseline model. The vertical bars indicate ground truth phone location from TIMIT. Each row of the upper image indicates frames selected by the attention mechanism to emit a phone symbol. The network has clearly learned to produce a left-to-right alignment with a tendency to look slightly ahead, and does not confuse between the repeated \u201ckclk\u201d phrase. Best viewed in color.", "b624504240fa52ab76167acfe3156150ca01cf3b/7-Figure4-1.png": "Figure 4: Results of force-aligning the concatenated utterances. Each dot represents a single utterance created by either concatenating multiple copies of the same utterance, or of different, randomly chosen utterances. We clearly see that the highest robustness is achieved when the hybrid attention mechanism is combined with the proposed sharpening technique (see the bottom-right plot.)", "b624504240fa52ab76167acfe3156150ca01cf3b/8-Figure5-1.png": "Figure 5: Phoneme error rates obtained on decoding long sequences. Each network was decoded with alignment sharpening techniques that produced proper forced alignments. The proposed ARSG\u2019s are clearly more robust to the length of the utterances than the baseline one is."}, "referred_figures_tables": [["b624504240fa52ab76167acfe3156150ca01cf3b/5-Figure2-1.png"]], "question_id": [12], "question": ["What is CTC-training?"], "question_section": ["1 Introduction"], "question_trigger_sentence": ["Excellent results by an HMM-less recognizer have recently been reported, with the\nsystem consisting of a CTC-trained neural network and a language model"], "question_type": ["Testing question"], "evidential_info": [[{"context": "Speech recognizers based on the connectionist temporal classification (CTC,[13]) and its extension, RNNTransducer\u00a0[14], are the closest to the ARSG modelconsidered in this paper. They follow earlier work on end-to-end trainable deeplearning over sequences with gradient signals flowing through the alignmentprocess\u00a0[15]. They have been shown to perform well on thephoneme recognition task\u00a0[16]. Furthermore, the CTC wasrecently found to be able to directly transcribe text from speech without anyintermediate phonetic representation\u00a0[17].", "rationale": "The proposed model is close to the existing speech recognizers based on CTC (connectionist temporal classification) [13] and its extension (RNN Transducer) [14]. These models are based on end-to-end trainable deep learning over sequences with gradient signals flowing through the alignment process."}, {"context": "The considered ARSG is different from both the CTC and RNN Transducer in twoways. First, whereas the attention mechanism deterministically aligns the inputand the output sequences, the CTC and RNN Transducer treat the alignment as alatent random variable over which MAP (maximum a posteriori) inference isperformed. This deterministic nature of the ARSG\u2019s alignment mechanism allowsbeam search procedure to be simpler. Furthermore, we empirically observe that amuch smaller beam width can be used with the deterministic mechanism, whichallows faster decoding (see Sec.\u00a04.2 andFig.\u00a02).Second, the alignment mechanism of both the CTC and RNN Transducer isconstrained to be \u201cmonotonic\u201d to keep marginalization of the alignmenttractable. On the other hand, the proposed attention mechanism can result innon-monotonic alignment, which makes it suitable for a larger variety of tasksother than speech recognition.", "rationale": "The CTC and RNN Transducer performs MAP inference, treating the alignment as a latent random variable. Also, their alignment mechanism is constrained to be monotonic to keep marginalization of the alignment tractable."}]], "composition": ["According to the related work section in the paper, CTC training is the deep-learning-based speech recognization model which performs MAP inference on the alignment as a latent random variable. There is no detailed information on how CTC training works in this paper and presumed to exist in [13]."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["1103"], "passages": ["Recently, attention-based recurrent networks have been successfully applied to awide variety of tasks, such as handwritingsynthesis\u00a0[1], machinetranslation\u00a0[2], image captiongeneration\u00a0[3] and visual objectclassification\u00a0[4].111An early version of this work was presented at the NIPS 2014 Deep LearningWorkshop [5].Such models iteratively process their input by selecting relevant content atevery step. This basic idea significantly extends the applicability range ofend-to-end training methods, for instance, making it possible to constructnetworks with external memory\u00a0[6, 7].", "We introduce extensions to attention-based recurrent networksthat make them applicable tospeech recognition. Learning to recognize speech can be viewed aslearning to generate a sequence (transcription) given another sequence (speech).From this perspective it is similar to machine translation and handwritingsynthesis tasks, for which attention-based methods have been found suitable[2, 1]. However, compared tomachine translation, speech recognition principally differs byrequesting much longer inputsequences (thousands of frames instead of dozens of words), which introduces achallenge of distinguishing similar speech fragments222Explained inmore detail in Sec.\u00a02.1. in a single utterance.It isalso different from handwriting synthesis, since the input sequence is muchnoisier and does not have as clear structure. For these reasons speechrecognition is an interesting testbed for developing new attention-basedarchitectures capable of processing long and noisy inputs.", "Application of attention-based models to speech recognition is also animportant step toward building fully end-to-end trainable speechrecognition systems, which is an active area of research. The dominantapproach is still basedon hybrid systems consisting of a deep neural acoustic model, a triphone HMMmodel and an n-gram languagemodel\u00a0[8, 9]. This requires dictionariesof hand-crafted pronunciation and phoneme lexicons, and a multi-stage trainingprocedure to make the components work together. Excellent results by an HMM-lessrecognizer have recently been reported, with the system consisting of aCTC-trained neural network and a language model\u00a0[10].Still, the language model was added only at the last stage in that work, thusleaving open a question of how much an acoustic model can benefit from beingaware of a language model during training.", "In this paper, we evaluate attention-based models on aphoneme recognition task using the widely-used TIMITdataset. At each time step in generating an output sequence (phonemes),an attention mechanism selects or weighs the signals producedby a trained feature extraction mechanism at potentially all of the time stepsin the input sequence (speech frames). The weighted feature vector thenhelps to condition the generation of the next element of the output sequence.Since the utterances in this dataset are rathershort (mostly under 5 seconds), we measure theability of the considered models in recognizing much longerutterances which were created by artificially concatenatingthe existing utterances.", "We start with a model proposed in[2] for the machine translation taskas the baseline. This model seems entirely vulnerable tothe issue of similar speech fragmentsbut despite ourexpectations it was competitive on the original test set, reaching 18.7% phonemeerror rate (PER). However, its performancedegraded quickly with longer, concatenated utterances. Weprovide evidence that this model adapted to track the absolutelocation in the input sequence of the content it isrecognizing, a strategy feasible for short utterances from theoriginal test set but inherently unscalable.", "In order to circumvent this undesired behavior, in thispaper, we propose to modify the attention mechanism suchthat it explicitly takes into account both (a) the location of thefocus from the previous step, as in\u00a0[6] and (b)the features of the input sequence, as in\u00a0[2].This is achieved by adding as inputs to the attention mechanismauxiliary convolutional features which are extractedby convolving the attentionweights from the previous step with trainable filters. We show that a modelwith such convolutional features performs significantlybetter on the considered task (18.0% PER). Moreimportantly, the model with convolutional features robustlyrecognized utterances many times longer than the onesfrom the training set, always staying below 20% PER.", "Therefore, the contribution of this work is three-fold. For one, we present anovel purely neural speech recognition architecture based on an attentionmechanism, whose performance is comparable to that of the conventionalapproaches on the TIMIT dataset. Moreover, we propose a generic method ofadding location awareness to the attention mechanism. Finally, we introduce amodification of the attention mechanism to avoid concentrating the attention ona single frame, and thus avoid obtaining less \u201ceffective training examples\u201d,bringing the PER down to 17.6%.", "An attention-based recurrent sequence generator (ARSG) is a recurrent neuralnetwork that stochastically generates an output sequence (y1,\u2026,yT)subscript\ud835\udc661\u2026subscript\ud835\udc66\ud835\udc47(y_{1},\\dots,y_{T})( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_y start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT )from an input x\ud835\udc65xitalic_x. In practice, x\ud835\udc65xitalic_x is often processed by an encoderwhich outputs a sequential input representation h=(h1,\u2026,hL)\u210esubscript\u210e1\u2026subscript\u210e\ud835\udc3fh=(h_{1},\\ldots,h_{L})italic_h = ( italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_h start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ) moresuitable for the attention mechanism to work with.", "In the context of this work, the output y\ud835\udc66yitalic_y is a sequence of phonemes, and theinput x=(x1,\u2026,xL\u2032)\ud835\udc65subscript\ud835\udc651\u2026subscript\ud835\udc65superscript\ud835\udc3f\u2032x=(x_{1},\\ldots,x_{L^{\\prime}})italic_x = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) is a sequence of feature vectors. Each featurevector is extracted from a small overlapping window of audio frames. The encoderis implemented as a deep bidirectional recurrent network (BiRNN), to form asequential representation h\u210ehitalic_h of length L=L\u2032\ud835\udc3fsuperscript\ud835\udc3f\u2032L=L^{\\prime}italic_L = italic_L start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.", "At the i\ud835\udc56iitalic_i-th step an ARSG generates an output yisubscript\ud835\udc66\ud835\udc56y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT by focusing on the relevantelements of h\u210ehitalic_h:\u03b1i=Attend(si\u22121,\u03b1i\u22121,h)subscript\ud835\udefc\ud835\udc56\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc51subscript\ud835\udc60\ud835\udc561subscript\ud835\udefc\ud835\udc561\u210e\\displaystyle\\alpha_{i}=Attend(s_{i-1},\\alpha_{i-1},h)italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_A italic_t italic_t italic_e italic_n italic_d ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_\u03b1 start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_h )(1)gi=\u2211j=1L\u03b1i,jhjsubscript\ud835\udc54\ud835\udc56superscriptsubscript\ud835\udc571\ud835\udc3fsubscript\ud835\udefc\ud835\udc56\ud835\udc57subscript\u210e\ud835\udc57\\displaystyle g_{i}=\\sum\\limits_{j=1}^{L}\\alpha_{i,j}h_{j}italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT(2)yi\u223cGenerate(si\u22121,gi),similar-tosubscript\ud835\udc66\ud835\udc56\ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52subscript\ud835\udc60\ud835\udc561subscript\ud835\udc54\ud835\udc56\\displaystyle y_{i}\\sim Generate(s_{i-1},g_{i}),italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u223c italic_G italic_e italic_n italic_e italic_r italic_a italic_t italic_e ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,(3)where si\u22121subscript\ud835\udc60\ud835\udc561s_{i-1}italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT is the (i\u22121)\ud835\udc561(i-1)( italic_i - 1 )-th state of the recurrent neural network to whichwe refer as the generator, \u03b1i\u2208\u211dLsubscript\ud835\udefc\ud835\udc56superscript\u211d\ud835\udc3f\\alpha_{i}\\in\\mathbb{R}^{L}italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT is a vector of theattention weights, also often called thealignment\u00a0[2]. Using the terminology from[4], we call gisubscript\ud835\udc54\ud835\udc56g_{i}italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT a glimpse. The step is completed bycomputing a new generator state:si=Recurrency(si\u22121,gi,yi)subscript\ud835\udc60\ud835\udc56\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66subscript\ud835\udc60\ud835\udc561subscript\ud835\udc54\ud835\udc56subscript\ud835\udc66\ud835\udc56\\displaystyle s_{i}=Recurrency(s_{i-1},g_{i},y_{i})italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_R italic_e italic_c italic_u italic_r italic_r italic_e italic_n italic_c italic_y ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )(4)Long short-term memory units (LSTM, [11]) and gated recurrentunits (GRU, [12]) are typically used as a recurrent activation, towhich we refer as a recurrency. The process is graphically illustrated inFig.\u00a01.", "Inspired by [6] we distinguish between location-based,content-based and hybrid attention mechanisms. Attend\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc51Attenditalic_A italic_t italic_t italic_e italic_n italic_d inEq.\u00a0(1) describes the most generic, hybrid attention. If theterm \u03b1i\u22121subscript\ud835\udefc\ud835\udc561\\alpha_{i-1}italic_\u03b1 start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT is dropped from Attend\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc51Attenditalic_A italic_t italic_t italic_e italic_n italic_d arguments, i.e.,\u03b1i=Attend(si\u22121,h)subscript\ud835\udefc\ud835\udc56\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc51subscript\ud835\udc60\ud835\udc561\u210e\\alpha_{i}=Attend(s_{i-1},h)italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_A italic_t italic_t italic_e italic_n italic_d ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_h ),we call it content-based (see, e.g., [2] or[3]). In this case, Attend\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc51Attenditalic_A italic_t italic_t italic_e italic_n italic_d is often implemented by scoringeach element in h\u210ehitalic_h separately and normalizing the scores:ei,j=Score(si\u22121,hj),subscript\ud835\udc52\ud835\udc56\ud835\udc57\ud835\udc46\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52subscript\ud835\udc60\ud835\udc561subscript\u210e\ud835\udc57\\displaystyle e_{i,j}=Score(s_{i-1},h_{j}),italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = italic_S italic_c italic_o italic_r italic_e ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ,(5)\u03b1i,j=exp\u2061(ei,j)/\u2211j=1Lexp\u2061(ei,j).subscript\ud835\udefc\ud835\udc56\ud835\udc57/subscript\ud835\udc52\ud835\udc56\ud835\udc57superscriptsubscript\ud835\udc571\ud835\udc3fsubscript\ud835\udc52\ud835\udc56\ud835\udc57\\displaystyle\\alpha_{i,j}=\\exp(e_{i,j})\\left/\\sum\\limits_{j=1}^{L}\\exp(e_{i,j})\\right..italic_\u03b1 start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = roman_exp ( italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ) / \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT roman_exp ( italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ) .(6)", "The main limitation of such scheme is that identical or very similar elements ofh\u210ehitalic_h are scored equally regardless of their position in thesequence. This is the issue of \u201csimilar speech fragments\u201d raised above.Often this issue is partially alleviated by an encoder suchas e.g. a BiRNN\u00a0[2] or a deep convolutionalnetwork\u00a0[3] that encode contextualinformation into every element of h\u210ehitalic_h . However, capacity ofh\u210ehitalic_h elements is always limited, and thus disambiguation bycontext is only possible to a limited extent.", "Alternatively, a location-based attention mechanism computes the alignment fromthe generator state and the previous alignment only such that\u03b1i=Attend(si\u22121,\u03b1i\u22121)subscript\ud835\udefc\ud835\udc56\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc51subscript\ud835\udc60\ud835\udc561subscript\ud835\udefc\ud835\udc561\\alpha_{i}=Attend(s_{i-1},\\alpha_{i-1})italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_A italic_t italic_t italic_e italic_n italic_d ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_\u03b1 start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ).For instance, Graves [1] used the location-basedattention mechanism using a Gaussian mixture model in his handwriting synthesismodel. In the case of speech recognition, this type of location-based attentionmechanism would have to predict the distance between consequentphonemes using si\u22121subscript\ud835\udc60\ud835\udc561s_{i-1}italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT only, which we expect to be hard due tolarge variance of this quantity.", "For these limitations associated with both content-based and location-basedmechanisms, we argue that a hybrid attention mechanism is a natural candidatefor speech recognition. Informally, we would like an attention model that usesthe previous alignment \u03b1i\u22121subscript\ud835\udefc\ud835\udc561\\alpha_{i-1}italic_\u03b1 start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT to select a short list of elements fromh\u210ehitalic_h, from which the content-based attention, inEqs.\u00a0(5)\u2013(6), will selectthe relevant ones without confusion.", "We start from the ARSG-based model with the content-based attention mechanismproposed in [2]. This model can be described byEqs.\u00a0(5)\u2013(6), whereei,j=w\u22a4tanh\u2061(Wsi\u22121+Vhj+b).subscript\ud835\udc52\ud835\udc56\ud835\udc57superscript\ud835\udc64top\ud835\udc4asubscript\ud835\udc60\ud835\udc561\ud835\udc49subscript\u210e\ud835\udc57\ud835\udc4f\\displaystyle e_{i,j}=w^{\\top}\\tanh(Ws_{i-1}+Vh_{j}+b).italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = italic_w start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT roman_tanh ( italic_W italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT + italic_V italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + italic_b ) .(7)w\ud835\udc64witalic_w and b\ud835\udc4fbitalic_b are vectors, W\ud835\udc4aWitalic_W and V\ud835\udc49Vitalic_V are matrices.", "We extend this content-based attention mechanism of the original model to belocation-aware by making it take into account the alignment produced at theprevious step. First, we extract k\ud835\udc58kitalic_k vectors fi,j\u2208\u211dksubscript\ud835\udc53\ud835\udc56\ud835\udc57superscript\u211d\ud835\udc58f_{i,j}\\in\\mathbb{R}^{k}italic_f start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT forevery position j\ud835\udc57jitalic_j of the previous alignment \u03b1i\u22121subscript\ud835\udefc\ud835\udc561\\alpha_{i-1}italic_\u03b1 start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT by convolving itwith a matrix F\u2208\u211dk\u00d7r\ud835\udc39superscript\u211d\ud835\udc58\ud835\udc5fF\\in\\mathbb{R}^{k\\times r}italic_F \u2208 blackboard_R start_POSTSUPERSCRIPT italic_k \u00d7 italic_r end_POSTSUPERSCRIPT:fi=F*\u03b1i\u22121.subscript\ud835\udc53\ud835\udc56\ud835\udc39subscript\ud835\udefc\ud835\udc561\\displaystyle f_{i}=F*\\alpha_{i-1}.italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_F * italic_\u03b1 start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT .(8)These additional vectors fi,jsubscript\ud835\udc53\ud835\udc56\ud835\udc57f_{i,j}italic_f start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT are then used by the scoring mechanism ei,jsubscript\ud835\udc52\ud835\udc56\ud835\udc57e_{i,j}italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT:ei,j=w\u22a4tanh\u2061(Wsi\u22121+Vhj+Ufi,j+b)subscript\ud835\udc52\ud835\udc56\ud835\udc57superscript\ud835\udc64top\ud835\udc4asubscript\ud835\udc60\ud835\udc561\ud835\udc49subscript\u210e\ud835\udc57\ud835\udc48subscript\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udc4f\\displaystyle e_{i,j}=w^{\\top}\\tanh(Ws_{i-1}+Vh_{j}+Uf_{i,j}+b)italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = italic_w start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT roman_tanh ( italic_W italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT + italic_V italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + italic_U italic_f start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT + italic_b )(9)", "There are three potential issues with the normalization inEq.\u00a0(6).", "First, when the input sequence h\u210ehitalic_h is long, the glimpse gisubscript\ud835\udc54\ud835\udc56g_{i}italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is likely tocontain noisy information from many irrelevant feature vectors hjsubscript\u210e\ud835\udc57h_{j}italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, as thenormalized scores \u03b1i,jsubscript\ud835\udefc\ud835\udc56\ud835\udc57\\alpha_{i,j}italic_\u03b1 start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT are all positive and sum to 1111. This makes itdifficult for the proposed ARSG to focus clearly on a few relevant frames ateach time i\ud835\udc56iitalic_i. Second, the attention mechanism is required to consider all theL\ud835\udc3fLitalic_L frames each time it decodes a single output yisubscript\ud835\udc66\ud835\udc56y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT while decoding the outputof length T\ud835\udc47Titalic_T, leading to a computational complexity of O(LT)\ud835\udc42\ud835\udc3f\ud835\udc47O(LT)italic_O ( italic_L italic_T ). This mayeasily become prohibitively expensive, when input utterances are long(and issue that is less serious for machine translation, because in thatcase the input sequence is made of words, not of 20ms acoustic frames).", "The other side of the coin is that the use of softmax normalization inEq.\u00a0(6) prefers to mostly focus on only a singlefeature vector hjsubscript\u210e\ud835\udc57h_{j}italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. This prevents the model from aggregating multipletop-scored frames to form a glimpse gisubscript\ud835\udc54\ud835\udc56g_{i}italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.", "There is a straightforward way to address the first issue of a noisy glimpse by\u201csharpening\u201d the scores \u03b1i,jsubscript\ud835\udefc\ud835\udc56\ud835\udc57\\alpha_{i,j}italic_\u03b1 start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT. One way to sharpen the weights is tointroduce an inverse temperature \u03b2>1\ud835\udefd1\\beta>1italic_\u03b2 > 1 to the softmax function suchthatai,j=exp\u2061(\u03b2ei,j)/\u2211j=1Lexp\u2061(\u03b2ei,j),subscript\ud835\udc4e\ud835\udc56\ud835\udc57/\ud835\udefdsubscript\ud835\udc52\ud835\udc56\ud835\udc57superscriptsubscript\ud835\udc571\ud835\udc3f\ud835\udefdsubscript\ud835\udc52\ud835\udc56\ud835\udc57a_{i,j}=\\exp(\\beta e_{i,j})\\left/\\sum_{j=1}^{L}\\exp(\\beta e_{i,j})\\right.,italic_a start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = roman_exp ( italic_\u03b2 italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ) / \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT roman_exp ( italic_\u03b2 italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ) ,or to keep only the top-k\ud835\udc58kitalic_k frames according to the scores and re-normalizethem. These sharpening methods, however, still requires us to compute the scoreof every frame each time (O(LT)\ud835\udc42\ud835\udc3f\ud835\udc47O(LT)italic_O ( italic_L italic_T )), and they worsen the second issue, of overly narrowfocus.", "We also propose and investigate a windowing technique.At each time i\ud835\udc56iitalic_i, the attention mechanism considers only a subsequenceh~=(hpi\u2212w,\u2026,hpi+w\u22121)~\u210esubscript\u210esubscript\ud835\udc5d\ud835\udc56\ud835\udc64\u2026subscript\u210esubscript\ud835\udc5d\ud835\udc56\ud835\udc641\\tilde{h}=(h_{p_{i}-w},\\ldots,h_{p_{i}+w-1})over~ start_ARG italic_h end_ARG = ( italic_h start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_w end_POSTSUBSCRIPT , \u2026 , italic_h start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_w - 1 end_POSTSUBSCRIPT ) of the whole sequence h\u210ehitalic_h, wherew\u226aLmuch-less-than\ud835\udc64\ud835\udc3fw\\ll Litalic_w \u226a italic_L is the predefined window width and pisubscript\ud835\udc5d\ud835\udc56p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the median of the alignment\u03b1i\u22121subscript\ud835\udefc\ud835\udc561\\alpha_{i-1}italic_\u03b1 start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. The scores for hj\u2209h~subscript\u210e\ud835\udc57~\u210eh_{j}\\notin\\tilde{h}italic_h start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u2209 over~ start_ARG italic_h end_ARG are not computed,resulting in a lower complexity of O(L+T)\ud835\udc42\ud835\udc3f\ud835\udc47O(L+T)italic_O ( italic_L + italic_T ).This windowing technique is similar to taking the top-k\ud835\udc58kitalic_k frames, and similarly,has the effect of sharpening.", "The proposed sharpening based on windowing can be used both during training andevaluation. Later, in the experiments, we only consider the case where it isused during evaluation.", "We observed that the proposed sharpening methods indeedhelped with long utterances. However, all of them, andespecially selecting the frame with the highest score,negatively affected the model\u2019s performance on the standarddevelopment set which mostly consists of short utterances.This observations let us hypothesize that it is helpful forthe model to aggregate selections from multiple top-scoredframes. In a sense this brings more diversity, i.e., moreeffective training examples, to the output part of the model,as more input locations are considered.To facilitate this effect, we replace theunbounded exponentialfunction of the softmax function inEq.\u00a0(6) with thebounded logistic sigmoid \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 such thatai,j=\u03c3(ei,j)/\u2211j=1L\u03c3(ei,j).subscript\ud835\udc4e\ud835\udc56\ud835\udc57/\ud835\udf0esubscript\ud835\udc52\ud835\udc56\ud835\udc57superscriptsubscript\ud835\udc571\ud835\udc3f\ud835\udf0esubscript\ud835\udc52\ud835\udc56\ud835\udc57a_{i,j}=\\sigma(e_{i,j})\\left/\\sum_{j=1}^{L}\\sigma(e_{i,j})\\right..italic_a start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = italic_\u03c3 ( italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ) / \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_\u03c3 ( italic_e start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ) .This has the effect of smoothing the focus found by the attentionmechanism.", "Speech recognizers based on the connectionist temporal classification (CTC,[13]) and its extension, RNNTransducer\u00a0[14], are the closest to the ARSG modelconsidered in this paper. They follow earlier work on end-to-end trainable deeplearning over sequences with gradient signals flowing through the alignmentprocess\u00a0[15]. They have been shown to perform well on thephoneme recognition task\u00a0[16]. Furthermore, the CTC wasrecently found to be able to directly transcribe text from speech without anyintermediate phonetic representation\u00a0[17].", "The considered ARSG is different from both the CTC and RNN Transducer in twoways. First, whereas the attention mechanism deterministically aligns the inputand the output sequences, the CTC and RNN Transducer treat the alignment as alatent random variable over which MAP (maximum a posteriori) inference isperformed. This deterministic nature of the ARSG\u2019s alignment mechanism allowsbeam search procedure to be simpler. Furthermore, we empirically observe that amuch smaller beam width can be used with the deterministic mechanism, whichallows faster decoding (see Sec.\u00a04.2 andFig.\u00a02).Second, the alignment mechanism of both the CTC and RNN Transducer isconstrained to be \u201cmonotonic\u201d to keep marginalization of the alignmenttractable. On the other hand, the proposed attention mechanism can result innon-monotonic alignment, which makes it suitable for a larger variety of tasksother than speech recognition.", "A hybrid attention model using a convolution operation was also proposed in[6] for neural Turing machines (NTM). At each time step, the NTMcomputes content-based attention weights which are then convolved with apredicted shifting distribution. Unlike the NTM\u2019s approach, the hybrid mechanismproposed here lets learning figure out how the content-based and location-basedaddressing be combined by a deep, parametric function (seeEq.\u00a0(9).)", "Sukhbaatar et al. [18] describes a similar hybrid attentionmechanism, where location embeddings are used as input to the attention model.This approach has an important disadvantage that the model cannot work with aninput sequence longer than those seen during training. Our approach, on theother hand, works well on sequences many times longer than those seen duringtraining (see Sec.\u00a05.)", "We closely followed the procedure in [16]. All experimentswere performed on the TIMIT corpus [19]. We used the train-dev-testsplit from the Kaldi [20] TIMIT s5 recipe. We trained on thestandard 462 speaker set with all SA utterances removed and used the 50 speakerdev set for early stopping. We tested on the 24 speaker core test set. Allnetworks were trained on 40 mel-scale filter-bank features together with theenergy in each frame, and first and second temporal differences, yielding intotal 123 features per frame. Each feature was rescaled to have zero mean andunit variance over the training set. Networks were trained on the full 61-phoneset extended with an extra \u201cend-of-sequence\u201d token that was appended to eachtarget sequence. Similarly, we appended an all-zero frame at the end of eachinput sequence to indicate the end of the utterance. Decoding was performedusing the 61+1 phoneme set, while scoring was done on the 39 phoneme set.", "One property of ARSG models is that different subsets of parameters are reuseddifferent number of times; L\ud835\udc3fLitalic_L times for those of the encoder, LT\ud835\udc3f\ud835\udc47LTitalic_L italic_T for theattention weights and T\ud835\udc47Titalic_T times for all the other parameters of the ARSG. Thismakes the scales of derivatives w.r.t. parameters vary significantly, and wehandle it by using an adaptive learning rate algorithm,AdaDelta\u00a0[21] which has two hyperparameters \u03f5italic-\u03f5\\epsilonitalic_\u03f5 and \u03c1\ud835\udf0c\\rhoitalic_\u03c1.All the weight matrices were initialized from a normal Gaussian distributionwith its standard deviation set to 0.010.010.010.01. Recurrent weights were furthermoreorthogonalized.", "As TIMIT is a relatively small dataset, proper regularization is crucial. We usedthe adaptive weight noise as a main regularizer\u00a0[22]. We firsttrained our models with a column norm constraint\u00a0[23] with themaximum norm 1111until the lowest development negative log-likelihood is achieved.333Applying the weight noise from the beginning of training caused severeunderfitting.During this time, \u03f5italic-\u03f5\\epsilonitalic_\u03f5 and \u03c1\ud835\udf0c\\rhoitalic_\u03c1 are set to 10\u22128superscript10810^{-8}10 start_POSTSUPERSCRIPT - 8 end_POSTSUPERSCRIPT and 0.950.950.950.95,respectively. At this point, we began using the adaptive weight noise, andscaled down the model complexity cost LCsubscript\ud835\udc3f\ud835\udc36L_{C}italic_L start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT by a factor of 10, while disablingthe column norm constraints. Oncethe new lowest development log-likelihood was reached, we fine-tuned the modelwith a smaller \u03f5=10\u221210italic-\u03f5superscript1010\\epsilon=10^{-10}italic_\u03f5 = 10 start_POSTSUPERSCRIPT - 10 end_POSTSUPERSCRIPT, until we didnot observe the improvement in the development phoneme error rate (PER) for 100Kweight updates. Batch size 1 was used throughout thetraining.", "We evaluated the ARSGs with different attention mechanisms. The encoder was a3-layer BiRNN with 256 GRU units in each direction, and the activations of the512 top-layer units were used as the representation h\u210ehitalic_h. The generator had asingle recurrent layer of 256 GRU units.Generate\ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52Generateitalic_G italic_e italic_n italic_e italic_r italic_a italic_t italic_e in Eq.\u00a0(3) had a hidden layer of 64 maxout units.The initial states of both the encoder and generator were treated as additionalparameters.", "Our baseline model is the one with a purely content-based attention mechanism(See Eqs.\u00a0(5)\u2013(7).) The scoringnetwork in Eq.\u00a0(7) had 512hidden units.The other two models use the convolutional features in Eq.\u00a0(8)with k=10\ud835\udc5810k=10italic_k = 10 and r=201\ud835\udc5f201r=201italic_r = 201. One of them uses the smoothing fromSec.\u00a02.3.", "A left-to-right beam search over phoneme sequenceswas used during decoding [24]. Beam search was stoppedwhen the \u201cend-of-sequence\u201d token \u27e8eos\u27e9delimited-\u27e8\u27e9eos\\left<\\text{eos}\\right>\u27e8 eos \u27e9 was emitted. Westarted with a beam width of 10, increasing it up to 40 when the network failedto produce \u27e8eos\u27e9delimited-\u27e8\u27e9eos\\left<\\text{eos}\\right>\u27e8 eos \u27e9 with the narrower beam. As shown inFig.\u00a02, decoding with a wider beam gives little-to-nonebenefit.", "All the models achieved competitivePERs (see Table\u00a01).With the convolutional features, we see 3.7% relative improvement over thebaseline and further 5.9% with the smoothing.", "To our surprise(see Sec.\u00a02.1.),the baseline model learned to align properly.An alignment produced by the baseline modelon a sequence with repeated phonemes (utterance FDHC0_SX209) is presented inFig.\u00a03 which demonstrates that the baseline model is notconfused by short-range repetitions. We can also seefrom the figure that itprefers to select frames that are near the beginning or even slightly before thephoneme location provided as a part of the dataset. The alignments produced bythe other models were very similar visually.", "The good performance of the baseline model led us to the question of how itdistinguishes between repetitions of similar phoneme sequences and how reliablyit decodes longer sequences with more repetitions. We created two datasets oflong utterances; one by repeating each test utterance, and the other byconcatenating randomly chosen utterances. In both cases, the waveforms werecross-faded with a 0.05s silence inserted as the \u201cpau\u201d phone. We concatenatedup to 15151515 utterances.", "First, we checked the forced alignment with these longer utterances by forcingthe generator to emit the correct phonemes. Each alignment was consideredcorrect if 90% of the alignment weight lies inside the ground-truth phonemewindow extended by 20 frames on each side. Under this definition, allphones but the\u27e8eos\u27e9delimited-\u27e8\u27e9eos\\left<\\text{eos}\\right>\u27e8 eos \u27e9 shown in Fig.\u00a03 are properlyaligned.", "The first column of Fig.\u00a04 shows the number ofcorrectly aligned frames w.r.t. the utterance length (in frames) for some of theconsidered models. One can see that the baseline model was able to decodesequences up to about 120 phones when a single utterance was repeated, and up toabout 150 phones when different utterances were concatenated. Even when itfailed, it correctly aligned about 50 phones. On the other hand, the model withthe hybrid attention mechanism with convolutional features was able to alignsequences up to 200 phones long. However, once it began to fail, the model wasnot able to align almost all phones. The model with the smoothing behavedsimilarly to the one with convolutional features only.", "We examined failed alignments to understand these two different modes offailure. Some of the examples are shown in the Supplementary Materials.", "We found that the baseline model properly aligns about 40 first phones, thenmakes a jump to the end of the recording and cycles over the last 10 phones.This behavior suggests that it learned to track its approximate location in thesource sequence. However, the tracking capability is limited to the lengthsobserved during training. Once the tracker saturates, it jumps to the end of therecording.", "In contrast, when the location-aware network failed it just stopped aligning \u2013no particular frames were selected for each phone. We attribute this behaviorto the issue of noisy glimpse discussed in Sec.\u00a02.3. With along utterance there are manyirrelevant frames negatively affecting the weight assigned to the correctframes. In line with this conjecture, the location-aware network works slightlybetter on the repetition of the same utterance, where all frames are somehowrelevant, than on the concatenation of different utterances, where eachmisaligned frame is irrelevant. ", "To gain more insightwe applied the alignment sharpening schemes described inSec.\u00a02.3. In the remaining columns ofFig.\u00a04, we see that the sharpeningmethods help the location-aware network to find proper alignments, while theyshow little effect on the baseline network.The windowing technique helps both the baseline and location-aware networks,with the location-aware network properly aligning nearly all sequences.", "During visual inspection, we noticed that in the middle of very long utterancesthe baseline model was confused by repetitions of similar content within thewindow, and that such confusions did not happen in the beginning. This supportsour conjecture above.", "We evaluated the models on long sequences. Each model was decoded using thealignment sharpening techniques that helped to obtain proper forced alignments.The results are presented in Fig.\u00a05. The baseline modelfails to decode long utterances, even when a narrow window is used to constrainthe alignments it produces. The two other location-aware networksare able to decode utterances formed by concatenating up to 11 test utterances.Better results were obtained with a wider window, presumably because itresembles more the training conditions when at each step the attention mechanismwas seeing the whole input sequence. With the wide window, both of the networksscored about 20% PER on the long utterances, indicating that the proposedlocation-aware attention mechanism can scale to sequences much longer than thosein the training set with only minor modifications required at the decodingstage.", "We proposed and evaluated a novel end-to-end trainable speech recognitionarchitecture based on a hybrid attention mechanism which combines both contentand location information in order to select the next position in the inputsequence for decoding. One desirable property of the proposed model is that itcan recognize utterances much longer than the ones it was trained on. In thefuture, we expect this model to be used to directly recognize text fromspeech\u00a0[10, 17], in which case it maybecome important to incorporate a monolingual language model to the ARSGarchitecture [26].", "This work has contributed two novel ideas for attention mechanisms: a betternormalization approach yielding smoother alignments and a generic principle forextracting and using features from the previous alignments.Both of these can potentially be applied beyond speech recognition. Forinstance, the proposed attention can be used withoutmodification in neural Turing machines, or by using 2\u2013Dconvolution instead of 1\u2013D, for improving image captiongeneration\u00a0[3].", "All experiments were conducted using Theano[27, 28], PyLearn2[29], and Blocks[30] libraries.", "The authors would like to acknowledge the support of the following agencies forresearch funding and computing support: National Science Center (Poland),NSERC, Calcul Qu\u00e9bec, Compute Canada,the Canada Research Chairs and CIFAR. Bahdanau also thanks PlanetIntelligent Systems GmbH and Yandex."], "figure_types": {"b624504240fa52ab76167acfe3156150ca01cf3b/3-Figure1-1.png": "schematic", "b624504240fa52ab76167acfe3156150ca01cf3b/5-Figure2-1.png": "plot", "b624504240fa52ab76167acfe3156150ca01cf3b/6-Figure3-1.png": "plot", "b624504240fa52ab76167acfe3156150ca01cf3b/7-Figure4-1.png": "plot", "b624504240fa52ab76167acfe3156150ca01cf3b/8-Figure5-1.png": "plot"}}, "1711.09020": {"paper_id": "paper_57", "title": "StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation", "arxiv_url": "https://arxiv.org/abs/1711.09020", "s2orc_url": "https://www.semanticscholar.org/paper/302207c149bdf7beb6e46e4d4afbd2fa9ac02c64", "all_figures_tables": {"302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/2-Figure2-1.png": "Figure 2. Comparison between cross-domain models and our proposed model, StarGAN. (a) To handle multiple domains, crossdomain models should be built for every pair of image domains. (b) StarGAN is capable of learning mappings among multiple domains using a single generator. The figure represents a star topology connecting multi-domains.", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/3-Figure3-1.png": "Figure 3. Overview of StarGAN, consisting of two modules, a discriminator D and a generator G. (a) D learns to distinguish between real and fake images and classify the real images to its corresponding domain. (b) G takes in as input both the image and target domain label and generates an fake image. The target domain label is spatially replicated and concatenated with the input image. (c) G tries to reconstruct the original image from the fake image given the original domain label. (d) G tries to generate images indistinguishable from real images and classifiable as target domain byD.", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/5-Figure4-1.png": "Figure 4. Facial attribute transfer results on the CelebA dataset. The first column shows the input image, next four columns show the single attribute transfer results, and rightmost columns show the multi-attribute transfer results. H: Hair color, G: Gender, A: Aged.", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/6-Figure5-1.png": "Figure 5. Facial expression synthesis results on the RaFD dataset.", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/7-Figure6-1.png": "Figure 6. Facial expression synthesis results of StarGAN-SNG and StarGAN-JNT on CelebA dataset.", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/7-Table1-1.png": "Table 1. AMT perceptual evaluation for ranking different models on a single attribute transfer task. Each column sums to 100%.", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/7-Table2-1.png": "Table 2. AMT perceptual evaluation for ranking different models on a multi-attribute transfer task. H: Hair color; G: Gender; A: Aged.", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/8-Figure7-1.png": "Figure 7. Learned role of the mask vector. All images are generated by StarGAN-JNT. The first row shows the result of applying the proper mask vector, and the last row shows the result of applying the wrong mask vector.", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/8-Table3-1.png": "Table 3. Classification errors [%] and the number of parameters on the RaFD dataset."}, "referred_figures_tables": [["302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/8-Table3-1.png"], ["302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/8-Table3-1.png"]], "question_id": [1, 0], "question": ["They perform only a qualitative analysis of the proposed model. Is it true?", "They propose StarGAN to overcome a limitation of high computational complexity of current image-to-image translation models. Is it true?"], "question_section": ["Conclusion", "Abstract"], "question_trigger_sentence": ["Besides the advantages in scalability, StarGAN generated images of higher visual quality compared to existing methods [16, 23, 33], owing to the generalization capability behind the multi-task learning setting.", "However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains."], "question_type": ["Shallow question", "Shallow question"], "evidential_info": [[{"context": "Qualitative evaluation. Fig.\u20094 shows the facial attribute transfer results on CelebA. We observed that our method provides a higher visual quality of translation results on test data compared to the cross-domain models. One possible reason is the regularization effect of StarGAN through a multi-task learning framework. In other words, rather than training a model to perform a fixed translation (e.g., brown-to-blond hair), which is prone to overfitting, we train our model to flexibly translate images according to the labels of the target domain. This allows our model to learn reliable features universally applicable to multiple domains of images with different facial attribute values.", "rationale": "They provided quantitive analyzing method for their paper."}, {"context": "Quantitative evaluation protocol. For quantitative evaluations, we performed two user studies in a survey format using Amazon Mechanical Turk (AMT) to assess single and multiple attribute transfer tasks. Given an input image, the Turkers were instructed to choose the best generated image based on perceptual realism, quality of transfer in attribute(s), and preservation of a figure\u2019s original identity. The options were four randomly shuffled images generated from four different methods. The generated images in one study have a single attribute transfer in either hair color (black, blond, brown), gender, or age. In another study, the generated images involve a combination of attribute transfers. Each Turker was asked 30 to 40 questions with a few simple yet logical questions for validating human effort. The number of validated Turkers in each user study is 146 and 100 in single and multiple transfer tasks, respectively.", "rationale": "They introduced two different paper, which is related to quantitive analysis"}, {"context": "Quantitative results. Tables\u20091 and 2 show the results of our AMT experiment on single- and multi-attribute transfer tasks, respectively. StarGAN obtained the majority of votes for best transferring attributes in all cases.In the case of gender changes in Table\u20091, the voting difference between our model and other models was marginal, e.g., 39.1% for StarGAN vs. 31.4% for DIAT. However, in multi-attribute changes, e.g., the \u2018G+A\u2019 case in Table\u20092, the performance difference becomes significant, e.g., 49.8% for StarGAN vs. 20.3% for IcGAN), clearly showing the advantages of StarGAN in more complicated, multi-attribute transfer tasks. This is because unlike the other methods, StarGAN can handle image translation involving multiple attribute changes by randomly generating a target domain label in the training phase.", "rationale": "They provided qualitative analysis result."}, {"context": "Qualitative evaluation. As seen in Fig.\u20095, StarGAN clearly generates the most natural-looking expressions while properly maintaining the personal identity and facial features of the input. While DIAT and CycleGAN mostly preserve the identity of the input, many of their results are shown blurry and do not maintain the degree of sharpness as seen in the input. IcGAN even fails to preserve the personal identity in the image by generating male images.", "rationale": "They provided qualitative analysis result."}, {"context": "Quantitative evaluation. For a quantitative evaluation, we compute the classification error of a facial expression on synthesized images. We trained a facial expression classifier on the RaFD dataset (90%/10% splitting for training and test sets) using a ResNet-18 architecture [5], resulting in a near-perfect accuracy of 99.55%. We then trained each of image translation models using the same training set and performed image translation on the same, unseen test set. Finally, we classified the expression of these translated images using the above-mentioned classifier. As can be seen in Table 3, our model achieves the lowest classification error, indicating that our model produces the most realistic facial expressions among all the methods compared.", "rationale": "They provided quantitive analysis result."}], [{"context": "Another important advantage of our model is the scalability in terms of the number of parameters required. The last column in Table 3 shows that the number of parameters required to learn all translations by StarGAN is seven times smaller than that of DIAT and fourteen times smaller than that of CycleGAN. This is because StarGAN requires only a single generator and discriminator pair, regardless of the number of domains, while in the case of cross-domain models such as CycleGAN, a completely different model should be trained for each source-target domain pair.", "rationale": "Table 3 shows that the number of parameters required to learn all translations by StarGAN is multiple times smaller than other previous models."}]], "composition": ["False. They provided not only qualitative analysis but also quantitive analysis for their model.", "It is false. There is no evidence that the motivation of StarGAN was made in order to overcome a limitation of high computational complexity, even it achieved that."], "Is_figure_in_evidence": [false, false], "Is_table_in_evidence": [true, true], "question_key": ["1110", "1120"], "passages": ["The task of image-to-image translation is to change a particular aspect of a given image to another, e.g., changing the facial expression of a person from smiling to frowning (see Fig.\u2009StarGAN: Unified Generative Adversarial Networks  for Multi-Domain Image-to-Image Translation). This task has experienced significant improvements following the introduction of generative adversarial networks (GANs), with results ranging from changing hair color\u00a0[9], reconstructing photos from edge maps\u00a0[7], and changing the seasons of scenery images\u00a0[33].", "Given training data from two different domains, these models learn to translate images from one domain to the other. We denote the terms attribute as a meaningful feature inherent in an image such as hair color, gender or age, and attribute value as a particular value of an attribute, e.g., black/blond/brown for hair color or male/female for gender. We further denote domain as a set of images sharing the same attribute value. For example, images of women can represent one domain while those of men represent another.", "Several image datasets come with a number of labeled attributes. For instance, the CelebA[19] dataset contains 40 labels related to facial attributes such as hair color, gender, and age, and the RaFD [13] dataset has 8 labels for facial expressions such as \u2018happy\u2019, \u2018angry\u2019 and \u2018sad\u2019. These settings enable us to perform more interesting tasks, namely multi-domain image-to-image translation, where we change images according to attributes from multiple domains. The first five columns in Fig.\u2009StarGAN: Unified Generative Adversarial Networks  for Multi-Domain Image-to-Image Translation show how a CelebA image can be translated according to any of the four domains, \u2018blond hair\u2019, \u2018gender\u2019, \u2018aged\u2019, and \u2018pale skin\u2019. We can further extend to training multiple domains from different datasets, such as jointly training CelebA and RaFD images to change a CelebA image\u2019s facial expression using features learned by training on RaFD, as in the rightmost columns of Fig.\u2009StarGAN: Unified Generative Adversarial Networks  for Multi-Domain Image-to-Image Translation. ", "However, existing models are both inefficient and ineffective in such multi-domain image translation tasks. Their inefficiency results from the fact that in order to learn all mappings among k\ud835\udc58kitalic_k domains, k(k\u22121)\ud835\udc58\ud835\udc581k(k\\mathbb{-}1)italic_k ( italic_k - 1 ) generators have to be trained. Fig.\u20092\u2009(a) illustrates how twelve distinct generator networks have to be trained to translate images among four different domains. Meanwhile, they are ineffective that even though there exist global features that can be learned from images of all domains such as face shapes, each generator cannot fully utilize the entire training data and only can learn from two domains out of k\ud835\udc58kitalic_k. Failure to fully utilize training data is likely to limit the quality of generated images. Furthermore, they are incapable of jointly training domains from different datasets because each dataset is partially labeled, which we further discuss in Section 3.2.", "As a solution to such problems we propose StarGAN, a novel and scalable approach capable of learning mappings among multiple domains. As demonstrated in Fig.\u20092\u2009(b), our model takes in training data of multiple domains, and learns the mappings between all available domains using only a single generator. The idea is simple. Instead of learning a fixed translation (e.g., black-to-blond hair), our generator takes in as inputs both image and domain information, and learns to flexibly translate the image into the corresponding domain. We use a label (e.g., binary or one-hot vector) to represent domain information. During training, we randomly generate a target domain label and train the model to flexibly translate an input image into the target domain. By doing so, we can control the domain label and translate the image into any desired domain at testing phase. ", "We also introduce a simple but effective approach that enables joint training between domains of different datasets by adding a mask vector to the domain label. Our proposed method ensures that the model can ignore unknown labels and focus on the label provided by a particular dataset. In this manner, our model can perform well on tasks such as synthesizing facial expressions of CelebA images using features learned from RaFD, as shown in the rightmost columns of Fig.\u2009StarGAN: Unified Generative Adversarial Networks  for Multi-Domain Image-to-Image Translation. As far as our knowledge goes, our work is the first to successfully perform multi-domain image translation across different datasets.", "Overall, our contributions are as follows:\u2219\u2219\\bullet\u2219We propose StarGAN, a novel generative adversarial network that learns the mappings among multiple domains using only a single generator and a discriminator, training effectively from images of all domains.\u2219\u2219\\bullet\u2219We demonstrate how we can successfully learn multi-domain image translation between multiple datasets by utilizing a mask vector method that enables StarGAN to control all available domain labels.\u2219\u2219\\bullet\u2219We provide both qualitative and quantitative results on facial attribute transfer and facial expression synthesis tasks using StarGAN, showing its superiority over baseline models.", "Generative Adversarial Networks. Generative adversarial networks (GANs) [3] have shown remarkable results in various computer vision tasks such as image generation [6, 24, 32, 8], image translation [7, 9, 33], super-resolution imaging [14], and face image synthesis [10, 16, 26, 31]. A typical GAN model consists of two modules: a discriminator and a generator. The discriminator learns to distinguish between real and fake samples, while the generator learns to generate fake samples that are indistinguishable from real samples. Our approach also leverages the adversarial loss to make the generated images as realistic as possible.", "Conditional GANs. GAN-based conditional image generation has also been actively studied. Prior studies have provided both the discriminator and generator with class information in order to generate samples conditioned on the class \u00a0[20, 21, 22]. Other recent approaches focused on generating particular images highly relevant to a given text description \u00a0[25, 30]. The idea of conditional image generation has also been successfully applied to domain transfer [9, 28], super-resolution imaging[14], and photo editing [2, 27].In this paper, we propose a scalable GAN framework that can flexibly steer the image translation to various target domains, by providing conditional domain information.", "Image-to-Image Translation. Recent work have achieved impressive results in image-to-image translation [7, 9, 17, 33]. For instance, pix2pix [7] learns this task in a supervised manner using cGANs[20]. It combines an adversarial loss with a L1 loss, thus requires paired data samples. To alleviate the problem of obtaining data pairs, unpaired image-to-image translation frameworks [9, 17, 33] have been proposed.UNIT [17] combines variational autoencoders (VAEs) [12] with CoGAN [18], a GAN framework where two generators share weights to learn the joint distribution of images in cross domains. CycleGAN [33] and DiscoGAN [9] preserve key attributes between the input and the translated image by utilizing a cycle consistency loss. However, all these frameworks are only capable of learning the relations between two different domains at a time. Their approaches have limited scalability in handling multiple domains since different models should be trained for each pair of domains. Unlike the aforementioned approaches, our framework can learn the relations among multiple domains using only a single model.", "We first describe our proposed StarGAN, a framework to address multi-domain image-to-image translation within a single dataset. Then, we discuss how StarGAN incorporates multiple datasets containing different label sets to flexibly perform image translations using any of these labels.", "Our goal is to train a single generator G\ud835\udc3aGitalic_G that learns mappings among multiple domains. To achieve this, we train G\ud835\udc3aGitalic_G to translate an input image x\ud835\udc65xitalic_x into an output image y\ud835\udc66yitalic_y conditioned on the target domain label c\ud835\udc50citalic_c, G(x,c)\u2192y\u2192\ud835\udc3a\ud835\udc65\ud835\udc50\ud835\udc66G(x,c)\\rightarrow yitalic_G ( italic_x , italic_c ) \u2192 italic_y.We randomly generate the target domain label c\ud835\udc50citalic_c so that G\ud835\udc3aGitalic_G learns to flexibly translate the input image. We also introduce an auxiliary classifier [22] that allows a single discriminator to control multiple domains. That is, our discriminator produces probability distributions over both sources and domain labels, D:x\u2192{Dsrc(x),Dcls(x)}:\ud835\udc37\u2192\ud835\udc65subscript\ud835\udc37\ud835\udc60\ud835\udc5f\ud835\udc50\ud835\udc65subscript\ud835\udc37\ud835\udc50\ud835\udc59\ud835\udc60\ud835\udc65D:x\\rightarrow\\{{D}_{src}(x),{D}_{cls}(x)\\}italic_D : italic_x \u2192 { italic_D start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT ( italic_x ) , italic_D start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT ( italic_x ) }. Fig.\u20093 illustrates the training process of our proposed approach.", "Adversarial Loss. To make the generated images indistinguishable from real images, we adopt an adversarial loss\u2112adv=\ud835\udd3cx[log\u2061Dsrc(x)]+\ud835\udd3cx,c[log\u2061(1\u2212Dsrc(G(x,c)))],subscript\u2112\ud835\udc4e\ud835\udc51\ud835\udc63subscript\ud835\udd3c\ud835\udc65delimited-[]subscript\ud835\udc37\ud835\udc60\ud835\udc5f\ud835\udc50\ud835\udc65subscript\ud835\udd3c\ud835\udc65\ud835\udc50delimited-[]1subscript\ud835\udc37\ud835\udc60\ud835\udc5f\ud835\udc50\ud835\udc3a\ud835\udc65\ud835\udc50\\begin{split}\\mathcal{L}_{adv}=&\\thinspace{\\mathbb{E}}_{x}\\left[\\log{{D}_{src}(x)}\\right]\\>\\>+\\\\&\\thinspace{\\mathbb{E}}_{x,c}[\\log{(1-{D}_{src}(G(x,c)))}],\\end{split}start_ROW start_CELL caligraphic_L start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT = end_CELL start_CELL blackboard_E start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT [ roman_log italic_D start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT ( italic_x ) ] + end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL blackboard_E start_POSTSUBSCRIPT italic_x , italic_c end_POSTSUBSCRIPT [ roman_log ( 1 - italic_D start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT ( italic_G ( italic_x , italic_c ) ) ) ] , end_CELL end_ROW(1)where G\ud835\udc3aGitalic_G generates an image G(x,c)\ud835\udc3a\ud835\udc65\ud835\udc50G(x,c)italic_G ( italic_x , italic_c ) conditioned on both the input image x\ud835\udc65xitalic_x and the target domain label c\ud835\udc50citalic_c, while D\ud835\udc37Ditalic_D tries to distinguish between real and fake images. In this paper, we refer to the term Dsrc(x)subscript\ud835\udc37\ud835\udc60\ud835\udc5f\ud835\udc50\ud835\udc65{D}_{src}(x)italic_D start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT ( italic_x ) as a probability distribution over sources given by D\ud835\udc37Ditalic_D. The generator G\ud835\udc3aGitalic_G tries to minimize this objective, while the discriminator D\ud835\udc37Ditalic_D tries to maximize it.", "Domain Classification Loss. For a given input image x\ud835\udc65xitalic_x and a target domain label c\ud835\udc50citalic_c, our goal is to translate x\ud835\udc65xitalic_x into an output image y\ud835\udc66yitalic_y, which is properly classified to the target domain c\ud835\udc50citalic_c. To achieve this condition, we add an auxiliary classifier on top of D\ud835\udc37Ditalic_D and impose the domain classification loss when optimizing both D\ud835\udc37Ditalic_D and G\ud835\udc3aGitalic_G. That is, we decompose the objective into two terms: a domain classification loss of real images used to optimize D\ud835\udc37Ditalic_D, and a domain classification loss of fake images used to optimize G\ud835\udc3aGitalic_G. In detail, the former is defined as\u2112clsr=\ud835\udd3cx,c\u2032[\u2212log\u2061Dcls(c\u2032|x)],superscriptsubscript\u2112\ud835\udc50\ud835\udc59\ud835\udc60\ud835\udc5fsubscript\ud835\udd3c\ud835\udc65superscript\ud835\udc50\u2032delimited-[]subscript\ud835\udc37\ud835\udc50\ud835\udc59\ud835\udc60conditionalsuperscript\ud835\udc50\u2032\ud835\udc65\\mathcal{L}_{cls}^{r}={\\mathbb{E}}_{x,c^{\\prime}}[-\\log{{D}_{cls}(c^{\\prime}|x)}],caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT italic_x , italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ - roman_log italic_D start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT ( italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT | italic_x ) ] ,(2)where the term Dcls(c\u2032|x)subscript\ud835\udc37\ud835\udc50\ud835\udc59\ud835\udc60conditionalsuperscript\ud835\udc50\u2032\ud835\udc65{D}_{cls}(c^{\\prime}|x)italic_D start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT ( italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT | italic_x ) represents a probability distribution over domain labels computed by D\ud835\udc37Ditalic_D. By minimizing this objective, D\ud835\udc37Ditalic_D learns to classify a real image x\ud835\udc65xitalic_x to its corresponding original domain c\u2032superscript\ud835\udc50\u2032c^{\\prime}italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT. We assume that the input image and domain label pair (x,c\u2032)\ud835\udc65superscript\ud835\udc50\u2032(x,c^{\\prime})( italic_x , italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) is given by the training data. On the other hand, the loss function for the domain classification of fake images is defined as\u2112clsf=\ud835\udd3cx,c[\u2212log\u2061Dcls(c|G(x,c))].superscriptsubscript\u2112\ud835\udc50\ud835\udc59\ud835\udc60\ud835\udc53subscript\ud835\udd3c\ud835\udc65\ud835\udc50delimited-[]subscript\ud835\udc37\ud835\udc50\ud835\udc59\ud835\udc60conditional\ud835\udc50\ud835\udc3a\ud835\udc65\ud835\udc50\\mathcal{L}_{cls}^{f}={\\mathbb{E}}_{x,c}[-\\log{{D}_{cls}(c|G(x,c))}].caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT = blackboard_E start_POSTSUBSCRIPT italic_x , italic_c end_POSTSUBSCRIPT [ - roman_log italic_D start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT ( italic_c | italic_G ( italic_x , italic_c ) ) ] .(3)In other words, G\ud835\udc3aGitalic_G tries to minimize this objective to generate images that can be classified as the target domain c\ud835\udc50citalic_c. ", "Reconstruction Loss. By minimizing the adversarial and classification losses, G\ud835\udc3aGitalic_G is trained to generate images that are realistic and classified to its correct target domain. However, minimizing the losses (Eqs.\u2009(1) and (3)) does not guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs. To alleviate this problem, we apply a cycle consistency loss [9, 33] to the generator, defined as\u2112rec=\ud835\udd3cx,c,c\u2032[\u2016x\u2212G(G(x,c),c\u2032)\u20161],subscript\u2112\ud835\udc5f\ud835\udc52\ud835\udc50subscript\ud835\udd3c\ud835\udc65\ud835\udc50superscript\ud835\udc50\u2032delimited-[]subscriptnorm\ud835\udc65\ud835\udc3a\ud835\udc3a\ud835\udc65\ud835\udc50superscript\ud835\udc50\u20321\\mathcal{L}_{rec}={\\mathbb{E}}_{x,c,c^{\\prime}}[{||x-G(G(x,c),c^{\\prime})||}_{1}],caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT italic_x , italic_c , italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ | | italic_x - italic_G ( italic_G ( italic_x , italic_c ) , italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) | | start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] ,(4)where G\ud835\udc3aGitalic_G takes in the translated image G(x,c)\ud835\udc3a\ud835\udc65\ud835\udc50G(x,c)italic_G ( italic_x , italic_c ) and the original domain label c\u2032superscript\ud835\udc50\u2032c^{\\prime}italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT as input and tries to reconstruct the original image x\ud835\udc65xitalic_x. We adopt the L1 norm as our reconstruction loss. Note that we use a single generator twice, first to translate an original image into an image in the target domain and then to reconstruct the original image from the translated image.", "Full Objective. Finally, the objective functions to optimize G\ud835\udc3aGitalic_G and D\ud835\udc37Ditalic_D are written, respectively, as\u2112D=\u2212\u2112adv+\u03bbcls\u2112clsr,subscript\u2112\ud835\udc37subscript\u2112\ud835\udc4e\ud835\udc51\ud835\udc63subscript\ud835\udf06\ud835\udc50\ud835\udc59\ud835\udc60superscriptsubscript\u2112\ud835\udc50\ud835\udc59\ud835\udc60\ud835\udc5f\\mathcal{L}_{D}=-\\mathcal{L}_{adv}+{\\lambda}_{cls}\\thinspace\\mathcal{L}_{cls}^{r},caligraphic_L start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT = - caligraphic_L start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT + italic_\u03bb start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ,(5)\u2112G=\u2112adv+\u03bbcls\u2112clsf+\u03bbrec\u2112rec,subscript\u2112\ud835\udc3asubscript\u2112\ud835\udc4e\ud835\udc51\ud835\udc63subscript\ud835\udf06\ud835\udc50\ud835\udc59\ud835\udc60superscriptsubscript\u2112\ud835\udc50\ud835\udc59\ud835\udc60\ud835\udc53subscript\ud835\udf06\ud835\udc5f\ud835\udc52\ud835\udc50subscript\u2112\ud835\udc5f\ud835\udc52\ud835\udc50\\mathcal{L}_{G}=\\mathcal{L}_{adv}+{\\lambda}_{cls}\\thinspace\\mathcal{L}_{cls}^{f}+{\\lambda}_{rec}\\thinspace\\mathcal{L}_{rec},caligraphic_L start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT + italic_\u03bb start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT + italic_\u03bb start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT ,(6)where \u03bbclssubscript\ud835\udf06\ud835\udc50\ud835\udc59\ud835\udc60{\\lambda}_{cls}italic_\u03bb start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT and \u03bbrecsubscript\ud835\udf06\ud835\udc5f\ud835\udc52\ud835\udc50{\\lambda}_{rec}italic_\u03bb start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT are hyper-parameters that control the relative importance of domain classification and reconstruction losses, respectively, compared to the adversarial loss. We use \u03bbcls=1subscript\ud835\udf06\ud835\udc50\ud835\udc59\ud835\udc601{\\lambda}_{cls}=1italic_\u03bb start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT = 1 and \u03bbrec=10subscript\ud835\udf06\ud835\udc5f\ud835\udc52\ud835\udc5010{\\lambda}_{rec}=10italic_\u03bb start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT = 10 in all of our experiments.", "An important advantage of StarGAN is that it simultaneously incorporates multiple datasets containing different types of labels, so that StarGAN can control all the labels at the test phase. An issue when learning from multiple datasets, however, is that the label information is only partially known to each dataset. In the case of CelebA\u2009[19] and RaFD\u2009[13], while the former contains labels for attributes such as hair color and gender, it does not have any labels for facial expressions such as \u2018happy\u2019 and \u2018angry\u2019, and vice versa for the latter. This is problematic because the complete information on the label vector c\u2032superscript\ud835\udc50\u2032c^{\\prime}italic_c start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is required when reconstructing the input image x\ud835\udc65xitalic_x from the translated image G(x,c)\ud835\udc3a\ud835\udc65\ud835\udc50G(x,c)italic_G ( italic_x , italic_c ) (See Eq.\u2009(4)).", "Mask Vector. To alleviate this problem, we introduce a mask vector m\ud835\udc5amitalic_m that allows StarGAN to ignore unspecified labels and focus on the explicitly known label provided by a particular dataset. In StarGAN, we use an n\ud835\udc5bnitalic_n-dimensional one-hot vector to represent m\ud835\udc5amitalic_m, with n\ud835\udc5bnitalic_n being the number of datasets. In addition, we define a unified version of the label as a vectorc~=[c1,\u2026,cn,m],~\ud835\udc50subscript\ud835\udc501\u2026subscript\ud835\udc50\ud835\udc5b\ud835\udc5a\\tilde{c}=[{c}_{1},...,{c}_{n},m],over~ start_ARG italic_c end_ARG = [ italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_c start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_m ] ,(7)where [\u22c5]delimited-[]\u22c5[\\cdot][ \u22c5 ] refers to concatenation, and cisubscript\ud835\udc50\ud835\udc56{c}_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represents a vector for the labels of the i\ud835\udc56iitalic_i-th dataset. The vector of the known label cisubscript\ud835\udc50\ud835\udc56{c}_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes. For the remaining n\u22121\ud835\udc5b1n\\mathbb{-}1italic_n - 1 unknown labels we simply assign zero values. In our experiments, we utilize the CelebA and RaFD datasets, where n\ud835\udc5bnitalic_n is two.", "Training Strategy. When training StarGAN with multiple datasets, we use the domain label c~~\ud835\udc50\\tilde{c}over~ start_ARG italic_c end_ARG defined in Eq.\u2009(7) as input to the generator. By doing so, the generator learns to ignore the unspecified labels, which are zero vectors, and focus on the explicitly given label. The structure of the generator is exactly the same as in training with a single dataset, except for the dimension of the input label c~~\ud835\udc50\\tilde{c}over~ start_ARG italic_c end_ARG. On the other hand, we extend the auxiliary classifier of the discriminator to generate probability distributions over labels for all datasets. Then, we train the model in a multi-task learning setting, where the discriminator tries to minimize only the classification error associated to the known label. For example, when training with images in CelebA, the discriminator minimizes only classification errors for labels related to CelebA attributes, and not facial expressions related to RaFD. Under these settings, by alternating between CelebA and RaFD the discriminator learns all of the discriminative features for both datasets, and the generator learns to control all the labels in both datasets.", "Improved GAN Training.To stabilize the training process and generate higher quality images, we replace Eq.\u2009(1) with Wasserstein GAN objective with gradient penalty [1, 4] defined as\u2112adv=\ud835\udd3cx[Dsrc(x)]\u2212\ud835\udd3cx,c[Dsrc(G(x,c))]\u2212\u03bbgp\ud835\udd3cx^[(\u2016\u25bdx^Dsrc(x^)\u20162\u22121)2],subscript\u2112\ud835\udc4e\ud835\udc51\ud835\udc63subscript\ud835\udd3c\ud835\udc65delimited-[]subscript\ud835\udc37\ud835\udc60\ud835\udc5f\ud835\udc50\ud835\udc65subscript\ud835\udd3c\ud835\udc65\ud835\udc50delimited-[]subscript\ud835\udc37\ud835\udc60\ud835\udc5f\ud835\udc50\ud835\udc3a\ud835\udc65\ud835\udc50subscript\ud835\udf06\ud835\udc54\ud835\udc5dsubscript\ud835\udd3c^\ud835\udc65delimited-[]superscriptsubscriptnormsubscript\u25bd^\ud835\udc65subscript\ud835\udc37\ud835\udc60\ud835\udc5f\ud835\udc50^\ud835\udc65212\\begin{split}\\mathcal{L}_{adv}=\\thinspace&{\\mathbb{E}}_{x}[{D}_{src}(x)]-{\\mathbb{E}}_{x,c}[{D}_{src}(G(x,c))]\\thinspace\\thinspace\\\\&-{\\lambda}_{gp}\\thinspace{\\mathbb{E}}_{\\hat{x}}[{{(||{\\triangledown}_{\\hat{x}}{D}_{src}(\\hat{x})||}_{2}-1)}^{2}]\\thinspace,\\end{split}start_ROW start_CELL caligraphic_L start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT = end_CELL start_CELL blackboard_E start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT [ italic_D start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT ( italic_x ) ] - blackboard_E start_POSTSUBSCRIPT italic_x , italic_c end_POSTSUBSCRIPT [ italic_D start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT ( italic_G ( italic_x , italic_c ) ) ] end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL - italic_\u03bb start_POSTSUBSCRIPT italic_g italic_p end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT over^ start_ARG italic_x end_ARG end_POSTSUBSCRIPT [ ( | | \u25bd start_POSTSUBSCRIPT over^ start_ARG italic_x end_ARG end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_s italic_r italic_c end_POSTSUBSCRIPT ( over^ start_ARG italic_x end_ARG ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] , end_CELL end_ROW(8)where x^^\ud835\udc65\\hat{x}over^ start_ARG italic_x end_ARG is sampled uniformly along a straight line between a pair of a real and a generated images. We use \u03bbgp=10subscript\ud835\udf06\ud835\udc54\ud835\udc5d10{\\lambda}_{gp}=10italic_\u03bb start_POSTSUBSCRIPT italic_g italic_p end_POSTSUBSCRIPT = 10 for all experiments.", "Network Architecture.Adapted from CycleGAN [33], StarGAN has the generator network composed of two convolutional layers with the stride size of two for downsampling, six residual blocks [5], and two transposed convolutional layers with the stride size of two for upsampling. We use instance normalization [29] for the generator but no normalization for the discriminator. We leverage PatchGANs [7, 15, 33] for the discriminator network, which classifies whether local image patches are real or fake. See the appendix (Section 7.2) for more details about the network architecture.", "In this section, we first compare StarGAN against recent methods on facial attribute transfer by conducting user studies. Next, we perform a classification experiment on facial expression synthesis. Lastly, we demonstrate empirical results that StarGAN can learn image-to-image translation from multiple datasets. All our experiments were conducted by using the model output from unseen images during the training phase.", "As our baseline models, we adopt DIAT [16] and CycleGAN [33], both of which performs image-to-image translation between two different domains. For comparison, we trained these models multiple times for every pair of two different domains. We also adopt IcGAN [23] as a baseline which can perform attribute transfer using a cGAN [22].", "DIAT uses an adversarial loss to learn the mapping from x\u2208X\ud835\udc65\ud835\udc4bx\\in Xitalic_x \u2208 italic_X to y\u2208Y\ud835\udc66\ud835\udc4cy\\in Yitalic_y \u2208 italic_Y, where x\ud835\udc65xitalic_x and y\ud835\udc66yitalic_y are face images in two different domains X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y, respectively. This method has a regularization term on the mapping as \u2016x\u2212F(G(x))\u20161subscriptnorm\ud835\udc65\ud835\udc39\ud835\udc3a\ud835\udc651{||x-F(G(x))||}_{1}| | italic_x - italic_F ( italic_G ( italic_x ) ) | | start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to preserve identity features of the source image, where F\ud835\udc39Fitalic_F is a feature extractor pretrained on a face recognition task.", "CycleGAN also uses an adversarial loss to learn the mapping between two different domains X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y. This method regularizes the mapping via cycle consistency losses, \u2016x\u2212(GYX(GXY(x)))\u20161subscriptnorm\ud835\udc65subscript\ud835\udc3a\ud835\udc4c\ud835\udc4bsubscript\ud835\udc3a\ud835\udc4b\ud835\udc4c\ud835\udc651{||x-({G}_{YX}({G}_{XY}(x)))||}_{1}| | italic_x - ( italic_G start_POSTSUBSCRIPT italic_Y italic_X end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT italic_X italic_Y end_POSTSUBSCRIPT ( italic_x ) ) ) | | start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and \u2016y\u2212(GXY(GYX(y)))\u20161subscriptnorm\ud835\udc66subscript\ud835\udc3a\ud835\udc4b\ud835\udc4csubscript\ud835\udc3a\ud835\udc4c\ud835\udc4b\ud835\udc661{||y-({G}_{XY}({G}_{YX}(y)))||}_{1}| | italic_y - ( italic_G start_POSTSUBSCRIPT italic_X italic_Y end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT italic_Y italic_X end_POSTSUBSCRIPT ( italic_y ) ) ) | | start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. This method requires two generators and discriminators for each pair of two different domains.", "IcGAN combines an encoder with a cGAN [22] model. cGAN learns the mapping G:{z,c}\u2192x:\ud835\udc3a\u2192\ud835\udc67\ud835\udc50\ud835\udc65G:\\{z,c\\}\\rightarrow xitalic_G : { italic_z , italic_c } \u2192 italic_x that generates an image x\ud835\udc65xitalic_x conditioned on both the latent vector z\ud835\udc67zitalic_z and the conditional vector c\ud835\udc50citalic_c. In addition, IcGAN introduces an encoder to learn the inverse mappings of cGAN, Ez:x\u2192z:subscript\ud835\udc38\ud835\udc67\u2192\ud835\udc65\ud835\udc67{E}_{z}:x\\rightarrow zitalic_E start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT : italic_x \u2192 italic_z and Ec:x\u2192c:subscript\ud835\udc38\ud835\udc50\u2192\ud835\udc65\ud835\udc50{E}_{c}:x\\rightarrow citalic_E start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT : italic_x \u2192 italic_c. This allows IcGAN to synthesis images by only changing the conditional vector and preserving the latent vector.", "CelebA. The CelebFaces Attributes (CelebA) dataset [19] contains 202,599 face images of celebrities, each annotated with 40 binary attributes. We crop the initial 178\u00d7218178218178\\times 218178 \u00d7 218 size images to 178\u00d7178178178178\\times 178178 \u00d7 178, then resize them as 128\u00d7128128128128\\times 128128 \u00d7 128. We randomly select 2,000 images as test set and use all remaining images for training data. We construct seven domains using the following attributes: hair color (black, blond, brown), gender (male/female), and age (young/old).", "RaFD. The Radboud Faces Database (RaFD) [13] consists of 4,824 images collected from 67 participants. Each participant makes eight facial expressions in three different gaze directions, which are captured from three different angles. We crop the images to 256\u00d7256256256256\\times 256256 \u00d7 256, where the faces are centered, and then resize them to 128\u00d7128128128128\\times 128128 \u00d7 128.", "All models are trained using Adam [11] with \u03b21=0.5subscript\ud835\udefd10.5{\\beta}_{1}=0.5italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.5 and \u03b22=0.999subscript\ud835\udefd20.999{\\beta}_{2}=0.999italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.999. For data augmentation we flip the images horizontally with a probability of 0.5. We perform one generator update after five discriminator updates as in [4]. The batch size is set to 16 for all experiments. For experiments on CelebA, we train all models with a learning rate of 0.0001 for the first 10 epochs and linearly decay the learning rate to 0 over the next 10 epochs. To compensate for the lack of data, when training with RaFD we train all models for 100 epochs with a learning rate of 0.0001 and apply the same decaying strategy over the next 100 epochs. Training takes about one day on a single NVIDIA Tesla M40 GPU.", "We first compare our proposed method to the baseline models on a single and multi-attribute transfer tasks. We train the cross-domain models such as DIAT and CycleGAN multiple times considering all possible attribute value pairs.In the case of DIAT and CycleGAN, we perform multi-step translations to synthesize multiple attributes (e.g. transferring a gender attribute after changing a hair color).", "Qualitative evaluation. Fig.\u20094 shows the facial attribute transfer results on CelebA. We observed that our method provides a higher visual quality of translation results on test data compared to the cross-domain models. One possible reason is the regularization effect of StarGAN through a multi-task learning framework. In other words, rather than training a model to perform a fixed translation (e.g., brown-to-blond hair), which is prone to overfitting, we train our model to flexibly translate images according to the labels of the target domain. This allows our model to learn reliable features universally applicable to multiple domains of images with different facial attribute values.", "Furthermore, compared to IcGAN, our model demonstrates an advantage in preserving the facial identity feature of an input. We conjecture that this is because our method maintains the spatial information by using activation maps from the convolutional layer as latent representation, rather than just a low-dimensional latent vector as in IcGAN.", "Quantitative evaluation protocol. For quantitative evaluations, we performed two user studies in a survey format using Amazon Mechanical Turk (AMT) to assess single and multiple attribute transfer tasks. Given an input image, the Turkers were instructed to choose the best generated image based on perceptual realism, quality of transfer in attribute(s), and preservation of a figure\u2019s original identity. The options were four randomly shuffled images generated from four different methods. The generated images in one study have a single attribute transfer in either hair color (black, blond, brown), gender, or age. In another study, the generated images involve a combination of attribute transfers. Each Turker was asked 30 to 40 questions with a few simple yet logical questions for validating human effort. The number of validated Turkers in each user study is 146 and 100 in single and multiple transfer tasks, respectively.", "Quantitative results. Tables\u20091 and 2 show the results of our AMT experiment on single- and multi-attribute transfer tasks, respectively. StarGAN obtained the majority of votes for best transferring attributes in all cases.In the case of gender changes in Table\u20091, the voting difference between our model and other models was marginal, e.g., 39.1% for StarGAN vs. 31.4% for DIAT. However, in multi-attribute changes, e.g., the \u2018G+A\u2019 case in Table\u20092, the performance difference becomes significant, e.g., 49.8% for StarGAN vs. 20.3% for IcGAN), clearly showing the advantages of StarGAN in more complicated, multi-attribute transfer tasks. This is because unlike the other methods, StarGAN can handle image translation involving multiple attribute changes by randomly generating a target domain label in the training phase.", "We next train our model on the RaFD dataset to learn the task of synthesizing facial expressions. To compare StarGAN and baseline models, we fix the input domain as the \u2018neutral\u2019 expression, but the target domain varies among the seven remaining expressions.", "Qualitative evaluation. As seen in Fig.\u20095, StarGAN clearly generates the most natural-looking expressions while properly maintaining the personal identity and facial features of the input. While DIAT and CycleGAN mostly preserve the identity of the input, many of their results are shown blurry and do not maintain the degree of sharpness as seen in the input. IcGAN even fails to preserve the personal identity in the image by generating male images.", "We believe that the superiority of StarGAN in the image quality is due to its implicit data augmentation effect from a multi-task learning setting. RaFD images contain a relatively small size of samples, e.g., 500 images per domain. When trained on two domains, DIAT and CycleGAN can only use 1,000 training images at a time, but StarGAN can use 4,000 images in total from all the available domains for its training. This allows StarGAN to properly learn how to maintain the quality and sharpness of the generated output.", "Quantitative evaluation. For a quantitative evaluation, we compute the classification error of a facial expression on synthesized images. We trained a facial expression classifier on the RaFD dataset (90%/10% splitting for training and test sets) using a ResNet-18 architecture [5], resulting in a near-perfect accuracy of 99.55%. We then trained each of image translation models using the same training set and performed image translation on the same, unseen test set. Finally, we classified the expression of these translated images using the above-mentioned classifier. As can be seen in Table 3, our model achieves the lowest classification error, indicating that our model produces the most realistic facial expressions among all the methods compared.", "Another important advantage of our model is the scalability in terms of the number of parameters required. The last column in Table 3 shows that the number of parameters required to learn all translations by StarGAN is seven times smaller than that of DIAT and fourteen times smaller than that of CycleGAN. This is because StarGAN requires only a single generator and discriminator pair, regardless of the number of domains, while in the case of cross-domain models such as CycleGAN, a completely different model should be trained for each source-target domain pair.", "Finally, we empirically demonstrate that our model can learn not only from multiple domains within a single dataset, but also from multiple datasets. We train our model jointly on the CelebA and RaFD datasets using the mask vector (see Section 3.2). To distinguish between the model trained only on RaFD and the model trained on both CelebA and RaFD, we denote the former as StarGAN-SNG (single) and the latter as StarGAN-JNT (joint).", "Effects of joint training. Fig.\u20096 shows qualitative comparisons between StarGAN-SNG and StarGAN-JNT, where the task is to synthesize facial expressions of images in CelebA. StarGAN-JNT exhibits emotional expressions with high visual quality, while StarGAN-SNG generates reasonable but blurry images with gray backgrounds. This difference is due to the fact that StarGAN-JNT learns to translate CelebA images during training but not StarGAN-SNG. In other words, StarGAN-JNT can leverage both datasets to improve shared low-level tasks such facial keypoint detection and segmentation. By utilizing both CelebA and RaFD, StarGAN-JNT can improve these low-level tasks, which is beneficial to learning facial expression synthesis.", "Learned role of mask vector. In this experiment, we gave a one-hot vector c\ud835\udc50citalic_c by setting the dimension of a particular facial expression (available from the second dataset, RaFD) to one. In this case, since the label associated with the second data set is explicitly given, the proper mask vector would be [0,1]01[0,1][ 0 , 1 ]. Fig.\u20097 shows the case where this proper mask vector was given and the opposite case where a wrong mask vector of [1,0]10[1,0][ 1 , 0 ] was given.When the wrong mask vector was used, StarGAN-JNT fails to synthesize facial expressions, and it manipulates the age of the input image. This is because the model ignores the facial expression label as unknown and treats the facial attribute label as valid by the mask vector. Note that since one of the facial attributes is \u2018young\u2019, the model translates the image from young to old when it takes in a zero vector as input. From this behavior, we can confirm that StarGAN properly learned the intended role of a mask vector in image-to-image translations when involving all the labels from multiple datasets altogether.", "In this paper, we proposed StarGAN, a scalable image-to-image translation model among multiple domains using a single generator and a discriminator. Besides the advantages in scalability, StarGAN generated images of higher visual quality compared to existing methods\u00a0[16, 23, 33], owing to the generalization capability behind the multi-task learning setting. In addition, the use of the proposed simple mask vector enables StarGAN to utilize multiple datasets with different sets of domain labels, thus handling all available labels from them. We hope our work to enable users to develop interesting image translation applications across multiple domains. ", "Acknowledgements. This work was mainly done while the first author did a research internship at Clova AI Research, NAVER. We thank all the researchers at NAVER, especially Donghyun Kwak, for insightful discussions. This work was partially supported by the National Research Foundation of Korea(NRF) grant funded by the Korean government (MSIP) (No. NRF2016R1C1B2015924). Jaegul Choo is the corresponding author.", "Fig.\u20098 shows an overview of StarGAN when learning from both the CelebA and RaFD datasets. As can be seen at the top of the figure, the label for CelebA contains binary attributes (Black, Blond, Brown, Male, and Young), while the label for RaFD provides information on categorical attributes (Angry, Fearful, Happy, Sad, and Disgusted). The mask vector is a two-dimensional one-hot vector which indicates whether the CelebA or RaFD label is valid.", "The network architectures of StarGAN are shown in Table 4 and 5. For the generator network, we use instance normalization in all layers except the last output layer. For the discriminator network, we use Leaky ReLU with a negative slope of 0.01. There are some notations; ndsubscript\ud835\udc5b\ud835\udc51{n}_{d}italic_n start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT: the number of domain, ncsubscript\ud835\udc5b\ud835\udc50{n}_{c}italic_n start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT: the dimension of domain labels (nd+2subscript\ud835\udc5b\ud835\udc512{n}_{d}+2italic_n start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT + 2 when training with both the CelebA and RaFD datasets, otherwise same as ndsubscript\ud835\udc5b\ud835\udc51{n}_{d}italic_n start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT), N: the number of output channels, K: kernel size, S: stride size, P: padding size, IN: instance normalization.", "Figs.\u20099,\u200910,\u200911, and\u200912 show additional images with 256\u00d7256256256256\\times 256256 \u00d7 256 resolutions generated by StarGAN. All images were generated by a single generator trained on both the CelebA and RaFD datasets. We trained StarGAN on a single NVIDIA Pascal M40 GPU for seven days."], "figure_types": {"302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/2-Figure2-1.png": "schematic", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/3-Figure3-1.png": "schematic", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/5-Figure4-1.png": "photograph(s)", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/6-Figure5-1.png": "photograph(s)", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/7-Figure6-1.png": "photograph(s)", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/7-Table1-1.png": "table", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/7-Table2-1.png": "table", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/8-Figure7-1.png": "photograph(s)", "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64/8-Table3-1.png": "table"}}, "2112.05364": {"paper_id": "paper_6", "title": "Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation", "arxiv_url": "https://arxiv.org/abs/2112.05364", "s2orc_url": "https://www.semanticscholar.org/paper/571469045c49877f78a4522f7f21e8c30e2f5c89", "all_figures_tables": {"571469045c49877f78a4522f7f21e8c30e2f5c89/16-Figure3-1.png": "Figure 3: Importance heatmap for the 6-layer 12-head model. Head 1-4 are injected with the patterns, where the highlighted boxes represent Matching Token (Green), Intra Sentence (Olive) and Positional (Blue) (-1, +1).", "571469045c49877f78a4522f7f21e8c30e2f5c89/16-Table4-1.png": "Table 4: The results for the summarization experiments under three settings with Trigram Blocking applied.", "571469045c49877f78a4522f7f21e8c30e2f5c89/16-Table5-1.png": "Table 5: Ablation study on the CNN/DM dataset (6- layer 8-head) with Trigram Blocking applied.", "571469045c49877f78a4522f7f21e8c30e2f5c89/17-Figure4-1.png": "Figure 4: PAL head importance with (a) and without (b) patterns injected, where the highlighted boxes represent Matching Token (Green), Intra Sentence (Olive) and Positional (Blue) (-1, +1)", "571469045c49877f78a4522f7f21e8c30e2f5c89/4-Figure1-1.png": "Figure 1: The overview of our proposed generic pipeline. Given (A) a trained model for a specific task, our pipeline can be divided into two main parts: (B) pattern discovery and (C) pattern injection.", "571469045c49877f78a4522f7f21e8c30e2f5c89/5-Figure2-1.png": "Figure 2: Example of Pattern Extraction in the extractive summarization case study.2 (A) We first find important heads, before (B) identifying the three interpretable patterns (highlighted in Green, Olive and Blue, respectively): (i) Matching token, (ii) Intra-Sentence, and (iii) Positional.. Finally, (C) each pattern is evaluated with the global relevance score (GR) on all of the attention heads. For the purpose of illustration, we display one attention head with significantly larger GR in for each of the three identified patterns.", "571469045c49877f78a4522f7f21e8c30e2f5c89/8-Table1-1.png": "Table 1: Results for the two tasks (four datasets) under different settings, where we report the average performance across the top-3 checkpoints. The parenthesis (e.g. 4/8) denotes the number of heads with patterns injected, while sparsity (\u03c1) is computed from the average of the 4 datasets.", "571469045c49877f78a4522f7f21e8c30e2f5c89/8-Table2-1.png": "Table 2: Ablation study on the CNN/DM dataset with the 6 Layer 8 Head transformer setting.", "571469045c49877f78a4522f7f21e8c30e2f5c89/9-Table3-1.png": "Table 3: ROUGE F-scores of PAL with pretrained models for extractive summarization. All metrics were significantly better than the baselines with a confidence level of 99% according to the Bootstrap Significance test (Dror et al., 2018)."}, "referred_figures_tables": [["571469045c49877f78a4522f7f21e8c30e2f5c89/8-Table1-1.png"]], "question_id": [6], "question": ["How was performance measured and was the performance of the human-guided knowledge distilled model significantly higher?"], "question_section": ["Introduction"], "question_trigger_sentence": ["Remarkably, the human-guided knowledge distilled model performs much better than the vanilla transformer (baseline), and can be competitive even with fine-tuned BERT distilled models, which by distilling all aspects of the model have an a priori substantial advantage over our technique that only distils attention patterns."], "question_type": ["Testing question"], "evidential_info": [[{"context": "In both scenarios, we argue the interpretability of the resulting model is improved. We provide a justification of our claim based on the Predictive, Descriptive, and Relevant (PDR) framework proposed by Murdoch et\u00a0al. (2019). Specifically, by injecting human-interpretable patterns into the model, we increase the model\u2019s descriptive accuracy by explicitly encoding useful relationships between input tokens in the attention weights while simultaneously improving the predictive accuracy in task performance. Further, the patterns are relevant for the problem since they are discovered in the human-in-the-loop process and are verified to be important for the task.", "rationale": "Interpretability is measured with the PDR framework."}, {"context": "As shown in Table\u00a01, our pattern-infused models outperform the plain transformer models for both the CNN/DM and NYT-50 datasets under all three settings(6 Layer 8 Heads, 6 Layer 12 Heads, and 6 Layer 12 Heads with BERT embeddings).Similarly for topic segmentation,results also show that the pattern-injection approach substantially outperforms the vanilla transformer across all metrics.It is worthemphasizing that the performance gain is slightly higher for summarization models. When normalized by the ROUGE scores of extractive oracle summaries555As reported by Liu and Lapata (2019), the ROUGE scores (R-1/R-2/R-L) of the oracle upper bound for CNN/DM and NYT-50 are respectively, 52.59/31.24/48.87 and 49.18/33.24/46.02., the pattern-infused summarization models achieve an average 15\\% improvement over the baselines, while the topic-segmentation models achieve a 12\\% improvement over the baselines.In-line with prior work (McCoy et\u00a0al., 2020), we also find that the performance is consistent across random seeds, where we report an extremely low standard deviation of0.03 (ROUGE) and 0.002 (F1) for extractive summarization and topic segmentation, respectively. Overall, the results from our experiments convincingly demonstrates the benefits of our approach and the generalizability of the patterns discovered by our pipeline.", "rationale": "Effectiveness and the efficiency of the proposed method are measured."}, {"context": "In summary, the key aim of our experiments was to verify consistent improvements over our own baselines under the same settings in order to probethe benefits (effectiveness and efficiency) of the discovered patterns for the task. Therefore, we do not perform extensive tuning to achieve the same results reported by Liu and Lapata (2019).", "rationale": "Summarization performance is measured by ROUGE scores; the proposed models achieve an average 15% improvement over baselines. Topic segmentation models achieve an average of 12% improvement over baselines, measured in F1 scores."}]], "composition": ["Interpretability is measured with the PDR framework. Summarization performance measured in ROUGE is 15% better. Topic segmentation performance measured in F1 is 12% better."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["1144"], "passages": ["With transformer-based models (Vaswani et\u00a0al., 2017) dominating the leaderboard for many key NLP tasks such as summarization (Liu and Lapata, 2019), topic segmentation (Lukasik et\u00a0al., 2020), and sentiment analysis Adhikari et\u00a0al. (2019), their core multi-head self-attention mechanism has also been thoroughly investigated. In particular, to explain why and how transformers work, researchers have analyzed the learnt self-attention matrices oftrained transformer models(e.g., Raganato and Tiedemann (2018); Kovaleva et\u00a0al. (2019)),with Vig and Belinkov (2019) for instance, exploring the attention patterns in BERT (Devlin et\u00a0al., 2019) and GPT-2 Radford et\u00a0al. (2019), as well as analyzing their alignment with syntax.", "Meanwhile, a parallel line of research has exploredinjecting predefined patterns into attention matrices of transformers in an attemptto reduce the run-time complexity of self-attention while maintaining competitive accuracy.This can be done by either replacing theattention weights with a fixed matrix (Raganato et\u00a0al., 2020; Tay et\u00a0al., 2021; Xiao et\u00a0al., 2020); or alternatively by guiding the attention weights through more flexible masking strategies (Mihaylov and Frank, 2019; Child et\u00a0al., 2019; Guo et\u00a0al., 2019; Li et\u00a0al., 2019; Beltagy et\u00a0al., 2020; Zaheer et\u00a0al., 2020; Bai et\u00a0al., 2021).", "In this work, we propose and test a novel human-in-the-loop pipeline that to the best of our knowledge is the first attemptto combine research on analyzing self-attention with work on injecting patterns into attention matrices.To start, human users visually explore the attention matrices of transformers to identify task-specific patterns that could be formalized as a predicate. After quantitatively evaluating the patterns on the validation set, they can be injected into attention heads of transformer models to simultaneously improve task accuracy and make the model more efficient by sparsifying the attention matrices111The implementation of our work is publicly available at: https://github.com/raymondzmc/Attention-Pattern-Exploitation. This is in contrast to previous work that mostly focuses on the trade-off between two metrics.", "In both scenarios, we argue the interpretability of the resulting model is improved. We provide a justification of our claim based on the Predictive, Descriptive, and Relevant (PDR) framework proposed by Murdoch et\u00a0al. (2019). Specifically, by injecting human-interpretable patterns into the model, we increase the model\u2019s descriptive accuracy by explicitly encoding useful relationships between input tokens in the attention weights while simultaneously improving the predictive accuracy in task performance. Further, the patterns are relevant for the problem since they are discovered in the human-in-the-loop process and are verified to be important for the task.", "In order to test the feasibility and potential benefits of our approach, we run two case studies on the tasks of extractive summarization and topic segmentation using BERT-based models,and we findthat:(i)Some of the important heads do have patterns with interpretable meaning,either lexical, local or positional. For instance, thematching token (i.e. the trend of attending to other tokens with the same id) is an important clue for the summarization model.(ii)We show that when the discoveredpatterns are injectedto the attention heads of transformer models, both the task accuracy and efficiency of the model can be significantly improved. (iii) Additionally, we also propose a strategy to improve the performance of pretrained transformer models by injecting patterns through PALs.", "In \u00a72.1 and \u00a72.2, we describe the two lines of research that our work aims to combine.\u00a72.3 summarizes recent trends on enhancing the interpretability of neural NLP models, while \u00a72.4 introduces the two NLP tasks used for our case studies.", "Various works have investigated the attention head matrices in transformers (Raganato and Tiedemann, 2018; Clark et\u00a0al., 2019; Kovaleva et\u00a0al., 2019; Zhao and Bethard, 2020; Xiao et\u00a0al., 2021), often with the aid of visualization tools (Vig, 2019; Hoover et\u00a0al., 2020; Li et\u00a0al., 2021). For examples, Vig and Belinkov (2019) visually explore attention patterns in BERT and GPT-2, analyzing their alignment with syntax. While Voita et\u00a0al. (2019) characterize the functions of the attention heads in Machine Translation (MT)models (positional, syntactic, and rare words), and evaluate the importance of those head functions. More recently, Bian et\u00a0al. (2021) find the redundancy in BERT\u2019s attention patterns to be both phase-independent (pretrainedand fine-tuned) and task-agnostic. Lastly, Huber and Carenini (2022) infer discourse structures from the attention patterns of language models(BERT and BART), and find discourse information to be consistently captured in the same heads even when fine-tuned fordifferenttasks.In this paper, we also aim to find task-specific important attention patterns, but in contrast to previous work that identifies and categorizes attention patterns, we propose a pipeline to leverage these patterns in improving models\u2019 performance and interpretability.", "We organize the related work on augmenting attention matrices into two categories.In the first category,attention weights are completely replaced with a fixed matrix. For example, Raganato et\u00a0al. (2020) use fixed positional patterns in MT models and demonstrate benefits for low-resource scenarios, while Tay et\u00a0al. (2021) replace the weights computed using dot-product self-attention with a random matrix, and report comparable performance with standard transformers. Later on, Xiao et\u00a0al. (2020) expand their work by using embedded RST-style discourse trees as fixed attention matrices and show the effectiveness of discourse-based attention matrices for extractive summarization.In contrast, in the second category of attention augmentation works, masks are applied on top of the attention weights to either inject linguistic information (Yang et\u00a0al., 2018; Mihaylov and Frank, 2019) or improve the efficiency of self-attention via fixed patterns (Child et\u00a0al., 2019; Guo et\u00a0al., 2019; Li et\u00a0al., 2019; Ainslie et\u00a0al., 2020). Just to describe a few prominent examples, Strubell et\u00a0al. (2018) use bi-affine attention to learn syntactic dependencies in attention heads, and Bai et\u00a0al. (2021) inject syntactic structures into BERT through extra attention layers. Concurrently, while Beltagy et\u00a0al. (2020) use diagonal/vertical/horizontal patterns to respectively model local and global context, Zaheer et\u00a0al. (2020) add patterns randomly by drawing inspiration from graph theory.In comparison, while in all previous works the designing of pre-defined patterns requires extensive trial and error, and only improve upon either the accuracy or efficiency at the expense of the other, we explore a strategy of discoveringand assessing important attention patterns interactively in this paper.Not only do the discoveredpatterns help improve performance in terms of both accuracy and efficiency, they also reveal valuable insights regarding the internal workings of pretrained language models.", "In the context of Machine Learning, interpretability can be defined as the description of the internals of a model in a way that is understandable to humans (Gilpin et\u00a0al., 2018).With the rise of deep learning, various techniques have been proposed to interpret the inner workings of neural NLP models. For example, probing classifiers are often used for finding linguistic or knowledge information learned by neural networks (Conneau et\u00a0al., 2018; Tenney et\u00a0al., 2019; Pimentel et\u00a0al., 2020; Voita and Titov, 2020; Hou and Sachan, 2021; Aghazadeh et\u00a0al., 2022), while behaviour testing aims at understanding how models behave through inferences under different controlled settings (McCoy et\u00a0al., 2019; Ross and Pavlick, 2019; Ribeiro et\u00a0al., 2020; Koh et\u00a0al., 2021; Goel et\u00a0al., 2021). In contrast, our work is an example of making interpretability an inherent attribute of the neural models (e.g. Chen and Ji (2020); Hu et\u00a0al. (2021)),with human-distinguishable patterns revealing insights regarding a subset of parameters in the model.", "Extractive summarization is the taskof picking the most representative sentences as the summary for the given document(s). Current state-of-the-art models, which are mostly based on large-scale pretrained language models Liu and Lapata (2019); Zhong et\u00a0al. (2020); Jia et\u00a0al. (2020); Ruan et\u00a0al. (2022), can deliver good performance, but why and how such models work so well still remain an open question. In our case study, we adoptthe popular BERTSum(Liu and Lapata, 2019).", "Topic segmentation is the task of breaking stretches of running text into smaller topical-coherent segments consisting of one or more sentences addressing a common topic.Recently, more research work frames the taskin the supervised learning paradigm and uses neural models such as Bi-LSTMs (Koshorek et\u00a0al., 2018; Xing et\u00a0al., 2020) and transformer (Glavas and Somasundaran, 2020; Lo et\u00a0al., 2021) as the backbone, due to the availability of large-scale labeled benchmarks sampled from Wikipedia. These proposed neural topic segmentation models achieve state-of-the-art performance on monologue text by formulating the problem as a sequence labeling task,where the predicted label of each sentence indicates whether or not it is the end of a segment.In our case study, we adopt Cross-Segment BERT (Lukasik et\u00a0al., 2020).", "As an overview, we firstbriefly describe the proposed pipeline (Figure\u00a01). Specifically, given a trained model,users areasked to first discoverimportant patterns usingthe visual interface\u00a0(Li et\u00a0al., 2021)by following three steps: Step 1 (\u00a73.1.1): Estimate the importance scores for all the heads on the validation set, and find important heads that stand out.Step 2 (\u00a73.1.2): Discoverrelevant patterns in the important heads, using criteria described in \u00a73.1.2.Step 3 (\u00a73.1.3): Evaluate and validate the patterns to confirm their global relevance.", "Once the important patterns are identified, there are two common approaches(i.e. fixing and masking)to injectthem as constraints to the attention matrices in the transformer-based neural models (see \u00a73.2). The pipeline alsoenables two scenarios,in which injectingthe patterns can be beneficial:the first one is to train a new model with the patterns injected, while the second one is to enhance the original model.", "In this section we provide details of the three steps for discoveringpatterns from the attention heads.The three steps are illustrated in Figure\u00a01 (B).", "Although the multi-head self attention mechanism in transformers allows the model to learn multiple types of relationships between input representations across a single hidden layer, the importance of the individual attention heads can vary depending on the downstream tasks.In practice, we propose the use of scalable gradient-based methods (Michel et\u00a0al., 2019; Voita et\u00a0al., 2019; Molchanov et\u00a0al., 2019) for an efficient estimation of head importance, and take the top-K heads at each layer to find important patterns for the task \u00a0(\u00a73.1.2). Note that K\ud835\udc3eKitalic_K can be adjusted based on the availability of human users and the size of the model.", "Once the the most important heads are identified, their attention distributions are inspected to look for patterns.", "We define an attention pattern to be interpretable iff it can be modeled as a predicate P\ud835\udc43Pitalic_P between any pair of input tokens (xi,xj)subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57(x_{i},x_{j})( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ).For instance, the positional pattern \u2018preceding token\u2019 would be true if xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT appears before xjsubscript\ud835\udc65\ud835\udc57x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. Candidate patterns can be discovered following two criteria:1) they are beneficial for the downstream task;2) they occur consistently among relevant tokens.", "With a pattern discoveredin \u00a73.1.2, this step confirms the pattern\u2019s global relevance by empirically measuring the proportion of attention values aligning with the pattern. For each attention head, the associated predicate is evaluated over the entire validation set to ensure the pattern is not appearing by chance on the certain data that the user happen to look at.", "Specifically, we define the global relevance (GR) of a pattern P\ud835\udc43Pitalic_P for a head h\u210ehitalic_h as follows:GR(P,h)=1|X|\u2211x\u2208X\u2211i|x|\u2211j|x|\u03b1i,j(x,h)\u22c5\ud835\udfd9P(xi,xj)|x|GR\ud835\udc43\u210e1\ud835\udc4bsubscript\ud835\udc65\ud835\udc4bsuperscriptsubscript\ud835\udc56\ud835\udc65superscriptsubscript\ud835\udc57\ud835\udc65\u22c5superscriptsubscript\ud835\udefc\ud835\udc56\ud835\udc57\ud835\udc65\u210esubscript1\ud835\udc43subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\ud835\udc65\\textrm{GR}(P,h)=\\frac{1}{|X|}\\sum_{x\\in X}\\frac{\\sum_{i}^{|x|}\\sum_{j}^{|x|}\\alpha_{i,j}^{(x,h)}\\cdot\\mathbbm{1}_{P(x_{i},x_{j})}}{|x|}GR ( italic_P , italic_h ) = divide start_ARG 1 end_ARG start_ARG | italic_X | end_ARG \u2211 start_POSTSUBSCRIPT italic_x \u2208 italic_X end_POSTSUBSCRIPT divide start_ARG \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_x | end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_x | end_POSTSUPERSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_x , italic_h ) end_POSTSUPERSCRIPT \u22c5 blackboard_1 start_POSTSUBSCRIPT italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT end_ARG start_ARG | italic_x | end_ARG(1)where the attention value from the token xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to xjsubscript\ud835\udc65\ud835\udc57x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT on the head h\u210ehitalic_h for an input sample x\ud835\udc65xitalic_x, denoted as \u03b1i,j(x,h)superscriptsubscript\ud835\udefc\ud835\udc56\ud835\udc57\ud835\udc65\u210e\\alpha_{i,j}^{(x,h)}italic_\u03b1 start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_x , italic_h ) end_POSTSUPERSCRIPT, is aggregated if and only if P(xi,xj)\ud835\udc43subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57P(x_{i},x_{j})italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) holds. To validate a pattern\u2019s generality, the relevance is averaged over the validation set X\ud835\udc4bXitalic_X.", "As illustrated in Figure\u00a01 (C), after extracting the patterns following the three steps in \u00a73.1, we propose to injectthe patterns into attention matriceswith two methods (\u00a73.2.1), and discuss two practical scenarios (\u00a73.2.2) where they can be beneficial for the downstream tasks.", "In this work, we injectthe discovered patternsby either fixing or masking the attention weights prior to the softmax function.For fixed attention weights, the attention logits in the scaled-dot-product attention is replaced with a fixed (possibly input dependent) matrix such that:FixAttn(V,X)=\u03c3(F(P)(X))VFixAttn\ud835\udc49\ud835\udc4b\ud835\udf0esuperscript\ud835\udc39\ud835\udc43\ud835\udc4b\ud835\udc49\\textrm{FixAttn}(V,X)=\\sigma(F^{(P)}(X))VFixAttn ( italic_V , italic_X ) = italic_\u03c3 ( italic_F start_POSTSUPERSCRIPT ( italic_P ) end_POSTSUPERSCRIPT ( italic_X ) ) italic_V(2)where \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 is the softmax operation, V\ud835\udc49Vitalic_V is the value vectors, and F(X)\u2208[0,1]\ud835\udc39\ud835\udc4b01F(X)\\in[0,1]italic_F ( italic_X ) \u2208 [ 0 , 1 ] computes a binary matrix from the input sequence X\ud835\udc4bXitalic_X based on the predicated P\ud835\udc43Pitalic_P for the specific pattern. Similarly, a pattern can also be injectedby casting a mask over the attention weights computed from the key and query vectors, as:MaskAttn(Q,K,V,X)=\u03c3(M(P)(X)+QKT)VMaskAttn\ud835\udc44\ud835\udc3e\ud835\udc49\ud835\udc4b\ud835\udf0esuperscript\ud835\udc40\ud835\udc43\ud835\udc4b\ud835\udc44superscript\ud835\udc3e\ud835\udc47\ud835\udc49\\textrm{MaskAttn}(Q,K,V,X)=\\sigma(M^{(P)}(X)+QK^{T})VMaskAttn ( italic_Q , italic_K , italic_V , italic_X ) = italic_\u03c3 ( italic_M start_POSTSUPERSCRIPT ( italic_P ) end_POSTSUPERSCRIPT ( italic_X ) + italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) italic_V(3)where M(X)\u2208[0,\u2212\u221e)\ud835\udc40\ud835\udc4b0M(X)\\in[0,-\\infty)italic_M ( italic_X ) \u2208 [ 0 , - \u221e ) computes the desired behaviour in the same fashion as F(X)\ud835\udc39\ud835\udc4bF(X)italic_F ( italic_X ), and is added to the attention logits to approximate the multiplication of the attention distribution by a weight.", "Although the two methods are very similar with respect to the improvement they contribute (see \u00a74), masking allows more flexibility and is generally used for patterns with a large number of applicable tokens, while fixing is more rigid and better suited for a small number of applicable tokens.", "In practice, patterns can be injectedin at least two scenarios: (i) injecting patterns directly into the attention heads of transformer-based models, and (ii) injecting patterns into pretrained transformer models using techniques such as the Projected Attention Layers (Stickland and Murray, 2019). We conduct case studies for these two scenarios in \u00a74.", "In this section,we demonstrate the effectiveness of our pipeline in two NLP tasks (extractive summarization and topic segmentation) and discuss our findings in detail.", "We adopt the popular BERTSum (Liu and Lapata, 2019) for extractive summarization. With the contextualized representation from BERT, the model uses a binary classifier to predict whether each sentence belongs in the summary. We train the model on the CNN/DM dataset (See et\u00a0al., 2017), and use ROUGE (Lin, 2004) as the evaluation metric.", "We adopt Cross-Segment BERT (Lukasik et\u00a0al., 2020) for topic segmentation, where a candidate segment boundary is first represented by its left and right context, and then passed through a binary classifier to predict whether the candidate is a topical segment boundary. The model is trained on the WikiSection dataset (Arnold et\u00a0al., 2019), and the F1-score is used as evaluation metric for validation.", "Using the two models from \u00a74.1, as we discoverthat similar attention patterns exist in the important heads for both tasks, the two case studies are presented together. Without loss of generality, we will use extractive summarization as the running example task (footnote\u00a03) to illustrate the process of pattern discovery. We also apply the same process to topic segmentation.", "We adapt the Taylor expansion method (Molchanov et\u00a0al., 2019) as a proxy score for the head importance estimation.Following Li et\u00a0al. (2021), we use the first-order expansion to avoid the overhead from computing the Hessian, where the gradient w.r.t. the validation loss is summed over all parameters of an attention head to estimate its importance.", "Theimportance score heatmap of all heads is visualized in footnote\u00a03 (A), revealing that head importance is not uniformly distributed, i.e. a small number of heads play a dominant role for the summarization task,as observed in Michel et\u00a0al. (2019).", "To discovertask-specific patterns,we analyze the top-3 most important heads of each layer, and look forhuman-interpretable relationships encoded in the attention weights. In practice, we use the instance-level interactions provided by the visual framework (Li et\u00a0al., 2021), and randomly select 5 validation examples per task for our analysis.The entire process takesless than one hour to complete for each task, where we manually examinethe attention weights for less than half of the tokens for each example.It is worth noting that detailed analysis regarding the trade-off between human costandpattern recallwould require extensive user studies beyond the scope of this work.", "After discovering patterns,we assess the global relevance of each patterns on the validation set, wherethe pattern is kept only if the corresponding predicate P\ud835\udc43Pitalic_P exists in at least one significantly relevant head.In our case studies, we use the 3-sigma ruleto determine the significance of a pattern. Specifically, patterns with at least one head over 3 standard deviations above the GR mean (over all the heads) are kept for further applications.", "After verifying on the validation set, we discoverthree patternsconsistently existing in both tasks (over 50% of important heads).This suggests that important patterns are generalizable across multiple NLP tasks, whichis consistent with the findings in Bian et\u00a0al. (2021). Further analysis also shows that the attention patterns are consistent after fine-tuning, where we report an average Jensen-Shannon Divergenceof 0.010.010.010.01 between the attention distributions of BERTSum across 3 random seeds.We hope our findings provide motivation for the in-depth study of pattern importance in different NLP tasks. Lastly, while it may be argued that this step of the pipeline can be automated by directly evaluating the importance and relevance of predefined patterns (e.g. syntax, discourse) based on intuitions, as indicated below, our interactive approach allows the discovery of interpretable patterns which otherwise would be hard to define due to the infinite search space of possible patterns. Next, we describe the three discovered patterns in detail.", "This pattern describes the \u201cattending to matching tokens\u201d behaviour, wherethe attention value \u03b1i,jhsuperscriptsubscript\ud835\udefc\ud835\udc56\ud835\udc57\u210e\\alpha_{i,j}^{h}italic_\u03b1 start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT between input tokens xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and xjsubscript\ud835\udc65\ud835\udc57x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT on the head h\u210ehitalic_h is high whenever xi=xjsubscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57x_{i}=x_{j}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. For example, as shown in footnote\u00a03 (i), the token \"photo\" mostly attends to other appearances of the token \"photo\" in the input sequence. To evaluate whether this pattern has a large global relevance for any head, we only consider tokens that appear at least twice within a single documents, and compute GR (Eq. 1), in which P(xi,xj)\ud835\udc43subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57P(x_{i},x_{j})italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) holds if and only if xi=xjsubscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57x_{i}=x_{j}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, i.e. \ud835\udfd9P(xi,xj)=(\ud835\udfd9freq(xi)>1)\u00d7(\ud835\udfd9xi=xj)subscript1\ud835\udc43subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57subscript1freqsubscript\ud835\udc65\ud835\udc561subscript1subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57\\mathbbm{1}_{P(x_{i},x_{j})}=(\\mathbbm{1}_{\\textrm{freq}(x_{i})>1})\\times(\\mathbbm{1}_{x_{i}=x_{j}})blackboard_1 start_POSTSUBSCRIPT italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT = ( blackboard_1 start_POSTSUBSCRIPT freq ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) > 1 end_POSTSUBSCRIPT ) \u00d7 ( blackboard_1 start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT ).", "The evaluation results show that there are several heads for which the matching token pattern has high global relevance(See the Green box in footnote\u00a03).Interestingly, these heads are more prominent (in the importance heatmap) for the extractive summarization task, suggesting this pattern is especially important for summarization models during inference.", "This pattern describes the behaviour of only attending to tokens within a text span.For summarization, these heads will focus on attending tokens within the same sentence (footnote\u00a03 (ii)). Similarly, the same heads in topic segmentation models will focus on attending tokens within the same context (left or right).To evaluate this pattern, GR is computed with P(xi,xj)\ud835\udc43subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57P(x_{i},x_{j})italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) holding iff xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and xjsubscript\ud835\udc65\ud835\udc57x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT occur within the same text span. footnote\u00a03\u00a0(C) reveals that this pattern appears more frequently in the mid to upper layers of the transformer encoder.", "Similar toKovaleva et\u00a0al. (2019), we also observe \u201cpositional heads\u201d, which focus specifically on either the preceding or following tokens, i.e., either \u03b1i,i\u22121hsuperscriptsubscript\ud835\udefc\ud835\udc56\ud835\udc561\u210e\\alpha_{i,i-1}^{h}italic_\u03b1 start_POSTSUBSCRIPT italic_i , italic_i - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT or \u03b1i,i+1hsuperscriptsubscript\ud835\udefc\ud835\udc56\ud835\udc561\u210e\\alpha_{i,i+1}^{h}italic_\u03b1 start_POSTSUBSCRIPT italic_i , italic_i + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT have high values (footnote\u00a03 (iii)). To evaluate this pattern, GR is computed with P(xi,xj)\ud835\udc43subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57P(x_{i},x_{j})italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) holding iff j=i\u22121\ud835\udc57\ud835\udc561j=i-1italic_j = italic_i - 1 for preceding postional heads and j=i+1\ud835\udc57\ud835\udc561j=i+1italic_j = italic_i + 1 for succeeding positional heads. The pattern is verified to exist in the lower layers of the encoder, shown in the blue boxes of footnote\u00a03 (C).", "In addition to the three patterns mentioned above, we also observe heads that focus on attending to special tokens (e.g. [CLS], [SEP]) or punctuations (e.g. periods). However, we find that attention heads with this behaviour are generally less important for the task (outside top-3), and therefore omitted them from the next step of our pipeline.", "On the other hand, we also find uninterpretable attention patterns in some of the important heads of each layer. As hypothesized by previous works (Clark et\u00a0al., 2019), these attention heads might be performing complex linguistic operations in combination with other heads. We leave the verification, interpretation and the efficient injection of these patterns into the models as a direction for future work.", "After uncovering potentially important patterns and confirming their relevance, we injectthem to transformer-based modelsfor thetask of summarizationand topic segmentationthrough masking and fixing the attention weights.While we only perform the pattern discoveryprocess on the CNN/DM and WikiSection datasets, we injectthe discovered patterns to two other datasets (NYT-50 (Sandhaus, 2008) for summarization and Wiki-727K (Arnold et\u00a0al., 2019) for topic segmentation) to demonstrate that our discovered patterns are generalizable in \u201ccross-dataset\u201d settings444Results shown in Sec. 4 are without the Trigram Blocking trick, and more results with it are in Appendix\u00a04.", "The patterns identified from our analysis can be injectedinto an attention head through masking or fixing its corresponding attention weight matrix. Specifically, for the matching token pattern, we apply an attention mask which enforces that when a token appears more than once in the document, it should attend only to other occurrences of itself:Mi,j(m)={1(xi=xj)\u2228(freq(xi)=1)0otherwisesubscriptsuperscript\ud835\udc40\ud835\udc5a\ud835\udc56\ud835\udc57cases1subscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc57freqsubscript\ud835\udc65\ud835\udc5610otherwiseM^{(m)}_{i,j}=\\begin{cases}1&(x_{i}=x_{j})\\lor(\\textrm{freq}(x_{i})=1)\\\\0&\\textrm{otherwise}\\end{cases}italic_M start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = { start_ROW start_CELL 1 end_CELL start_CELL ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) \u2228 ( freq ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = 1 ) end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL otherwise end_CELL end_ROW(4)where the constraint is removed for tokens occurring only once in the document.", "Similarly, for intra-sentence/intra-context attention, the attention mask specifies that only tokens within the same boundary can attend to each others, where:Mi,j(s)={1SameBoundary(xi,xj)0otherwisesubscriptsuperscript\ud835\udc40\ud835\udc60\ud835\udc56\ud835\udc57cases1SameBoundarysubscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc570otherwiseM^{(s)}_{i,j}=\\begin{cases}1&\\textrm{SameBoundary}(x_{i},x_{j})\\\\0&\\textrm{otherwise}\\end{cases}italic_M start_POSTSUPERSCRIPT ( italic_s ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = { start_ROW start_CELL 1 end_CELL start_CELL SameBoundary ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL otherwise end_CELL end_ROW(5)", "Lastly, we use a fixed attention matrix to encode the two positional patterns with:Fi,j(\u22121)={1j=i\u221210otherwisesubscriptsuperscript\ud835\udc391\ud835\udc56\ud835\udc57cases1\ud835\udc57\ud835\udc5610otherwiseF^{(-1)}_{i,j}=\\begin{cases}1&j=i-1\\\\0&\\textrm{otherwise}\\end{cases}italic_F start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = { start_ROW start_CELL 1 end_CELL start_CELL italic_j = italic_i - 1 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL otherwise end_CELL end_ROW(6)", "With Fi,j(+1)subscriptsuperscript\ud835\udc391\ud835\udc56\ud835\udc57F^{(+1)}_{i,j}italic_F start_POSTSUPERSCRIPT ( + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT being the same, but equal to 1 for j=i+1\ud835\udc57\ud835\udc561j=i+1italic_j = italic_i + 1. We use fixed attention matrices for these patterns to save computational overhead since it has the same effect as applying the mask (each row is a one-hot vector). This is similar to the method proposed by Raganato et\u00a0al. (2020), but we only fix for the preceding and succeedingtoken patterns.", "In the first round of experiments, we injectthe four patternson smaller transformer models to demonstrate their effectivenesson both tasks. Since the goal of these experiments is to assess the benefits brought by these patterns, we do not perform extensive hyper-parameter search when injectingthese patterns (e.g. on which layer, etc.).", "Under both settings, each of the four patterns (including two positional patterns) is injectedin a separate attention head across all layers in the model. Motivated by studies on the trade-off between sparsity ratio and task performance, we adopt the sparsity ratio used by previous works (Shi et\u00a0al., 2021; Wang et\u00a0al., 2022): \u03c1=1\u2212|M|/N2\ud835\udf0c1\ud835\udc40superscript\ud835\udc412\\rho=1-|M|/N^{2}italic_\u03c1 = 1 - | italic_M | / italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, where |M|\ud835\udc40|M|| italic_M | denotes the number of non-zero elements in the attention mask, and N\ud835\udc41Nitalic_N denotes the length of the example. Given the sparsity \u03c1\ud835\udf0c\\rhoitalic_\u03c1, the complexity of self-attention is thus reduced to \ud835\udcaa((1\u2212\u03c1)n2)\ud835\udcaa1\ud835\udf0csuperscript\ud835\udc5b2\\mathcal{O}\\big{(}(1-\\rho)n^{2}\\big{)}caligraphic_O ( ( 1 - italic_\u03c1 ) italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) (Shi et\u00a0al., 2021).To investigate how the sparsity ratio affects the performance of our pattern-infused models, we experiment with different number of heads to injectour patterns, where the sparsity ratio increases along with the number of heads (with patterns).", "As shown in Table\u00a01, our pattern-infused models outperform the plain transformer models for both the CNN/DM and NYT-50 datasets under all three settings(6 Layer 8 Heads, 6 Layer 12 Heads, and 6 Layer 12 Heads with BERT embeddings).Similarly for topic segmentation,results also show that the pattern-injection approach substantially outperforms the vanilla transformer across all metrics.It is worthemphasizing that the performance gain is slightly higher for summarization models. When normalized by the ROUGE scores of extractive oracle summaries555As reported by Liu and Lapata (2019), the ROUGE scores (R-1/R-2/R-L) of the oracle upper bound for CNN/DM and NYT-50 are respectively, 52.59/31.24/48.87 and 49.18/33.24/46.02., the pattern-infused summarization models achieve an average 15%percent1515\\%15 % improvement over the baselines, while the topic-segmentation models achieve a 12%percent1212\\%12 % improvement over the baselines.In-line with prior work (McCoy et\u00a0al., 2020), we also find that the performance is consistent across random seeds, where we report an extremely low standard deviation of0.03 (ROUGE) and 0.002 (F1) for extractive summarization and topic segmentation, respectively. Overall, the results from our experiments convincingly demonstrates the benefits of our approach and the generalizability of the patterns discovered by our pipeline.", "In addition, while a higher sparsity ratio causes a slight decrease in performance under some scenarios, we find that even with a ratio of 0.860.860.860.86,our model still significantly outperforms the vanilla transformer across all settings. This is in contrast to the findings by previous work (Child et\u00a0al., 2019; Guo et\u00a0al., 2019; Li et\u00a0al., 2019; Beltagy et\u00a0al., 2020; Zaheer et\u00a0al., 2020; Shi et\u00a0al., 2021), where the high sparsity ratio from fixed patterns often results in performance degradation from the vanilla transformer. These findings from our work provide crucial insights for designing more energy efficient models in the future.", "Overall, with the discovered patterns injected,our models are arguably more interpretable thanplain transformers on both tasks, as we know with certaintythe information encoded in each masked/fixed attention heads. To further justify our claim of interpretability, the attention headswith patterns injectedtend to have higher importance scores than the other heads666An illustrative example is shown in Appendix\u00a0C.1, suggesting that such patterns are effectively leveraged by the model.", "To study the contribution of individual patterns, we perform an ablation study by injectingall combinations of patterns on CNN/DM using the transformer model with 6 layers and 8 heads777Ablation study results for topic segmentation (WikiSection) can be found in Appendix E.From Table\u00a02,we observe that injectingmatching token and intra-sentence together achieves the strongest improvement in accuracyamong all combinations, only slightly lower than injectingall patterns. Meanwhile, the gains from injectingpatterns separately are only marginal. One intriguing explanation is that these two patterns allows the model to learn sentence-level features based on term frequency (plausibly similar to TF-IDF (Jones, 1972)), where higher scores are assigned to sentences containing frequently appearing tokens. Additionally, although injectingonly the positional patterns causes the performance to degrade, it works better when combined with the two other patterns.Wehypothesize thatpositional patterns need to be combined with patterns with more global context in orderto be more effectively utilized.", "We then experiment with injecting the patterns back into the pre-trained transformer encoder.In particular, we injectthem through additional attention heads in the form of a Projected Attention Layer (PAL) (Stickland and Murray, 2019), along with the parameters of the original model. Details regarding PALs are described in Appendix A.", "The hidden size of our PALs is 256256256256, which consists of 4444 additional attention heads (dk=dv=dq=64subscript\ud835\udc51\ud835\udc58subscript\ud835\udc51\ud835\udc63subscript\ud835\udc51\ud835\udc5e64d_{k}=d_{v}=d_{q}=64italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = 64). PAL is added in each of the 12 BERT layers, where our patterns are injectedin the 4 PAL attention heads. To ensure the changes in performance are due to the patterns rather than the additional parameters, we also compare against adding PAL without injectingthe patterns.", "Results in Table\u00a03 indicate that injectingthe patterns in PAL (+PAL+Patterns) surprisingly improves BERTSum\u2019s performance on both datasets, where the performance gains on the NYT-50 are similar (or even slightly better) than on the in-domain CNN/DM dataset, supporting the generality of the discovered patterns.Additionally, as it was the case for thetransformerswith patterns injected, visualizing the head importance scores reveals that the PAL heads with patterns injectedare significantly more important (by two orders of magnitude) than the PAL heads without patterns injected888An illustrative example is shown in Appendix\u00a0C.2, indicating that the interpretable patterns are important features during model inference.", "In summary, the key aim of our experiments was to verify consistent improvements over our own baselines under the same settings in order to probethe benefits (effectiveness and efficiency) of the discovered patterns for the task. Therefore, we do not perform extensive tuning to achieve the same results reported by Liu and Lapata (2019).", "In this paper, we propose a generic human-in-the-loop pipeline, which combinestwo popular research directions, where the findings from an analysis of the multi-head self-attention mechanism in transformers can be utilized to create more accurate and interpretable transformer models. A humananalyzes the attention heads of a task-specific model, discovers and verifies potentially meaningful patterns, and injects them into the attention heads of models.By running a case study on two NLP tasks, we show the effectiveness of our pipeline. We do discovermeaningful patterns in some important heads, and the relationships encoded in the patterns help us understand the features used by the model for both tasks.Furthermore, by injectingthe patterns into the smaller models and the original model, the performance and interpretability get improved in both cases.", "As future work, we plan to apply our pipeline to other NLP tasks (e.g. language modeling, abstractive summarization) and explore and verify whether the important patterns from one task can be transferable to another task. Similarly, we also plan to apply our pipeline to different model variants to examine and compare the patterns encoded in the attention weights.In the long term, our pipeline could be naturally automated by replacing the pattern discovery step with evaluating predefined linguistic patterns. However, assessing the efficiency gains from injecting such patterns (requiring ground-truth annotations) would require more in-depth studies beyond the scope of this paper.Finally, since human factors are an important aspect of interpretability, we plan to conduct extensive user studies across different NLP tasks and model sizes to examine the trade-off between human-cost and the coverage of discovered patterns."], "figure_types": {"571469045c49877f78a4522f7f21e8c30e2f5c89/16-Figure3-1.png": "table", "571469045c49877f78a4522f7f21e8c30e2f5c89/16-Table4-1.png": "table", "571469045c49877f78a4522f7f21e8c30e2f5c89/16-Table5-1.png": "table", "571469045c49877f78a4522f7f21e8c30e2f5c89/17-Figure4-1.png": "plot", "571469045c49877f78a4522f7f21e8c30e2f5c89/4-Figure1-1.png": "schematic", "571469045c49877f78a4522f7f21e8c30e2f5c89/5-Figure2-1.png": "schematic", "571469045c49877f78a4522f7f21e8c30e2f5c89/8-Table1-1.png": "table", "571469045c49877f78a4522f7f21e8c30e2f5c89/8-Table2-1.png": "table", "571469045c49877f78a4522f7f21e8c30e2f5c89/9-Table3-1.png": "table"}}, "1508.06615": {"paper_id": "paper_61", "title": "Character-Aware Neural Language Models", "arxiv_url": "https://arxiv.org/abs/1508.06615", "s2orc_url": "https://www.semanticscholar.org/paper/891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "all_figures_tables": {"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/2-Figure1-1.png": "Figure 1: Architecture of our language model applied to an example sentence. Best viewed in color. Here the model takes absurdity as the current input and combines it with the history (as represented by the hidden state) to predict the next word, is. First layer performs a lookup of character embeddings (of dimension four) and stacks them to form the matrix Ck. Then convolution operations are applied betweenCk and multiple filter matrices. Note that in the above example we have twelve filters\u2014three filters of width two (blue), four filters of width three (yellow), and five filters of width four (red). A max-over-time pooling operation is applied to obtain a fixed-dimensional representation of the word, which is given to the highway network. The highway network\u2019s output is used as the input to a multi-layer LSTM. Finally, an affine transformation followed by a softmax is applied over the hidden representation of the LSTM to obtain the distribution over the next word. Cross entropy loss between the (predicted) distribution over next word and the actual next word is minimized. Element-wise addition, multiplication, and sigmoid operators are depicted in circles, and affine transformations (plus nonlinearities where appropriate) are represented by solid arrows.", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/4-Table1-1.png": "Table 1: Corpus statistics. |V| =word vocabulary size; |C| = character vocabulary size; T = number of tokens in training set. The small English data is from the Penn Treebank and the Arabic data is from the News-Commentary corpus. The rest are from the 2013 ACL Workshop on Machine Translation. |C| is large because of (rarely occurring) special characters.", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/4-Table2-1.png": "Table 2: Architecture of the small and large models. d = dimensionality of character embeddings; w = filter widths; h = number of filter matrices, as a function of filter width (so the large model has filters of width [1, 2, 3, 4, 5, 6, 7] of size [50, 100, 150, 200, 200, 200, 200] for a total of 1100 filters); f, g = nonlinearity functions; l = number of layers; m = number of hidden units.", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/5-Table3-1.png": "Table 3: Performance of our model versus other neural language models on the English Penn Treebank test set. PPL refers to perplexity (lower is better) and size refers to the approximate number of parameters in the model. KN-5 is a Kneser-Ney 5-gram language model which serves as a non-neural baseline. \u2020For these models the authors did not explicitly state the number of parameters, and hence sizes shown here are estimates based on our understanding of their papers or private correspondence with the respective authors.", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/5-Table4-1.png": "Table 4: Test set perplexities for DATA-S. First two rows are from Botha (2014) (except on Arabic where we trained our own KN-4 model) while the last six are from this paper. KN-4 is a Kneser-Ney 4-gram language model, and MLBL is the best performing morphological logbilinear model from Botha (2014). Small/Large refer to model size (see Table 2), and Word/Morph/Char are models with words/morphemes/characters as inputs respectively.", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/6-Table5-1.png": "Table 5: Test set perplexities on DATA-L. First two rows are from Botha (2014), while the last three rows are from the small LSTM models described in the paper. KN4 is a Kneser-Ney 4-gram language model, and MLBL is the best performing morphological logbilinear model from Botha (2014). Word/Morph/Char are models with words/morphemes/characters as inputs respectively.", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/7-Figure2-1.png": "Figure 2: Plot of character n-gram representations via PCA for English. Colors correspond to: prefixes (red), suffixes (blue), hyphenated (orange), and all others (grey). Prefixes refer to character n-grams which start with the start-of-word character. Suffixes likewise refer to character n-grams which end with the end-of-word character.", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/7-Table6-1.png": "Table 6: Nearest neighbor words (based on cosine similarity) of word representations from the large word-level and characterlevel (before and after highway layers) models trained on the PTB. Last three words are OOV words, and therefore they do not have representations in the word-level model.", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/7-Table7-1.png": "Table 7: Perplexity on the Penn Treebank for small/large models trained with/without highway layers.", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/7-Table8-1.png": "Table 8: Perplexity reductions by going from small wordlevel to character-level models based on different corpus/vocabulary sizes on German (DE). |V| is the vocabulary size and T is the number of tokens in the training set. The full vocabulary of the 1m dataset was less than 100k and hence that scenario is unavailable."}, "referred_figures_tables": [["891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/5-Table3-1.png"], ["891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/5-Table3-1.png"]], "question_id": [2, 4], "question": ["Does large model always shows better performance than small model?", "How many times better performance is the model than the baseline?"], "question_section": ["Results", "Introduction"], "question_trigger_sentence": ["Architecture of the small (LSTM-Char-Small) and large (LSTM-Char-Large) models is summarized in Table 2. As another baseline, we also train two comparable LSTM models that use word embeddings only (LSTM-Word-Small, LSTM-Word-Large). LSTM-Word-Small uses 200 hidden units and LSTM-WordLarge uses 650 hidden units.", "on morphologically rich languages (Arabic, Czech, French, German, Spanish, and Russian), our model outperforms various baselines (Kneser-Ney, wordlevel/morpheme-level LSTM), again with fewer parameters."], "question_type": ["Shallow question", "Testing question"], "evidential_info": [[{"context": "As can be seen from Table 3, our large model is on par with the existing state-of-the-art (Zaremba et al. 2014), despite having approximately 60% fewer parameters. Our small model signi\ufb01cantly outperforms other NLMs of sim- ilar size, even though it is penalized by the fact that the dataset already has OOV words replaced with < unk > (other models are purely word-level models). While lower perplex- ities have been reported with model ensembles (Mikolov and Zweig 2012), we do not include them here as they are not comparable to the current work.", "rationale": "They said that their large model is 60% smaller then baseline. However, their model is on par with them."}], [{"context": "As can be seen from Table 3, our large model is on par with the existing state-of-the-art (Zaremba et al. 2014), despite having approximately 60% fewer parameters. Our small model signi\ufb01cantly outperforms other NLMs of sim- ilar size, even though it is penalized by the fact that the dataset already has OOV words replaced with < unk > (other models are purely word-level models). While lower perplex- ities have been reported with model ensembles (Mikolov and Zweig 2012), we do not include them here as they are not comparable to the current work.", "rationale": "Their model has the almost same performance as baseline models."}]], "composition": ["No.", "almost same"], "Is_figure_in_evidence": [false, false], "Is_table_in_evidence": [true, true], "question_key": ["1162", "1164"], "passages": [], "figure_types": {"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/2-Figure1-1.png": "schematic", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/4-Table1-1.png": "table", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/4-Table2-1.png": "table", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/5-Table3-1.png": "table", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/5-Table4-1.png": "table", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/6-Table5-1.png": "table", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/7-Figure2-1.png": "plot", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/7-Table6-1.png": "table", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/7-Table7-1.png": "table", "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7/7-Table8-1.png": "table"}}, "1611.01603": {"paper_id": "paper_62", "title": "Bidirectional Attention Flow for Machine Comprehension", "arxiv_url": "https://arxiv.org/abs/1611.01603", "s2orc_url": "https://www.semanticscholar.org/paper/3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "all_figures_tables": {"3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/12-Table4-1.png": "Table 4: Error analysis on SQuAD. We randomly selected EM-incorrect answers and classified them into 6 different categories. Only relevant sentence(s) from the context shown for brevity.", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/13-Table5-1.png": "Table 5: Variations of similarity function \u03b1 (Equation 1) and fusion function \u03b2 (Equation 2) and their performance on the dev data of SQuAD. See Appendix B for the details of each variation.", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/2-Figure1-1.png": "Figure 1: BiDirectional Attention Flow Model (best viewed in color)", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/6-Table1-1.png": "Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al. (2016)a, Yu et al. (2016)b, Yang et al. (2016)c, Wang & Jiang (2016)d, IBM Watsone (unpublished), Xiong et al. (2016b)f , and Microsoft Research Asiag (unpublished) on the SQuAD test set. Results shown here reflect the SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single runs.", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/7-Figure2-1.png": "Figure 2: (a) t-SNE visualizations of the months names embedded in the two feature spaces. The contextual embedding layer is able to distinguish the two usages of the word May using context from the surrounding text. (b) Venn diagram of the questions answered correctly by our model and the more traditional baseline (Rajpurkar et al., 2016). (c) Correctly answered questions broken down by the 10 most frequent first words in the question.", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/7-Table2-1.png": "Table 2: Closest context words to a given query word, using a cosine similarity metric computed in the Word Embedding feature space and the Phrase Embedding feature space.", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/8-Figure3-1.png": "Figure 3: Attention matrices for question-context tuples. The left palette shows the context paragraph (correct answer in red and underlined), the middle palette shows the attention matrix (each row is a question word, each column is a context word), and the right palette shows the top attention points for each question word, above a threshold.", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/9-Table3-1.png": "Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods (marked with \u2217) for completeness."}, "referred_figures_tables": [["3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/2-Figure1-1.png"]], "question_id": [6], "question": ["Why did the author add one more direction in attention flow?"], "question_section": ["Abstract"], "question_trigger_sentence": ["In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query-aware context representation without early summarization."], "question_type": ["Deep/complex question"], "evidential_info": [[{"context": "In this paper, we introduce the Bi-Directional Attention Flow\u00a0 (BiDAF) network, a hierarchical multi-stage architecture for modeling the representations of the context paragraph at different levels of granularity (Figure\u00a01).BiDAF\u00a0includes character-level, word-level, and contextual embeddings, and uses bi-directional attention flow to obtain a query-aware context representation.Our attention mechanism offers following improvements to the previously popular attention paradigms.First, our attention layer is not used to summarize the context paragraph into a fixed-size vector.Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to flow through to the subsequent modeling layer.This reduces the information loss caused by early summarization.Second, we use a memory-less attention mechanism.That is, while we iteratively compute attention through time as in\u00a0Bahdanau et\u00a0al. (2015), the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step.We hypothesize that this simplification leads to the division of labor between the attention layer and the modeling layer.It forces the attention layer to focus on learning the attention between the query and the context, and enables the modeling layer to focus on learning the interaction within the query-aware context representation (the output of the attention layer).It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps.Our experiments show that memory-less attention gives a clear advantage over dynamic attention.Third, we use attention mechanisms in both directions, query-to-context and context-to-query, which provide complimentary information to each other.", "rationale": "BiDAF uses bi-directional attention flow in order to obtain a query-aware context representation."}, {"context": "In this paper, we introduce BiDAF, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization. The experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. The ablation analyses demonstrate the importance of each component in our model. The visualizations and discussions show that our model is learning a suitable representation for MC and is capable of answering complex questions by attending to correct locations in the given paragraph. Future work involves extending our approach to incorporate multiple hops of the attention layer.", "rationale": "BiDAF, uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization."}]], "composition": ["In order to obtain a query-aware context representation, author used bi-directional attention flow."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["1175"], "passages": ["The tasks of machine comprehension (MC) and question answering (QA) have gained significant popularity over the past few years within the natural language processing and computer vision communities. Systems trained end-to-end now achieve promising results on a variety of tasks in the text and image domains.One of the key factors to the advancement has been the use of neural attention mechanism, which enables the system to focus on a targeted area within a context paragraph (for MC) or within an image (for Visual QA), that is most relevant to answer the question\u00a0(Weston et\u00a0al., 2015; Antol et\u00a0al., 2015; Xiong et\u00a0al., 2016a).Attention mechanisms in previous works typically have one or more of the following characteristics.First, the computed attention weights are often used to extract the most relevant information from the context for answering the question by summarizing the context into a fixed-size vector.Second, in the text domain, they are often temporally dynamic, whereby the attention weights at the current time step are a function of the attended vector at the previous time step.Third, they are usually uni-directional, wherein the query attends on the context paragraph or the image.", "In this paper, we introduce the Bi-Directional Attention Flow\u00a0 (BiDAF) network, a hierarchical multi-stage architecture for modeling the representations of the context paragraph at different levels of granularity (Figure\u00a01).BiDAF\u00a0includes character-level, word-level, and contextual embeddings, and uses bi-directional attention flow to obtain a query-aware context representation.Our attention mechanism offers following improvements to the previously popular attention paradigms.First, our attention layer is not used to summarize the context paragraph into a fixed-size vector.Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to flow through to the subsequent modeling layer.This reduces the information loss caused by early summarization.Second, we use a memory-less attention mechanism.That is, while we iteratively compute attention through time as in\u00a0Bahdanau et\u00a0al. (2015), the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step.We hypothesize that this simplification leads to the division of labor between the attention layer and the modeling layer.It forces the attention layer to focus on learning the attention between the query and the context, and enables the modeling layer to focus on learning the interaction within the query-aware context representation (the output of the attention layer).It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps.Our experiments show that memory-less attention gives a clear advantage over dynamic attention.Third, we use attention mechanisms in both directions, query-to-context and context-to-query, which provide complimentary information to each other.", "Our BiDAF\u00a0model111Our code and interactive demo are available at: allenai.github.io/bi-att-flow/ outperforms all previous approaches on the highly-competitive Stanford Question Answering Dataset (SQuAD) test set leaderboard at the time of submission.With a modification to only the output layer, BiDAF\u00a0achieves the state-of-the-art results on the CNN/DailyMail cloze test.We also provide an in-depth ablation study of our model on the SQuAD development set, visualize the intermediate feature spaces in our model, and analyse its performance as compared to a more traditional language model for machine comprehension\u00a0(Rajpurkar et\u00a0al., 2016).", "Our machine comprehension model is a hierarchical multi-stage process and consists of six layers (Figure\u00a01):1.Character Embedding Layer maps each word to a vector space using character-level CNNs. 2.Word Embedding Layer maps each word to a vector space using a pre-trained word embedding model. 3.Contextual Embedding Layer utilizes contextual cues from surrounding words to refine the embedding of the words. These first three layers are applied to both the query and context.4.Attention Flow Layer couples the query and context vectors and produces a set of query-aware feature vectors for each word in the context.5.Modeling Layer employs a Recurrent Neural Network to scan the context.6.Output Layer provides an answer to the query.", "Character embedding layer is responsible for mapping each word to a high-dimensional vector space. Let {\ud835\udc991,\u2026\ud835\udc99T}subscript\ud835\udc991\u2026subscript\ud835\udc99\ud835\udc47\\{\\bm{x}_{1},\\dots\\bm{x}_{T}\\}{ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT } and {\ud835\udc921,\u2026\ud835\udc92J}subscript\ud835\udc921\u2026subscript\ud835\udc92\ud835\udc3d\\{\\bm{q}_{1},\\dots\\bm{q}_{J}\\}{ bold_italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 bold_italic_q start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT } represent the words in the input context paragraph and query, respectively.Following\u00a0Kim (2014), we obtain the character-level embedding of each word using Convolutional Neural Networks (CNN). Characters are embedded into vectors, which can be considered as 1D inputs to the CNN, and whose size is the input channel size of the CNN. The outputs of the CNN are max-pooled over the entire width to obtain a fixed-size vector for each word.", "Word embedding layer also maps each word to a high-dimensional vector space. We use pre-trained word vectors, GloVe\u00a0(Pennington et\u00a0al., 2014), to obtain the fixed word embedding of each word.", "The concatenation of the character and word embedding vectors is passed to a two-layer Highway Network\u00a0(Srivastava et\u00a0al., 2015).The outputs of the Highway Network are two sequences of d\ud835\udc51ditalic_d-dimensional vectors, or more conveniently, two matrices: \ud835\udc17\u2208\u211dd\u00d7T\ud835\udc17superscript\u211d\ud835\udc51\ud835\udc47{\\bf X}\\in\\mathbb{R}^{d\\times T}bold_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_T end_POSTSUPERSCRIPT for the context and \ud835\udc10\u2208\u211dd\u00d7J\ud835\udc10superscript\u211d\ud835\udc51\ud835\udc3d{\\bf Q}\\in\\mathbb{R}^{d\\times J}bold_Q \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_J end_POSTSUPERSCRIPT for the query.", "We use a Long Short-Term Memory Network (LSTM)\u00a0(Hochreiter & Schmidhuber, 1997) on top of the embeddings provided by the previous layers to model the temporal interactions between words. We place an LSTM in both directions, and concatenate the outputs of the two LSTMs. Hence we obtain \ud835\udc07\u2208\u211d2d\u00d7T\ud835\udc07superscript\u211d2\ud835\udc51\ud835\udc47{\\bf H}\\in\\mathbb{R}^{2d\\times T}bold_H \u2208 blackboard_R start_POSTSUPERSCRIPT 2 italic_d \u00d7 italic_T end_POSTSUPERSCRIPT from the context word vectors \ud835\udc17\ud835\udc17{\\bf X}bold_X, and \ud835\udc14\u2208\u211d2d\u00d7J\ud835\udc14superscript\u211d2\ud835\udc51\ud835\udc3d{\\bf U}\\in\\mathbb{R}^{2d\\times J}bold_U \u2208 blackboard_R start_POSTSUPERSCRIPT 2 italic_d \u00d7 italic_J end_POSTSUPERSCRIPT from query word vectors \ud835\udc10\ud835\udc10{\\bf Q}bold_Q.Note that each column vector of \ud835\udc07\ud835\udc07{\\bf H}bold_H and \ud835\udc14\ud835\udc14{\\bf U}bold_U is 2d2\ud835\udc512d2 italic_d-dimensional because of the concatenation of the outputs of the forward and backward LSTMs, each with d\ud835\udc51ditalic_d-dimensional output.", "It is worth noting that the first three layers of the model are computing features from the query and context at different levels of granularity, akin to the multi-stage feature computation of convolutional neural networks in the computer vision field.", "Attention flow layer is responsible for linking and fusing information from the context and the query words.Unlike previously popular attention mechanisms\u00a0(Weston et\u00a0al., 2015; Hill et\u00a0al., 2016; Sordoni et\u00a0al., 2016; Shen et\u00a0al., 2016), the attention flow layer is not used to summarize the query and context into single feature vectors.Instead, the attention vector at each time step, along with the embeddings from previous layers, are allowed to flow through to the subsequent modeling layer.This reduces the information loss caused by early summarization.", "The inputs to the layer are contextual vector representations of the context \ud835\udc07\ud835\udc07{\\bf H}bold_H and the query \ud835\udc14\ud835\udc14{\\bf U}bold_U.The outputs of the layer are the query-aware vector representations of the context words, \ud835\udc06\ud835\udc06{\\bf G}bold_G, along with the contextual embeddings from the previous layer.", "In this layer, we compute attentions in two directions: from context to query as well as from query to context.Both of these attentions, which will be discussed below, are derived from a shared similarity matrix, \ud835\udc12\u2208\u211dT\u00d7J\ud835\udc12superscript\u211d\ud835\udc47\ud835\udc3d{\\bf S}\\in\\mathbb{R}^{T\\times J}bold_S \u2208 blackboard_R start_POSTSUPERSCRIPT italic_T \u00d7 italic_J end_POSTSUPERSCRIPT, between the contextual embeddings of the context (\ud835\udc07\ud835\udc07{\\bf H}bold_H) and the query (\ud835\udc14\ud835\udc14{\\bf U}bold_U),where \ud835\udc12tjsubscript\ud835\udc12\ud835\udc61\ud835\udc57{\\bf S}_{tj}bold_S start_POSTSUBSCRIPT italic_t italic_j end_POSTSUBSCRIPT indicates the similarity between t\ud835\udc61titalic_t-th context word and j\ud835\udc57jitalic_j-th query word.The similarity matrix is computed by\ud835\udc12tj=\u03b1(\ud835\udc07:t,\ud835\udc14:j)\u2208\u211dsubscript\ud835\udc12\ud835\udc61\ud835\udc57\ud835\udefcsubscript\ud835\udc07:absent\ud835\udc61subscript\ud835\udc14:absent\ud835\udc57\u211d{\\bf S}_{tj}=\\alpha({\\bf H}_{:t},{\\bf U}_{:j})\\in\\mathbb{R}bold_S start_POSTSUBSCRIPT italic_t italic_j end_POSTSUBSCRIPT = italic_\u03b1 ( bold_H start_POSTSUBSCRIPT : italic_t end_POSTSUBSCRIPT , bold_U start_POSTSUBSCRIPT : italic_j end_POSTSUBSCRIPT ) \u2208 blackboard_R(1)where \u03b1\ud835\udefc\\alphaitalic_\u03b1 is a trainable scalar function that encodes the similarity between its two input vectors,\ud835\udc07:tsubscript\ud835\udc07:absent\ud835\udc61{\\bf H}_{:t}bold_H start_POSTSUBSCRIPT : italic_t end_POSTSUBSCRIPT is t\ud835\udc61titalic_t-th column vector of \ud835\udc07\ud835\udc07{\\bf H}bold_H, and\ud835\udc14:jsubscript\ud835\udc14:absent\ud835\udc57{\\bf U}_{:j}bold_U start_POSTSUBSCRIPT : italic_j end_POSTSUBSCRIPT is j\ud835\udc57jitalic_j-th column vector of \ud835\udc14\ud835\udc14{\\bf U}bold_U,We choose \u03b1(\ud835\udc21,\ud835\udc2e)=\ud835\udc30(\ud835\udc12)\u22a4[\ud835\udc21;\ud835\udc2e;\ud835\udc21\u2218\ud835\udc2e]\ud835\udefc\ud835\udc21\ud835\udc2esubscriptsuperscript\ud835\udc30top\ud835\udc12\ud835\udc21\ud835\udc2e\ud835\udc21\ud835\udc2e\\alpha({\\bf h},{\\bf u})={\\bf w}^{\\top}_{({\\bf S})}[{\\bf h};{\\bf u};{\\bf h}\\circ{\\bf u}]italic_\u03b1 ( bold_h , bold_u ) = bold_w start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( bold_S ) end_POSTSUBSCRIPT [ bold_h ; bold_u ; bold_h \u2218 bold_u ],where \ud835\udc30(\ud835\udc12)\u2208\u211d6dsubscript\ud835\udc30\ud835\udc12superscript\u211d6\ud835\udc51{\\bf w}_{({\\bf S})}\\in\\mathbb{R}^{6d}bold_w start_POSTSUBSCRIPT ( bold_S ) end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT 6 italic_d end_POSTSUPERSCRIPT is a trainable weight vector,\u2218\\circ\u2218 is elementwise multiplication,[;][;][ ; ] is vector concatenation across row,and implicit multiplication is matrix multiplication.Now we use \ud835\udc12\ud835\udc12{\\bf S}bold_S to obtain the attentions and the attended vectors in both directions.", "Context-to-query Attention.Context-to-query (C2Q) attention signifies which query words are most relevant to each context word.Let \ud835\udc1at\u2208\u211dJsubscript\ud835\udc1a\ud835\udc61superscript\u211d\ud835\udc3d{\\bf a}_{t}\\in\\mathbb{R}^{J}bold_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_J end_POSTSUPERSCRIPT represent the attention weights on the query words by t\ud835\udc61titalic_t-th context word, \u2211\ud835\udc1atj=1subscript\ud835\udc1a\ud835\udc61\ud835\udc571\\sum{\\bf a}_{tj}=1\u2211 bold_a start_POSTSUBSCRIPT italic_t italic_j end_POSTSUBSCRIPT = 1 for all t\ud835\udc61titalic_t. The attention weight is computed by \ud835\udc1at=softmax(\ud835\udc12t:)\u2208\u211dJsubscript\ud835\udc1a\ud835\udc61softmaxsubscript\ud835\udc12:\ud835\udc61absentsuperscript\u211d\ud835\udc3d{\\bf a}_{t}=\\mathrm{softmax}({\\bf S}_{t:})\\in\\mathbb{R}^{J}bold_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = roman_softmax ( bold_S start_POSTSUBSCRIPT italic_t : end_POSTSUBSCRIPT ) \u2208 blackboard_R start_POSTSUPERSCRIPT italic_J end_POSTSUPERSCRIPT,and subsequently each attended query vector is \ud835\udc14~:t=\u2211j\ud835\udc1atj\ud835\udc14:jsubscript~\ud835\udc14:absent\ud835\udc61subscript\ud835\udc57subscript\ud835\udc1a\ud835\udc61\ud835\udc57subscript\ud835\udc14:absent\ud835\udc57\\tilde{{\\bf U}}_{:t}=\\sum_{j}{\\bf a}_{tj}{\\bf U}_{:j}over~ start_ARG bold_U end_ARG start_POSTSUBSCRIPT : italic_t end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_a start_POSTSUBSCRIPT italic_t italic_j end_POSTSUBSCRIPT bold_U start_POSTSUBSCRIPT : italic_j end_POSTSUBSCRIPT.Hence \ud835\udc14~~\ud835\udc14\\tilde{{\\bf U}}over~ start_ARG bold_U end_ARG is a 2d2\ud835\udc512d2 italic_d-by-T\ud835\udc47Titalic_T matrix containing the attended query vectors for the entire context.", "Query-to-context Attention.Query-to-context (Q2C) attention signifies which context words have the closest similarity to one of the query words and are hence critical for answering the query. We obtain the attention weights on the context words by \ud835\udc1b=softmax(maxcol\u2061(\ud835\udc12))\u2208\u211dT\ud835\udc1bsoftmaxsubscript\ud835\udc50\ud835\udc5c\ud835\udc59\ud835\udc12superscript\u211d\ud835\udc47{\\bf b}=\\mathrm{softmax}(\\max_{col}({\\bf S}))\\in\\mathbb{R}^{T}bold_b = roman_softmax ( roman_max start_POSTSUBSCRIPT italic_c italic_o italic_l end_POSTSUBSCRIPT ( bold_S ) ) \u2208 blackboard_R start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT, where the maximum function (maxcolsubscript\ud835\udc50\ud835\udc5c\ud835\udc59\\max_{col}roman_max start_POSTSUBSCRIPT italic_c italic_o italic_l end_POSTSUBSCRIPT) is performed across the column. Then the attended context vector is \ud835\udc21~=\u2211t\ud835\udc1bt\ud835\udc07:t\u2208\u211d2d~\ud835\udc21subscript\ud835\udc61subscript\ud835\udc1b\ud835\udc61subscript\ud835\udc07:absent\ud835\udc61superscript\u211d2\ud835\udc51\\tilde{\\bf h}=\\sum_{t}{\\bf b}_{t}{\\bf H}_{:t}\\in\\mathbb{R}^{2d}over~ start_ARG bold_h end_ARG = \u2211 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_b start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_H start_POSTSUBSCRIPT : italic_t end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT 2 italic_d end_POSTSUPERSCRIPT. This vector indicates the weighted sum of the most important words in the context with respect to the query.\ud835\udc21~~\ud835\udc21\\tilde{\\bf h}over~ start_ARG bold_h end_ARG is tiled T\ud835\udc47Titalic_T times across the column, thus giving \ud835\udc07~\u2208\u211d2d\u00d7T~\ud835\udc07superscript\u211d2\ud835\udc51\ud835\udc47\\tilde{\\bf H}\\in\\mathbb{R}^{2d\\times T}over~ start_ARG bold_H end_ARG \u2208 blackboard_R start_POSTSUPERSCRIPT 2 italic_d \u00d7 italic_T end_POSTSUPERSCRIPT.", "Finally, the contextual embeddings and the attention vectors are combined together to yield \ud835\udc06\ud835\udc06{\\bf G}bold_G, where each column vector can be considered as the query-aware representation of each context word.We define \ud835\udc06\ud835\udc06{\\bf G}bold_G by\ud835\udc06:t=\ud835\udf37(\ud835\udc07:t,\ud835\udc14~:t,\ud835\udc07~:t)\u2208\u211dd\ud835\udc06subscript\ud835\udc06:absent\ud835\udc61\ud835\udf37subscript\ud835\udc07:absent\ud835\udc61subscript~\ud835\udc14:absent\ud835\udc61subscript~\ud835\udc07:absent\ud835\udc61superscript\u211dsubscript\ud835\udc51\ud835\udc06{\\bf G}_{:t}={\\bm{\\beta}}({\\bf H}_{:t},\\tilde{\\bf U}_{:t},\\tilde{\\bf H}_{:t})\\in\\mathbb{R}^{d_{\\bf G}}bold_G start_POSTSUBSCRIPT : italic_t end_POSTSUBSCRIPT = bold_italic_\u03b2 ( bold_H start_POSTSUBSCRIPT : italic_t end_POSTSUBSCRIPT , over~ start_ARG bold_U end_ARG start_POSTSUBSCRIPT : italic_t end_POSTSUBSCRIPT , over~ start_ARG bold_H end_ARG start_POSTSUBSCRIPT : italic_t end_POSTSUBSCRIPT ) \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT bold_G end_POSTSUBSCRIPT end_POSTSUPERSCRIPT(2)where \ud835\udc06:tsubscript\ud835\udc06:absent\ud835\udc61{\\bf G}_{:t}bold_G start_POSTSUBSCRIPT : italic_t end_POSTSUBSCRIPT is the t\ud835\udc61titalic_t-th column vector (corresponding to t\ud835\udc61titalic_t-th context word),\ud835\udf37\ud835\udf37{\\bm{\\beta}}bold_italic_\u03b2 is a trainable vector function that fuses its (three) input vectors,and d\ud835\udc06subscript\ud835\udc51\ud835\udc06d_{\\bf G}italic_d start_POSTSUBSCRIPT bold_G end_POSTSUBSCRIPT is the output dimension of the \ud835\udf37\ud835\udf37{\\bm{\\beta}}bold_italic_\u03b2 function.While the \ud835\udf37\ud835\udf37{\\bm{\\beta}}bold_italic_\u03b2 function can be an arbitrary trainable neural network, such as multi-layer perceptron,a simple concatenation as following still shows good performance in our experiments: \ud835\udf37(\ud835\udc21,\ud835\udc2e~,\ud835\udc21~)=[\ud835\udc21;\ud835\udc2e~;\ud835\udc21\u2218\ud835\udc2e~;\ud835\udc21\u2218\ud835\udc21~]\u2208\u211d8d\u00d7T\ud835\udf37\ud835\udc21~\ud835\udc2e~\ud835\udc21\ud835\udc21~\ud835\udc2e\ud835\udc21~\ud835\udc2e\ud835\udc21~\ud835\udc21superscript\u211d8\ud835\udc51\ud835\udc47{\\bm{\\beta}}({\\bf h},\\tilde{\\bf u},\\tilde{\\bf h})=[{\\bf h};\\tilde{\\bf u};{\\bf h}\\circ\\tilde{\\bf u};{\\bf h}\\circ\\tilde{\\bf h}]\\in\\mathbb{R}^{8d\\times T}bold_italic_\u03b2 ( bold_h , over~ start_ARG bold_u end_ARG , over~ start_ARG bold_h end_ARG ) = [ bold_h ; over~ start_ARG bold_u end_ARG ; bold_h \u2218 over~ start_ARG bold_u end_ARG ; bold_h \u2218 over~ start_ARG bold_h end_ARG ] \u2208 blackboard_R start_POSTSUPERSCRIPT 8 italic_d \u00d7 italic_T end_POSTSUPERSCRIPT (i.e., d\ud835\udc06=8dsubscript\ud835\udc51\ud835\udc068\ud835\udc51d_{\\bf G}=8ditalic_d start_POSTSUBSCRIPT bold_G end_POSTSUBSCRIPT = 8 italic_d).", "The input to the modeling layer is \ud835\udc06\ud835\udc06{\\bf G}bold_G, which encodes the query-aware representations of context words.The output of the modeling layer captures the interaction among the context words conditioned on the query.This is different from the contextual embedding layer, which captures the interaction among context words independent of the query.We use two layers of bi-directional LSTM, with the output size of d\ud835\udc51ditalic_d for each direction.Hence we obtain a matrix \ud835\udc0c\u2208\u211d2d\u00d7T\ud835\udc0csuperscript\u211d2\ud835\udc51\ud835\udc47{\\bf M}\\in\\mathbb{R}^{2d\\times T}bold_M \u2208 blackboard_R start_POSTSUPERSCRIPT 2 italic_d \u00d7 italic_T end_POSTSUPERSCRIPT, which is passed onto the output layer to predict the answer.Each column vector of \ud835\udc0c\ud835\udc0c{\\bf M}bold_M is expected to contain contextual information about the word with respect to the entire context paragraph and the query.", "The output layer is application-specific.The modular nature of BiDAF\u00a0allows us to easily swap out the output layer based on the task, with the rest of the architecture remaining exactly the same. Here, we describe the output layer for the QA task. In section\u00a05, we use a slight modification of this output layer for cloze-style comprehension. ", "The QA task requires the model to find a sub-phrase of the paragraph to answer the query. The phrase is derived by predicting the start and the end indices of the phrase in the paragraph. We obtain the probability distribution of the start index over the entire paragraph by\ud835\udc291=softmax(\ud835\udc30(\ud835\udc291)\u22a4[\ud835\udc06;\ud835\udc0c]),superscript\ud835\udc291softmaxsuperscriptsubscript\ud835\udc30superscript\ud835\udc291top\ud835\udc06\ud835\udc0c{\\bf p}^{1}=\\mathrm{softmax}({\\bf w}_{({\\bf p}^{1})}^{\\top}[{\\bf G};{\\bf M}]),bold_p start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = roman_softmax ( bold_w start_POSTSUBSCRIPT ( bold_p start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT [ bold_G ; bold_M ] ) ,(3)where \ud835\udc30(\ud835\udc291)\u2208\u211d10dsubscript\ud835\udc30superscript\ud835\udc291superscript\u211d10\ud835\udc51{\\bf w}_{({\\bf p}^{1})}\\in\\mathbb{R}^{10d}bold_w start_POSTSUBSCRIPT ( bold_p start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT 10 italic_d end_POSTSUPERSCRIPT is a trainable weight vector.For the end index of the answer phrase, we pass \ud835\udc0c\ud835\udc0c{\\bf M}bold_M to another bidirectional LSTM layer and obtain \ud835\udc0c2\u2208\u211d2d\u00d7Tsuperscript\ud835\udc0c2superscript\u211d2\ud835\udc51\ud835\udc47{\\bf M}^{2}\\in\\mathbb{R}^{2d\\times T}bold_M start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT 2 italic_d \u00d7 italic_T end_POSTSUPERSCRIPT.Then we use \ud835\udc0c2superscript\ud835\udc0c2{\\bf M}^{2}bold_M start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT to obtain the probability distribution of the end index in a similar manner:\ud835\udc292=softmax(\ud835\udc30(\ud835\udc292)\u22a4[\ud835\udc06;\ud835\udc0c2])superscript\ud835\udc292softmaxsuperscriptsubscript\ud835\udc30superscript\ud835\udc292top\ud835\udc06superscript\ud835\udc0c2{\\bf p}^{2}=\\mathrm{softmax}({\\bf w}_{({\\bf p}^{2})}^{\\top}[{\\bf G};{\\bf M}^{2}])bold_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = roman_softmax ( bold_w start_POSTSUBSCRIPT ( bold_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT [ bold_G ; bold_M start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] )(4)", "Training.We define the training loss (to be minimized) as the sum of the negative log probabilities of the true start and end indices by the predicted distributions, averaged over all examples:L(\u03b8)=\u22121N\u2211iNlog\u2061(\ud835\udc29yi11)+log\u2061(\ud835\udc29yi22)\ud835\udc3f\ud835\udf031\ud835\udc41subscriptsuperscript\ud835\udc41\ud835\udc56subscriptsuperscript\ud835\udc291subscriptsuperscript\ud835\udc661\ud835\udc56subscriptsuperscript\ud835\udc292subscriptsuperscript\ud835\udc662\ud835\udc56L(\\theta)=-\\frac{1}{N}\\sum^{N}_{i}\\log({\\bf p}^{1}_{y^{1}_{i}})+\\log({\\bf p}^{2}_{y^{2}_{i}})italic_L ( italic_\u03b8 ) = - divide start_ARG 1 end_ARG start_ARG italic_N end_ARG \u2211 start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log ( bold_p start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) + roman_log ( bold_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT )(5)where \u03b8\ud835\udf03\\thetaitalic_\u03b8 is the set of all trainable weights in the model (the weights and biases of CNN filters and LSTM cells, \ud835\udc30(\ud835\udc12)subscript\ud835\udc30\ud835\udc12{\\bf w}_{({\\bf S})}bold_w start_POSTSUBSCRIPT ( bold_S ) end_POSTSUBSCRIPT, \ud835\udc30(\ud835\udc291)subscript\ud835\udc30superscript\ud835\udc291{\\bf w}_{({\\bf p}^{1})}bold_w start_POSTSUBSCRIPT ( bold_p start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT and \ud835\udc30(\ud835\udc292)subscript\ud835\udc30superscript\ud835\udc292{\\bf w}_{({\\bf p}^{2})}bold_w start_POSTSUBSCRIPT ( bold_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT),N\ud835\udc41Nitalic_N is the number of examples in the dataset,yi1subscriptsuperscript\ud835\udc661\ud835\udc56y^{1}_{i}italic_y start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and yi2subscriptsuperscript\ud835\udc662\ud835\udc56y^{2}_{i}italic_y start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are the true start and end indices of the i\ud835\udc56iitalic_i-th example, respectively,and \ud835\udc29ksubscript\ud835\udc29\ud835\udc58{\\bf p}_{k}bold_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT indicates the k\ud835\udc58kitalic_k-th value of the vector \ud835\udc29\ud835\udc29{\\bf p}bold_p.", "Test.The answer span (k,l)\ud835\udc58\ud835\udc59(k,l)( italic_k , italic_l ) where k\u2264l\ud835\udc58\ud835\udc59k\\leq litalic_k \u2264 italic_l with the maximum value of \ud835\udc29k1\ud835\udc29l2subscriptsuperscript\ud835\udc291\ud835\udc58subscriptsuperscript\ud835\udc292\ud835\udc59{\\bf p}^{1}_{k}{\\bf p}^{2}_{l}bold_p start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT is chosen, which can be computed in linear time with dynamic programming.", "A significant contributor to the advancement of MC models has been the availability of large datasets. Early datasets such as MCTest\u00a0(Richardson et\u00a0al., 2013) were too small to train end-to-end neural models.Massive cloze test datasets (CNN/DailyMail by Hermann et\u00a0al. (2015) and Childrens Book Test by Hill et\u00a0al. (2016)), enabled the application of deep neural architectures to this task.More recently, Rajpurkar et\u00a0al. (2016) released the Stanford Question Answering (SQuAD) dataset with over 100,000 questions. We evaluate the performance of our comprehension system on both SQuAD and CNN/DailyMail datasets.", "Previous works in end-to-end machine comprehension use attention mechanisms in three distinct ways.The first group (largely inspired by\u00a0Bahdanau et\u00a0al. (2015)) uses a dynamic attention mechanism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention.Hermann et\u00a0al. (2015) argue that the dynamic attention model performs better than using a single fixed query vector to attend on context words on CNN & DailyMail datasets.Chen et\u00a0al. (2016) show that simply using bilinear term for computing the attention weights in the same model drastically improves the accuracy.Wang & Jiang (2016) reverse the direction of the attention (attending on query words as the context RNN progresses) for SQuAD. In contrast to these models, BiDAF\u00a0uses a memory-less attention mechanism.", "The second group computes the attention weights once, which are then fed into an output layer for final prediction (e.g.,\u00a0Kadlec et\u00a0al. (2016)).Attention-over-attention model (Cui et\u00a0al., 2016) uses a 2D similarity matrix between the query and context words (similar to Equation\u00a01) to compute the weighted average of query-to-context attention. In contrast to these models, BiDAF\u00a0does not summarize the two modalities in the attention layer and instead lets the attention vectors flow into the modeling (RNN) layer. ", "The third group (considered as variants of Memory Network\u00a0(Weston et\u00a0al., 2015)) repeats computing an attention vector between the query and the context through multiple layers, typically referred to as multi-hop (Sordoni et\u00a0al., 2016; Dhingra et\u00a0al., 2016). Shen et\u00a0al. (2016) combine Memory Networks with Reinforcement Learning in order to dynamically control the number of hops. One can also extend our BiDAF\u00a0model to incorporate multiple hops.", "The task of question answering has also gained a lot of interest in the computer vision community. Early works on visual question answering (VQA) involved encoding the question using an RNN, encoding the image using a CNN and combining them to answer the question\u00a0(Antol et\u00a0al., 2015; Malinowski et\u00a0al., 2015). Attention mechanisms have also been successfully employed for the VQA task and can be broadly clustered based on the granularity of their attention and the approach to construct the attention matrix. At the coarse level of granularity, the question attends to different patches in the image\u00a0(Zhu et\u00a0al., 2016; Xiong et\u00a0al., 2016a). At a finer level, each question word attends to each image patch and the highest attention value for each spatial location\u00a0(Xu & Saenko, 2016) is adopted. A hybrid approach is to combine questions representations at multiple levels of granularity (unigrams, bigrams, trigrams)\u00a0(Yang et\u00a0al., 2015). Several approaches to constructing the attention matrix have been used including element-wise product, element-wise sum, concatenation and Multimodal Compact Bilinear Pooling\u00a0(Fukui et\u00a0al., 2016).", "Lu et\u00a0al. (2016) have recently shown that in addition to attending from the question to image patches, attending from the image back to the question words provides an improvement on the VQA task. This finding in the visual domain is consistent with our finding in the language domain, where our bi-directional attention between the query and context provides improved results.Their model, however, uses the attention weights directly in the output layer and does not take advantage of the attention flow to the modeling layer.", "In this section, we evaluate our model on the task of question answering using the recently released SQuAD\u00a0(Rajpurkar et\u00a0al., 2016), which has gained a huge attention over a few months. In the next section, we evaluate our model on the task of cloze-style reading comprehension.", "SQuAD is a machine comprehension dataset on a large set of Wikipedia articles, with more than 100,000 questions. The answer to each question is always a span in the context.The model is given a credit if its answer matches one of the human written answers.Two metrics are used to evaluate models: Exact Match (EM) and a softer metric, F1 score, which measures the weighted average of the precision and recall rate at character level.The dataset consists of 90k/10k train/dev question-context tuples with a large hidden test set.It is one of the largest available MC datasets with human-written questions and serves as a great test bed for our model.", "The model architecture used for this task is depicted in Figure\u00a01. Each paragraph and question are tokenized by a regular-expression-based word tokenizer (PTB Tokenizer) and fed into the model. We use 100 1D filters for CNN char embedding, each with a width of 5.The hidden state size (d\ud835\udc51ditalic_d) of the model is 100.The model has about 2.6 million parameters.We use the AdaDelta\u00a0(Zeiler, 2012) optimizer, with a minibatch size of 60 and an initial learning rate of 0.50.50.50.5, for 12 epochs.A dropout\u00a0(Srivastava et\u00a0al., 2014) rate of 0.20.20.20.2 is used for the CNN, all LSTM layers, and the linear transformation before the softmax for the answers.During training, the moving averages of all weights of the model are maintained with the exponential decay rate of 0.9990.9990.9990.999.At test time, the moving averages instead of the raw weights are used.The training process takes roughly 20 hours on a single Titan X GPU. We also train an ensemble model consisting of 12 training runs with the identical architecture and hyper-parameters.At test time, we choose the answer with the highest sum of confidence scores amongst the 12 runs for each question.", "The results of our model and competing approaches on the hidden test are summarized in Table\u00a01(a). BiDAF\u00a0(ensemble) achieves an EM score of 73.3 and an F1 score of 81.1, outperforming all previous approaches.", "Table\u00a01(b) shows the performance of our model and its ablations on the SQuAD dev set. Both char-level and word-level embeddings contribute towards the model\u2019s performance. We conjecture that word-level embedding is better at representing the semantics of each word as a whole, while char-level embedding can better handle out-of-vocab (OOV) or rare words. To evaluate bi-directional attention, we remove C2Q and Q2C attentions. For ablating C2Q attention, we replace the attended question vector \ud835\udc14~~\ud835\udc14\\tilde{\\bf U}over~ start_ARG bold_U end_ARG with the average of the output vectors of the question\u2019s contextual embedding layer (LSTM). C2Q attention proves to be critical with a drop of more than 10 points on both metrics. For ablating Q2C attention, the output of the attention layer, \ud835\udc06\ud835\udc06{\\bf G}bold_G, does not include terms that have the attended Q2C vectors, \ud835\udc07~~\ud835\udc07\\tilde{\\bf H}over~ start_ARG bold_H end_ARG. To evaluate the attention flow, we study a dynamic attention model, where the attention is dynamically computed within the modeling layer\u2019s LSTM, following previous work\u00a0(Bahdanau et\u00a0al., 2015; Wang & Jiang, 2016). This is in contrast with our approach, where the attention is pre-computed before flowing to the modeling layer. Despite being a simpler attention mechanism, our proposed static attention outperforms the dynamically computed attention by more than 3 points. We conjecture that separating out the attention layer results in a richer set of features computed in the first 4 layers which are then incorporated by the modeling layer.We also show the performance of BiDAF\u00a0with several different definitions of \u03b1\ud835\udefc\\alphaitalic_\u03b1 and \ud835\udf37\ud835\udf37{\\bm{\\beta}}bold_italic_\u03b2 functions (Equation\u00a01 and\u00a02) in Appendix\u00a0B.", "We now provide a qualitative analysis of our model on the SQuAD dev set. First, we visualize the feature spaces after the word and contextual embedding layers. These two layers are responsible for aligning the embeddings between the query and context words which are the inputs to the subsequent attention layer. To visualize the embeddings, we choose a few frequent query words in the dev data and look at the context words that have the highest cosine similarity to the query words (Table\u00a02). At the word embedding layer, query words such as When, Where and Who are not well aligned to possible answers in the context, but this dramatically changes in the contextual embedding layer which has access to context from surrounding words and is just 1 layer below the attention layer. When begins to match years, Where matches locations, and Who matches names.", "We also visualize these two feature spaces using t-SNE in Figure\u00a02. t-SNE is performed on a large fraction of dev data but we only plot data points corresponding to the months of the year.An interesting pattern emerges in the Word space, where May is separated from the rest of the months because May has multiple meanings in the English language.The contextual embedding layer uses contextual cues from surrounding words and is able to separate the usages of the word May. Finally we visualize the attention matrices for some question-context tuples in the dev data in Figure\u00a03. In the first example, Where matches locations and in the second example, many matches quantities and numerical symbols. Also, entities in the question typically attend to the same entities in the context, thus providing a feature for the model to localize possible answers.", "We analyse the performance of our our model with a traditional language-feature-based baseline\u00a0(Rajpurkar et\u00a0al., 2016). Figure\u00a02b shows a Venn diagram of the dev set questions correctly answered by the models. Our model is able to answer more than 86% of the questions correctly answered by the baseline. The 14% that are incorrectly answered does not have a clear pattern.This suggests that neural architectures are able to exploit much of the information captured by the language features.We also break this comparison down by the first words in the questions (Figure\u00a02c). Our model outperforms the traditional baseline comfortably in every category.", "We randomly select 50 incorrect questions (based on EM) and categorize them into 6 classes.50% of errors are due to the imprecise boundaries of the answers,28% involve syntactic complications and ambiguities,14% are paraphrase problems,4% require external knowledge,2% need multiple sentences to answer,and 2% are due to mistakes during tokenization.See Appendix\u00a0A for the examples of the error modes.", "We also evaluate our model on the task of cloze-style reading comprehension using the CNN and Daily Mail datasets\u00a0(Hermann et\u00a0al., 2015).", "In a cloze test, the reader is asked to fill in words that have been removed from a passage, for measuring one\u2019s ability to comprehend text. Hermann et\u00a0al. (2015) have recently compiled a massive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test) examples from CNN and DailyMail news articles, respectively. Each example has a news article and an incomplete sentence extracted from the human-written summary of the article. To distinguish this task from language modeling and force one to refer to the article to predict the correct missing word, the missing word is always a named entity, anonymized with a random ID.Also, the IDs must be shuffled constantly during test, which is also critical for full anonymization.", "The model architecture used for this task is very similar to that for SQuAD (Section\u00a04) with only a few small changes to adapt it to the cloze test.Since each answer in the CNN/DailyMail datasets is always a single word (entity), we only need to predict the start index (\ud835\udc291superscript\ud835\udc291{\\bf p}^{1}bold_p start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT); the prediction for the end index (\ud835\udc292superscript\ud835\udc292{\\bf p}^{2}bold_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT) is omitted from the loss function.Also, we mask out all non-entity words in the final classification layer so that they are forced to be excluded from possible answers. Another important difference from SQuAD is that the answer entity might appear more than once in the context paragraph. To address this, we follow a similar strategy from\u00a0Kadlec et\u00a0al. (2016).During training, after we obtain \ud835\udc291superscript\ud835\udc291{\\bf p}^{1}bold_p start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT, we sum all probability values of the entity instances in the context that correspond to the correct answer.Then the loss function is computed from the summed probability.We use a minibatch size of 48 and train for 8 epochs, with early stop when the accuracy on validation data starts to drop.Inspired by the window-based method\u00a0(Hill et\u00a0al., 2016),we split each article into short sentences where each sentence is a 19-word window around each entity (hence the same word might appear in multiple sentences).The RNNs in BiDAF\u00a0are not feed-forwarded or back-propagated across sentences, which speed up the training process by parallelization.The entire training process takes roughly 60 hours on eight Titan X GPUs. The other hyper-parameters are identical to the model described in Section\u00a04.", "The results of our single-run models and competing approaches on the CNN/DailyMail datasets are summarized in Table\u00a03.*{}^{*}start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT indicates ensemble methods.BiDAF\u00a0outperforms previous single-run models on both datasets for both val and test data. On the DailyMail test, our single-run model even outperforms the best ensemble method.", "In this paper, we introduce BiDAF, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization. The experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. The ablation analyses demonstrate the importance of each component in our model. The visualizations and discussions show that our model is learning a suitable representation for MC and is capable of answering complex questions by attending to correct locations in the given paragraph. Future work involves extending our approach to incorporate multiple hops of the attention layer.", "This research was supported by the NSF (IIS 1616112), NSF (III 1703166), Allen Institute for AI (66-9175), Allen Distinguished Investigator Award, Google Research Faculty Award, and Samsung GRO Award. We thank the anonymous reviewers for their helpful comments."], "figure_types": {"3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/12-Table4-1.png": "table", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/13-Table5-1.png": "table", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/2-Figure1-1.png": "schematic", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/6-Table1-1.png": "table", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/7-Figure2-1.png": "plot", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/7-Table2-1.png": "table", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/8-Figure3-1.png": "plot", "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4/9-Table3-1.png": "table"}}, "2202.03036": {"paper_id": "paper_63", "title": "Structure-Aware Transformer for Graph Representation Learning", "arxiv_url": "https://arxiv.org/abs/2202.03036", "s2orc_url": "https://www.semanticscholar.org/paper/ea0e4a9778e33b7f8e7b3246d63071330950995a", "all_figures_tables": {"ea0e4a9778e33b7f8e7b3246d63071330950995a/17-Table4-1.png": "Table 4: Hyperparameters for SAT models trained on different datasets. RWPE-p indicates using p steps in the random walk positional encoding, which results in a p-dimensional vector as the positional representation for each node.", "ea0e4a9778e33b7f8e7b3246d63071330950995a/18-Table5-1.png": "Table 5: Number of parameters and training time per epoch for k-subtree SAT models using the hyperparameters in Table 4. Various GNNs are used as the base GNN in SAT.", "ea0e4a9778e33b7f8e7b3246d63071330950995a/18-Table6-1.png": "Table 6: Test MAE for SAT models using different structure extractors and readout methods on the ZINC dataset.", "ea0e4a9778e33b7f8e7b3246d63071330950995a/19-Table7-1.png": "Table 7: Comparison of SAT and SOTA methods on the OGBG-PPA dataset. All results are computed from 10 different runs.", "ea0e4a9778e33b7f8e7b3246d63071330950995a/19-Table8-1.png": "Table 8: Comparison of SAT and SOTA methods on the OGBG-CODE2 dataset. All results are computed from 10 different runs.", "ea0e4a9778e33b7f8e7b3246d63071330950995a/2-Figure1-1.png": "Figure 1: Position-aware vs. structure-aware: Using a positional encoding based on shortest paths in G1 and G2 respectively (assuming all edges have equal weight), node u and v would receive identical encodings since their shortest paths to all other nodes are the same in both graphs. However, their structures are different, with v forming a triangle with its red neighbors.", "ea0e4a9778e33b7f8e7b3246d63071330950995a/21-Figure5-1.png": "Figure 5: Attention visualization of SAT and the Transformer. The middle column shows the attention weights of the [CLS] node learned by our SAT model and the right column shows the attention weights learned by the classic Transformer with RWPE.", "ea0e4a9778e33b7f8e7b3246d63071330950995a/4-Figure2-1.png": "Figure 2: Overview of an example SAT layer that uses the k-subgraph GNN extractor as its structure extractor. The structure extractor generates structure-aware node representations which are used to compute the query (Q) and key (K) matrices in the Transformer layer. Structure-aware node representations are generated in the k-subgraph GNN extractor by first extracting the k-hop subgraph centered at each node (here, k = 1) and then using a GNN on each subgraph to generate a node representations using the full subgraph information. While the structure extractor can use any class of subgraphs, the one illustrated here defined on the class of k-hop subgraphs has a reasonable computation-expressiveness trade-off.", "ea0e4a9778e33b7f8e7b3246d63071330950995a/6-Table1-1.png": "Table 1: Comparison of SAT to SOTA methods on graph regression and classification tasks. ZINC results use edge weights where applicable, otherwise without edge weights. ? indicates we obtained the results ourselves by adapting the code provided by the original paper. means that higher is better for the performance metric; indicates lower is better.", "ea0e4a9778e33b7f8e7b3246d63071330950995a/7-Table2-1.png": "Table 2: Comparison of SAT to SOTA methods on OGB datasets.", "ea0e4a9778e33b7f8e7b3246d63071330950995a/8-Table3-1.png": "Table 3: Since SAT uses a GNN to extract structures, we compare the performance of the original sparse GNN to SAT which uses that GNN (\u201cbase GNN\u201d). Across different choices of GNNs, we observe that both k-subtree and k-subgraph SATs always outperform the original sparse GNN it uses. The evaluation metrics are the same as in Table 1.", "ea0e4a9778e33b7f8e7b3246d63071330950995a/9-Figure3-1.png": "Figure 3: We provide an analysis of the different drivers of performance in SAT on the ZINC dataset (lower is better). In Figure 3a, we show how changing the size of k affects performance (k=0 is equivalent to a vanilla Transformer that is not structure-aware). Figure 3b shows the effect of different absolute encoding methods, and Figure 3c shows the effect of different readout methods.", "ea0e4a9778e33b7f8e7b3246d63071330950995a/9-Figure4-1.png": "Figure 4: Attention visualization of SAT and the Transformer. The center column shows the attention weights of the [CLS] node learned by our SAT model and the right column shows the attention weights learned by the classic Transformer with the random walk positional encoding (RWPE)."}, "referred_figures_tables": [["ea0e4a9778e33b7f8e7b3246d63071330950995a/4-Figure2-1.png"]], "question_id": [7], "question": ["What is the meaning of \"using graph structures explicitly\"?"], "question_section": ["backgroud"], "question_trigger_sentence": ["While GNNs use the graph structure explicitly, Transformers remove that explicit structure, and instead infer relations between nodes by leveraging the node attributes"], "question_type": ["Testing question"], "evidential_info": [[{"context": "In this work, we address the critical question of how to encode structural information into a Transformer architecture. Our principal contribution is to introduce a flexible structure-aware self-attention mechanism that explicitly considers the graph structure and thus captures structural interaction between nodes. The resulting class of Transformers, which we call the Structure-Aware Transformer (SAT), can provide structure-aware representations of graphs, in contrast to most existing position-aware Transformers for graph-structured data. Specifically:\u2022We reformulate the self-attention mechanism in Vaswani et\u00a0al. (2017) as a kernel smoother andextend the original exponential kernel on node features to also account for local structures, by extracting a subgraph representation centered around each node.\u2022We propose several methods for automatically generating the subgraph representations, enabling the resulting kernel smoother to simultaneously capture structural and attributed similarities between nodes. The resulting representations are theoretically guaranteed to be at least as expressive as the subgraph representations.\u2022We demonstrate the effectiveness of SAT models on five graph and node property prediction benchmarks by showing it achieves better performance than state-of-the-art GNNs and Transformers. Furthermore, we show how SAT can easily leverage any GNN to compute the node representations which incorporate subgraph information and outperform the base GNN, making it an effortless enhancer of any existing GNN.\u2022Finally, we show that we can attribute the performance gains to the structure-aware aspect of our architecture, and showcase how SAT is more interpretable than the classic Transformer with an absolute encoding.", "rationale": "Our principal contribution is to introduce a flexible structure-aware self-attention mechanism that explicitly considers the graph structure and thus captures structural interaction between nodes."}, {"context": "The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention. Here, we seek to demonstrate that this information provides crucial predictive information, and study how the choice of k affects the results. Figure\u00a02(a) shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset. All models use the RWPE. k=0 corresponds to the vanilla Transformer only using absolute positional encoding, i.e.\u00a0not using structure. We find that incorporating structural information leads to substantial improvement in performance, with optimal performance around k=3 for both k-subtree and k-subgraph extractors. As k increases beyond k=4, the performance in k-subtree extractors deteriorated, which is consistent with the observed phenomenon that GNNs work best in shallower networks\u00a0(Kipf & Welling, 2017). We observe that k-subgraph does not suffer as much from this issue, underscoring a new aspect of its usefulness. On the other hand, k-subtree extractors are more computationally efficient and scalable to larger OGB datasets.", "rationale": "The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention."}]], "composition": ["The meaning of using graph structures explicitly is to explicity incorporate structural information into the self-attention.\nThe reason is that both P3 and P7 state the main contribution of SAT with paraphrasing.\nP3 indicates that to consider graph structure explicitly is a main idea of SAT, and P7 emphasizes it as to incorporate structural information in the self-attention."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["1184"], "passages": ["Graph neural networks (GNNs) have been established as powerful and flexible tools for graph representation learning, with successful applications in drug discovery\u00a0(Gaudelet et\u00a0al., 2021), protein design\u00a0(Ingraham et\u00a0al., 2019), social network analysis\u00a0(Fan et\u00a0al., 2019), and so on. A large class of GNNs build multilayer models, where each layer operates on the previous layer to generate new representations using a message-passing mechanism\u00a0(Gilmer et\u00a0al., 2017) to aggregate local neighborhood information.", "While many different message-passing strategies have been proposed, some critical limitations have been uncovered in this class of GNNs. These include the limited expressiveness of GNNs\u00a0(Xu et\u00a0al., 2019; Morris et\u00a0al., 2019), as well as known problems such as over-smoothing\u00a0(Li et\u00a0al., 2018, 2019; Chen et\u00a0al., 2020; Oono & Suzuki, 2020) and over-squashing\u00a0(Alon & Yahav, 2021).Over-smoothing manifests as all node representations converging to a constant after sufficiently many layers, while over-squashing occurs when messages from distant nodes are not effectively propagated through certain \u201cbottlenecks\u201d in a graph, since too many messages get compressed into a single fixed-length vector. Designing new architectures beyond neighborhood aggregation is thus essential to solve these problems.", "Transformers\u00a0(Vaswani et\u00a0al., 2017), which have proved to be successful in natural language understanding\u00a0(Vaswani et\u00a0al., 2017), computer vision\u00a0(Dosovitskiy et\u00a0al., 2020), and biological sequence modeling\u00a0(Rives et\u00a0al., 2021), offer the potential to address these issues. Rather than only aggregating local neighborhood information in the message-passing mechanism, the Transformer architecture is able to capture interaction information between any node pair via a single self-attention layer.Moreover, in contrast to GNNs, the Transformer avoids introducing any structural inductive bias at intermediate layers, addressing the expressivity limitation of GNNs. Instead, it encodes structural or positional information about nodes only into input node features, albeit limiting how much information it can learn from the graph structure. Integrating information about the graph structure into the Transformer architecture has thus gained growing attention in the graph representation learning field. However, most existing approaches only encode positional relationships between nodes, rather than explicitly encoding the structural relationships. As a result, they may not identify structural similarities between nodes and could fail to model the structural interaction between nodes (see Figure\u00a01). This could explain why their performance was dominated by sparse GNNs in several tasks\u00a0(Dwivedi et\u00a0al., 2022).", "In this work, we address the critical question of how to encode structural information into a Transformer architecture. Our principal contribution is to introduce a flexible structure-aware self-attention mechanism that explicitly considers the graph structure and thus captures structural interaction between nodes. The resulting class of Transformers, which we call the Structure-Aware Transformer (SAT), can provide structure-aware representations of graphs, in contrast to most existing position-aware Transformers for graph-structured data. Specifically:\u2022We reformulate the self-attention mechanism in Vaswani et\u00a0al. (2017) as a kernel smoother andextend the original exponential kernel on node features to also account for local structures, by extracting a subgraph representation centered around each node.\u2022We propose several methods for automatically generating the subgraph representations, enabling the resulting kernel smoother to simultaneously capture structural and attributed similarities between nodes. The resulting representations are theoretically guaranteed to be at least as expressive as the subgraph representations.\u2022We demonstrate the effectiveness of SAT models on five graph and node property prediction benchmarks by showing it achieves better performance than state-of-the-art GNNs and Transformers. Furthermore, we show how SAT can easily leverage any GNN to compute the node representations which incorporate subgraph information and outperform the base GNN, making it an effortless enhancer of any existing GNN.\u2022Finally, we show that we can attribute the performance gains to the structure-aware aspect of our architecture, and showcase how SAT is more interpretable than the classic Transformer with an absolute encoding.", "We will present the related work and relevant background in Sections\u00a02 and 3 before presenting our method in Section\u00a04 and our experimental findings in Section\u00a05.", "We present here the work most related to ours, namely the work stemming from message passing GNNs, positional representations on graphs, and graph Transformers.", "Message passing graph neural networks have recently been one of the leading methods for graph representation learning. An early seminal example is the GCN\u00a0(Kipf & Welling, 2017), which was based on performing convolutions on the graph. Gilmer et\u00a0al. (2017) reformulated the early GNNs into a framework of message passing GNNs, which has since then become the predominant framework of GNNs in use today, with extensive examples (Hamilton et\u00a0al., 2017; Xu et\u00a0al., 2019; Corso et\u00a0al., 2020; Hu et\u00a0al., 2020b; Veli\u010dkovi\u0107 et\u00a0al., 2018; Li et\u00a0al., 2020a; Yang et\u00a0al., 2022). However, as mentioned above, they suffer from problems of limited expressiveness, over-smoothing, and over-squashing.", "Because of the limited expressiveness of GNNs, there has been some recent research into the use of absolute encoding\u00a0(Shaw et\u00a0al., 2018), which consists of adding or concatenating positional or structural representations to the input node features. While it is often called an absolute positional encoding, we refer to it more generally as an absolute encoding to include both positional and structural encoding, which are both important in graph modeling. Absolute encoding primarily considers position or location relationships between nodes. Examples of position-based methods include the Laplacian positional encoding\u00a0(Dwivedi & Bresson, 2021; Kreuzer et\u00a0al., 2021), Weisfeiler\u2013Lehman-based positional encoding\u00a0(Zhang et\u00a0al., 2020), and random walk positional encoding (RWPE)\u00a0(Li et\u00a0al., 2020b; Dwivedi et\u00a0al., 2022), while distance-based methods include distances to a predefined set of nodes\u00a0(You et\u00a0al., 2019) and shortest path distances between pairs of nodes (Zhang et\u00a0al., 2020; Li et\u00a0al., 2020b). Dwivedi et\u00a0al. (2022) extend these ideas by using a trainable absolute encoding.", "While the absolute encoding methods listed above can be used with message passing GNNs, they also play a crucial role in the (graph) Transformer architecture. Graph Transformer\u00a0(Dwivedi & Bresson, 2021) provided an early example of how to generalize the Transformer architecture to graphs, using Laplacian eigenvectors as an absolute encoding and computing attention on the immediate neighborhood of each node, rather than on the full graph. SAN\u00a0(Kreuzer et\u00a0al., 2021) also used the Laplacian eigenvectors for computing an absolute encoding, but computed attention on the full graph, while distinguishing between true and created edges. Many graph Transformer methods also use a relative encoding\u00a0(Shaw et\u00a0al., 2018) in addition to absolute encoding. This strategy incorporates representations of the relative position or distances between nodes on the graph directly into the self-attention mechanism, as opposed to the absolute encoding which is only applied once to the input node features. Mialon et\u00a0al. (2021) propose a relative encoding by means of kernels on graphs to bias the self-attention calculation, which is then able to incorporate positional information into Transformers via the choice of kernel function. Other recent work seeks to incorporate structural information into the graph Transformer, for example by encoding some carefully selected graph theoretic properties such as centrality measures and shortest path distances as positional representations\u00a0(Ying et\u00a0al., 2021) or by using GNNs to integrate the graph structure\u00a0(Rong et\u00a0al., 2020; Jain et\u00a0al., 2021; Mialon et\u00a0al., 2021; Shi et\u00a0al., 2021).", "In this work, we combine the best of both worlds from message passing GNNs and from the Transformer architecture. We incorporate both an absolute as well as a novel relative encoding that explicitly incorporates the graph structure, thereby designing a Transformer architecture that takes both local and global information into account.", "In the following, we refer to a graph as G=(V,E,\ud835\udc17)\ud835\udc3a\ud835\udc49\ud835\udc38\ud835\udc17G=(V,E,\\mathbf{X})italic_G = ( italic_V , italic_E , bold_X ), where the node attributes for node u\u2208V\ud835\udc62\ud835\udc49u\\in Vitalic_u \u2208 italic_V is denoted by xu\u2208\ud835\udcb3\u2282dsubscript\ud835\udc65\ud835\udc62\ud835\udcb3superscript\ud835\udc51absentx_{u}\\in{\\mathcal{X}}\\subset^{d}italic_x start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT \u2208 caligraphic_X \u2282 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and the node attributes for all nodes are stored in \ud835\udc17\u2208n\u00d7dsuperscript\ud835\udc5b\ud835\udc51\ud835\udc17absent\\mathbf{X}\\in^{n\\times d}bold_X \u2208 start_POSTSUPERSCRIPT italic_n \u00d7 italic_d end_POSTSUPERSCRIPT for a graph with n\ud835\udc5bnitalic_n nodes.", "While GNNs use the graph structure explicitly, Transformers remove that explicit structure, and instead infer relations between nodes by leveraging the node attributes. In this sense, the Transformer\u00a0(Vaswani et\u00a0al., 2017) ignores the graph structure and rather considers the graph as a (multi-) set of nodes, and uses the self-attention mechanism to infer the similarity between nodes. The Transformer itself is composed of two main blocks: a self-attention module followed by a feed-forward neural network. In the self-attention module, the input node features \ud835\udc17\ud835\udc17\\mathbf{X}bold_X are first projected to query (\ud835\udc10\ud835\udc10\\mathbf{Q}bold_Q), key (\ud835\udc0a\ud835\udc0a\\mathbf{K}bold_K) and value (\ud835\udc15\ud835\udc15\\mathbf{V}bold_V) matrices through a linear projection such that \ud835\udc10=\ud835\udc17\ud835\udc16\ud835\udc10\ud835\udc10subscript\ud835\udc17\ud835\udc16\ud835\udc10\\mathbf{Q}=\\mathbf{X}\\mathbf{W_{Q}}bold_Q = bold_XW start_POSTSUBSCRIPT bold_Q end_POSTSUBSCRIPT, \ud835\udc0a=\ud835\udc17\ud835\udc16\ud835\udc0a\ud835\udc0asubscript\ud835\udc17\ud835\udc16\ud835\udc0a\\mathbf{K}=\\mathbf{X}\\mathbf{W_{K}}bold_K = bold_XW start_POSTSUBSCRIPT bold_K end_POSTSUBSCRIPT and \ud835\udc15=\ud835\udc17\ud835\udc16\ud835\udc15\ud835\udc15subscript\ud835\udc17\ud835\udc16\ud835\udc15\\mathbf{V}=\\mathbf{X}\\mathbf{W_{V}}bold_V = bold_XW start_POSTSUBSCRIPT bold_V end_POSTSUBSCRIPT respectively. We can compute the self-attention viaAttn(\ud835\udc17):=softmax(\ud835\udc10\ud835\udc0aTdout)\ud835\udc15\u2208n\u00d7dout,assignAttn\ud835\udc17softmaxsuperscript\ud835\udc10\ud835\udc0a\ud835\udc47subscript\ud835\udc51\ud835\udc5c\ud835\udc62\ud835\udc61\ud835\udc15superscript\ud835\udc5bsubscript\ud835\udc51\ud835\udc5c\ud835\udc62\ud835\udc61absent\\mathrm{Attn}(\\mathbf{X}):=\\mathrm{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d_{out}}})\\mathbf{V}\\in^{n\\times d_{out}},roman_Attn ( bold_X ) := roman_softmax ( divide start_ARG bold_QK start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT end_ARG end_ARG ) bold_V \u2208 start_POSTSUPERSCRIPT italic_n \u00d7 italic_d start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ,(1)where doutsubscript\ud835\udc51\ud835\udc5c\ud835\udc62\ud835\udc61d_{out}italic_d start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT refers to the dimension of \ud835\udc10\ud835\udc10\\mathbf{Q}bold_Q, and \ud835\udc16\ud835\udc10,\ud835\udc16\ud835\udc0a,\ud835\udc16\ud835\udc15subscript\ud835\udc16\ud835\udc10subscript\ud835\udc16\ud835\udc0asubscript\ud835\udc16\ud835\udc15\\mathbf{W_{Q}},\\mathbf{W_{K}},\\mathbf{W_{V}}bold_W start_POSTSUBSCRIPT bold_Q end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT bold_K end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT bold_V end_POSTSUBSCRIPT are trainable parameters. It is common to use multi-head attention, which concatenates multiple instances of Eq.\u00a0(1) and has shown to be effective in practice\u00a0(Vaswani et\u00a0al., 2017). Then, the output of the self-attention is followed by a skip-connection and a feed-forward network (FFN), which jointly compose a Transformer layer, as shown below:\ud835\udc17\u2032superscript\ud835\udc17\u2032\\displaystyle\\mathbf{X}^{\\prime}bold_X start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT=\ud835\udc17+Attn(\ud835\udc17),absent\ud835\udc17Attn\ud835\udc17\\displaystyle=\\mathbf{X}+\\mathrm{Attn}(\\mathbf{X}),= bold_X + roman_Attn ( bold_X ) ,(2)\ud835\udc17\u2032\u2032superscript\ud835\udc17\u2032\u2032\\displaystyle\\mathbf{X}^{\\prime\\prime}bold_X start_POSTSUPERSCRIPT \u2032 \u2032 end_POSTSUPERSCRIPT=FFN(\ud835\udc17\u2032):=ReLU(\ud835\udc17\u2032W1)W2.absentFFNsuperscript\ud835\udc17\u2032assignReLUsuperscript\ud835\udc17\u2032subscript\ud835\udc4a1subscript\ud835\udc4a2\\displaystyle=\\mathrm{FFN}(\\mathbf{X}^{\\prime}):=\\text{ReLU}(\\mathbf{X}^{\\prime}W_{1})W_{2}.= roman_FFN ( bold_X start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) := ReLU ( bold_X start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .Multiple layers can be stacked to form a Transformer model, which ultimately provides node-level representations of the graph. As the self-attention is equivariant to permutations of the input nodes, the Transformer will always generate the same representations for nodes with the same attributes regardless of their locations and surrounding structures in the graph. It is thus necessary to incorporate such information into the Transformer, generally via absolute encoding.", "Absolute encoding refers to adding or concatenating the positional or structural representations of the graph to the input node features before the main Transformer model, such as the Laplacian positional encoding\u00a0(Dwivedi & Bresson, 2021) or RWPE\u00a0(Dwivedi et\u00a0al., 2022). The main shortcoming of these encoding methods is that they generally do not provide a measure of the structural similarity between nodes and their neighborhoods.", "As noticed by\u00a0Mialon et\u00a0al. (2021), the self-attention in Eq.\u00a0(1) can be rewritten as a kernel smootherAttn(xv)=\u2211u\u2208V\u03baexp(xv,xu)\u2211w\u2208V\u03baexp(xv,xw)f(xu),\u2200v\u2208V,formulae-sequenceAttnsubscript\ud835\udc65\ud835\udc63subscript\ud835\udc62\ud835\udc49subscript\ud835\udf05subscript\ud835\udc65\ud835\udc63subscript\ud835\udc65\ud835\udc62subscript\ud835\udc64\ud835\udc49subscript\ud835\udf05subscript\ud835\udc65\ud835\udc63subscript\ud835\udc65\ud835\udc64\ud835\udc53subscript\ud835\udc65\ud835\udc62for-all\ud835\udc63\ud835\udc49\\mathrm{Attn}(x_{v})=\\sum_{u\\in V}\\frac{\\kappa_{\\exp}(x_{v},x_{u})}{\\sum_{w\\in V}\\kappa_{\\exp}(x_{v},x_{w})}f(x_{u}),~{}\\forall v\\in V,roman_Attn ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) = \u2211 start_POSTSUBSCRIPT italic_u \u2208 italic_V end_POSTSUBSCRIPT divide start_ARG italic_\u03ba start_POSTSUBSCRIPT roman_exp end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_w \u2208 italic_V end_POSTSUBSCRIPT italic_\u03ba start_POSTSUBSCRIPT roman_exp end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) end_ARG italic_f ( italic_x start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) , \u2200 italic_v \u2208 italic_V ,(3)where f(x)=\ud835\udc16\ud835\udc15x\ud835\udc53\ud835\udc65subscript\ud835\udc16\ud835\udc15\ud835\udc65f(x)=\\mathbf{W_{V}}xitalic_f ( italic_x ) = bold_W start_POSTSUBSCRIPT bold_V end_POSTSUBSCRIPT italic_x is the linear value function and \u03baexpsubscript\ud835\udf05\\kappa_{\\exp}italic_\u03ba start_POSTSUBSCRIPT roman_exp end_POSTSUBSCRIPT is a (non-symmetric) exponential kernel on \u00d7dd{}^{d}\\times^{d}start_FLOATSUPERSCRIPT italic_d end_FLOATSUPERSCRIPT \u00d7 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT parameterized by \ud835\udc16\ud835\udc10subscript\ud835\udc16\ud835\udc10\\mathbf{W_{Q}}bold_W start_POSTSUBSCRIPT bold_Q end_POSTSUBSCRIPT and \ud835\udc16\ud835\udc0asubscript\ud835\udc16\ud835\udc0a\\mathbf{W_{K}}bold_W start_POSTSUBSCRIPT bold_K end_POSTSUBSCRIPT:\u03baexp(x,x\u2032):=exp\u2061(\u27e8\ud835\udc16\ud835\udc10x,\ud835\udc16\ud835\udc0ax\u2032\u27e9/dout),assignsubscript\ud835\udf05\ud835\udc65superscript\ud835\udc65\u2032subscript\ud835\udc16\ud835\udc10\ud835\udc65subscript\ud835\udc16\ud835\udc0asuperscript\ud835\udc65\u2032subscript\ud835\udc51\ud835\udc5c\ud835\udc62\ud835\udc61\\kappa_{\\exp}(x,x^{\\prime}):=\\exp\\left(\\langle\\mathbf{W_{Q}}x,\\mathbf{W_{K}}x^{\\prime}\\rangle/\\sqrt{d_{out}}\\right),italic_\u03ba start_POSTSUBSCRIPT roman_exp end_POSTSUBSCRIPT ( italic_x , italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) := roman_exp ( \u27e8 bold_W start_POSTSUBSCRIPT bold_Q end_POSTSUBSCRIPT italic_x , bold_W start_POSTSUBSCRIPT bold_K end_POSTSUBSCRIPT italic_x start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u27e9 / square-root start_ARG italic_d start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT end_ARG ) ,(4)where \u27e8\u22c5,\u22c5\u27e9\u22c5\u22c5\\langle\\cdot,\\cdot\\rangle\u27e8 \u22c5 , \u22c5 \u27e9 is the dot product on d\ud835\udc51{}^{d}start_FLOATSUPERSCRIPT italic_d end_FLOATSUPERSCRIPT. With this form, Mialon et\u00a0al. (2021) propose a relative positional encoding strategy via the product of this kernel and a diffusion kernel on the graph, which consequently captures the positional similarity between nodes. However, this method is only position-aware, in contrast to our structure-aware encoding that will be presented in Section\u00a04.", "In this section, we will describe how to encode the graph structure into the self-attention mechanism and provide a class of Transformer models based on this framework.", "As presented above, self-attention in the Transformer can be rewritten as a kernel smoother where the kernel is a trainable exponential kernel defined on node features, and which only captures attributed similarity between a pair of nodes. The problem with this kernel smoother is that it cannot filter out nodes that are structurally different from the node of interest when they have the same or similar node features. In order to also incorporate the structural similarity between nodes, we consider a more generalized kernel that additionally accounts for the local substructures around each node. By introducing a set of subgraphs centered at each node, we define our structure-aware attention as:SA-attn(v):=\u2211u\u2208V\u03bagraph(SG(v),SG(u))\u2211w\u2208V\u03bagraph(SG(v),SG(w))f(xu),assignSA-attn\ud835\udc63subscript\ud835\udc62\ud835\udc49subscript\ud835\udf05graphsubscript\ud835\udc46\ud835\udc3a\ud835\udc63subscript\ud835\udc46\ud835\udc3a\ud835\udc62subscript\ud835\udc64\ud835\udc49subscript\ud835\udf05graphsubscript\ud835\udc46\ud835\udc3a\ud835\udc63subscript\ud835\udc46\ud835\udc3a\ud835\udc64\ud835\udc53subscript\ud835\udc65\ud835\udc62\\text{SA-attn}(v):=\\sum_{u\\in V}\\frac{\\kappa_{\\text{graph}}(S_{G}(v),S_{G}(u))}{\\sum_{w\\in V}\\kappa_{\\text{graph}}(S_{G}(v),S_{G}(w))}f(x_{u}),SA-attn ( italic_v ) := \u2211 start_POSTSUBSCRIPT italic_u \u2208 italic_V end_POSTSUBSCRIPT divide start_ARG italic_\u03ba start_POSTSUBSCRIPT graph end_POSTSUBSCRIPT ( italic_S start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_v ) , italic_S start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u ) ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_w \u2208 italic_V end_POSTSUBSCRIPT italic_\u03ba start_POSTSUBSCRIPT graph end_POSTSUBSCRIPT ( italic_S start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_v ) , italic_S start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_w ) ) end_ARG italic_f ( italic_x start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) ,(5)where SG(v)subscript\ud835\udc46\ud835\udc3a\ud835\udc63S_{G}(v)italic_S start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_v ) denotes a subgraph in G\ud835\udc3aGitalic_G centered at a node v\ud835\udc63vitalic_v associated with node features \ud835\udc17\ud835\udc17\\mathbf{X}bold_X and \u03bagraphsubscript\ud835\udf05graph\\kappa_{\\text{graph}}italic_\u03ba start_POSTSUBSCRIPT graph end_POSTSUBSCRIPT can be any kernel that compares a pair of subgraphs. This new self-attention function not only takes the attributed similarity into account but also the structural similarity between subgraphs. It thus generates more expressive node representations than the original self-attention, as we will show in Section\u00a04.4. Moreover, this self-attention is no longer equivariant to any permutation of nodes but only to nodes whose features and subgraphs coincide, which is a desirable property.", "In the rest of the paper, we will consider the following form of \u03bagraphsubscript\ud835\udf05graph\\kappa_{\\text{graph}}italic_\u03ba start_POSTSUBSCRIPT graph end_POSTSUBSCRIPT that already includes a large class of expressive and computationally tractable models:\u03bagraph(SG(v),SG(u))=\u03baexp(\u03c6(v,G),\u03c6(u,G)),subscript\ud835\udf05graphsubscript\ud835\udc46\ud835\udc3a\ud835\udc63subscript\ud835\udc46\ud835\udc3a\ud835\udc62subscript\ud835\udf05\ud835\udf11\ud835\udc63\ud835\udc3a\ud835\udf11\ud835\udc62\ud835\udc3a\\kappa_{\\text{graph}}(S_{G}(v),S_{G}(u))=\\kappa_{\\exp}(\\varphi(v,G),\\varphi(u,G)),italic_\u03ba start_POSTSUBSCRIPT graph end_POSTSUBSCRIPT ( italic_S start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_v ) , italic_S start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u ) ) = italic_\u03ba start_POSTSUBSCRIPT roman_exp end_POSTSUBSCRIPT ( italic_\u03c6 ( italic_v , italic_G ) , italic_\u03c6 ( italic_u , italic_G ) ) ,(6)where \u03c6(u,G)\ud835\udf11\ud835\udc62\ud835\udc3a\\varphi(u,G)italic_\u03c6 ( italic_u , italic_G ) is a structure extractor that extracts vector representations of some subgraph centered at u\ud835\udc62uitalic_u with node features \ud835\udc17\ud835\udc17\\mathbf{X}bold_X. We provide several alternatives of the structure extractor below. It is worth noting that our structure-aware self-attention is flexible enough to be combined with any model that generates representations of subgraphs, including GNNs and (differentiable) graph kernels. For notational simplicity, we assume there are no edge attributes, but our method can easily incorporate edge attributes as long as the structure extractor can accommodate them. The edge attributes are consequently not considered in the self-attention computation, but are incorporated into the structure-aware node representations. In the structure extractors presented in this paper, this means that edge attributes were included whenever the base GNN was able to handle edge attributes.", "A straightforward way to extract local structural information at node u\ud835\udc62uitalic_u is to apply any existing GNN model to the input graph with node features \ud835\udc17\ud835\udc17\\mathbf{X}bold_X and take the output node representation at u\ud835\udc62uitalic_u as the subgraph representation at u\ud835\udc62uitalic_u. More formally, if we denote by GNNG(k)superscriptsubscriptGNN\ud835\udc3a\ud835\udc58\\text{GNN}_{G}^{(k)}GNN start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT an arbitrary GNN model with k\ud835\udc58kitalic_k layers applied to G\ud835\udc3aGitalic_G with node features \ud835\udc17\ud835\udc17\\mathbf{X}bold_X, then\u03c6(u,G)=GNNG(k)(u).\ud835\udf11\ud835\udc62\ud835\udc3asubscriptsuperscriptGNN\ud835\udc58\ud835\udc3a\ud835\udc62\\varphi(u,G)=\\text{GNN}^{(k)}_{G}(u).italic_\u03c6 ( italic_u , italic_G ) = GNN start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u ) .(7)This extractor is able to represent the k\ud835\udc58kitalic_k-subtree structure rooted at u\ud835\udc62uitalic_u\u00a0(Xu et\u00a0al., 2019). While this class of structure extractors is fast to compute and can flexibly leverage any existing GNN, they cannot be more expressive than the Weisfeiler\u2013Lehman test due to the expressiveness limitation of message passing GNNs\u00a0(Xu et\u00a0al., 2019). In practice, a small value of k\ud835\udc58kitalic_k already leads to good performance, while not suffering from over-smoothing or over-squashing.", "A more expressive extractor is to use a GNN to directly compute the representation of the entire k\ud835\udc58kitalic_k-hop subgraph centered at u\ud835\udc62uitalic_u rather than just the node representation u\ud835\udc62uitalic_u. Recent work has explored the idea of using subgraphs rather than subtrees around a node in GNNs, with positive experimental results (Zhang & Li, 2021; Wijesinghe & Wang, 2022), as well as being strictly more powerful than the 1-WL test (Zhang & Li, 2021). We follow the same setup as is done in Zhang & Li (2021), and adapt our GNN extractor to utilize the entire k\ud835\udc58kitalic_k-hop subgraph.The k\ud835\udc58kitalic_k-subgraph GNN extractor aggregates the updated node representations of all nodes within the k\ud835\udc58kitalic_k-hop neighborhood using a pooling function such as summation. Formally, if we denote by \ud835\udca9k(u)subscript\ud835\udca9\ud835\udc58\ud835\udc62{\\mathcal{N}}_{k}(u)caligraphic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_u ) the k\ud835\udc58kitalic_k-hop neighborhood of node u\ud835\udc62uitalic_u including itself, the representation of a node u\ud835\udc62uitalic_u is:\u03c6(u,G)=\u2211v\u2208\ud835\udca9k(u)GNNG(k)(v).\ud835\udf11\ud835\udc62\ud835\udc3asubscript\ud835\udc63subscript\ud835\udca9\ud835\udc58\ud835\udc62subscriptsuperscriptGNN\ud835\udc58\ud835\udc3a\ud835\udc63\\varphi(u,G)=\\sum_{v\\in{\\mathcal{N}}_{k}(u)}\\text{GNN}^{(k)}_{G}(v).italic_\u03c6 ( italic_u , italic_G ) = \u2211 start_POSTSUBSCRIPT italic_v \u2208 caligraphic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_u ) end_POSTSUBSCRIPT GNN start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_v ) .(8)", "We observe that prior to the pooling function, the k\ud835\udc58kitalic_k-subgraph GNN extractor is equivalent to using the k\ud835\udc58kitalic_k-subtree GNN extractor within each k\ud835\udc58kitalic_k-hop subgraph. So as to capture the attributed similarity as well as structural similarity, we augment the node representation from k\ud835\udc58kitalic_k-subgraph GNN extractor with the original node features via concatenation. While this extractor provides more expressive subgraph representations than the k\ud835\udc58kitalic_k-subtree extractor, it requires enumerating all k\ud835\udc58kitalic_k-hop subgraphs, and consequently does not scale as well as the k\ud835\udc58kitalic_k-subtree extractor to large datasets.", "Finally, we present a list of other potential structure extractors for different purposes. One possible choice is to directly learn a number of \u201chidden graphs\u201d as the \u201canchor subgraphs\u201d to represent subgraphs for better model interpretability, by using the concepts introduced in\u00a0Nikolentzos & Vazirgiannis (2020). While Nikolentzos & Vazirgiannis (2020) obtain a vector representation of the input graph by counting the number of matching walks between the whole graph and each of the hidden graphs, one could extend this to the node level by comparing the hidden graphs to the k\ud835\udc58kitalic_k-hop subgraph centered around each node. The adjacency matrix of the hidden graphs is a trainable parameter in the network, thereby enabling end-to-end training to identify which subgraph structures are predictive. Then, for a trained model, visualizing the learned hidden graphs provides useful insights about the structural motifs in the dataset.", "Furthermore, more domain-specific GNNs could also be used to extract potentially more expressive subgraph representations. For instance, Bodnar et\u00a0al. (2021) recently proposed a new kind of message passing scheme operating on regular cell complexes which benefits from provably stronger expressivity for molecules. Our self-attention mechanism can fully benefit from the development of more domain-specific and expressive GNNs.", "Finally, another possible structure extractor is to use a non-parametric graph kernel (e.g.\u00a0a Weisfeiler-Lehman graph kernel) on the k\ud835\udc58kitalic_k-hop subgraphs centered around each node. This provides a flexible way to combine graph kernels and deep learning, which might offer new theoretical insights into the link between the self-attention and kernel methods.", "Having defined our structure-aware self-attention function, the other components of the Structure-Aware Transformer follow the Transformer architecture as described in Section\u00a03.1; see Figure\u00a02 for a visual overview. Specifically, the self-attention function is followed by a skip-connection, a FFN and two normalization layers before and after the FFN. In addition, we also include the degree factor in the skip-connection, which was found useful for reducing the overwhelming influence of highly connected graph components\u00a0(Mialon et\u00a0al., 2021), i.e.,xv\u2032=xv+1/dvSA-attn(v),superscriptsubscript\ud835\udc65\ud835\udc63\u2032subscript\ud835\udc65\ud835\udc631subscript\ud835\udc51\ud835\udc63SA-attn\ud835\udc63x_{v}^{\\prime}=x_{v}+1/\\sqrt{d_{v}}\\,\\text{SA-attn}(v),italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT + 1 / square-root start_ARG italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_ARG SA-attn ( italic_v ) ,(9)where dvsubscript\ud835\udc51\ud835\udc63d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT denotes the degree of node v\ud835\udc63vitalic_v. After a Transformer layer, we obtain a new graph with the same structure but different node features G\u2032=(V,E,\ud835\udc17\u2032)superscript\ud835\udc3a\u2032\ud835\udc49\ud835\udc38superscript\ud835\udc17\u2032G^{\\prime}=(V,E,\\mathbf{X}^{\\prime})italic_G start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = ( italic_V , italic_E , bold_X start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ), where \ud835\udc17\u2032superscript\ud835\udc17\u2032\\mathbf{X}^{\\prime}bold_X start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT corresponds to the output of the Transformer layer.", "Finally, for graph property prediction, there are various ways to aggregate node-level representations into a graph representation, such as by taking the average or sum. Alternatively, one can use the embedding of a virtual [CLS] node (Jain et\u00a0al., 2021) that is attached to the input graph without any connectivity to other nodes. We compare these approaches in Section\u00a05.", "While the self-attention in Eq.\u00a0(5) is structure-aware, most absolute encoding techniques are only position-aware and could therefore provide complementary information. Indeed, we find that the combination leads to further performance improvements, which we show in Section\u00a05. We choose to use the RWPE\u00a0(Dwivedi et\u00a0al., 2022), though any other absolute positional representations, including learnable ones, can also be used.", "We further argue that only using absolute positional encoding with the Transformer would exhibit a too relaxed structural inductive bias which is not guaranteed to generate similar node representations even if two nodes have similar local structures. This is due to the fact that distance or Laplacian-based positional representations generally serve as structural or positional signatures but do not provide a measure of structural similarity between nodes, especially in the inductive case where two nodes are from different graphs. This is also empirically affirmed in Section\u00a05 by their relatively worse performance without using our structural encoding. In contrast, the subgraph representations used in the structure-aware attention can be tailored to measure the structural similarity between nodes, and thus generate similar node-level representations if they possess similar attributes and surrounding structures. We can formally state this in the following theorem:", "The proof is provided in the Appendix. The metric D\ud835\udc37Ditalic_D is an optimal matching metric between two multisets which measures how different they are. This theorem shows that two node representations from the SA-attn are similar if the graphs that they belong to have similar multisets of node features and subgraph representations overall, and at the same time, the subgraph representations at these two nodes are similar. In particular, if two nodes belong to the same graph, i.e.\u00a0G=G\u2032\ud835\udc3asuperscript\ud835\udc3anormal-\u2032G=G^{\\prime}italic_G = italic_G start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT, then the second and last terms on the right side of Eq.\u00a0(10) are equal to zero and the distance between their representations is thus constrained by the distance between their corresponding subgraph representations. However, for Transformers with absolute positional encoding, the distance between two node representations is not constrained by their structural similarity, as the distance between two positional representations does not necessarily characterize how structurally similar two nodes are. Despite stronger inductive biases, we will show that our model is still sufficiently expressive in the next section.", "The expressive power of graph Transformers compared to classic GNNs has hardly been studied, since the soft structural inductive bias introduced in absolute encoding is generally hard to characterize. Thanks to the unique design of our SAT, which relies on a subgraph structure extractor, it becomes possible to study the expressiveness of the output representations. More specifically, we formally show that the node representation from a structure-aware attention layer is at least as expressive as its subgraph representation given by the structure extractor, following the injectivity of the attention function with respect to the query:", "Note that the assumptions made in the theorem are mild as one can always add some absolute encoding or random noise to make the attributes of one node different from all other nodes, and similarly for subgraph representations. The countable assumption on \ud835\udcb3\ud835\udcb3{\\mathcal{X}}caligraphic_X is generally adopted for expressivity analysis of GNNs (e.g.\u00a0Xu et\u00a0al. (2019)). We assume f\ud835\udc53fitalic_f to be any mapping rather than just a linear function as in the definition of the self-attention function since it can be practically approximated by a FFN in multi-layer Transformers through the universal approximation theorem\u00a0(Hornik, 1991). Theorem\u00a02 suggests that if the structure extractor is sufficiently expressive, the resulting SAT model can also be at least equally expressive. Furthermore, more expressive extractors could lead to more expressively powerful SAT models and thus better prediction performance, which is also empirically confirmed in Section\u00a05.", "In this section, we evaluate SAT models versus several SOTA methods for graph representation learning, including GNNs and Transformers, on five graph and node prediction tasks, as well as analyze the different components of our architecture to identify what drives the performance. In summary, we discovered the following aspects about SAT:", "\u2022The structure-aware framework achieves SOTA performance on graph and node classification tasks, outperforming SOTA graph Transformers and sparse GNNs.\u2022Both instances of the SAT, namely k\ud835\udc58kitalic_k-subtree and k\ud835\udc58kitalic_k-subgraph SAT, always improve upon the base GNN it is built upon, highlighting the improved expressiveness of our structure-aware approach.\u2022We show that incorporating the structure via our structure-aware attention brings a notable improvement relative to the vanilla Transformer with RWPE that just uses node attribute similarity instead of also incorporating structural similarity. We also show that a small value of k\ud835\udc58kitalic_k already leads to good performance, while not suffering from over-smoothing or over-squashing.\u2022We show that choosing a proper absolute positional encoding and a readout method improves performance, but to a much lesser extent than incorporating the structure into the approach.", "Furthermore, we note that SAT achieves SOTA performance while only considering a small hyperparameter search space. Performance could likely be further improved with more hyperparameter tuning.", "We assess the performance of our method with five medium to large benchmark datasets for node and graph property prediction, including ZINC\u00a0(Dwivedi et\u00a0al., 2020), CLUSTER\u00a0(Dwivedi et\u00a0al., 2020), PATTERN\u00a0(Dwivedi et\u00a0al., 2020), OGBG-PPA\u00a0(Hu et\u00a0al., 2020a) and OGBG-CODE2\u00a0(Hu et\u00a0al., 2020a).", "We compare our method to the following GNNs: GCN\u00a0(Kipf & Welling, 2017), GraphSAGE\u00a0(Hamilton et\u00a0al., 2017), GAT\u00a0(Veli\u010dkovi\u0107 et\u00a0al., 2018), GIN\u00a0(Xu et\u00a0al., 2019), PNA\u00a0(Corso et\u00a0al., 2020), DeeperGCN\u00a0 (Li et\u00a0al., 2020a), and ExpC\u00a0(Yang et\u00a0al., 2022). Our comparison partners also include several recently proposed Transformers on graphs, including the original Transformer with RWPE\u00a0(Dwivedi et\u00a0al., 2022), Graph Transformer\u00a0(Dwivedi & Bresson, 2021), SAN\u00a0(Kreuzer et\u00a0al., 2021), Graphormer\u00a0(Ying et\u00a0al., 2021) and GraphTrans\u00a0(Jain et\u00a0al., 2021), a model that uses the vanilla Transformer on top of a GNN.", "All results for the comparison methods are either taken from the original paper or from Dwivedi et\u00a0al. (2020) if not available. We consider k\ud835\udc58kitalic_k-subtree and k\ud835\udc58kitalic_k-subgraph SAT equipped with different GNN extractors, including GCN, GIN, GraphSAGE and PNA. For OGBG-PPA and OGBG-CODE2, we do not run experiments for k\ud835\udc58kitalic_k-subgraph SAT models due to large memory requirements. Full details on the datasets, experimental setup, and hyperparameters are provided in the Appendix.", "We show the performance of SATs compared to other GNNs and Transformers in Table\u00a01 and 2. SAT models consistently outperform SOTA methods on these datasets, showing its ability to combine the benefits of both GNNs and Transformers. In particular, for the CODE2 dataset, our SAT models outperform SOTA methods by a large margin despite a relatively small number of parameters and minimal hyperparameter tuning, which will put it at the first place on the OGB leaderboard.", "Table\u00a03 summarizes the performance of SAT relative to the sparse GNN it uses to extract the subgraph representations, across different GNNs. We observe that both variations of SAT consistently bring large performance gains to its base GNN counterpart, making it a systematic enhancer of any GNN model. Furthermore, PNA, which is the most expressive GNN we considered, has consistently the best performance when used with SAT, empirically validating our theoretical finding in Section\u00a04.4. k\ud835\udc58kitalic_k-subgraph SAT also outperforms or performs equally as k\ud835\udc58kitalic_k-subtree SAT in almost all the cases, showing its superior expressiveness.", "While Table\u00a03 showcases the added value of the SAT relative to sparse GNNs, we now dissect the components of SAT on the ZINC dataset to identify which aspects of the architecture bring the biggest performance gains.", "The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention. Here, we seek to demonstrate that this information provides crucial predictive information, and study how the choice of k\ud835\udc58kitalic_k affects the results. Figure\u00a02(a) shows how the test MAE is impacted by varying k\ud835\udc58kitalic_k for k\ud835\udc58kitalic_k-subtree and k\ud835\udc58kitalic_k-subgraph extractors using PNA on the ZINC dataset. All models use the RWPE. k=0\ud835\udc580k=0italic_k = 0 corresponds to the vanilla Transformer only using absolute positional encoding, i.e.\u00a0not using structure. We find that incorporating structural information leads to substantial improvement in performance, with optimal performance around k=3\ud835\udc583k=3italic_k = 3 for both k\ud835\udc58kitalic_k-subtree and k\ud835\udc58kitalic_k-subgraph extractors. As k\ud835\udc58kitalic_k increases beyond k=4\ud835\udc584k=4italic_k = 4, the performance in k\ud835\udc58kitalic_k-subtree extractors deteriorated, which is consistent with the observed phenomenon that GNNs work best in shallower networks\u00a0(Kipf & Welling, 2017). We observe that k\ud835\udc58kitalic_k-subgraph does not suffer as much from this issue, underscoring a new aspect of its usefulness. On the other hand, k\ud835\udc58kitalic_k-subtree extractors are more computationally efficient and scalable to larger OGB datasets.", "We assess here whether the absolute encoding brought complementary information to SAT. In Figure\u00a02(b), we conduct an ablation study showing the results of SAT with and without absolute positional encoding, including RWPE and Laplacian PE\u00a0(Dwivedi et\u00a0al., 2020). Our SAT with a positional encoding outperforms its counterpart without it, confirming the complementary nature of the two encodings. However, we also note that the performance gain brought by the absolute encoding is far less than the gain obtained by using our structure-aware attention, as shown in Figure\u00a02(a) (comparing the instance of k=0\ud835\udc580k=0italic_k = 0 to k>0\ud835\udc580k>0italic_k > 0), emphasizing that our structure-aware attention is the more important aspect of the model.", "Finally, we compare the performance of SAT models using different readout methods for aggregating node-level representations on the ZINC dataset in Figure\u00a02(c), including the CLS pooling discussed in Section\u00a04.2. Unlike the remarkable influence of the readout method in GNNs\u00a0(Xu et\u00a0al., 2019), we observe very little impact in SAT models.", "In addition to performance improvement, we show that SAT offers better model interpretability compared to the classic Transformer with only absolute positional encoding. We respectively train a SAT model and a Transformer with a CLS readout on the Mutagenicity dataset, and visualize the attention scores between the [CLS] node and other nodes learned by SAT and the Transformer in Figure\u00a04. The salient difference between the two models is that SAT has structure-aware node embeddings, and thus we can attribute the following interpretability gains to that. While both models manage to identify some chemical motifs known for mutagenicity, such as NO22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT and NH22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT, the attention scores learned by SAT are sparser and more informative, meaning that SAT puts more attention weights on these known mutagenic motifs than the Transformer with RWPE. The vanilla Transformer even fails to put attention on some important atoms such as the H atoms in the NH22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT group. The only H atoms highlighted by SAT are those in the NH22{}_{2}start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT group, suggesting that our SAT indeed takes the structure into account. More focus on these discriminative motifs makes the SAT model less influenced by other chemical patterns that commonly exist in the dataset, such as benzene, and thus leads to overall improved performance. More results are provided in the Appendix.", "We introduced the SAT model, which successfully incorporates structural information into the Transformer architecture and overcomes the limitations of the absolute encoding. In addition to SOTA empirical performance with minimal hyperparameter tuning, SAT also provides better interpretability than the Transformer.", "As mentioned above, k\ud835\udc58kitalic_k-subgraph SAT has higher memory requirements than k\ud835\udc58kitalic_k-subtree SAT, which can restrict its applicability if access to high memory GPUs is restricted. We see the main limitation of SAT is that it suffers from the same drawbacks as the Transformer, namely the quadratic complexity of the self-attention computation.", "Because SAT can be combined with any GNN, a natural extension of our work is to combine SAT with structure extractors which have shown to be strictly more expressive than the 1-WL test, such as the recent topological GNN introduced by Horn et\u00a0al. (2021). Additionally, the SAT framework is flexible and can incorporate any structure extractor which produces structure-aware node representations, and could even be extended beyond using GNNs, such as differentiable graph kernels.", "Another important area for future work is to focus on reducing the high memory cost and time complexity of the self-attention computation, as is being done in recent efforts for developing a so-called linear transformer, which has linear complexity in both time and space requirements (Tay et\u00a0al., 2020; Wang et\u00a0al., 2020; Qin et\u00a0al., 2022)."], "figure_types": {"ea0e4a9778e33b7f8e7b3246d63071330950995a/17-Table4-1.png": "table", "ea0e4a9778e33b7f8e7b3246d63071330950995a/18-Table5-1.png": "table", "ea0e4a9778e33b7f8e7b3246d63071330950995a/18-Table6-1.png": "table", "ea0e4a9778e33b7f8e7b3246d63071330950995a/19-Table7-1.png": "table", "ea0e4a9778e33b7f8e7b3246d63071330950995a/19-Table8-1.png": "table", "ea0e4a9778e33b7f8e7b3246d63071330950995a/2-Figure1-1.png": "schematic", "ea0e4a9778e33b7f8e7b3246d63071330950995a/21-Figure5-1.png": "schematic", "ea0e4a9778e33b7f8e7b3246d63071330950995a/4-Figure2-1.png": "schematic", "ea0e4a9778e33b7f8e7b3246d63071330950995a/6-Table1-1.png": "table", "ea0e4a9778e33b7f8e7b3246d63071330950995a/7-Table2-1.png": "table", "ea0e4a9778e33b7f8e7b3246d63071330950995a/8-Table3-1.png": "table", "ea0e4a9778e33b7f8e7b3246d63071330950995a/9-Figure3-1.png": "plot", "ea0e4a9778e33b7f8e7b3246d63071330950995a/9-Figure4-1.png": "schematic"}}, "1706.02216": {"paper_id": "paper_65", "title": "Inductive Representation Learning on Large Graphs", "arxiv_url": "https://arxiv.org/abs/1706.02216", "s2orc_url": "https://www.semanticscholar.org/paper/6b7d6e6416343b2a122f8416e69059ce919026ef", "all_figures_tables": {"6b7d6e6416343b2a122f8416e69059ce919026ef/19-Figure3-1.png": "Figure 3: Accuracy (in F1-score) for different approaches on the citation data as the feature matrix is incrementally replaced with random Gaussian noise.", "6b7d6e6416343b2a122f8416e69059ce919026ef/2-Figure1-1.png": "Figure 1: Visual illustration of the GraphSAGE sample and aggregate approach.", "6b7d6e6416343b2a122f8416e69059ce919026ef/7-Table1-1.png": "Table 1: Prediction results for the three datasets (micro-averaged F1 scores). Results for unsupervised and fully supervised GraphSAGE are shown. Analogous trends hold for macro-averaged scores."}, "referred_figures_tables": [["6b7d6e6416343b2a122f8416e69059ce919026ef/2-Figure1-1.png"]], "question_id": [3], "question": ["In what ways can it be said that the concatenation acts as a skip connection?"], "question_section": ["Proposed method"], "question_trigger_sentence": ["This concatenation can be viewed as a simple form of a \u201cskip connection\u201d [13] between the different \u201csearch depths\u201d, or \u201clayers\u201d of the GraphSAGE algorithm, and it leads to significant gains in performance (Section 4).\n"], "question_type": ["Deep/complex question"], "evidential_info": [[{"context": "In this section, we describe the embedding generation, or forward propagation algorithm (Algorithm 1), which assumes that the model has already been trained and that the parameters are fixed.In particular, we assume that we have learned the parameters of K aggregator functions (denoted \\textsc{aggregate}_{k},\\forall k\\in\\{1,...,K\\}), which aggregate information from node neighbors, as well as a set of weight matrices \\mathbf{W}^{k},\\forall k\\in\\{1,...,K\\}, which are used to propagate information between different layers of the model or \u201csearch depths\u201d.Section 3.2 describes how we train these parameters.", "rationale": "In particular, we assume that we have learned the parameters of K aggregator functions (denoted \\textsc{aggregate}_{k},\\forall k\\in\\{1,...,K\\}), which aggregate information from node neighbors, as well as a set of weight matrices \\mathbf{W}^{k},\\forall k\\in\\{1,...,K\\}, which are used to propagate information  between different layers of the model or \u201csearch depths"}, {"context": "Instead of training a distinct embedding vector for each node, we train a set of aggregator functions that learn to aggregate feature information from a node\u2019s local neighborhood (Figure 1).Each aggregator function aggregates information from a different number of hops, or search depth, away from a given node.At test, or inference time, we use our trained system to generate embeddings for entirely unseen nodes by applying the learned aggregation functions.Following previous work on generating node embeddings, we design an unsupervised loss function that allows GraphSAGE\u00a0to be trained without task-specific supervision.We also show that GraphSAGE\u00a0can be trained in a fully supervised manner.", "rationale": "Each aggregator function aggregates information from a different number of hops, or search depth, away from a given node."}, {"context": "Paragraph 10 :", "rationale": "This concatenation can be"}]], "composition": ["Skip connection is to consider information from different search depths or layers simultaneously.\nGraphSAGE use a set of weight matrices and concatenation to consider information from diverse search depths. \nIt can be interpreted as a skip connection.\nThe reason is that a set of weight matrices are used to propagate information \n between different layers of the model or search depths, while considering different search depth is a kind of skip-connection."], "Is_figure_in_evidence": [true], "Is_table_in_evidence": [false], "question_key": ["1207"], "passages": ["Low-dimensional vector embeddings of nodes in large graphs111While it is common to refer to these data structures as social or biological networks, we use the term graph to avoid ambiguity with neural network terminology. have proved extremely useful as feature inputs for a wide variety of prediction and graph analysis tasks [5, 11, 28, 35, 36].The basic idea behind node embedding approaches is to use dimensionality reduction techniques to distill the high-dimensional information about a node\u2019s graph neighborhood into a dense vector embedding.These node embeddings can then be fed to downstream machine learning systems and aid in tasks such as node classification, clustering, and link prediction [11, 28, 35].", "However, previous works have focused on embedding nodes from a single fixed graph, and many real-world applications require embeddings to be quickly generated for unseen nodes, or entirely new (sub)graphs.This inductive capability is essential for high-throughput, production machine learning systems, which operate on evolving graphs and constantly encounter unseen nodes (e.g., posts on Reddit, users and videos on Youtube).An inductive approach to generating node embeddings also facilitates generalization across graphs with the same form of features:for example, one could train an embedding generator on protein-protein interaction graphs derived from a model organism, and then easily produce node embeddings for data collected on new organisms using the trained model.", "The inductive node embedding problem is especially difficult, compared to the transductive setting, because generalizing to unseen nodes requires \u201caligning\u201d newly observed subgraphs to the node embeddings that the algorithm has already optimized on.An inductive framework must learn to recognize structural properties of a node\u2019s neighborhood that reveal both the node\u2019s local role in the graph, as well as its global position.", "Most existing approaches to generating node embeddings are inherently transductive.The majority of these approaches directly optimize the embeddings for each node using matrix-factorization-based objectives, and do not naturally generalize to unseen data, since they make predictions on nodes in a single, fixed graph [5, 11, 23, 28, 35, 36, 37, 39].These approaches can be modified to operate in an inductive setting (e.g., [28]), but these modifications tend to be computationally expensive, requiring additional rounds of gradient descent before new predictions can be made.There are also recent approaches to learning over graph structures using convolution operators that offer promise as an embedding methodology [17].So far, graph convolutional networks (GCNs) have only been applied in the transductive setting with fixed graphs [17, 18].In this work we both extend GCNs to the task of inductive unsupervised learning and propose a framework that generalizes the GCN approach to use trainable aggregation functions (beyond simple convolutions).", "Present work. We propose a general framework, called GraphSAGE\u00a0(sample and aggregate), for inductive node embedding.Unlike embedding approaches that are based on matrix factorization, we leverage node features (e.g., text attributes, node profile information, node degrees) in order to learn an embedding function that generalizes to unseen nodes.By incorporating node features in the learning algorithm, we simultaneously learn the topological structure of each node\u2019s neighborhood as well as the distribution of node features in the neighborhood.While we focus on feature-rich graphs (e.g., citation data with text attributes, biological data with functional/molecular markers), our approach can also make use of structural features that are present in all graphs (e.g., node degrees).Thus, our algorithm can also be applied to graphs without node features.", "Instead of training a distinct embedding vector for each node, we train a set of aggregator functions that learn to aggregate feature information from a node\u2019s local neighborhood (Figure 1).Each aggregator function aggregates information from a different number of hops, or search depth, away from a given node.At test, or inference time, we use our trained system to generate embeddings for entirely unseen nodes by applying the learned aggregation functions.Following previous work on generating node embeddings, we design an unsupervised loss function that allows GraphSAGE\u00a0to be trained without task-specific supervision.We also show that GraphSAGE\u00a0can be trained in a fully supervised manner.", "We evaluate our algorithm on three node-classification benchmarks, which test GraphSAGE\u2019s ability to generate useful embeddings on unseen data.We use two evolving document graphs based on citation data and Reddit post data (predicting paper and post categories, respectively), and a multi-graph generalization experiment based on a dataset of protein-protein interactions (predicting protein functions).Using these benchmarks, we show that our approach is able to effectively generate representations for unseen nodes and outperform relevant baselines by a significant margin: across domains, our supervised approach improves classification F1-scores by an average of 51% compared to using node features alone and GraphSAGE\u00a0consistently outperforms a strong, transductive baseline [28], despite this baseline taking \u223c100\u00d7{\\sim}100\\times\u223c 100 \u00d7 longer to run on unseen nodes.We also show that the new aggregator architectures we propose provide significant gains (7.4% on average) compared to an aggregator inspired by graph convolutional networks [17].Lastly, we probe the expressive capability of our approach and show, through theoretical analysis, that GraphSAGE\u00a0is capable of learning structural information about a node\u2019s role in a graph, despite the fact that it is inherently based on features (Section 5).", "Our algorithm is conceptually related to previous node embedding approaches, general supervised approaches to learning over graphs, and recent advancements in applying convolutional neural networks to graph-structured data.222In the time between this papers original submission to NIPS 2017 and the submission of the final, accepted (i.e., \u201ccamera-ready\u201d) version, there have been a number of closely related (e.g., follow-up) works published on pre-print servers. For temporal clarity, we do not review or compare against these papers in detail.", "Factorization-based embedding approaches.There are a number of recent node embedding approaches that learn low-dimensional embeddings using random walk statistics and matrix factorization-based learning objectives [5, 11, 28, 35, 36].These methods also bear close relationships to more classic approaches to spectral clustering [23], multi-dimensional scaling [19], as well as the PageRank algorithm [25].Since these embedding algorithms directly train node embeddings for individual nodes, they are inherently transductive and, at the very least, require expensive additional training (e.g., via stochastic gradient descent) to make predictions on new nodes.In addition, for many of these approaches (e.g., [11, 28, 35, 36]) the objective function is invariant to orthogonal transformations of the embeddings, which means that the embedding space does not naturally generalize between graphs and can drift during re-training.One notable exception to this trend is the Planetoid-I algorithm introduced by Yang et al.\u00a0[40], which is an inductive, embedding-based approach to semi-supervised learning.However, Planetoid-I does not use any graph structural information during inference; instead, it uses the graph structure as a form of regularization during training.Unlike these previous approaches, we leverage feature information in order to train a model to produce embeddings for unseen nodes.", "Supervised learning over graphs.Beyond node embedding approaches, there is a rich literature on supervised learning over graph-structured data.This includes a wide variety of kernel-based approaches, where feature vectors for graphs are derived from various graph kernels (see [32] and references therein).There are also a number of recent neural network approaches to supervised learning over graph structures [7, 10, 21, 31].Our approach is conceptually inspired by a number of these algorithms.However, whereas these previous approaches attempt to classify entire graphs (or subgraphs), the focus of this work is generating useful representations for individual nodes.", "Graph convolutional networks.In recent years, several convolutional neural network architectures for learning over graphs have been proposed (e.g., [4, 9, 8, 17, 24]).The majority of these methods do not scale to large graphs or are designed for whole-graph classification (or both) [4, 9, 8, 24].However, our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al. [17, 18].The original GCN algorithm [17] is designed for semi-supervised learning in a transductive setting, and the exact algorithm requires that the full graph Laplacian is known during training.A simple variant of our algorithm can be viewed as an extension of the GCN framework to the inductive setting, a point which we revisit in Section 3.3.", "The key idea behind our approach is that we learn how to aggregate feature information from a node\u2019s local neighborhood (e.g., the degrees or text attributes of nearby nodes).We first describe the GraphSAGE\u00a0embedding generation (i.e., forward propagation) algorithm, which generates embeddings for nodes assuming that the GraphSAGE\u00a0model parameters are already learned (Section 3.1).We then describe how the GraphSAGE\u00a0model parameters can be learned using standard stochastic gradient descent and backpropagation techniques (Section 3.2).", "In this section, we describe the embedding generation, or forward propagation algorithm (Algorithm 1), which assumes that the model has already been trained and that the parameters are fixed.In particular, we assume that we have learned the parameters of K\ud835\udc3eKitalic_K aggregator functions (denoted aggregatek,\u2200k\u2208{1,\u2026,K}subscriptaggregate\ud835\udc58for-all\ud835\udc581\u2026\ud835\udc3e\\textsc{aggregate}_{k},\\forall k\\in\\{1,...,K\\}aggregate start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , \u2200 italic_k \u2208 { 1 , \u2026 , italic_K }), which aggregate information from node neighbors, as well as a set of weight matrices \ud835\udc16k,\u2200k\u2208{1,\u2026,K}superscript\ud835\udc16\ud835\udc58for-all\ud835\udc581\u2026\ud835\udc3e\\mathbf{W}^{k},\\forall k\\in\\{1,...,K\\}bold_W start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , \u2200 italic_k \u2208 { 1 , \u2026 , italic_K }, which are used to propagate information between different layers of the model or \u201csearch depths\u201d.Section 3.2 describes how we train these parameters.", "The intuition behind Algorithm 1 is that at each iteration, or search depth, nodes aggregate information from their local neighbors, and as this process iterates, nodes incrementally gain more and more information from further reaches of the graph.", "Algorithm 1 describes the embedding generation process in the case where the entire graph, \ud835\udca2=(\ud835\udcb1,\u2130)\ud835\udca2\ud835\udcb1\u2130\\mathcal{G}=(\\mathcal{V},\\mathcal{E})caligraphic_G = ( caligraphic_V , caligraphic_E ), and features for all nodes \ud835\udc31v,\u2200v\u2208\ud835\udcb1subscript\ud835\udc31\ud835\udc63for-all\ud835\udc63\ud835\udcb1\\mathbf{x}_{v},\\forall v\\in\\mathcal{V}bold_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , \u2200 italic_v \u2208 caligraphic_V, are provided as input.We describe how to generalize this to the minibatch setting below.Each step in the outer loop of Algorithm 1 proceeds as follows, where k\ud835\udc58kitalic_k denotes the current step in the outer loop (or the depth of the search) and \ud835\udc21ksuperscript\ud835\udc21\ud835\udc58\\mathbf{h}^{k}bold_h start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT denotes a node\u2019s representation at this step: First, each node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V aggregates the representations of the nodes in its immediate neighborhood, {\ud835\udc21uk\u22121,\u2200u\u2208\ud835\udca9(v)}subscriptsuperscript\ud835\udc21\ud835\udc581\ud835\udc62for-all\ud835\udc62\ud835\udca9\ud835\udc63\\{\\mathbf{h}^{k-1}_{u},\\forall u\\in\\mathcal{N}(v)\\}{ bold_h start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT , \u2200 italic_u \u2208 caligraphic_N ( italic_v ) }, into a single vector \ud835\udc21\ud835\udca9(v)k\u22121subscriptsuperscript\ud835\udc21\ud835\udc581\ud835\udca9\ud835\udc63\\mathbf{h}^{k-1}_{\\mathcal{N}(v)}bold_h start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_N ( italic_v ) end_POSTSUBSCRIPT.Note that this aggregation step depends on the representations generated at the previous iteration of the outer loop (i.e., k\u22121\ud835\udc581k-1italic_k - 1), and the k=0\ud835\udc580k=0italic_k = 0 (\u201cbase case\u201d) representations are defined as the input node features.After aggregating the neighboring feature vectors, GraphSAGE\u00a0then concatenates the node\u2019s current representation, \ud835\udc21vk\u22121superscriptsubscript\ud835\udc21\ud835\udc63\ud835\udc581\\mathbf{h}_{v}^{k-1}bold_h start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT, with the aggregated neighborhood vector, \ud835\udc21\ud835\udca9(v)k\u22121subscriptsuperscript\ud835\udc21\ud835\udc581\ud835\udca9\ud835\udc63\\mathbf{h}^{k-1}_{\\mathcal{N}(v)}bold_h start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_N ( italic_v ) end_POSTSUBSCRIPT, and this concatenated vector is fed through a fully connected layer with nonlinear activation function \u03c3\ud835\udf0e\\sigmaitalic_\u03c3, which transforms the representations to be used at the next step of the algorithm (i.e., \ud835\udc21vk,\u2200v\u2208\ud835\udcb1superscriptsubscript\ud835\udc21\ud835\udc63\ud835\udc58for-all\ud835\udc63\ud835\udcb1\\mathbf{h}_{v}^{k},\\forall v\\in\\mathcal{V}bold_h start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , \u2200 italic_v \u2208 caligraphic_V).For notational convenience, we denote the final representations output at depth K\ud835\udc3eKitalic_K as \ud835\udc33v\u2261\ud835\udc21vK,\u2200v\u2208\ud835\udcb1formulae-sequencesubscript\ud835\udc33\ud835\udc63subscriptsuperscript\ud835\udc21\ud835\udc3e\ud835\udc63for-all\ud835\udc63\ud835\udcb1\\mathbf{z}_{v}\\equiv\\mathbf{h}^{K}_{v},\\forall v\\in\\mathcal{V}bold_z start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2261 bold_h start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , \u2200 italic_v \u2208 caligraphic_V.The aggregation of the neighbor representations can be done by a variety of aggregator architectures (denoted by the aggregate placeholder in Algorithm 1), and we discuss different architecture choices in Section 3.3 below.", "To extend Algorithm 1 to the minibatch setting, given a set of input nodes, we first forward sample the required neighborhood sets (up to depth K\ud835\udc3eKitalic_K) and then we run the inner loop (line 3 in Algorithm 1), but instead of iterating over all nodes, we compute only the representations that are necessary to satisfy the recursion at each depth (Appendix A contains complete minibatch pseudocode).", "Relation to the Weisfeiler-Lehman Isomorphism Test.The GraphSAGE\u00a0algorithm is conceptually inspired by a classic algorithm for testing graph isomorphism.If, in Algorithm 1, we (i) set K=|\ud835\udcb1|\ud835\udc3e\ud835\udcb1K=|\\mathcal{V}|italic_K = | caligraphic_V |, (ii) set the weight matrices as the identity, and (iii) use an appropriate hash function as an aggregator (with no non-linearity), then Algorithm 1 is an instance of the Weisfeiler-Lehman (WL) isomorphism test, also known as \u201cnaive vertex refinement\u201d [32].If the set of representations {\ud835\udc33v,\u2200v\u2208\ud835\udcb1}subscript\ud835\udc33\ud835\udc63for-all\ud835\udc63\ud835\udcb1\\{\\mathbf{z}_{v},\\forall v\\in\\mathcal{V}\\}{ bold_z start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , \u2200 italic_v \u2208 caligraphic_V } output by Algorithm 1 for two subgraphs are identical then the WL test declares the two subgraphs to be isomorphic.This test is known to fail in some cases, but is valid for a broad class of graphs [32].GraphSAGE\u00a0is a continuous approximation to the WL test, where we replace the hash function with trainable neural network aggregators.Of course, we use GraphSAGE\u00a0to generate useful node representations\u2013not to test graph isomorphism.Nevertheless, the connection between GraphSAGE\u00a0and the classic WL test provides theoretical context for our algorithm design to learn the topological structure of node neighborhoods.", "Neighborhood definition.In this work, we uniformly sample a fixed-size set of neighbors, instead of using full neighborhood sets in Algorithm 1, in order to keep the computational footprint of each batch fixed.333Exploring non-uniform samplers is an important direction for future work.That is, using overloaded notation, we define \ud835\udca9(v)\ud835\udca9\ud835\udc63\\mathcal{N}(v)caligraphic_N ( italic_v ) as a fixed-size, uniform draw from the set {u\u2208\ud835\udcb1:(u,v)\u2208\u2130}conditional-set\ud835\udc62\ud835\udcb1\ud835\udc62\ud835\udc63\u2130\\{u\\in\\mathcal{V}:(u,v)\\in\\mathcal{E}\\}{ italic_u \u2208 caligraphic_V : ( italic_u , italic_v ) \u2208 caligraphic_E }, and we draw different uniform samples at each iteration, k\ud835\udc58kitalic_k, in Algorithm 1.Without this sampling the memory and expected runtime of a single batch is unpredictable and in the worst case O(|\ud835\udcb1|)\ud835\udc42\ud835\udcb1O(|\\mathcal{V}|)italic_O ( | caligraphic_V | ).In contrast, the per-batch space and time complexity for GraphSAGE\u00a0is fixed at O(\u220fi=1KSi)\ud835\udc42superscriptsubscriptproduct\ud835\udc561\ud835\udc3esubscript\ud835\udc46\ud835\udc56O(\\prod_{i=1}^{K}S_{i})italic_O ( \u220f start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), where Si,i\u2208{1,\u2026,K}subscript\ud835\udc46\ud835\udc56\ud835\udc561\u2026\ud835\udc3eS_{i},i\\in\\{1,...,K\\}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i \u2208 { 1 , \u2026 , italic_K } and K\ud835\udc3eKitalic_K are user-specified constants.Practically speaking we found that our approach could achieve high performance with K=2\ud835\udc3e2K=2italic_K = 2 and S1\u22c5S2\u2264500\u22c5subscript\ud835\udc461subscript\ud835\udc462500S_{1}\\cdot S_{2}\\leq 500italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u22c5 italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2264 500 (see Section\u00a04.4 for details).", "In order to learn useful, predictive representations in a fully unsupervised setting, we apply a graph-based loss function to the output representations, \ud835\udc33u,\u2200u\u2208\ud835\udcb1subscript\ud835\udc33\ud835\udc62for-all\ud835\udc62\ud835\udcb1\\mathbf{z}_{u},\\forall u\\in{\\mathcal{V}}bold_z start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT , \u2200 italic_u \u2208 caligraphic_V, and tune the weight matrices, \ud835\udc16k,\u2200k\u2208{1,\u2026,K}superscript\ud835\udc16\ud835\udc58for-all\ud835\udc581\u2026\ud835\udc3e\\mathbf{W}^{k},\\forall k\\in\\{1,...,K\\}bold_W start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , \u2200 italic_k \u2208 { 1 , \u2026 , italic_K }, and parameters of the aggregator functions via stochastic gradient descent.The graph-based loss function encourages nearby nodes to have similar representations, while enforcing that the representations of disparate nodes are highly distinct:J\ud835\udca2(\ud835\udc33u)=\u2212log\u2061(\u03c3(\ud835\udc33u\u22a4\ud835\udc33v))\u2212Q\u22c5\ud835\udd3cvn\u223cPn(v)log\u2061(\u03c3(\u2212\ud835\udc33u\u22a4\ud835\udc33vn)),subscript\ud835\udc3d\ud835\udca2subscript\ud835\udc33\ud835\udc62\ud835\udf0esubscriptsuperscript\ud835\udc33top\ud835\udc62subscript\ud835\udc33\ud835\udc63\u22c5\ud835\udc44subscript\ud835\udd3csimilar-tosubscript\ud835\udc63\ud835\udc5bsubscript\ud835\udc43\ud835\udc5b\ud835\udc63\ud835\udf0esubscriptsuperscript\ud835\udc33top\ud835\udc62subscript\ud835\udc33subscript\ud835\udc63\ud835\udc5bJ_{\\mathcal{G}}(\\mathbf{z}_{u})=-\\log\\left(\\sigma(\\mathbf{z}^{\\top}_{u}\\mathbf{z}_{v})\\right)-Q\\cdot\\mathbb{E}_{v_{n}\\sim P_{n}(v)}\\log\\left(\\sigma(-\\mathbf{z}^{\\top}_{u}\\mathbf{z}_{v_{n}})\\right),italic_J start_POSTSUBSCRIPT caligraphic_G end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) = - roman_log ( italic_\u03c3 ( bold_z start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) ) - italic_Q \u22c5 blackboard_E start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT \u223c italic_P start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_v ) end_POSTSUBSCRIPT roman_log ( italic_\u03c3 ( - bold_z start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ) ,(1)where v\ud835\udc63vitalic_v is a node that co-occurs near u\ud835\udc62uitalic_u on fixed-length random walk, \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 is the sigmoid function, Pnsubscript\ud835\udc43\ud835\udc5bP_{n}italic_P start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT is a negative sampling distribution, and Q\ud835\udc44Qitalic_Q defines the number of negative samples.Importantly, unlike previous embedding approaches, the representations \ud835\udc33usubscript\ud835\udc33\ud835\udc62\\mathbf{z}_{u}bold_z start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT that we feed into this loss function are generated from the features contained within a node\u2019s local neighborhood, rather than training a unique embedding for each node (via an embedding look-up).", "This unsupervised setting emulates situations where node features are provided to downstream machine learning applications, as a service or in a static repository.In cases where representations are to be used only on a specific downstream task, the unsupervised loss (Equation 1) can simply be replaced, or augmented, by a task-specific objective (e.g., cross-entropy loss).", "Unlike machine learning over N-D lattices (e.g., sentences, images, or 3-D volumes), a node\u2019s neighbors have no natural ordering; thus, the aggregator functions in Algorithm 1 must operate over an unordered set of vectors.Ideally, an aggregator function would be symmetric (i.e., invariant to permutations of its inputs) while still being trainable and maintaining high representational capacity.The symmetry property of the aggregation function ensures that our neural network model can be trained and applied to arbitrarily ordered node neighborhood feature sets.We examined three candidate aggregator functions:", "Mean aggregator. Our first candidate aggregator function is the mean operator, where we simply take the elementwise mean of the vectors in {\ud835\udc21uk\u22121,\u2200u\u2208\ud835\udca9(v)}superscriptsubscript\ud835\udc21\ud835\udc62\ud835\udc581for-all\ud835\udc62\ud835\udca9\ud835\udc63\\{\\mathbf{h}_{u}^{k-1},\\forall u\\in\\mathcal{N}(v)\\}{ bold_h start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT , \u2200 italic_u \u2208 caligraphic_N ( italic_v ) }.The mean aggregator is nearly equivalent to the convolutional propagation rule used in the transductive GCN framework [17].In particular, we can derive an inductive variant of the GCN approach by replacing lines 4 and 5 in Algorithm 1 with the following:444Note that this differs from Kipf et al\u2019s exact equation by a minor normalization constant [17].\ud835\udc21vk\u2190\u03c3(\ud835\udc16\u22c5mean({\ud835\udc21vk\u22121}\u222a{\ud835\udc21uk\u22121,\u2200u\u2208\ud835\udca9(v)}).\\mathbf{h}^{k}_{v}\\leftarrow\\sigma(\\mathbf{W}\\cdot\\textsc{mean}(\\{\\mathbf{h}^{k-1}_{v}\\}\\cup\\{\\mathbf{h}_{u}^{k-1},\\forall u\\in{\\mathcal{N}(v)}\\}).bold_h start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2190 italic_\u03c3 ( bold_W \u22c5 mean ( { bold_h start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } \u222a { bold_h start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT , \u2200 italic_u \u2208 caligraphic_N ( italic_v ) } ) .(2)We call this modified mean-based aggregator convolutional since it is a rough, linear approximation of a localized spectral convolution [17].An important distinction between this convolutional aggregator and our other proposed aggregators is that it does not perform the concatenation operation in line 5 of Algorithm 1\u2014i.e., the convolutional aggregator does concatenate the node\u2019s previous layer representation \ud835\udc21vk\u22121subscriptsuperscript\ud835\udc21\ud835\udc581\ud835\udc63\\mathbf{h}^{k-1}_{v}bold_h start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT with the aggregated neighborhood vector \ud835\udc21\ud835\udca9(v)ksubscriptsuperscript\ud835\udc21\ud835\udc58\ud835\udca9\ud835\udc63\\mathbf{h}^{k}_{\\mathcal{N}(v)}bold_h start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_N ( italic_v ) end_POSTSUBSCRIPT.This concatenation can be viewed as a simple form of a \u201cskip connection\u201d [13] between the different \u201csearch depths\u201d, or \u201clayers\u201d of the GraphSAGE algorithm, and it leads to significant gains in performance (Section 4).", "LSTM aggregator. We also examined a more complex aggregator based on an LSTM architecture [14].Compared to the mean aggregator, LSTMs have the advantage of larger expressive capability. However, it is important to note that LSTMs are not inherently symmetric (i.e., they are not permutation invariant), since they process their inputs in a sequential manner.We adapt LSTMs to operate on an unordered set by simply applying the LSTMs to a random permutation of the node\u2019s neighbors.", "Pooling aggregator. The final aggregator we examine is both symmetric and trainable.In this pooling approach, each neighbor\u2019s vector is independently fed through a fully-connected neural network; following this transformation, an elementwise max-pooling operation is applied to aggregate information across the neighbor set:aggregatekpool=max\u2061({\u03c3(\ud835\udc16pool\ud835\udc21uik+\ud835\udc1b),\u2200ui\u2208\ud835\udca9(v)}),superscriptsubscriptaggregate\ud835\udc58pool\ud835\udf0esubscript\ud835\udc16poolsubscriptsuperscript\ud835\udc21\ud835\udc58subscript\ud835\udc62\ud835\udc56\ud835\udc1bfor-allsubscript\ud835\udc62\ud835\udc56\ud835\udca9\ud835\udc63\\textsc{aggregate}_{k}^{\\textrm{pool}}=\\max(\\{\\sigma\\left(\\mathbf{W}_{\\textrm{pool}}\\mathbf{h}^{k}_{u_{i}}+\\mathbf{b}\\right),\\forall u_{i}\\in\\mathcal{N}(v)\\}),aggregate start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT pool end_POSTSUPERSCRIPT = roman_max ( { italic_\u03c3 ( bold_W start_POSTSUBSCRIPT pool end_POSTSUBSCRIPT bold_h start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT + bold_b ) , \u2200 italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 caligraphic_N ( italic_v ) } ) ,(3)where max\\maxroman_max denotes the element-wise max operator and \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 is a nonlinear activation function.In principle, the function applied before the max pooling can be an arbitrarily deep multi-layer perceptron, but we focus on simple single-layer architectures in this work.This approach is inspired by recent advancements in applying neural network architectures to learn over general point sets [29].Intuitively, the multi-layer perceptron can be thought of as a set of functions that compute features for each of the node representations in the neighbor set. By applying the max-pooling operator to each of the computed features, the model effectively captures different aspects of the neighborhood set.Note also that, in principle, any symmetric vector function could be used in place of the max\\maxroman_max operator (e.g., an element-wise mean).We found no significant difference between max- and mean-pooling in developments test and thus focused on max-pooling for the rest of our experiments.", "We test the performance of GraphSAGE\u00a0on three benchmark tasks: (i) classifying academic papers into different subjects using the Web of Sciencecitation dataset, (ii) classifying Reddit posts as belonging to different communities, and (iii) classifying protein functions across various biological protein-protein interaction (PPI) graphs. Sections 4.1 and 4.2 summarize the datasets, and the supplementary material contains additional information.In all these experiments, we perform predictions on nodes that are not seen during training, and, in the case of the PPI dataset, we test on entirely unseen graphs.", "Experimental set-up.To contextualize the empirical results on our inductive benchmarks, we compare against four baselines:a random classifer, a logistic regression feature-based classifier (that ignores graph structure), the DeepWalk algorithm [28] as a representative factorization-based approach, and aconcatenation of the raw features and DeepWalk embeddings.We also compare four variants of GraphSAGE\u00a0that use the different aggregator functions (Section 3.3).Since, the \u201cconvolutional\u201d variant of GraphSAGE\u00a0is an extended, inductive version of Kipf et al\u2019s semi-supervised GCN [17], we term this variant GraphSAGE-GCN.We test unsupervised variants of GraphSAGE\u2009 trained according to the loss in Equation (1), as well as supervised variants that are trained directly on classification cross-entropy loss.For all the GraphSAGE\u00a0variants we used rectified linear units as the non-linearity and set K=2\ud835\udc3e2K=2italic_K = 2 with neighborhood sample sizes S1=25subscript\ud835\udc46125S_{1}=25italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 25 and S2=10subscript\ud835\udc46210S_{2}=10italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 10 (see Section 4.4 for sensitivity analyses).", "For the Reddit and citation datasets, we use \u201conline\u201d training for DeepWalk as described in Perozzi et al.\u00a0[28], where we run a new round of SGD optimization to embed the new test nodes before making predictions (see the Appendix for details).In the multi-graph setting, we cannot apply DeepWalk, since the embedding spaces generated by running the DeepWalk algorithm on different disjoint graphs can be arbitrarily rotated with respect to each other (Appendix D).", "All models were implemented in TensorFlow [1] with the Adam optimizer [16] (except DeepWalk, which performed better with the vanilla gradient descent optimizer).We designed our experiments with the goals of (i) verifying the improvement of GraphSAGE\u00a0over the baseline approaches (i.e., raw features and DeepWalk) and (ii) providing a rigorous comparison of the different GraphSAGE\u00a0aggregator architectures.In order to provide a fair comparison, all models share an identical implementation of their minibatch iterators, loss function and neighborhood sampler (when applicable).Moreover, in order to guard against unintentional \u201chyperparameter hacking\u201d in the comparisons between GraphSAGE\u00a0aggregators, we sweep over the same set of hyperparameters for all GraphSAGE\u00a0variants (choosing the best setting for each variant according to performance on a validation set).The set of possible hyperparameter values was determined on early validation tests using subsets of the citation and Reddit data that we then discarded from our analyses.The appendix contains further implementation details.555Code and links to the datasets: http://snap.stanford.edu/graphsage/", "Our first two experiments are on classifying nodes in evolving information graphs, a task that is especially relevant to high-throughput production systems, which constantly encounter unseen data.", "Citation data.Our first task is predicting paper subject categories on a large citation dataset.We use an undirected citation graph dataset derived from the Thomson Reuters Web of Science Core Collection, corresponding to all papers in six biology-related fields for the years 2000-2005.The node labels for this dataset correspond to the six different field labels.In total, this is dataset contains 302,424 nodes with an average degree of 9.15.We train all the algorithms on the 2000-2004 data and use the 2005 data for testing (with 30% used for validation).For features, we used node degrees and processed the paper abstracts according Arora et al.\u2019s\u00a0[2] sentence embedding approach, with 300-dimensional word vectors trained using the GenSim word2vec implementation [30].", "Reddit data.In our second task, we predict which community different Reddit posts belong to.Reddit is a large online discussion forum where users post and comment on content in different topical communities.We constructed a graph dataset from Reddit posts made in the month of September, 2014.The node label in this case is the community, or \u201csubreddit\u201d, that a post belongs to.We sampled 50 large communities and built a post-to-post graph, connecting posts if the same user comments on both.In total this dataset contains 232,965 posts with an average degree of 492.We use the first 20 days for training and the remaining days for testing (with 30% used for validation).For features, we use off-the-shelf 300-dimensional GloVe CommonCrawl word vectors [27]; for each post, we concatenated (i) the average embedding of the post title, (ii) the average embedding of all the post\u2019s comments (iii) the post\u2019s score, and (iv) the number of comments made on the post.", "The first four columns of Table 1 summarize the performance of GraphSAGE\u2009 as well as the baseline approaches on these two datasets.We find that GraphSAGE\u00a0outperforms all the baselines by a significant margin, and the trainable, neural network aggregators provide significant gains compared to the GCN approach.For example, the unsupervised variant GraphSAGE-pool outperforms the concatenation of the DeepWalk embeddings and the raw features by 13.8% on the citation data and 29.1% on the Reddit data, while the supervised version provides a gain of 19.7% and 37.2%, respectively.Interestingly, the LSTM based aggregator shows strong performance, despite the fact that it is designed for sequential data and not unordered sets.Lastly, we see that the performance of unsupervised GraphSAGE\u00a0is reasonably competitive with the fully supervised version, indicating that our framework can achieve strong performance without task-specific fine-tuning.", "We now consider the task of generalizing across graphs, which requires learning about node roles rather than community structure.We classify protein roles\u2014in terms of their cellular functions from gene ontology\u2014in variousprotein-protein interaction (PPI) graphs, with each graph corresponding to a different human tissue [41].We use positional gene sets, motif gene sets and immunological signatures as features and geneontology sets as labels (121 in total), collected from the Molecular Signatures Database [34].The average graph contains 2373 nodes, with an average degree of 28.8.We train all algorithms on 20 graphs and then average prediction F1 scores on two test graphs (with two other graphs used for validation).", "The final two columns of Table 1 summarize the accuracies of the various approaches on this data.Again we see that GraphSAGE\u00a0significantly outperforms the baseline approaches, with the LSTM- and pooling-based aggregators providing substantial gains over the mean- and GCN-based aggregators.666Note that in very recent follow-up work Chen and Zhu [6] achieve superior performance by optimizing the GraphSAGE\u00a0hyperparameters specifically for the PPI task and implementing new training techniques (e.g., dropout, layer normalization, and a new sampling scheme). We refer the reader to their work for the current state-of-the-art numbers on the PPI dataset that are possible using a variant of the GraphSAGE\u00a0approach.", "Figure 2.A summarizes the training and test runtimes for the different approaches.The training time for the methods are comparable (with GraphSAGE-LSTM being the slowest).However, the need to sample new random walks and run new rounds of SGD to embed unseen nodes makes DeepWalk 100-500\u00d7100\\text{-}500\\times100 - 500 \u00d7 slower at test time.", "For the GraphSAGE\u00a0variants, we found that setting K=2\ud835\udc3e2K=2italic_K = 2 provided a consistent boost in accuracy of around 10-15%10-percent1510\\text{-}15\\%10 - 15 %, on average, comparedto K=1\ud835\udc3e1K=1italic_K = 1; however, increasing K\ud835\udc3eKitalic_K beyond 2 gave marginal returns in performance (0-5%0-percent50\\text{-}5\\%0 - 5 %)while increasing the runtime by a prohibitively large factor of 10-100\u00d710\\text{-}100{\\times}10 - 100 \u00d7, dependingon the neighborhood sample size.We also found diminishing returns for sampling large neighborhoods (Figure2.B).Thus, despitethe higher variance induced by sub-sampling neighborhoods, GraphSAGE\u00a0is still able to maintain strong predictive accuracy, while significantly improving the runtime.", "Overall, we found that the LSTM- and pool-based aggregators performed the best, in terms of both average performance and number of experimental settings where they were the top-performing method (Table 1).To give more quantitative insight into these trends, we consider each of the six different experimental settings (i.e., (3 datasets)\u00d7(unsupervised vs.\u00a0supervised)(3 datasets)unsupervised vs.\u00a0supervised\\textrm{(3 datasets)}\\times(\\textrm{unsupervised vs.\\ supervised})(3 datasets) \u00d7 ( unsupervised vs. supervised )) as trials and consider what performance trends are likely to generalize.In particular, we use the non-parametric Wilcoxon Signed-Rank Test [33] to quantify the differences between the different aggregators across trials, reporting the T\ud835\udc47Titalic_T-statistic and p\ud835\udc5dpitalic_p-value where applicable.Note that this method is rank-based and essentially tests whether we would expect one particular approach to outperform another in a new experimental setting.Given our small sample size of only 6 different settings, this significance test is somewhat underpowered; nonetheless, the T\ud835\udc47Titalic_T-statistic and associated p\ud835\udc5dpitalic_p-values are useful quantitative measures to assess the aggregators\u2019 relative performances.", "We see that LSTM-, pool- and mean-based aggregators all provide statistically significant gains over the GCN-based approach (T=1.0\ud835\udc471.0T=1.0italic_T = 1.0, p=0.02\ud835\udc5d0.02p=0.02italic_p = 0.02 for all three).However, the gains of the LSTM and pool approaches over the mean-based aggregator are more marginal (T=1.5\ud835\udc471.5T=1.5italic_T = 1.5, p=0.03\ud835\udc5d0.03p=0.03italic_p = 0.03, comparing LSTM to mean; T=4.5\ud835\udc474.5T=4.5italic_T = 4.5, p=0.10\ud835\udc5d0.10p=0.10italic_p = 0.10, comparing pool to mean).There is no significant difference between the LSTM and pool approaches (T=10.0\ud835\udc4710.0T=10.0italic_T = 10.0, p=0.46\ud835\udc5d0.46p=0.46italic_p = 0.46).However, GraphSAGE-LSTM is significantly slower than GraphSAGE-pool (by a factor of \u22482\u00d7{\\approx}2{\\times}\u2248 2 \u00d7), perhaps giving the pooling-based aggregator a slight edge overall.", "In this section, we probe the expressive capabilities of GraphSAGE\u00a0in order toprovide insight into how GraphSAGE\u00a0can learn about graph structure, even though it is inherently based on features.As a case-study, we consider whether GraphSAGE\u00a0can learn to predict the clustering coefficient of a node, i.e., the proportion of triangles that are closed within the node\u2019s 1-hop neighborhood [38]. The clustering coefficient is a popular measure of how clustered a node\u2019s local neighborhood is, and it serves as a building block for many more complicated structural motifs [3]. We can show that Algorithm 1 is capable of approximating clustering coefficients to an arbitrary degree of precision:", "Theorem 1 states that for any graph there exists a parameter setting for Algorithm 1 such that it can approximate clustering coefficients in that graph to an arbitrary precision, if the features for every node are distinct (and if the model is sufficiently high-dimensional).The full proof of Theorem 1 is in the Appendix.Note that as a corollary of Theorem 1, GraphSAGE\u00a0can learn about local graph structure, even when the node feature inputs are sampled from an absolutely continuous random distribution (see the Appendix for details).The basic idea behind the proof is that if each node has a unique feature representation, then we can learn to map nodes to indicator vectors and identify node neighborhoods.The proof of Theorem 1 relies on some properties of the pooling aggregator, which also provides insight into why GraphSAGE-pool outperforms the GCN and mean-based aggregators.", "We introduced a novel approach that allows embeddings to be efficiently generated for unseen nodes.GraphSAGE\u00a0consistently outperforms state-of-the-art baselines, effectively trades off performance and runtime by sampling node neighborhoods, and our theoretical analysis provides insight into how our approach can learn about local graph structures.A number of extensions and potential improvements are possible, such as extending GraphSAGE\u00a0to incorporate directed or multi-modal graphs.A particularly interesting direction for future work is exploring non-uniform neighborhood sampling functions, and perhaps even learning these functions as part of the GraphSAGE\u00a0optimization.", "The authors thank Austin Benson, Aditya Grover, Bryan He, Dan Jurafsky, Alex Ratner, Marinka Zitnik, and Daniel Selsam for their helpful discussions and comments on early drafts.The authors would also like to thank Ben Johnson for his many useful questions and comments on our code and Nikhil Mehta and Yuhui Ding for catching some minor errors in a previous version of the appendix.This research has been supported in part by NSF IIS-1149837, DARPA SIMPLEX,Stanford Data Science Initiative, Huawei, and Chan Zuckerberg Biohub.WLH was also supported by the SAP Stanford Graduate Fellowship and an NSERC PGS-D grant.The views and conclusions expressed in this material are those of the authorsand should not be interpreted as necessarily representing the official policies or endorsements, either expressed orimplied, of the above funding agencies, corporations, or the U.S. and Canadian governments."], "figure_types": {"6b7d6e6416343b2a122f8416e69059ce919026ef/19-Figure3-1.png": "plot", "6b7d6e6416343b2a122f8416e69059ce919026ef/2-Figure1-1.png": "schematic", "6b7d6e6416343b2a122f8416e69059ce919026ef/7-Table1-1.png": "table"}}, "1703.10593": {"paper_id": "paper_70", "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks", "arxiv_url": "https://arxiv.org/abs/1703.10593", "s2orc_url": "https://www.semanticscholar.org/paper/c43d954cf8133e6254499f3d68e45218067e4941", "all_figures_tables": {"c43d954cf8133e6254499f3d68e45218067e4941/11-Figure10-1.png": "Figure 10: Collection style transfer I: we transfer input images into the artistic styles of Monet, Van Gogh, Cezanne, and Ukiyo-e. Please see our website for additional examples.", "c43d954cf8133e6254499f3d68e45218067e4941/12-Figure11-1.png": "Figure 11: Collection style transfer II: we transfer input images into the artistic styles of Monet, Van Gogh, Cezanne, Ukiyo-e. Please see our website for additional examples.", "c43d954cf8133e6254499f3d68e45218067e4941/13-Figure12-1.png": "Figure 12: Relatively successful results on mapping Monet\u2019s paintings to photographs. Please see our website for additional examples.", "c43d954cf8133e6254499f3d68e45218067e4941/14-Figure13-1.png": "Figure 13: Our method applied to several translation problems. These images are selected as relatively successful results \u2013 please see our website for more comprehensive and random results. In the top two rows, we show results on object transfiguration between horses and zebras, trained on 939 images from the wild horse class and 1177 images from the zebra class in Imagenet [41]. The middle two rows show results on season transfer, trained on winter and summer photos of Yosemite from Flickr. In the bottom two rows, we train our method on 996 apple images and 1020 navel orange images from ImageNet.", "c43d954cf8133e6254499f3d68e45218067e4941/15-Figure14-1.png": "Figure 14: Photo enhancement: mapping from a set of iPhone snaps to professional DSLR photographs, the system often learns to produce shallow focus. Here we show some of the most successful results in our test set \u2013 average performance is considerably worse. Please see our website for more comprehensive and random examples.", "c43d954cf8133e6254499f3d68e45218067e4941/15-Figure15-1.png": "Figure 15: We compare our method with neural style transfer [12] on photo stylization. Left to right: input image, results from [12] using two different representative artworks as style images, results from [12] using the entire collection of the artist, and CycleGAN (ours).", "c43d954cf8133e6254499f3d68e45218067e4941/16-Figure16-1.png": "Figure 16: We compare our method with neural style transfer [12] on various applications. From top to bottom: apple\u2192orange, horse\u2192zebra, and Monet\u2192photo. Left to right: input image, results from [12] using two different images as style images, results from [12] using all the images from the target domain, and CycleGAN (ours).", "c43d954cf8133e6254499f3d68e45218067e4941/16-Figure17-1.png": "Figure 17: Typical failure cases of our method. Please see our website for more comprehensive results.", "c43d954cf8133e6254499f3d68e45218067e4941/2-Figure2-1.png": "Figure 2: Paired training data (left) consists of training examples {xi, yi}Ni=1, where the correspondence between xi and yi exists [21]. We instead consider unpaired training data (right), consisting of a source set {xi}Ni=1 (xi \u2208 X) and a target set {yj}j=1 (yj \u2208 Y ), with no information provided as to which xi matches which yj .", "c43d954cf8133e6254499f3d68e45218067e4941/3-Figure3-1.png": "Figure 3: (a) Our model contains two mapping functions G : X \u2192 Y and F : Y \u2192 X , and associated adversarial discriminators DY and DX . DY encourages G to translate X into outputs indistinguishable from domain Y , and vice versa for DX and F . To further regularize the mappings, we introduce two cycle consistency losses that capture the intuition that if we translate from one domain to the other and back again we should arrive at where we started: (b) forward cycle-consistency loss: x\u2192 G(x)\u2192 F (G(x)) \u2248 x, and (c) backward cycle-consistency loss: y \u2192 F (y)\u2192 G(F (y)) \u2248 y", "c43d954cf8133e6254499f3d68e45218067e4941/4-Figure4-1.png": "Figure 4: The generated images G(x) and the reconstructed images F (G(x)) from various experiments. From top to bottom: photo\u2194Cezanne, horses\u2194zebras, winter\u2192summer Yosemite, aerial photos\u2194maps on Google Maps.", "c43d954cf8133e6254499f3d68e45218067e4941/6-Figure5-1.png": "Figure 5: Different methods for mapping labels\u2194photos trained on Cityscapes images. From left to right: input, BiGAN/ALI [6, 8], CoGAN [30], feature loss + GAN, SimGAN [45], CycleGAN (ours), pix2pix [21] trained on paired data, and ground truth.", "c43d954cf8133e6254499f3d68e45218067e4941/6-Figure6-1.png": "Figure 6: Different methods for mapping aerial photos\u2194maps on Google Maps. From left to right: input, BiGAN/ALI [6, 8], CoGAN [30], feature loss + GAN, SimGAN [45], CycleGAN (ours), pix2pix [21] trained on paired data, and ground truth.", "c43d954cf8133e6254499f3d68e45218067e4941/7-Table1-1.png": "Table 1: AMT \u201creal vs fake\u201d test on maps\u2194aerial photos at 256\u00d7 256 resolution.", "c43d954cf8133e6254499f3d68e45218067e4941/7-Table2-1.png": "Table 2: FCN-scores for different methods, evaluated on Cityscapes labels\u2192photo.", "c43d954cf8133e6254499f3d68e45218067e4941/7-Table3-1.png": "Table 3: Classification performance of photo\u2192labels for different methods on cityscapes.", "c43d954cf8133e6254499f3d68e45218067e4941/7-Table4-1.png": "Table 4: Ablation study: FCN-scores for different variants of our method, evaluated on Cityscapes labels\u2192photo.", "c43d954cf8133e6254499f3d68e45218067e4941/8-Figure7-1.png": "Figure 7: Different variants of our method for mapping labels\u2194photos trained on cityscapes. From left to right: input, cycleconsistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss (F (G(x)) \u2248 x), GAN + backward cycle-consistency loss (G(F (y)) \u2248 y), CycleGAN (our full method), and ground truth. Both Cycle alone and GAN + backward fail to produce images similar to the target domain. GAN alone and GAN + forward suffer from mode collapse, producing identical label maps regardless of the input photo.", "c43d954cf8133e6254499f3d68e45218067e4941/8-Figure8-1.png": "Figure 8: Example results of CycleGAN on paired datasets used in \u201cpix2pix\u201d [21] such as architectural labels\u2194photos and edges\u2194shoes.", "c43d954cf8133e6254499f3d68e45218067e4941/9-Figure9-1.png": "Figure 9: The effect of the identity mapping loss on Monet\u2019s painting\u2192 photos. From left to right: input paintings, CycleGAN without identity mapping loss, CycleGAN with identity mapping loss. The identity mapping loss helps preserve the color of the input paintings."}, "referred_figures_tables": [["c43d954cf8133e6254499f3d68e45218067e4941/8-Figure7-1.png", "c43d954cf8133e6254499f3d68e45218067e4941/2-Figure2-1.png", "c43d954cf8133e6254499f3d68e45218067e4941/7-Table4-1.png"], ["c43d954cf8133e6254499f3d68e45218067e4941/8-Figure7-1.png", "c43d954cf8133e6254499f3d68e45218067e4941/7-Table4-1.png"]], "question_id": [15, 10], "question": ["How can we check if the model suffers from mode collapse?", "Normally, GAN training is unstable. Does this framework help to make the model stable?"], "question_section": ["5.1.4 Analysis of the loss function", "3.3. Full Objective"], "question_trigger_sentence": ["We also evaluate our method with the cy- cle loss in only one direction: GAN + forward cycle loss Ex\u223cpdata (x) [\u2225F (G(x))\u2212x\u22251 ], or GAN + backward cycle loss Ey\u223cpdata(y)[\u2225G(F(y))\u2212y\u22251] (Equation 2) and find that it of- ten incurs training instability and causes mode collapse, es- pecially for the direction of the mapping that was removed. Figure 7 shows several qualitative examples.", "In Section 5.1.4, we compare our method against ab- lations of the full objective, including the adversarial loss LGAN alone and the cycle consistency loss Lcyc alone, and empirically show that both objectives play critical roles in arriving at high-quality results. We also evaluate our method with only cycle loss in one direction and show that a single cycle is not sufficient to regularize the training for this under-constrained problem."], "question_type": ["Shallow Question", "Testing Question"], "evidential_info": [[{"context": "In Table\u00a04 and Table\u00a05, we compare against ablations of our full loss. Removing the GAN loss substantially degrades results, as does removing the cycle-consistency loss. We therefore conclude that both terms are critical to our results. We also evaluate our method with the cycle loss in only one direction: GAN + forward cycle loss \\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[\\lVert F(G(x))-x\\rVert_{1}], or GAN + backward cycle loss \\mathbb{E}_{y\\sim p_{\\text{data}}(y)}[\\lVert G(F(y))-y\\rVert_{1}] (Equation\u00a02) and find that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed. Figure\u00a07 shows several qualitative examples.", "rationale": "[The paper evaluates its method with the cycle loss in only one direction: GAN + forward cycle loss, or GAN + backward cycle loss (in Equation 2) and finds that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed.]"}, {"context": "[We therefore seek an algorithm that can learn to translate between domains without paired input-output examples (Figure 2, right). We assume there is some underlying relationship between the domains \u2013 for example, that they are two different renderings of the same underlying scene \u2013 and seek to learn that relationship. Although we lack supervision in the form of paired examples, we can exploit supervision at the level of sets: we are given one set of images in domain X and a different set in domain Y . We may train a mapping G : X \u2192 Y such that the output y\u02c6 = G(x), x \u2208 X, is indistinguishable from images y \u2208 Y by an adversary trained to classify y\u02c6 apart from y. In theory, this objective can induce an output distribution over y\u02c6 that matches the empirical distribution pdata(y) (in general, this requires G to be stochastic). The optimal G thereby translates the domain X to a domain Y\u02c6 distributed identically to Y . However, such a translation does not guarantee that an individual input x and output y are paired up in a meaningful way \u2013 there are infinitely many mappings G that will induce the same distribution over y\u02c6. Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the wellknown problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress.]", "rationale": "[In practice, the paper found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the well-known problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress.]"}], [{"context": "We apply two techniques from recent works to stabilize our model training procedure. First, for \\mathcal{L}_{\\text{GAN}} (Equation\u00a01), we replace the negative log likelihood objective by a least-squares loss\u00a0[35]. This loss is more stable during training and generates higher quality results. In particular, for a GAN loss \\mathcal{L}_{\\text{GAN}}(G,D,X,Y), we train the G to minimize \\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[(D(G(x))-1)^{2}] and train the D to minimize \\mathbb{E}_{y\\sim p_{\\text{data}}(y)}[(D(y)-1)^{2}]+\\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[D(G(x))^{2}].", "rationale": "[The paper applies two techniques to stabilize its model training procedure. First, for \\mathcal{L}_{\\text{GAN}} (Equation 1), the paper replaces the negative log likelihood objective by a least-squares loss.]"}, {"context": "Second, to reduce model oscillation\u00a0[15], we follow Shrivastava et al.\u2019s strategy\u00a0[46] and update the discriminators using a history of generated images rather than the ones produced by thelatest generators. We keep an image buffer that stores the 50 previously created images.", "rationale": "[The paper evaluate its method with the cycle loss in only one direction and finds that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed. ]"}, {"context": "In Table\u00a04 and Table\u00a05, we compare against ablations of our full loss. Removing the GAN loss substantially degrades results, as does removing the cycle-consistency loss. We therefore conclude that both terms are critical to our results. We also evaluate our method with the cycle loss in only one direction: GAN + forward cycle loss \\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[\\lVert F(G(x))-x\\rVert_{1}], or GAN + backward cycle loss \\mathbb{E}_{y\\sim p_{\\text{data}}(y)}[\\lVert G(F(y))-y\\rVert_{1}] (Equation\u00a02) and find that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed. Figure\u00a07 shows several qualitative examples.", "rationale": "[Secondly, to reduce model oscillation, the paper follows Shrivastava et al.\u2019s strategy and updates the discriminators using a history of generated images rather than the ones produced by the latest generators. The paper keeps an image buffer that stores the 50 previously created images.]"}]], "composition": ["[If all input images map to the same output image and the optimization fails to make progress, then the model is suffering from \u201cmode collapse\u201d. For example, the paper evaluates its method with the cycle loss in only one direction: GAN + forward cycle loss, or GAN + backward cycle loss (in Equation 2) and finds that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed.]", "[The paper applies two techniques to stabilize its model training procedure. First, for \\mathcal{L}_{\\text{GAN}} (Equation 1), the paper replaces the negative log likelihood objective by a least-squares loss. Secondly, to reduce model oscillation, the paper follows Shrivastava et al.\u2019s strategy and updates the discriminators using a history of generated images rather than the ones produced by the latest generators. The paper keeps an image buffer that stores the 50 previously created images. The paper also evaluates its method with the cycle loss in only one direction and finds that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed.]"], "Is_figure_in_evidence": [true, false], "Is_table_in_evidence": [false, true], "question_key": ["1241", "1251"], "passages": ["What did Claude Monet see as he placed his easel by the bank of the Seine near Argenteuil on a lovely spring day in 1873 (Figure\u00a01, top-left)? A color photograph, had it been invented, may have documented a crisp blue sky and a glassy river reflecting it. Monet conveyed his impression of this same scene through wispy brush strokes and a bright palette.\u2020\u2020* indicates equal contribution", "What if Monet had happened upon the little harbor in Cassis on a cool summer evening (Figure\u00a01, bottom-left)? A brief stroll through a gallery of Monet paintings makes it possible to imagine how he would have rendered the scene: perhaps in pastel shades, with abrupt dabs of paint, and a somewhat flattened dynamic range.", "We can imagine all this despite never having seen a side by side example of a Monet painting next to a photo of the scene he painted. Instead, we have knowledge of the set of Monet paintings and of the set of landscape photographs. We can reason about the stylistic differences between these two sets, and thereby imagine what a scene might look like if we were to \u201ctranslate\u201d it from one set into the other.", "In this paper, we present a method that can learn to do the same: capturing special characteristics of one image collection and figuring out how these characteristics could be translated into the other image collection, all in the absence of any paired training examples.", "This problem can be more broadly described as image-to-image translation\u00a0[22], converting an image from one representation of a given scene, x\ud835\udc65xitalic_x, to another, y\ud835\udc66yitalic_y, e.g., grayscale to color, image to semantic labels, edge-map to photograph.Years of research in computer vision, image processing, computational photography, and graphics have produced powerful translation systems in the supervised setting, where example image pairs {xi,yi}i=1Nsuperscriptsubscriptsubscript\ud835\udc65\ud835\udc56subscript\ud835\udc66\ud835\udc56\ud835\udc561\ud835\udc41\\{x_{i},y_{i}\\}_{i=1}^{N}{ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT are available (Figure\u00a02, left), e.g.,[11, 19, 22, 23, 28, 33, 45, 56, 58, 62].However, obtaining paired training data can be difficult and expensive. For example, only a couple of datasets exist for tasks like semantic segmentation (e.g.,\u00a0[4]), and they are relatively small.Obtaining input-output pairs for graphics tasks like artistic stylization can be even more difficult since the desired output is highly complex, typically requiring artistic authoring. For many tasks, like object transfiguration (e.g., zebra\u2194\u2194\\leftrightarrow\u2194horse, \u00a0Figure\u00a01 top-middle), the desired output is not even well-defined.", "We therefore seek an algorithm that can learn to translate between domains without paired input-output examples (Figure\u00a02, right). We assume there is some underlying relationship between the domains \u2013 for example, that they are two different renderings of the same underlying scene \u2013 and seek to learn that relationship. Although we lack supervision in the form of paired examples, we can exploit supervision at the level of sets: we are given one set of images in domain X\ud835\udc4bXitalic_X and a different set in domain Y\ud835\udc4cYitalic_Y. We may train a mapping G:X\u2192Y:\ud835\udc3a\u2192\ud835\udc4b\ud835\udc4cG:X\\rightarrow Yitalic_G : italic_X \u2192 italic_Y such that the output y^=G(x)^\ud835\udc66\ud835\udc3a\ud835\udc65\\hat{y}=G(x)over^ start_ARG italic_y end_ARG = italic_G ( italic_x ), x\u2208X\ud835\udc65\ud835\udc4bx\\in Xitalic_x \u2208 italic_X, is indistinguishable from images y\u2208Y\ud835\udc66\ud835\udc4cy\\in Yitalic_y \u2208 italic_Y by an adversary trained to classify y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG apart from y\ud835\udc66yitalic_y. In theory, this objective can induce an output distribution over y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG that matches the empirical distribution pdata(y)subscript\ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e\ud835\udc66p_{data}(y)italic_p start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ( italic_y ) (in general, this requires G\ud835\udc3aGitalic_G to be stochastic)\u00a0[16]. The optimal G\ud835\udc3aGitalic_G thereby translates the domain X\ud835\udc4bXitalic_X to a domain Y^^\ud835\udc4c\\hat{Y}over^ start_ARG italic_Y end_ARG distributed identically to Y\ud835\udc4cYitalic_Y. However, such a translation does not guarantee that an individual input x\ud835\udc65xitalic_x and output y\ud835\udc66yitalic_y are paired up in a meaningful way \u2013 there are infinitely many mappings G\ud835\udc3aGitalic_G that will induce the same distribution over y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG. Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the well-known problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress\u00a0[15].", "These issues call for adding more structure to our objective. Therefore, we exploit the property that translation should be \u201ccycle consistent\u201d, in the sense that if we translate, e.g., a sentence from English to French, and then translate it back from French to English, we should arrive back at the original sentence\u00a0[3]. Mathematically, if we have a translator G:X\u2192Y:\ud835\udc3a\u2192\ud835\udc4b\ud835\udc4cG:X\\rightarrow Yitalic_G : italic_X \u2192 italic_Y and another translator F:Y\u2192X:\ud835\udc39\u2192\ud835\udc4c\ud835\udc4bF:Y\\rightarrow Xitalic_F : italic_Y \u2192 italic_X, then G\ud835\udc3aGitalic_G and F\ud835\udc39Fitalic_F should be inverses of each other, and both mappings should be bijections. We apply this structural assumption by training both the mapping G\ud835\udc3aGitalic_G and F\ud835\udc39Fitalic_F simultaneously, and adding a cycle consistency loss\u00a0[64] that encourages F(G(x))\u2248x\ud835\udc39\ud835\udc3a\ud835\udc65\ud835\udc65F(G(x))\\approx xitalic_F ( italic_G ( italic_x ) ) \u2248 italic_x and G(F(y))\u2248y\ud835\udc3a\ud835\udc39\ud835\udc66\ud835\udc66G(F(y))\\approx yitalic_G ( italic_F ( italic_y ) ) \u2248 italic_y. Combining this loss with adversarial losses on domains X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y yields our full objective for unpaired image-to-image translation.", "We apply our method to a wide range of applications, including collection style transfer, object transfiguration, season transfer and photo enhancement. We also compare against previous approaches that rely either on hand-defined factorizations of style and content, or on shared embedding functions, and show that our method outperforms these baselines. We provide both PyTorch and Torch implementations. Check out more results at our website.", "Generative Adversarial Networks (GANs)\u00a0[16, 63] have achieved impressive results in image generation\u00a0[6, 39], image editing\u00a0[66], and representation learning\u00a0[39, 43, 37].\u3000Recent methods adopt the same idea for conditional image generation applications, such as text2image\u00a0[41], image inpainting\u00a0[38], and future prediction\u00a0[36], as well as to other domains like videos\u00a0[54] and 3D data\u00a0[57].The key to GANs\u2019 success is the idea of an adversarial loss that forces the generated images to be, in principle, indistinguishable from real photos.This loss is particularly powerful for image generation tasks,as this is exactly the objective that much of computer graphics aims to optimize. We adopt an adversarial loss to learn the mapping such that the translated images cannot be distinguished from images in the target domain.", "Image-to-Image TranslationThe idea of image-to-image translation goes back at least to Hertzmann et al.\u2019s Image Analogies\u00a0[19], who employ a non-parametric texture model\u00a0[10] on a single input-output training image pair.More recent approaches use a dataset of input-output examples to learn a parametric translation function using CNNs (e.g.,\u00a0[33]). Our approach builds on the \u201cpix2pix\u201d framework of Isola et al.\u00a0[22], which uses a conditional generative adversarial network\u00a0[16] to learn a mapping from input to output images. Similar ideas have been applied to various tasks such as generating photographs from sketches\u00a0[44] or from attribute and semantic layouts\u00a0[25]. However, unlike the above prior work, we learn the mapping without paired training examples.", "Unpaired Image-to-Image TranslationSeveral other methods also tackle the unpaired setting, where the goal is to relate two data domains: X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y. Rosales et al.\u00a0[42] propose a Bayesian framework that includes a prior based on a patch-based Markov random field computed from a source image and a likelihood term obtained from multiple style images. More recently, CoGAN\u00a0[32] and cross-modal scene networks\u00a0[1] use a weight-sharing strategy to learn a common representation across domains.Concurrent to our method, Liu et al.\u00a0[31] extends the above framework with a combination of variational autoencoders\u00a0[27] and generative adversarial networks\u00a0[16].Another line of concurrent work\u00a0[46, 49, 2] encourages the input and output to share specific \u201ccontent\u201d features even though they may differ in \u201cstyle\u201c. These methods also use adversarial networks, with additional terms to enforce the output to be close to the input in a predefined metric space, such as class label space\u00a0[2], image pixel space\u00a0[46], and image feature space\u00a0[49].", "Unlike the above approaches, our formulation does not rely on any task-specific, predefined similarity function between the input and output, nor do we assume that the input and output have to lie in the same low-dimensional embedding space. This makes our method a general-purpose solution for many vision and graphics tasks. We directly compare against several prior and contemporary approaches in Section\u00a05.1.", "Cycle ConsistencyThe idea of using transitivity as a way to regularize structured data has a long history. In visual tracking, enforcing simple forward-backward consistency has been a standard trick for decades\u00a0[24, 48]. In the language domain, verifying and improving translations via \u201cback translation and reconciliation\u201d is a technique used by human translators\u00a0[3] (including, humorously, by Mark Twain\u00a0[51]), as well as by machines\u00a0[17].More recently, higher-order cycle consistency has been used instructure from motion\u00a0[61],3D shape matching\u00a0[21], co-segmentation\u00a0[55], dense semantic alignment\u00a0[65, 64], and depth estimation\u00a0[14]. Of these, Zhou et al.\u00a0[64] and Godard et al.\u00a0[14] are most similar to our work, as they use a cycle consistency loss as a way of using transitivity to supervise CNN training.In this work, we are introducing a similar loss to push G\ud835\udc3aGitalic_G and F\ud835\udc39Fitalic_F to be consistent with each other. Concurrent with our work, in these same proceedings, Yi et al.\u00a0[59] independently use a similar objective for unpaired image-to-image translation, inspired by dual learning in machine translation\u00a0[17].", "Neural Style Transfer\u00a0[13, 23, 52, 12] is another way to perform image-to-image translation, which synthesizes a novel image by combining the content of one image with the style of another image (typically a painting) based on matching the Gram matrix statistics of pre-trained deep features. Our primary focus, on the other hand, is learning the mapping between two image collections, rather than between two specific images, by trying to capture correspondences between higher-level appearance structures. Therefore, our method can be applied to other tasks, such as painting\u2192\u2192\\rightarrow\u2192 photo, object transfiguration, etc. where single sample transfer methods do not perform well. We compare these two methods in \u00a0Section\u00a05.2.", "Our goal is to learn mapping functions between two domains X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y given training samples {xi}i=1Nsuperscriptsubscriptsubscript\ud835\udc65\ud835\udc56\ud835\udc561\ud835\udc41\\{x_{i}\\}_{i=1}^{N}{ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT where xi\u2208Xsubscript\ud835\udc65\ud835\udc56\ud835\udc4bx_{i}\\in Xitalic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 italic_X and {yj}j=1Msuperscriptsubscriptsubscript\ud835\udc66\ud835\udc57\ud835\udc571\ud835\udc40\\{y_{j}\\}_{j=1}^{M}{ italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT where yj\u2208Ysubscript\ud835\udc66\ud835\udc57\ud835\udc4cy_{j}\\in Yitalic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u2208 italic_Y111We often omit the subscript i\ud835\udc56iitalic_i and j\ud835\udc57jitalic_j for simplicity.. We denote the data distribution as x\u223cpdata(x)similar-to\ud835\udc65subscript\ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e\ud835\udc65x\\sim p_{data}(x)italic_x \u223c italic_p start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ( italic_x ) and y\u223cpdata(y)similar-to\ud835\udc66subscript\ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e\ud835\udc66y\\sim p_{data}(y)italic_y \u223c italic_p start_POSTSUBSCRIPT italic_d italic_a italic_t italic_a end_POSTSUBSCRIPT ( italic_y ). As illustrated in Figure\u00a03 (a), our model includes two mappings G:X\u2192Y:\ud835\udc3a\u2192\ud835\udc4b\ud835\udc4cG:X\\rightarrow Yitalic_G : italic_X \u2192 italic_Y and F:Y\u2192X:\ud835\udc39\u2192\ud835\udc4c\ud835\udc4bF:Y\\rightarrow Xitalic_F : italic_Y \u2192 italic_X. In addition, we introduce two adversarial discriminators DXsubscript\ud835\udc37\ud835\udc4bD_{X}italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT and DYsubscript\ud835\udc37\ud835\udc4cD_{Y}italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT, where DXsubscript\ud835\udc37\ud835\udc4bD_{X}italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT aims to distinguish between images {x}\ud835\udc65\\{x\\}{ italic_x } and translated images {F(y)}\ud835\udc39\ud835\udc66\\{F(y)\\}{ italic_F ( italic_y ) }; in the same way, DYsubscript\ud835\udc37\ud835\udc4cD_{Y}italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT aims to discriminate between {y}\ud835\udc66\\{y\\}{ italic_y } and {G(x)}\ud835\udc3a\ud835\udc65\\{G(x)\\}{ italic_G ( italic_x ) }. Our objective contains two types of terms: adversarial losses\u00a0[16] for matching the distribution of generated images to the data distribution in the target domain; and cycle consistency losses to prevent the learned mappings G\ud835\udc3aGitalic_G and F\ud835\udc39Fitalic_F from contradicting each other.", "We apply adversarial losses\u00a0[16] to both mapping functions. For the mapping function G:X\u2192Y:\ud835\udc3a\u2192\ud835\udc4b\ud835\udc4cG:X\\rightarrow Yitalic_G : italic_X \u2192 italic_Y and its discriminator DYsubscript\ud835\udc37\ud835\udc4cD_{Y}italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT, we express the objective as:\u2112GAN(G,DY,X,Y)=subscript\u2112GAN\ud835\udc3asubscript\ud835\udc37\ud835\udc4c\ud835\udc4b\ud835\udc4cabsent\\displaystyle\\mathcal{L}_{\\text{GAN}}(G,D_{Y},X,Y)=caligraphic_L start_POSTSUBSCRIPT GAN end_POSTSUBSCRIPT ( italic_G , italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT , italic_X , italic_Y ) =\ud835\udd3cy\u223cpdata(y)[log\u2061DY(y)]subscript\ud835\udd3csimilar-to\ud835\udc66subscript\ud835\udc5ddata\ud835\udc66delimited-[]subscript\ud835\udc37\ud835\udc4c\ud835\udc66\\displaystyle\\ \\mathbb{E}_{y\\sim p_{\\text{data}}(y)}[\\log D_{Y}(y)]blackboard_E start_POSTSUBSCRIPT italic_y \u223c italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( italic_y ) end_POSTSUBSCRIPT [ roman_log italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT ( italic_y ) ]+\\displaystyle++\ud835\udd3cx\u223cpdata(x)[log(1\u2212DY(G(x))],\\displaystyle\\ \\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[\\log(1-D_{Y}(G(x))],blackboard_E start_POSTSUBSCRIPT italic_x \u223c italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( italic_x ) end_POSTSUBSCRIPT [ roman_log ( 1 - italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT ( italic_G ( italic_x ) ) ] ,(1)where G\ud835\udc3aGitalic_G tries to generate images G(x)\ud835\udc3a\ud835\udc65G(x)italic_G ( italic_x ) that look similar to images from domain Y\ud835\udc4cYitalic_Y, while DYsubscript\ud835\udc37\ud835\udc4cD_{Y}italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT aims to distinguish between translated samples G(x)\ud835\udc3a\ud835\udc65G(x)italic_G ( italic_x ) and real samples y\ud835\udc66yitalic_y. G\ud835\udc3aGitalic_G aims to minimize this objective against an adversary D\ud835\udc37Ditalic_D that tries to maximize it, i.e., minG\u2061maxDY\u2061\u2112GAN(G,DY,X,Y)subscript\ud835\udc3asubscriptsubscript\ud835\udc37\ud835\udc4csubscript\u2112GAN\ud835\udc3asubscript\ud835\udc37\ud835\udc4c\ud835\udc4b\ud835\udc4c\\min_{G}\\max_{D_{Y}}\\mathcal{L}_{\\text{GAN}}(G,D_{Y},X,Y)roman_min start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT GAN end_POSTSUBSCRIPT ( italic_G , italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT , italic_X , italic_Y ).We introduce a similar adversarial loss for the mapping function F:Y\u2192X:\ud835\udc39\u2192\ud835\udc4c\ud835\udc4bF:Y\\rightarrow Xitalic_F : italic_Y \u2192 italic_X and its discriminator DXsubscript\ud835\udc37\ud835\udc4bD_{X}italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT as well: i.e., minF\u2061maxDX\u2061\u2112GAN(F,DX,Y,X)subscript\ud835\udc39subscriptsubscript\ud835\udc37\ud835\udc4bsubscript\u2112GAN\ud835\udc39subscript\ud835\udc37\ud835\udc4b\ud835\udc4c\ud835\udc4b\\min_{F}\\max_{D_{X}}\\mathcal{L}_{\\text{GAN}}(F,D_{X},Y,X)roman_min start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT GAN end_POSTSUBSCRIPT ( italic_F , italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_Y , italic_X ).", "Adversarial training can, in theory, learn mappings G\ud835\udc3aGitalic_G and F\ud835\udc39Fitalic_F that produce outputs identically distributed as target domains Y\ud835\udc4cYitalic_Y and X\ud835\udc4bXitalic_X respectively (strictly speaking, this requires G\ud835\udc3aGitalic_G and F\ud835\udc39Fitalic_F to be stochastic functions)\u00a0[15]. However, with large enough capacity, a network can map the same set of input images to any random permutation of images in the target domain, where any of the learned mappings can induce an output distribution that matches the target distribution. Thus, adversarial losses alone cannot guarantee that the learned function can map an individual input xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to a desired output yisubscript\ud835\udc66\ud835\udc56y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. To further reduce the space of possible mapping functions, we argue that the learned mapping functions should be cycle-consistent: as shown in Figure\u00a03 (b), for each image x\ud835\udc65xitalic_x from domain X\ud835\udc4bXitalic_X, the image translation cycle should be able to bring x\ud835\udc65xitalic_x back to the original image, i.e., x\u2192G(x)\u2192F(G(x))\u2248x\u2192\ud835\udc65\ud835\udc3a\ud835\udc65\u2192\ud835\udc39\ud835\udc3a\ud835\udc65\ud835\udc65x\\rightarrow G(x)\\rightarrow F(G(x))\\approx xitalic_x \u2192 italic_G ( italic_x ) \u2192 italic_F ( italic_G ( italic_x ) ) \u2248 italic_x. We call this forward cycle consistency. Similarly, as illustrated in Figure\u00a03 (c), for each image y\ud835\udc66yitalic_y from domain Y\ud835\udc4cYitalic_Y, G\ud835\udc3aGitalic_G and F\ud835\udc39Fitalic_F should also satisfy backward cycle consistency: y\u2192F(y)\u2192G(F(y))\u2248y\u2192\ud835\udc66\ud835\udc39\ud835\udc66\u2192\ud835\udc3a\ud835\udc39\ud835\udc66\ud835\udc66y\\rightarrow F(y)\\rightarrow G(F(y))\\approx yitalic_y \u2192 italic_F ( italic_y ) \u2192 italic_G ( italic_F ( italic_y ) ) \u2248 italic_y.We incentivize this behavior using a cycle consistency loss:\u2112cyc(G,F)=subscript\u2112cyc\ud835\udc3a\ud835\udc39absent\\displaystyle\\mathcal{L}_{\\text{cyc}}(G,F)=caligraphic_L start_POSTSUBSCRIPT cyc end_POSTSUBSCRIPT ( italic_G , italic_F ) =\ud835\udd3cx\u223cpdata(x)[\u2225F(G(x))\u2212x\u22251]subscript\ud835\udd3csimilar-to\ud835\udc65subscript\ud835\udc5ddata\ud835\udc65delimited-[]subscriptdelimited-\u2225\u2225\ud835\udc39\ud835\udc3a\ud835\udc65\ud835\udc651\\displaystyle\\ \\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[\\lVert F(G(x))-x\\rVert_{1}]blackboard_E start_POSTSUBSCRIPT italic_x \u223c italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( italic_x ) end_POSTSUBSCRIPT [ \u2225 italic_F ( italic_G ( italic_x ) ) - italic_x \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ]+\\displaystyle++\ud835\udd3cy\u223cpdata(y)[\u2225G(F(y))\u2212y\u22251].subscript\ud835\udd3csimilar-to\ud835\udc66subscript\ud835\udc5ddata\ud835\udc66delimited-[]subscriptdelimited-\u2225\u2225\ud835\udc3a\ud835\udc39\ud835\udc66\ud835\udc661\\displaystyle\\ \\mathbb{E}_{y\\sim p_{\\text{data}}(y)}[\\lVert G(F(y))-y\\rVert_{1}].blackboard_E start_POSTSUBSCRIPT italic_y \u223c italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( italic_y ) end_POSTSUBSCRIPT [ \u2225 italic_G ( italic_F ( italic_y ) ) - italic_y \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] .(2)In preliminary experiments, we also tried replacing the L1 norm in this loss with an adversarial loss between F(G(x))\ud835\udc39\ud835\udc3a\ud835\udc65F(G(x))italic_F ( italic_G ( italic_x ) ) and x\ud835\udc65xitalic_x, and between G(F(y))\ud835\udc3a\ud835\udc39\ud835\udc66G(F(y))italic_G ( italic_F ( italic_y ) ) and y\ud835\udc66yitalic_y, but did not observe improved performance.", "The behavior induced by the cycle consistency loss can be observed in Figure\u00a04: the reconstructed images F(G(x))\ud835\udc39\ud835\udc3a\ud835\udc65F(G(x))italic_F ( italic_G ( italic_x ) ) end up matching closely to the input images x\ud835\udc65xitalic_x.", "", "Our full objective is:\u2112(G,F,DX,DY)=\u2112\ud835\udc3a\ud835\udc39subscript\ud835\udc37\ud835\udc4bsubscript\ud835\udc37\ud835\udc4cabsent\\displaystyle\\mathcal{L}(G,F,D_{X},D_{Y})=caligraphic_L ( italic_G , italic_F , italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT ) =\u3001\u2112GAN(G,DY,X,Y)\u3001subscript\u2112GAN\ud835\udc3asubscript\ud835\udc37\ud835\udc4c\ud835\udc4b\ud835\udc4c\\displaystyle\u3001\\mathcal{L}_{\\text{GAN}}(G,D_{Y},X,Y)\u3001 caligraphic_L start_POSTSUBSCRIPT GAN end_POSTSUBSCRIPT ( italic_G , italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT , italic_X , italic_Y )+\\displaystyle++\u2112GAN(F,DX,Y,X)subscript\u2112GAN\ud835\udc39subscript\ud835\udc37\ud835\udc4b\ud835\udc4c\ud835\udc4b\\displaystyle\\ \\mathcal{L}_{\\text{GAN}}(F,D_{X},Y,X)caligraphic_L start_POSTSUBSCRIPT GAN end_POSTSUBSCRIPT ( italic_F , italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_Y , italic_X )+\\displaystyle++\u03bb\u2112cyc(G,F),\ud835\udf06subscript\u2112cyc\ud835\udc3a\ud835\udc39\\displaystyle\\ \\lambda\\mathcal{L}_{\\text{cyc}}(G,F),italic_\u03bb caligraphic_L start_POSTSUBSCRIPT cyc end_POSTSUBSCRIPT ( italic_G , italic_F ) ,(3)where \u03bb\ud835\udf06\\lambdaitalic_\u03bb controls the relative importance of the two objectives. We aim to solve:G*,F*=arg\u2061minG,F\u2061maxDx,DY\u2061\u2112(G,F,DX,DY).superscript\ud835\udc3asuperscript\ud835\udc39subscript\ud835\udc3a\ud835\udc39subscriptsubscript\ud835\udc37\ud835\udc65subscript\ud835\udc37\ud835\udc4c\u2112\ud835\udc3a\ud835\udc39subscript\ud835\udc37\ud835\udc4bsubscript\ud835\udc37\ud835\udc4cG^{*},F^{*}=\\arg\\min_{G,F}\\max_{D_{x},D_{Y}}\\mathcal{L}(G,F,D_{X},D_{Y}).italic_G start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_F start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT = roman_arg roman_min start_POSTSUBSCRIPT italic_G , italic_F end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L ( italic_G , italic_F , italic_D start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT ) .(4)", "Notice that our model can be viewed as training two \u201cautoencoders\u201d\u00a0[20]: we learn one autoencoder F\u2218G:X\u2192X:\ud835\udc39\ud835\udc3a\u2192\ud835\udc4b\ud835\udc4bF\\circ G:X\\rightarrow Xitalic_F \u2218 italic_G : italic_X \u2192 italic_X jointly with another G\u2218F:Y\u2192Y:\ud835\udc3a\ud835\udc39\u2192\ud835\udc4c\ud835\udc4cG\\circ F:Y\\rightarrow Yitalic_G \u2218 italic_F : italic_Y \u2192 italic_Y. However, these autoencoders each have special internal structures: they map an image to itself via an intermediate representation that is a translation of the image into another domain. Such a setup can also be seen as a special case of \u201cadversarial autoencoders\u201d\u00a0[34], which use an adversarial loss to train the bottleneck layer of an autoencoder to match an arbitrary target distribution. In our case, the target distribution for the X\u2192X\u2192\ud835\udc4b\ud835\udc4bX\\rightarrow Xitalic_X \u2192 italic_X autoencoder is that of the domain Y\ud835\udc4cYitalic_Y.", "In Section\u00a05.1.4, we compare our method against ablations of the full objective, including the adversarial loss \u2112GANsubscript\u2112GAN\\mathcal{L}_{\\text{GAN}}caligraphic_L start_POSTSUBSCRIPT GAN end_POSTSUBSCRIPT alone and the cycle consistency loss \u2112cycsubscript\u2112cyc\\mathcal{L}_{\\text{cyc}}caligraphic_L start_POSTSUBSCRIPT cyc end_POSTSUBSCRIPT alone, and empirically show that both objectives play critical roles in arriving at high-quality results. We also evaluate our method with only cycle loss in one direction and show that a single cycle is not sufficient to regularize the training for this under-constrained problem.", "We adopt the architecture for our generative networks from Johnson et al.\u00a0[23] who have shown impressive results for neural style transfer and super-resolution. This network contains three convolutions, several residual blocks\u00a0[18], two fractionally-strided convolutions with stride 1212\\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG, and one convolution that maps features to RGB. We use 6666 blocks for 128\u00d7128128128128\\times 128128 \u00d7 128 images and 9999 blocks for 256\u00d7256256256256\\times 256256 \u00d7 256 and higher-resolution training images. Similar to Johnson et al.\u00a0[23], we use instance normalization\u00a0[53]. For the discriminator networks we use 70\u00d770707070\\times 7070 \u00d7 70 PatchGANs\u00a0[22, 30, 29], which aim to classify whether 70\u00d770707070\\times 7070 \u00d7 70 overlapping image patches are real or fake. Such a patch-level discriminator architecture has fewer parameters than a full-image discriminator and can work on arbitrarily-sized images in a fully convolutional fashion\u00a0[22].", "We apply two techniques from recent works to stabilize our model training procedure. First, for \u2112GANsubscript\u2112GAN\\mathcal{L}_{\\text{GAN}}caligraphic_L start_POSTSUBSCRIPT GAN end_POSTSUBSCRIPT (Equation\u00a01), we replace the negative log likelihood objective by a least-squares loss\u00a0[35]. This loss is more stable during training and generates higher quality results. In particular, for a GAN loss \u2112GAN(G,D,X,Y)subscript\u2112GAN\ud835\udc3a\ud835\udc37\ud835\udc4b\ud835\udc4c\\mathcal{L}_{\\text{GAN}}(G,D,X,Y)caligraphic_L start_POSTSUBSCRIPT GAN end_POSTSUBSCRIPT ( italic_G , italic_D , italic_X , italic_Y ), we train the G\ud835\udc3aGitalic_G to minimize \ud835\udd3cx\u223cpdata(x)[(D(G(x))\u22121)2]subscript\ud835\udd3csimilar-to\ud835\udc65subscript\ud835\udc5ddata\ud835\udc65delimited-[]superscript\ud835\udc37\ud835\udc3a\ud835\udc6512\\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[(D(G(x))-1)^{2}]blackboard_E start_POSTSUBSCRIPT italic_x \u223c italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( italic_x ) end_POSTSUBSCRIPT [ ( italic_D ( italic_G ( italic_x ) ) - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] and train the D\ud835\udc37Ditalic_D to minimize \ud835\udd3cy\u223cpdata(y)[(D(y)\u22121)2]+\ud835\udd3cx\u223cpdata(x)[D(G(x))2]subscript\ud835\udd3csimilar-to\ud835\udc66subscript\ud835\udc5ddata\ud835\udc66delimited-[]superscript\ud835\udc37\ud835\udc6612subscript\ud835\udd3csimilar-to\ud835\udc65subscript\ud835\udc5ddata\ud835\udc65delimited-[]\ud835\udc37superscript\ud835\udc3a\ud835\udc652\\mathbb{E}_{y\\sim p_{\\text{data}}(y)}[(D(y)-1)^{2}]+\\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[D(G(x))^{2}]blackboard_E start_POSTSUBSCRIPT italic_y \u223c italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( italic_y ) end_POSTSUBSCRIPT [ ( italic_D ( italic_y ) - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] + blackboard_E start_POSTSUBSCRIPT italic_x \u223c italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( italic_x ) end_POSTSUBSCRIPT [ italic_D ( italic_G ( italic_x ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ].", "Second, to reduce model oscillation\u00a0[15], we follow Shrivastava et al.\u2019s strategy\u00a0[46] and update the discriminators using a history of generated images rather than the ones produced by thelatest generators. We keep an image buffer that stores the 50505050 previously created images.", "For all the experiments, we set \u03bb=10\ud835\udf0610\\lambda=10italic_\u03bb = 10 in Equation\u00a03.We use the Adam solver\u00a0[26] with a batch size of 1111. All networks were trained from scratch with a learning rate of 0.00020.00020.00020.0002. We keep the same learning rate for the first 100100100100 epochs and linearly decay the rate to zero over the next 100100100100 epochs.Please see the appendix (Section\u00a07) for more details about the datasets, architectures, and training procedures.", "We first compare our approach against recent methods for unpaired image-to-image translation on paired datasets where ground truth input-output pairs are available for evaluation. We then study the importance of both the adversarial loss and the cycle consistency loss and compare our full method against several variants. Finally, we demonstrate the generality of our algorithm on a wide range of applications where paired data does not exist. For brevity, we refer to our method as CycleGAN. The PyTorch and Torch code, models, and full results can be found at our website.", "Using the same evaluation datasets and metrics as \u201cpix2pix\u201d\u00a0[22], we compare our method against several baselines both qualitatively and quantitatively. The tasks include semantic labels\u2194\u2194\\leftrightarrow\u2194photo on the Cityscapes dataset\u00a0[4], and map\u2194\u2194\\leftrightarrow\u2194aerial photo on data scraped from Google Maps. We also perform ablation study on the full loss function.", "AMT perceptual studies On the map\u2194\u2194\\leftrightarrow\u2194aerial photo task, we run \u201creal vs fake\u201d perceptual studies on Amazon Mechanical Turk (AMT) to assess the realism of our outputs. We follow the same perceptual study protocol from Isola et al.\u00a0[22], except we only gather data from 25252525 participants per algorithm we tested. Participants were shown a sequence of pairs of images, one a real photo or map and one fake (generated by our algorithm or a baseline), and asked to click on the image they thought was real. The first 10101010 trials of each session were practice and feedback was given as to whether the participant\u2019s response was correct or incorrect. The remaining 40404040 trials were used to assess the rate at which each algorithm fooled participants. Each session only tested a single algorithm, and participants were only allowed to complete a single session. The numbers we report here are not directly comparable to those in\u00a0[22] as our ground truth images were processed slightly differently222We train all the models on 256\u00d7256256256256\\times 256256 \u00d7 256 images while in pix2pix\u00a0[22], the model was trained on 256\u00d7256256256256\\times 256256 \u00d7 256 patches of 512\u00d7512512512512\\times 512512 \u00d7 512 images, and run convolutionally on the 512\u00d7512512512512\\times 512512 \u00d7 512 images at test time. We choose 256\u00d7256256256256\\times 256256 \u00d7 256 in our experiments as many baselines cannot scale up to high-resolution images, and CoGAN cannot be tested fully convolutionally. and the participant pool we tested may be differently distributed from those tested in\u00a0[22] (due to running the experiment at a different date and time). Therefore, our numbers should only be used to compare our current method against the baselines (which were run under identical conditions), rather than against\u00a0[22].", "FCN score Although perceptual studies may be the gold standard for assessing graphical realism, we also seek an automatic quantitative measure that does not require human experiments. For this, we adopt the \u201cFCN score\u201d from\u00a0[22], and use it to evaluate the Cityscapes labels\u2192\u2192\\rightarrow\u2192photo task. The FCN metric evaluates how interpretable the generated photos are according to an off-the-shelf semantic segmentation algorithm (the fully-convolutional network, FCN, from\u00a0[33]). The FCN predicts a label map for a generated photo. This label map can then be compared against the input ground truth labels using standard semantic segmentation metrics described below. The intuition is that if we generate a photo from a label map of \u201ccar on the road\u201d, then we have succeeded if the FCN applied to the generated photo detects \u201ccar on the road\u201d.", "Semantic segmentation metrics To evaluate the performance of photo\u2192\u2192\\rightarrow\u2192labels, we use the standard metrics from the Cityscapes benchmark\u00a0[4], including per-pixel accuracy, per-class accuracy, and mean class Intersection-Over-Union (Class IOU)\u00a0[4].", "CoGAN\u00a0[32] This method learns one GAN generator for domain X\ud835\udc4bXitalic_X and one for domain Y\ud835\udc4cYitalic_Y, with tied weights on the first few layers for shared latent representations. Translation from X\ud835\udc4bXitalic_X to Y\ud835\udc4cYitalic_Y can be achieved by finding a latent representation that generates image X\ud835\udc4bXitalic_X and then rendering this latent representation into style Y\ud835\udc4cYitalic_Y.", "SimGAN\u00a0[46] Like our method, Shrivastava et al.[46] uses an adversarial loss to train a translation from X\ud835\udc4bXitalic_X to Y\ud835\udc4cYitalic_Y. The regularization term \u2225x\u2212G(x)\u22251subscriptdelimited-\u2225\u2225\ud835\udc65\ud835\udc3a\ud835\udc651\\lVert x-G(x)\\rVert_{1}\u2225 italic_x - italic_G ( italic_x ) \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT i s used to penalize making large changes at pixel level.", "Feature loss + GAN We also test a variant of SimGAN\u00a0[46] where the L1 loss is computed over deep image features using a pretrained network (VGG-16 relu4_2\u00a0[47]), rather than over RGB pixel values. Computing distances in deep feature space, like this, is also sometimes referred to as using a \u201cperceptual loss\u201d\u00a0[8, 23].", "BiGAN/ALI\u00a0[9, 7] Unconditional GANs\u00a0[16] learn a generator G:Z\u2192X:\ud835\udc3a\u2192\ud835\udc4d\ud835\udc4bG:Z\\rightarrow Xitalic_G : italic_Z \u2192 italic_X, that maps a random noise z\ud835\udc67zitalic_z to an image x\ud835\udc65xitalic_x. The BiGAN\u00a0[9] and ALI\u00a0[7] propose to also learn the inverse mapping function F:X\u2192Z:\ud835\udc39\u2192\ud835\udc4b\ud835\udc4dF:X\\rightarrow Zitalic_F : italic_X \u2192 italic_Z. Though they were originally designed for mapping a latent vector z\ud835\udc67zitalic_z to an image x\ud835\udc65xitalic_x, we implemented the same objective for mapping a source image x\ud835\udc65xitalic_x to a target image y\ud835\udc66yitalic_y.", "pix2pix\u00a0[22] We also compare against pix2pix\u00a0[22], which is trained on paired data, to see how close we can get to this \u201cupper bound\u201d without using any paired data.", "For a fair comparison, we implement all the baselines using the same architecture and details as our method, except for CoGAN\u00a0[32]. CoGAN builds on generators that produce images from a shared latent representation, which is incompatible with our image-to-image network. We use the public implementation of CoGAN instead.", "As can be seen in Figure\u00a05 and Figure\u00a06, we were unable to achieve compelling results with any of the baselines. Our method, on the other hand, can produce translations that are often of similar quality to the fully supervised pix2pix.", "Table\u00a01 reports performance regarding the AMT perceptual realism task. Here, we see that our method can fool participants on around a quarter of trials, in both the maps\u2192\u2192\\rightarrow\u2192aerial photos direction and the aerial photos\u2192\u2192\\rightarrow\u2192maps direction at 256\u00d7256256256256\\times 256256 \u00d7 256 resolution333We also train CycleGAN and pix2pix at 512\u00d7512512512512\\times 512512 \u00d7 512 resolution, and observe the comparable performance: maps\u2192\u2192\\rightarrow\u2192aerial photos: CycleGAN: 37.5%\u00b13.6%plus-or-minuspercent37.5percent3.637.5\\%\\pm 3.6\\%37.5 % \u00b1 3.6 % and pix2pix: 33.9%\u00b13.1%plus-or-minuspercent33.9percent3.133.9\\%\\pm 3.1\\%33.9 % \u00b1 3.1 %; aerial photos\u2192\u2192\\rightarrow\u2192maps: CycleGAN: 16.5%\u00b14.1%plus-or-minuspercent16.5percent4.116.5\\%\\pm 4.1\\%16.5 % \u00b1 4.1 % and pix2pix: 8.5%\u00b12.6%plus-or-minuspercent8.5percent2.68.5\\%\\pm 2.6\\%8.5 % \u00b1 2.6 %. All the baselines almost never fooled participants.", "Table\u00a02 assesses the performance of the labels\u2192\u2192\\rightarrow\u2192photo task on the Cityscapes and Table\u00a03 evaluates the opposite mapping (photos\u2192\u2192\\rightarrow\u2192labels). In both cases, our method again outperforms the baselines.", "In Table\u00a04 and Table\u00a05, we compare against ablations of our full loss. Removing the GAN loss substantially degrades results, as does removing the cycle-consistency loss. We therefore conclude that both terms are critical to our results. We also evaluate our method with the cycle loss in only one direction: GAN + forward cycle loss \ud835\udd3cx\u223cpdata(x)[\u2225F(G(x))\u2212x\u22251]subscript\ud835\udd3csimilar-to\ud835\udc65subscript\ud835\udc5ddata\ud835\udc65delimited-[]subscriptdelimited-\u2225\u2225\ud835\udc39\ud835\udc3a\ud835\udc65\ud835\udc651\\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[\\lVert F(G(x))-x\\rVert_{1}]blackboard_E start_POSTSUBSCRIPT italic_x \u223c italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( italic_x ) end_POSTSUBSCRIPT [ \u2225 italic_F ( italic_G ( italic_x ) ) - italic_x \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ], or GAN + backward cycle loss \ud835\udd3cy\u223cpdata(y)[\u2225G(F(y))\u2212y\u22251]subscript\ud835\udd3csimilar-to\ud835\udc66subscript\ud835\udc5ddata\ud835\udc66delimited-[]subscriptdelimited-\u2225\u2225\ud835\udc3a\ud835\udc39\ud835\udc66\ud835\udc661\\mathbb{E}_{y\\sim p_{\\text{data}}(y)}[\\lVert G(F(y))-y\\rVert_{1}]blackboard_E start_POSTSUBSCRIPT italic_y \u223c italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( italic_y ) end_POSTSUBSCRIPT [ \u2225 italic_G ( italic_F ( italic_y ) ) - italic_y \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] (Equation\u00a02) and find that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed. Figure\u00a07 shows several qualitative examples.", "In Figure\u00a04, we show a few random samples of the reconstructed images F(G(x))\ud835\udc39\ud835\udc3a\ud835\udc65F(G(x))italic_F ( italic_G ( italic_x ) ). We observed that the reconstructed images were often close to the original inputs x\ud835\udc65xitalic_x, at both training and testing time, even in cases where one domain represents significantly more diverse information, such as map\u2194\u2194\\leftrightarrow\u2194aerial photos.", "Figure\u00a08 shows some example results on other paired datasets used in \u201cpix2pix\u201d\u00a0[22], such as architectural labels\u2194\u2194\\leftrightarrow\u2194photos from the CMP Facade Database\u00a0[40], and edges\u2194\u2194\\leftrightarrow\u2194shoes from the UT Zappos50Kdataset\u00a0[60]. The image quality of our results is close to those produced by the fully supervised pix2pix while our method learns the mapping without paired supervision.", "We demonstrate our method on several applications where paired training data does not exist. Please refer to the appendix (Section\u00a07) for more details about the datasets. We observe that translations on training data are often more appealing than those on test data, and full results of all applications on both training and test data can be viewed on our project website.", "Collection style transfer (Figure\u00a010 and \u00a0Figure\u00a011) We train the model on landscape photographs downloaded from Flickr and WikiArt. Unlike recent work on \u201cneural style transfer\u201d\u00a0[13], our method learns to mimic the style of an entire collection of artworks, rather than transferring the style of a single selected piece of art. Therefore, we can learn to generate photos in the style of, e.g., Van Gogh, rather than just in the style of Starry Night. The size of the dataset for each artist/style was 526526526526, 1073107310731073, 400400400400, and 563563563563 for Cezanne, Monet, Van Gogh, and Ukiyo-e.", "Object transfiguration (Figure\u00a013) The model is trained to translate one object class from ImageNet\u00a0[5] to another (each class contains around 1000100010001000 training images). Turmukhambetov et al.\u00a0[50] propose a subspace model to translate one object into another object of the same category, while our method focuses on object transfiguration between two visually similar categories.", "Season transfer (Figure\u00a013) The model is trained on 854854854854 winter photos and 1273127312731273 summer photos of Yosemite downloaded from Flickr.", "Photo generation from paintings (Figure\u00a012)For painting\u2192\u2192\\rightarrow\u2192photo, we find that it is helpful to introduce an additional loss to encourage the mapping to preserve color composition between the input and output. In particular, we adopt the technique of Taigman et al.\u00a0[49] and regularize the generator to be near an identity mapping when real samples of the target domain are provided as the input to the generator: i.e., \u2112identity(G,F)=\ud835\udd3cy\u223cpdata(y)[\u2225G(y)\u2212y\u22251]+\ud835\udd3cx\u223cpdata(x)[\u2225F(x)\u2212x\u22251].subscript\u2112identity\ud835\udc3a\ud835\udc39subscript\ud835\udd3csimilar-to\ud835\udc66subscript\ud835\udc5ddata\ud835\udc66delimited-[]subscriptdelimited-\u2225\u2225\ud835\udc3a\ud835\udc66\ud835\udc661subscript\ud835\udd3csimilar-to\ud835\udc65subscript\ud835\udc5ddata\ud835\udc65delimited-[]subscriptdelimited-\u2225\u2225\ud835\udc39\ud835\udc65\ud835\udc651\\mathcal{L}_{\\text{identity}}(G,F)=\\mathbb{E}_{y\\sim p_{\\text{data}}(y)}[\\lVert G(y)-y\\rVert_{1}]+\\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[\\lVert F(x)-x\\rVert_{1}].caligraphic_L start_POSTSUBSCRIPT identity end_POSTSUBSCRIPT ( italic_G , italic_F ) = blackboard_E start_POSTSUBSCRIPT italic_y \u223c italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( italic_y ) end_POSTSUBSCRIPT [ \u2225 italic_G ( italic_y ) - italic_y \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] + blackboard_E start_POSTSUBSCRIPT italic_x \u223c italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT ( italic_x ) end_POSTSUBSCRIPT [ \u2225 italic_F ( italic_x ) - italic_x \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] .", "Without \u2112identitysubscript\u2112identity\\mathcal{L}_{\\text{identity}}caligraphic_L start_POSTSUBSCRIPT identity end_POSTSUBSCRIPT, the generator G\ud835\udc3aGitalic_G and F\ud835\udc39Fitalic_F are free to change the tint of input images when there is no need to. For example, when learning the mapping between Monet\u2019s paintings and Flickr photographs, the generator often maps paintings of daytime to photographs taken during sunset, because such a mapping may be equally valid under the adversarial loss and cycle consistency loss.The effect of this identity mapping loss are shown in Figure\u00a09.", "In Figure\u00a012, we show additional results translating Monet\u2019s paintings to photographs. This figure and Figure\u00a09 show results on paintings that were included in the training set, whereas for all other experiments in the paper, we only evaluate and show test set results. Because the training set does not include paired data, coming up with a plausible translation for a training set painting is a nontrivial task. Indeed, since Monet is no longer able to create new paintings, generalization to unseen, \u201ctest set\u201d, paintings is not a pressing problem.", "Photo enhancement (Figure\u00a014) We show that our method can be used to generate photos with shallower depth of field. We train the model on flower photos downloaded from Flickr. The source domain consists of flower photos taken by smartphones, which usually have deep DoF due to a small aperture. The target contains photos captured by DSLRs with a larger aperture. Our model successfully generates photos with shallower depth of field from the photos taken by smartphones.", "Comparison with Gatys et al.\u00a0[13]In \u00a0Figure\u00a015, we compare our results with neural style transfer\u00a0[13] on photo stylization. For each row, we first use two representative artworks as the style images for \u00a0[13]. Our method, on the other hand, can produce photos in the style of entire collection. To compare against neural style transfer of an entire collection, we compute the average Gram Matrix across the target domain and use this matrix to transfer the \u201caverage style\u201d with Gatys et al\u00a0[13].", "Figure\u00a016 demonstrates similar comparisons for other translation tasks.We observe that Gatys et al.\u00a0[13] requires finding target style images that closely match the desired output, but still often fails to produce photorealistic results, while our method succeeds to generate natural-looking results, similar to the target domain.", "Although our method can achieve compelling results in many cases, the results are far from uniformly positive. Figure\u00a017 shows several typical failure cases. On translation tasks that involve color and texture changes, as many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog\u2192\u2192\\rightarrow\u2192cat transfiguration, the learned translation degenerates into making minimal changes to the input (Figure\u00a017). This failure might be caused by our generator architectures which are tailored for good performance on the appearance changes. Handling more varied and extreme transformations, especially geometric changes, is an important problem for future work.", "Some failure cases are caused by the distribution characteristics of the training datasets. For example, our method has got confused in the horse \u2192\u2192\\rightarrow\u2192 zebra example (Figure\u00a017, right), because our model was trained on the wild horse and zebra synsets of ImageNet, which does not contain images of a person riding a horse or zebra.", "We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method. In some cases, this gap may be very hard \u2013 or even impossible \u2013 to close: for example, our method sometimes permutes the labels for tree and building in the output of the photos\u2192\u2192\\rightarrow\u2192labels task. Resolving this ambiguity may require some form of weak semantic supervision. Integrating weak or semi-supervised data may lead to substantially more powerful translators, still at a fraction of the annotation cost of the fully-supervised systems.", "Nonetheless, in many cases completely unpaired data is plentifully available and should be made use of. This paper pushes the boundaries of what is possible in this \u201cunsupervised\u201d setting.", "Acknowledgments: We thank Aaron Hertzmann, Shiry Ginosar, Deepak Pathak, Bryan Russell, Eli Shechtman, Richard Zhang, and Tinghui Zhou for many helpful comments. This work was supported in part by NSF SMA-1514512, NSF IIS-1633310, a Google Research Award, Intel Corp, and hardware donations from NVIDIA. JYZ is supported by the Facebook Graduate Fellowship and TP is supported by the Samsung Scholarship. The photographs used for style transfer were taken by AE, mostly in France.", "We train our networks from scratch, with a learning rate of 0.00020.00020.00020.0002. In practice, we divide the objective by 2222 while optimizing D\ud835\udc37Ditalic_D, which slows down the rate at which D\ud835\udc37Ditalic_D learns, relative to the rate of G\ud835\udc3aGitalic_G. We keep the same learning rate for the first 100100100100 epochs and linearly decay the rate to zero over the next 100100100100 epochs. Weights are initialized from a Gaussian distribution \ud835\udca9(0,0.02)\ud835\udca900.02\\mathcal{N}(0,0.02)caligraphic_N ( 0 , 0.02 ).", "Cityscapes label\u2194normal-\u2194\\leftrightarrow\u2194Photo 2975297529752975 training images from the Cityscapes training set [4] with image size 128\u00d7128128128128\\times 128128 \u00d7 128. We used the Cityscapes val set for testing.", "Maps\u2194normal-\u2194\\leftrightarrow\u2194aerial photograph 1096109610961096 training images were scraped from Google Maps\u00a0[22] with image size 256\u00d7256256256256\\times 256256 \u00d7 256. Images were sampled from in and around New York City. Data was then split into train and test about the median latitude of the sampling region (with a buffer region added to ensure that no training pixel appeared in the test set).", "Architectural facades labels\u2194normal-\u2194\\leftrightarrow\u2194photo 400400400400 training images from the CMP Facade Database\u00a0[40].", "Edges\u2192normal-\u2192\\rightarrow\u2192shoes around 50,0005000050,00050 , 000 training images from UT Zappos50K dataset\u00a0[60]. The model was trained for 5555 epochs.", "Horse\u2194normal-\u2194\\leftrightarrow\u2194Zebra and Apple\u2194normal-\u2194\\leftrightarrow\u2194Orange We downloaded the images from ImageNet\u00a0[5] using keywords wild horse, zebra, apple, and navel orange. The images were scaled to 256\u00d7256256256256\\times 256256 \u00d7 256 pixels. The training set size of each class: 939939939939 (horse), 1177117711771177 (zebra), 996996996996 (apple), and 1020102010201020 (orange).", "Summer\u2194normal-\u2194\\leftrightarrow\u2194Winter Yosemite The images were downloaded using Flickr API with the tag yosemite and the datetaken field. Black-and-white photos were pruned. The images were scaled to 256\u00d7256256256256\\times 256256 \u00d7 256 pixels. The training size of each class: 1273127312731273 (summer) and 854854854854 ( winter).", "Photo\u2194normal-\u2194\\leftrightarrow\u2194Art for style transfer The art images were downloaded from Wikiart.org. Some artworks that were sketches or too obscene were pruned by hand. The photos were downloaded from Flickr using the combination of tags landscape and landscapephotography. Black-and-white photos were pruned. The images were scaled to 256\u00d7256256256256\\times 256256 \u00d7 256 pixels. The training set size of each class was 1074107410741074 (Monet), 584584584584 (Cezanne), 401401401401 (Van Gogh), 1433143314331433 (Ukiyo-e), and 6853685368536853 (Photographs). The Monet dataset was particularly pruned to include only landscape paintings, and the Van Gogh dataset included only his later works that represent his most recognizable artistic style.", "Monet\u2019s paintings\u2192normal-\u2192\\rightarrow\u2192photos To achieve high resolution while conserving memory, we used random square crops of the original images for training. To generate results, we passed images of width 512512512512 pixels with correct aspect ratio to the generator network as input. The weight for the identity mapping loss was 0.5\u03bb0.5\ud835\udf060.5\\lambda0.5 italic_\u03bb where \u03bb\ud835\udf06\\lambdaitalic_\u03bb was the weight for cycle consistency loss. We set \u03bb=10\ud835\udf0610\\lambda=10italic_\u03bb = 10.", "Flower photo enhancement Flower images taken on smartphones were downloaded from Flickr by searching for the photos taken byApple iPhone 5, 5s, or 6, with search text flower. DSLR images with shallow DoF were also downloaded from Flickr by search tag flower, dof. The images were scaled to 360360360360 pixels by width. The identity mapping loss of weight 0.5\u03bb0.5\ud835\udf060.5\\lambda0.5 italic_\u03bb was used. The training set size of the smartphone and DSLR dataset were 1813181318131813 and 3326332633263326, respectively. We set \u03bb=10\ud835\udf0610\\lambda=10italic_\u03bb = 10.", "We provide both PyTorch and Torch implementations.", "Generator architecturesWe adopt our architectures from Johnson et al.\u00a0[23]. We use 6666 residual blocks for 128\u00d7128128128128\\times 128128 \u00d7 128 training images, and 9999 residual blocks for 256\u00d7256256256256\\times 256256 \u00d7 256 or higher-resolution training images. Below, we follow the naming convention used in the Johnson et al.\u2019s Github repository.", "Let c7s1-k denote a 7\u00d77777\\times 77 \u00d7 7 Convolution-InstanceNorm-ReLU layer with k\ud835\udc58kitalic_k filters and stride 1111. dk denotes a 3\u00d73333\\times 33 \u00d7 3 Convolution-InstanceNorm-ReLU layer with k\ud835\udc58kitalic_k filters and stride 2222. Reflection padding was used to reduce artifacts. Rk denotes a residual block that contains two 3\u00d73333\\times 33 \u00d7 3 convolutional layers with the same number of filters on both layer. uk denotes a 3\u00d73333\\times 33 \u00d7 3 fractional-strided-Convolution-InstanceNorm-ReLU layer with k\ud835\udc58kitalic_k filters and stride 1212\\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG.", "The network with 6 residual blocks consists of:c7s1-64,d128,d256,R256,R256,R256,R256,R256,R256,u128,u64,c7s1-3", "The network with 9 residual blocks consists of:c7s1-64,d128,d256,R256,R256,R256,R256,R256,R256,R256,R256,R256,u128u64,c7s1-3", "Discriminator architecturesFor discriminator networks, we use 70\u00d770707070\\times 7070 \u00d7 70 PatchGAN\u00a0[22].Let Ck denote a 4\u00d74444\\times 44 \u00d7 4 Convolution-InstanceNorm-LeakyReLU layer with k filters and stride 2222. After the last layer, we apply a convolution to produce a 1111-dimensional output. We do not use InstanceNorm for the first C64 layer. We use leaky ReLUs with a slope of 0.20.20.20.2. The discriminator architecture is:C64-C128-C256-C512"], "figure_types": {"c43d954cf8133e6254499f3d68e45218067e4941/11-Figure10-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/12-Figure11-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/13-Figure12-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/14-Figure13-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/15-Figure14-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/15-Figure15-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/16-Figure16-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/16-Figure17-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/2-Figure2-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/3-Figure3-1.png": "schematic", "c43d954cf8133e6254499f3d68e45218067e4941/4-Figure4-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/6-Figure5-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/6-Figure6-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/7-Table1-1.png": "table", "c43d954cf8133e6254499f3d68e45218067e4941/7-Table2-1.png": "table", "c43d954cf8133e6254499f3d68e45218067e4941/7-Table3-1.png": "table", "c43d954cf8133e6254499f3d68e45218067e4941/7-Table4-1.png": "table", "c43d954cf8133e6254499f3d68e45218067e4941/8-Figure7-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/8-Figure8-1.png": "photograph(s)", "c43d954cf8133e6254499f3d68e45218067e4941/9-Figure9-1.png": "photograph(s)"}}, "1703.05175": {"paper_id": "paper_72", "title": "Prototypical Networks for Few-shot Learning", "arxiv_url": "https://arxiv.org/abs/1703.05175", "s2orc_url": "https://www.semanticscholar.org/paper/c269858a7bb34e8350f2442ccf37797856ae9bca", "all_figures_tables": {"c269858a7bb34e8350f2442ccf37797856ae9bca/2-Figure1-1.png": "Figure 1: Prototypical Networks in the few-shot and zero-shot scenarios. Left: Few-shot prototypes ck are computed as the mean of embedded support examples for each class. Right: Zero-shot prototypes ck are produced by embedding class meta-data vk. In either case, embedded query points are classified via a softmax over distances to class prototypes: p\u03c6(y = k|x) \u221d exp(\u2212d(f\u03c6(x), ck)).", "c269858a7bb34e8350f2442ccf37797856ae9bca/5-Figure2-1.png": "Figure 2: A t-SNE visualization of the embeddings learned by Prototypical networks on the Omniglot dataset. A subset of the Tengwar script is shown (an alphabet in the test set). Class prototypes are indicated in black. Several misclassified characters are highlighted in red along with arrows pointing to the correct prototype.", "c269858a7bb34e8350f2442ccf37797856ae9bca/6-Table1-1.png": "Table 1: Few-shot classification accuracies on Omniglot. \u2217Uses non-standard train/test splits.", "c269858a7bb34e8350f2442ccf37797856ae9bca/6-Table2-1.png": "Table 2: Few-shot classification accuracies on miniImageNet. All accuracy results are averaged over 600 test episodes and are reported with 95% confidence intervals. \u2217Results reported by [24].", "c269858a7bb34e8350f2442ccf37797856ae9bca/7-Figure3-1.png": "Figure 3: Comparison showing the effect of distance metric and number of classes per training episode on 5-way classification accuracy for both Matching Networks and Prototypical Networks on miniImageNet. The x-axis indicates configuration of the training episodes (way, distance, and shot), and the y-axis indicates 5-way test accuracy for the corresponding shot. Error bars indicate 95% confidence intervals as computed over 600 test episodes. Note that Matching Networks and Prototypical Networks are identical in the 1-shot case.", "c269858a7bb34e8350f2442ccf37797856ae9bca/7-Table3-1.png": "Table 3: Zero-shot classification accuracies on CUB-200."}, "referred_figures_tables": [["c269858a7bb34e8350f2442ccf37797856ae9bca/2-Figure1-1.png"]], "question_id": [11], "question": ["Why is it beneficial to fix the prototype embedding g to have unit length?"], "question_section": ["2.7 Zero-Shot Learning"], "question_trigger_sentence": ["Since the meta-data vector and query point come from different input domains, we found it was helpful empirically to fix the prototype embedding g to have unit length, however we do not constrain the query embedding f ."], "question_type": ["Deep/complex Question"], "evidential_info": [[{"context": "[Zero-shot learning differs from few-shot learning in that instead of being given a support set of training points, we are given a class meta-data vector vk for each class. These could be determined in advance, or they could be learned from e.g., raw text [7]. Modifying prototypical networks to deal with the zero-shot case is straightforward: we simply define ck = g\u03d1(vk) to be a separate embedding of the meta-data vector. An illustration of the zero-shot procedure for prototypical networks as it relates to the few-shot procedure is shown in Figure 1. Since the meta-data vector and query point come from different input domains, we found it was helpful empirically to fix the prototype embedding g to have unit length, however we do not constrain the query embedding f. ]", "rationale": "[In Zero-shot learning, since the meta-data vector and query point come from different input domains, the paper found it was helpful empirically to fix the prototype embedding g to have unit length, however the paper did not constrain the query embedding f.]"}]], "composition": ["[In Zero-shot learning, since the meta-data vector and query point come from different input domains, the paper found it empirically beneficial to fix the prototype embedding g to have unit length, however the paper did not constrain the query embedding f.]"], "Is_figure_in_evidence": [true], "Is_table_in_evidence": [false], "question_key": ["1260"], "passages": ["Few-shot classification (Miller et\u00a0al., 2000; Lake et\u00a0al., 2011; Koch, 2015) is a task in which a classifier must be adapted to accommodate new classes not seen in training, given only a few examples of each of these classes. A naive approach, such as re-training the model on the new data, would severely overfit. While the problem is quite difficult, it has been demonstrated that humans have the ability to perform even one-shot classification, where only a single example of each new class is given, with a high degree of accuracy\u00a0(Lake et\u00a0al., 2011).", "Two recent approaches have made significant progress in few-shot learning.Vinyals et\u00a0al. (2016) proposed matching networks, which uses an attention mechanism over a learned embedding of the labeled set of examples (the support set) to predict classes for the unlabeled points (the query set). Matching networks can be interpreted as a weighted nearest-neighbor classifier applied within an embedding space. Notably, this model utilizes sampled mini-batches called episodes during training, where each episode is designed to mimic the few-shot task by subsampling classes as well as data points. The use of episodes makes the training problem more faithful to the test environment and thereby improves generalization.Ravi and Larochelle (2017) take the episodic training idea further and propose a meta-learning approach to few-shot learning. Their approach involves training an LSTM\u00a0Hochreiter and Schmidhuber (1997) to produce the updates to a classifier, given an episode, such that it will generalize well to a test-set. Here, rather than training a single model over multiple episodes, the LSTM meta-learner learns to train a custom model for each episode.", "We attack the problem of few-shot learning by addressing the key issue of overfitting. Since data is severely limited, we work under the assumption that a classifier should have a very simple inductive bias. Our approach, prototypical networks, is based on the idea that there exists an embedding in which points cluster around a single prototype representation for each class. In order to do this, we learn a non-linear mapping of the input into an embedding space using a neural network and take a class\u2019s prototype to be the mean of its support set in the embedding space. Classification is then performed for an embedded query point by simply finding the nearest class prototype. We follow the same approach to tackle zero-shot learning; here each class comes with meta-data giving a high-level description of the class rather than a small number of labeled examples. We therefore learn an embedding of the meta-data into a shared space to serve as the prototype for each class. Classification is performed, as in the few-shot scenario, by finding the nearest class prototype for an embedded query point.", "In this paper, we formulate prototypical networks for both the few-shot and zero-shot settings. We draw connections to matching networks in the one-shot setting, and analyze the underlying distance function used in the model. In particular, we relate prototypical networks to clustering Banerjee et\u00a0al. (2005) in order to justify the use of class means as prototypes when distances are computed with a Bregman divergence, such as squared Euclidean distance. We find empirically that the choice of distance is vital, as Euclidean distance greatly outperforms the more commonly used cosine similarity. On several benchmark tasks, we achieve state-of-the-art performance. Prototypical networks are simpler and more efficient than recent meta-learning algorithms, making them an appealing approach to few-shot and zero-shot learning.", "In few-shot classification we are given a small support set of N\ud835\udc41Nitalic_N labeled examples S={(\ud835\udc311,y1),\u2026,(\ud835\udc31N,yN)}\ud835\udc46subscript\ud835\udc311subscript\ud835\udc661\u2026subscript\ud835\udc31\ud835\udc41subscript\ud835\udc66\ud835\udc41S=\\{(\\mathbf{x}_{1},y_{1}),\\ldots,(\\mathbf{x}_{N},y_{N})\\}italic_S = { ( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , \u2026 , ( bold_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ) } where each \ud835\udc31i\u2208\u211dDsubscript\ud835\udc31\ud835\udc56superscript\u211d\ud835\udc37\\mathbf{x}_{i}\\in\\mathbb{R}^{D}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT is the D\ud835\udc37Ditalic_D-dimensional feature vector of an example and yi\u2208{1,\u2026,K}subscript\ud835\udc66\ud835\udc561\u2026\ud835\udc3ey_{i}\\in\\{1,\\ldots,K\\}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 { 1 , \u2026 , italic_K } is the corresponding label. Sksubscript\ud835\udc46\ud835\udc58S_{k}italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT denotes the set of examples labeled with class k\ud835\udc58kitalic_k.", "Prototypical networks compute an M\ud835\udc40Mitalic_M-dimensional representation \ud835\udc1ck\u2208\u211dMsubscript\ud835\udc1c\ud835\udc58superscript\u211d\ud835\udc40\\mathbf{c}_{k}\\in\\mathbb{R}^{M}bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT, or prototype, of each class through an embedding function f\u03d5:\u211dD\u2192\u211dM:subscript\ud835\udc53bold-italic-\u03d5\u2192superscript\u211d\ud835\udc37superscript\u211d\ud835\udc40f_{\\bm{\\phi}}:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{M}italic_f start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT \u2192 blackboard_R start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT with learnable parameters \u03d5bold-italic-\u03d5\\bm{\\phi}bold_italic_\u03d5. Each prototype is the mean vector of the embedded support points belonging to its class:\ud835\udc1ck=1|Sk|\u2211(\ud835\udc31i,yi)\u2208Skf\u03d5(\ud835\udc31i)subscript\ud835\udc1c\ud835\udc581subscript\ud835\udc46\ud835\udc58subscriptsubscript\ud835\udc31\ud835\udc56subscript\ud835\udc66\ud835\udc56subscript\ud835\udc46\ud835\udc58subscript\ud835\udc53bold-italic-\u03d5subscript\ud835\udc31\ud835\udc56\\mathbf{c}_{k}=\\frac{1}{|S_{k}|}\\sum_{(\\mathbf{x}_{i},y_{i})\\in S_{k}}f_{\\bm{\\phi}}(\\mathbf{x}_{i})bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG | italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | end_ARG \u2211 start_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) \u2208 italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )(1)Given a distance function d:\u211dM\u00d7\u211dM\u2192[0,+\u221e):\ud835\udc51\u2192superscript\u211d\ud835\udc40superscript\u211d\ud835\udc400d:\\mathbb{R}^{M}\\times\\mathbb{R}^{M}\\rightarrow[0,+\\infty)italic_d : blackboard_R start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT \u00d7 blackboard_R start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT \u2192 [ 0 , + \u221e ), prototypical networks produce a distribution over classes for a query point \ud835\udc31\ud835\udc31\\mathbf{x}bold_x based on a softmax over distances to the prototypes in the embedding space:p\u03d5(y=k|\ud835\udc31)=exp\u2061(\u2212d(f\u03d5(\ud835\udc31),\ud835\udc1ck))\u2211k\u2032exp\u2061(\u2212d(f\u03d5(\ud835\udc31),\ud835\udc1ck\u2032))subscript\ud835\udc5dbold-italic-\u03d5\ud835\udc66conditional\ud835\udc58\ud835\udc31\ud835\udc51subscript\ud835\udc53bold-italic-\u03d5\ud835\udc31subscript\ud835\udc1c\ud835\udc58subscriptsuperscript\ud835\udc58\u2032\ud835\udc51subscript\ud835\udc53bold-italic-\u03d5\ud835\udc31subscript\ud835\udc1csuperscript\ud835\udc58\u2032p_{\\bm{\\phi}}(y=k\\,|\\,\\mathbf{x})=\\frac{\\exp(-d(f_{\\bm{\\phi}}(\\mathbf{x}),\\mathbf{c}_{k}))}{\\sum_{k^{\\prime}}\\exp(-d(f_{\\bm{\\phi}}(\\mathbf{x}),\\mathbf{c}_{k^{\\prime}}))}italic_p start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( italic_y = italic_k | bold_x ) = divide start_ARG roman_exp ( - italic_d ( italic_f start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_x ) , bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_exp ( - italic_d ( italic_f start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_x ) , bold_c start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) ) end_ARG(2)Learning proceeds by minimizing the negative log-probability J(\u03d5)=\u2212log\u2061p\u03d5(y=k|\ud835\udc31)\ud835\udc3dbold-italic-\u03d5subscript\ud835\udc5dbold-italic-\u03d5\ud835\udc66conditional\ud835\udc58\ud835\udc31J(\\bm{\\phi})=-\\log p_{\\bm{\\phi}}(y=k\\,|\\,\\mathbf{x})italic_J ( bold_italic_\u03d5 ) = - roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( italic_y = italic_k | bold_x ) of the true class k\ud835\udc58kitalic_k via SGD. Training episodes are formed by randomly selecting a subset of classes from the training set, then choosing a subset of examples within each class to act as the support set and a subset of the remainder to serve as query points. Pseudocode to compute the loss J(\u03d5)\ud835\udc3dbold-italic-\u03d5J(\\bm{\\phi})italic_J ( bold_italic_\u03d5 ) for a training episode is provided in Algorithm\u00a01.", "For a particular class of distance functions, known as regular Bregman divergences Banerjee et\u00a0al. (2005), the prototypical networks algorithm is equivalent to performing mixture density estimation on the support set with an exponential family density. A regular Bregman divergence d\u03c6subscript\ud835\udc51\ud835\udf11d_{\\varphi}italic_d start_POSTSUBSCRIPT italic_\u03c6 end_POSTSUBSCRIPT is defined as:d\u03c6(\ud835\udc33,\ud835\udc33\u2032)=\u03c6(\ud835\udc33)\u2212\u03c6(\ud835\udc33\u2032)\u2212(\ud835\udc33\u2212\ud835\udc33\u2032)T\u2207\u03c6(\ud835\udc33\u2032),subscript\ud835\udc51\ud835\udf11\ud835\udc33superscript\ud835\udc33\u2032\ud835\udf11\ud835\udc33\ud835\udf11superscript\ud835\udc33\u2032superscript\ud835\udc33superscript\ud835\udc33\u2032\ud835\udc47\u2207\ud835\udf11superscript\ud835\udc33\u2032d_{\\varphi}(\\mathbf{z},\\mathbf{z}^{\\prime})=\\varphi(\\mathbf{z})-\\varphi(\\mathbf{z}^{\\prime})-(\\mathbf{z}-\\mathbf{z}^{\\prime})^{T}\\nabla\\varphi(\\mathbf{z}^{\\prime}),italic_d start_POSTSUBSCRIPT italic_\u03c6 end_POSTSUBSCRIPT ( bold_z , bold_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) = italic_\u03c6 ( bold_z ) - italic_\u03c6 ( bold_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) - ( bold_z - bold_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT \u2207 italic_\u03c6 ( bold_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) ,(3)where \u03c6\ud835\udf11\\varphiitalic_\u03c6 is a differentiable, strictly convex function of the Legendre type. Examples of Bregman divergences include squared Euclidean distance \u2016\ud835\udc33\u2212\ud835\udc33\u2032\u20162superscriptnorm\ud835\udc33superscript\ud835\udc33\u20322\\|\\mathbf{z}-\\mathbf{z}^{\\prime}\\|^{2}\u2225 bold_z - bold_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT and Mahalanobis distance.", "Prototype computation can be viewed in terms of hard clustering on the support set, with one cluster per class and each support point assigned to its corresponding class cluster. It has been shown Banerjee et\u00a0al. (2005) for Bregman divergences that the cluster representative achieving minimal distance to its assigned points is the cluster mean. Thus the prototype computation in Equation (1) yields optimal cluster representatives given the support set labels when a Bregman divergence is used.", "Moreover, any regular exponential family distribution p\u03c8(\ud835\udc33|\ud835\udf3d)subscript\ud835\udc5d\ud835\udf13conditional\ud835\udc33\ud835\udf3dp_{\\psi}(\\mathbf{z}|\\bm{\\theta})italic_p start_POSTSUBSCRIPT italic_\u03c8 end_POSTSUBSCRIPT ( bold_z | bold_italic_\u03b8 ) with parameters \ud835\udf3d\ud835\udf3d\\bm{\\theta}bold_italic_\u03b8 and cumulant function \u03c8\ud835\udf13\\psiitalic_\u03c8 can be written in terms of a uniquely determined regular Bregman divergence Banerjee et\u00a0al. (2005):p\u03c8(\ud835\udc33|\ud835\udf3d)=exp\u2061{\ud835\udc33T\ud835\udf3d\u2212\u03c8(\ud835\udf3d)\u2212g\u03c8(\ud835\udc33)}=exp\u2061{\u2212d\u03c6(\ud835\udc33,\ud835\udf41(\ud835\udf3d))\u2212g\u03c6(\ud835\udc33)}subscript\ud835\udc5d\ud835\udf13conditional\ud835\udc33\ud835\udf3dsuperscript\ud835\udc33\ud835\udc47\ud835\udf3d\ud835\udf13\ud835\udf3dsubscript\ud835\udc54\ud835\udf13\ud835\udc33subscript\ud835\udc51\ud835\udf11\ud835\udc33\ud835\udf41\ud835\udf3dsubscript\ud835\udc54\ud835\udf11\ud835\udc33p_{\\psi}(\\mathbf{z}|\\bm{\\theta})=\\exp\\{\\mathbf{z}^{T}\\bm{\\theta}-\\psi(\\bm{\\theta})-g_{\\psi}(\\mathbf{z})\\}=\\exp\\{-d_{\\varphi}(\\mathbf{z},\\bm{\\mu}(\\bm{\\theta}))-g_{\\varphi}(\\mathbf{z})\\}italic_p start_POSTSUBSCRIPT italic_\u03c8 end_POSTSUBSCRIPT ( bold_z | bold_italic_\u03b8 ) = roman_exp { bold_z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_\u03b8 - italic_\u03c8 ( bold_italic_\u03b8 ) - italic_g start_POSTSUBSCRIPT italic_\u03c8 end_POSTSUBSCRIPT ( bold_z ) } = roman_exp { - italic_d start_POSTSUBSCRIPT italic_\u03c6 end_POSTSUBSCRIPT ( bold_z , bold_italic_\u03bc ( bold_italic_\u03b8 ) ) - italic_g start_POSTSUBSCRIPT italic_\u03c6 end_POSTSUBSCRIPT ( bold_z ) }(4)Consider now a regular exponential family mixture model with parameters \ud835\udeaa={\ud835\udf3dk,\u03c0k}k=1K\ud835\udeaasuperscriptsubscriptsubscript\ud835\udf3d\ud835\udc58subscript\ud835\udf0b\ud835\udc58\ud835\udc581\ud835\udc3e\\bm{\\Gamma}=\\{\\bm{\\theta}_{k},\\pi_{k}\\}_{k=1}^{K}bold_\u0393 = { bold_italic_\u03b8 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_\u03c0 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT:p(\ud835\udc33|\ud835\udeaa)=\u2211k=1K\u03c0kp\u03c8(\ud835\udc33|\ud835\udf3dk)=\u2211k=1K\u03c0kexp\u2061(\u2212d\u03c6(\ud835\udc33,\ud835\udf41(\ud835\udf3dk))\u2212g\u03c6(\ud835\udc33))\ud835\udc5dconditional\ud835\udc33\ud835\udeaasuperscriptsubscript\ud835\udc581\ud835\udc3esubscript\ud835\udf0b\ud835\udc58subscript\ud835\udc5d\ud835\udf13conditional\ud835\udc33subscript\ud835\udf3d\ud835\udc58superscriptsubscript\ud835\udc581\ud835\udc3esubscript\ud835\udf0b\ud835\udc58subscript\ud835\udc51\ud835\udf11\ud835\udc33\ud835\udf41subscript\ud835\udf3d\ud835\udc58subscript\ud835\udc54\ud835\udf11\ud835\udc33p(\\mathbf{z}|\\bm{\\Gamma})=\\sum_{k=1}^{K}\\pi_{k}p_{\\psi}(\\mathbf{z}|\\bm{\\theta}_{k})=\\sum_{k=1}^{K}\\pi_{k}\\exp(-d_{\\varphi}(\\mathbf{z},\\bm{\\mu}(\\bm{\\theta}_{k}))-g_{\\varphi}(\\mathbf{z}))italic_p ( bold_z | bold_\u0393 ) = \u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_\u03c8 end_POSTSUBSCRIPT ( bold_z | bold_italic_\u03b8 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = \u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT roman_exp ( - italic_d start_POSTSUBSCRIPT italic_\u03c6 end_POSTSUBSCRIPT ( bold_z , bold_italic_\u03bc ( bold_italic_\u03b8 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) - italic_g start_POSTSUBSCRIPT italic_\u03c6 end_POSTSUBSCRIPT ( bold_z ) )(5)Given \ud835\udeaa\ud835\udeaa\\bm{\\Gamma}bold_\u0393, inference of the cluster assignment y\ud835\udc66yitalic_y for an unlabeled point \ud835\udc33\ud835\udc33\\mathbf{z}bold_z becomes:p(y=k|\ud835\udc33)=\u03c0kexp\u2061(\u2212d\u03c6(\ud835\udc33,\ud835\udf41(\ud835\udf3dk)))\u2211k\u2032\u03c0k\u2032exp\u2061(\u2212d\u03c6(\ud835\udc33,\ud835\udf41(\ud835\udf3dk)))\ud835\udc5d\ud835\udc66conditional\ud835\udc58\ud835\udc33subscript\ud835\udf0b\ud835\udc58subscript\ud835\udc51\ud835\udf11\ud835\udc33\ud835\udf41subscript\ud835\udf3d\ud835\udc58subscriptsuperscript\ud835\udc58\u2032subscript\ud835\udf0bsuperscript\ud835\udc58\u2032subscript\ud835\udc51\ud835\udf11\ud835\udc33\ud835\udf41subscript\ud835\udf3d\ud835\udc58p(y=k|\\mathbf{z})=\\frac{\\pi_{k}\\exp(-d_{\\varphi}(\\mathbf{z},\\bm{\\mu}(\\bm{\\theta}_{k})))}{\\sum_{k^{\\prime}}\\pi_{k^{\\prime}}\\exp(-d_{\\varphi}(\\mathbf{z},\\bm{\\mu}(\\bm{\\theta}_{k})))}italic_p ( italic_y = italic_k | bold_z ) = divide start_ARG italic_\u03c0 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT roman_exp ( - italic_d start_POSTSUBSCRIPT italic_\u03c6 end_POSTSUBSCRIPT ( bold_z , bold_italic_\u03bc ( bold_italic_\u03b8 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_\u03c0 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_exp ( - italic_d start_POSTSUBSCRIPT italic_\u03c6 end_POSTSUBSCRIPT ( bold_z , bold_italic_\u03bc ( bold_italic_\u03b8 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) ) end_ARG(6)For an equally-weighted mixture model with one cluster per class, cluster assignment inference (6) is equivalent to query class prediction (2) with f\u03d5(\ud835\udc31)=\ud835\udc33subscript\ud835\udc53italic-\u03d5\ud835\udc31\ud835\udc33f_{\\phi}(\\mathbf{x})=\\mathbf{z}italic_f start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_x ) = bold_z and \ud835\udc1ck=\ud835\udf41(\ud835\udf3dk)subscript\ud835\udc1c\ud835\udc58\ud835\udf41subscript\ud835\udf3d\ud835\udc58\\mathbf{c}_{k}=\\bm{\\mu}(\\bm{\\theta}_{k})bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_\u03bc ( bold_italic_\u03b8 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ).In this case, prototypical networks are effectively performing mixture density estimation with an exponential family distribution determined by d\u03c6subscript\ud835\udc51\ud835\udf11d_{\\varphi}italic_d start_POSTSUBSCRIPT italic_\u03c6 end_POSTSUBSCRIPT. The choice of distance therefore specifies modeling assumptions about the class-conditional data distribution in the embedding space.", "A simple analysis is useful in gaining insight into the nature of the learned classifier.When we use Euclidean distance d(\ud835\udc33,\ud835\udc33\u2032)=\u2016\ud835\udc33\u2212\ud835\udc33\u2032\u20162\ud835\udc51\ud835\udc33superscript\ud835\udc33\u2032superscriptnorm\ud835\udc33superscript\ud835\udc33\u20322d(\\mathbf{z},\\mathbf{z^{\\prime}})=\\|\\mathbf{z}-\\mathbf{z}^{\\prime}\\|^{2}italic_d ( bold_z , bold_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) = \u2225 bold_z - bold_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, then the model in Equation (2) is equivalent to a linear model with a particular parameterization\u00a0Mensink et\u00a0al. (2013). To see this, expand the term in the exponent:\u2212\u2016f\u03d5(\ud835\udc31)\u2212\ud835\udc1ck\u20162superscriptnormsubscript\ud835\udc53bold-italic-\u03d5\ud835\udc31subscript\ud835\udc1c\ud835\udc582\\displaystyle-\\|f_{\\bm{\\phi}}(\\mathbf{x})-\\mathbf{c}_{k}\\|^{2}- \u2225 italic_f start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_x ) - bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT=\u2212f\u03d5(\ud835\udc31)\u22a4f\u03d5(\ud835\udc31)+2\ud835\udc1ck\u22a4f\u03d5(\ud835\udc31)\u2212\ud835\udc1ck\u22a4\ud835\udc1ckabsentsubscript\ud835\udc53bold-italic-\u03d5superscript\ud835\udc31topsubscript\ud835\udc53bold-italic-\u03d5\ud835\udc312superscriptsubscript\ud835\udc1c\ud835\udc58topsubscript\ud835\udc53bold-italic-\u03d5\ud835\udc31superscriptsubscript\ud835\udc1c\ud835\udc58topsubscript\ud835\udc1c\ud835\udc58\\displaystyle=-f_{\\bm{\\phi}}(\\mathbf{x})^{\\top}f_{\\bm{\\phi}}(\\mathbf{x})+2\\mathbf{c}_{k}^{\\top}f_{\\bm{\\phi}}(\\mathbf{x})-\\mathbf{c}_{k}^{\\top}\\mathbf{c}_{k}= - italic_f start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_x ) start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_x ) + 2 bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_x ) - bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT(7)The first term in Equation (7) is constant with respect to the class k\ud835\udc58kitalic_k, so it does not affect the softmax probabilities. We can write the remaining terms as a linear model as follows:2\ud835\udc1ck\u22a4f\u03d5(\ud835\udc31)\u2212\ud835\udc1ck\u22a4\ud835\udc1ck=\ud835\udc30k\u22a4f\u03d5(\ud835\udc31)+bk, where\u00a0\ud835\udc30k=2\ud835\udc1ck\u00a0and\u00a0bk=\u2212\ud835\udc1ck\u22a4\ud835\udc1ck2superscriptsubscript\ud835\udc1c\ud835\udc58topsubscript\ud835\udc53bold-italic-\u03d5\ud835\udc31superscriptsubscript\ud835\udc1c\ud835\udc58topsubscript\ud835\udc1c\ud835\udc58superscriptsubscript\ud835\udc30\ud835\udc58topsubscript\ud835\udc53bold-italic-\u03d5\ud835\udc31subscript\ud835\udc4f\ud835\udc58, where\u00a0subscript\ud835\udc30\ud835\udc582subscript\ud835\udc1c\ud835\udc58\u00a0and\u00a0subscript\ud835\udc4f\ud835\udc58superscriptsubscript\ud835\udc1c\ud835\udc58topsubscript\ud835\udc1c\ud835\udc582\\mathbf{c}_{k}^{\\top}f_{\\bm{\\phi}}(\\mathbf{x})-\\mathbf{c}_{k}^{\\top}\\mathbf{c}_{k}=\\mathbf{w}_{k}^{\\top}f_{\\bm{\\phi}}(\\mathbf{x})+b_{k}\\mbox{, where }\\mathbf{w}_{k}=2\\mathbf{c}_{k}\\mbox{ and }b_{k}=-\\mathbf{c}_{k}^{\\top}\\mathbf{c}_{k}2 bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_x ) - bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_x ) + italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , where bold_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 2 bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = - bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT(8)We focus primarily on squared Euclidean distance (corresponding to spherical Gaussian densities) in this work. Our results indicate that Euclidean distance is an effective choice despite the equivalence to a linear model. We hypothesize this is because all of the required non-linearity can be learned within the embedding function.Indeed, this is the approach that modern neural network classification systems currently use, e.g., (Krizhevsky et\u00a0al., 2012; Szegedy et\u00a0al., 2015).", "Prototypical networks differ from matching networks in the few-shot case with equivalence in the one-shot scenario. Matching networks Vinyals et\u00a0al. (2016) produce a weighted nearest neighbor classifier given the support set, while prototypical networks produce a linear classifier when squared Euclidean distance is used. In the case of one-shot learning, \ud835\udc1ck=\ud835\udc31ksubscript\ud835\udc1c\ud835\udc58subscript\ud835\udc31\ud835\udc58\\mathbf{c}_{k}=\\mathbf{x}_{k}bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT since there is only one support point per class, and matching networks and prototypical networks become equivalent.", "A natural question is whether it makes sense to use multiple prototypes per class instead of just one.If the number of prototypes per class is fixed and greater than 1111, then this would require a partitioning scheme to further cluster the support points within a class. This has been proposed in Mensink et\u00a0al. (2013) and Rippel et\u00a0al. (2016); however both methods require a separate partitioning phase that is decoupled from the weight updates, while our approach is simple to learn with ordinary gradient descent methods.", "Vinyals et\u00a0al. (2016) propose a number of extensions, including decoupling the embedding functions of the support and query points, and using a second-level, fully-conditional embedding (FCE) that takes into account specific points in each episode. These could likewise be incorporated into prototypical networks, however they increase the number of learnable parameters, and FCE imposes an arbitrary ordering on the support set using a bi-directional LSTM. Instead, we show that it is possible to achieve the same level of performance using simple design choices, which we outline next.", "Vinyals et\u00a0al. (2016) and Ravi and Larochelle (2017) apply matching networks using cosine distance.However for both prototypical and matching networks any distance is permissible, and we found that using squared Euclidean distance can greatly improve results for both. We conjecture this is primarily due to cosine distance not being a Bregman divergence, and thus the equivalence to mixture density estimation discussed in Section\u00a02.3 does not hold.", "A straightforward way to construct episodes, used in Vinyals et\u00a0al. (2016) and Ravi and Larochelle (2017), is to choose Ncsubscript\ud835\udc41\ud835\udc50N_{c}italic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT classes and NSsubscript\ud835\udc41\ud835\udc46N_{S}italic_N start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT support points per class in order to match the expected situation at test-time. That is, if we expect at test-time to perform 5555-way classification and 1111-shot learning, then training episodes could be comprised of Nc=5subscript\ud835\udc41\ud835\udc505N_{c}=5italic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = 5, NS=1subscript\ud835\udc41\ud835\udc461N_{S}=1italic_N start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = 1. We have found, however, that it can be extremely beneficial to train with a higher Ncsubscript\ud835\udc41\ud835\udc50N_{c}italic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, or \u201cway\u201d, than will be used at test-time. In our experiments, we tune the training Ncsubscript\ud835\udc41\ud835\udc50N_{c}italic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT on a held-out validation set. Another consideration is whether to match NSsubscript\ud835\udc41\ud835\udc46N_{S}italic_N start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT, or \u201cshot\u201d, at train and test-time.For prototypical networks, we found that it is usually best to train and test with the same \u201cshot\u201d number.", "Zero-shot learning differs from few-shot learning in that instead of being given a support set of training points, we are given a class meta-data vector \ud835\udc2fksubscript\ud835\udc2f\ud835\udc58\\mathbf{v}_{k}bold_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT for each class. These could be determined in advance, or they could be learned from e.g., raw text\u00a0(Elhoseiny et\u00a0al., 2013). Modifying prototypical networks to deal with the zero-shot case is straightforward: we simply define \ud835\udc1ck=g\u03d1(\ud835\udc2fk)subscript\ud835\udc1c\ud835\udc58subscript\ud835\udc54bold-italic-\u03d1subscript\ud835\udc2f\ud835\udc58\\mathbf{c}_{k}=g_{\\bm{\\vartheta}}(\\mathbf{v}_{k})bold_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_g start_POSTSUBSCRIPT bold_italic_\u03d1 end_POSTSUBSCRIPT ( bold_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) to be a separate embedding of the meta-data vector. An illustration of the zero-shot procedure for prototypical networks as it relates to the few-shot procedure is shown in Figure\u00a01. Since the meta-data vector and query point come from different input domains, we found it was helpful empirically to fix the prototype embedding g\ud835\udc54gitalic_g to have unit length, however we do not constrain the query embedding f\ud835\udc53fitalic_f.", "For few-shot learning, we performed experiments on Omniglot (Lake et\u00a0al., 2011) and the miniImageNet version of ILSVRC-2012 (Russakovsky et\u00a0al., 2015) with the splits proposed by Ravi and Larochelle (2017). We perform zero-shot experiments on the 2011 version of the Caltech UCSD bird dataset (CUB-200 2011) (Welinder et\u00a0al., 2010).", "Omniglot (Lake et\u00a0al., 2011) is a dataset of 1623 handwritten characters collected from 50 alphabets. There are 20 examples associated with each character, where each example is drawn by a different human subject. We follow the procedure of Vinyals et\u00a0al. (2016) by resizing the grayscale images to 28 \u00d7\\times\u00d7 28 and augmenting the character classes with rotations in multiples of 90 degrees. We use 1200 characters plus rotations for training (4,800 classes in total) and the remaining classes, including rotations, for test. Our embedding architecture mirrors that used by Vinyals et\u00a0al. (2016) and is composed of four convolutional blocks. Each block comprises a 64-filter 3 \u00d7\\times\u00d7 3 convolution, batch normalization layer (Ioffe and Szegedy, 2015), a ReLU nonlinearity and a 2 \u00d7\\times\u00d7 2 max-pooling layer. When applied to the 28 \u00d7\\times\u00d7 28 Omniglot images this architecture results in a 64-dimensional output space. We use the same encoder for embedding both support and query points. All of our models were trained via SGD with Adam (Kingma and Ba, 2014). We used an initial learning rate of 10\u22123superscript10310^{-3}10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT and cut the learning rate in half every 2000 episodes. No regularization was used other than batch normalization.", "We trained prototypical networks using Euclidean distance in the 1-shot and 5-shot scenarios with training episodes containing 60 classes and 5 query points per class. We found that it is advantageous to match the training-shot with the test-shot, and to use more classes (higher \u201cway\u201d) per training episode rather than fewer. We compare against various baselines, including the neural statistician (Edwards and Storkey, 2017) and both the fine-tuned and non-fine-tuned versions of matching networks (Vinyals et\u00a0al., 2016). We computed classification accuracy for our models averaged over 1000 randomly generated episodes from the test set. The results are shown in Table\u00a01 and to our knowledge they represent the state-of-the-art on this dataset.", "The miniImageNet dataset, originally proposed by Vinyals et\u00a0al. (2016), is derived from the larger ILSVRC-12 dataset\u00a0(Russakovsky et\u00a0al., 2015). The splits used by Vinyals et\u00a0al. (2016) consist of 60,000 color images of size 84 \u00d7\\times\u00d7 84 divided into 100 classes with 600 examples each. For our experiments, we use the splits introduced by Ravi and Larochelle (2017) in order to directly compare with state-of-the-art algorithms for few-shot learning. Their splits use a different set of 100 classes, divided into 64 training, 16 validation, and 20 test classes. We follow their procedure by training on the 64 training classes and using the 16 validation classes for monitoring generalization performance only.", "We use the same four-block embedding architecture as in our Omniglot experiments, though here it results in a 1600-dimensional output space due to the increased size of the images. We also use the same learning rate schedule as in our Omniglot experiments and train until validation loss stops improving. We train using 30-way episodes for 1-shot classification and 20-way episodes for 5-shot classification. We match train shot to test shot and each class contains 15 query points per episode. We compare to the baselines as reported by Ravi and Larochelle (2017), which include a simple nearest neighbor approach on top of features learned by a classification network on the 64 training classes. The other baselines are two non-fine-tuned variants of matching networks (both ordinary and FCE) and the Meta-Learner LSTM.As can be seen in Table\u00a02, prototypical networks achieves state-of-the-art here by a wide margin.", "We conducted further analysis, to determine the effect of distance metric and the number of training classes per episode on the performance of prototypical networks and matching networks. To make the methods comparable, we use our own implementation of matching networks that utilizes the same embedding architecture as our prototypical networks.In Figure\u00a02 we compare cosine vs. Euclidean distance and 5-way vs. 20-way training episodes in the 1-shot and 5-shot scenarios, with 15 query points per class per episode. We note that 20-way achieves higher accuracy than 5-way and conjecture that the increased difficulty of 20-way classification helps the network to generalize better, because it forces the model to make more fine-grained decisions in the embedding space. Also, using Euclidean distance improves performance substantially over cosine distance. This effect is even more pronounced for prototypical networks, in which computing the class prototype as the mean of embedded support points is more naturally suited to Euclidean distances since cosine distance is not a Bregman divergence.", "In order to assess the suitability of our approach for zero-shot learning, we also run experiments on the Caltech-UCSD Birds (CUB) 200-2011 dataset (Welinder et\u00a0al., 2010). The CUB dataset contains 11,788 images of 200 bird species. We closely follow the procedure of Reed et\u00a0al. (2016) in preparing the data. We use their splits to divide the classes into 100 training, 50 validation, and 50 test. For images we use 1,024-dimensional features extracted by applying GoogLeNet (Szegedy et\u00a0al., 2015) to middle, upper left, upper right, lower left, and lower right crops of the original and horizontally-flipped image222Features downloaded from https://github.com/reedscot/cvpr2016.. At test time we use only the middle crop of the original image. For class meta-data we use the 312-dimensional continuous attribute vectors provided with the CUB dataset. These attributes encode various characteristics of the bird species such as their color, shape, and feather patterns.", "We learned a simple linear mapping on top of both the 1024-dimensional image features and the 312-dimensional attribute vectors to produce a 1,024-dimensional output space. For this dataset we found it helpful to normalize the class prototypes (embedded attribute vectors) to be of unit length, since the attribute vectors come from a different domain than the images. Training episodes were constructed with 50 classes and 10 query images per class. The embeddings were optimized via SGD with Adam at a fixed learning rate of 10\u22124superscript10410^{-4}10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT and weight decay of 10\u22125superscript10510^{-5}10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT. Early stopping on validation loss was used to determine the optimal number of epochs for retraining on the training plus validation set.", "Table\u00a03 shows that we achieve state-of-the-art results by a large margin when compared to methods utilizing attributes as class meta-data. We compare our method to other embedding approaches, such as ALE (Akata et\u00a0al., 2013), SJE (Akata et\u00a0al., 2015), and DS-SJE/DA-SJE (Reed et\u00a0al., 2016). We also compare to a recent clustering approach (Liao et\u00a0al., 2016) which trains an SVM on a learned feature space obtained by fine-tuning AlexNet (Krizhevsky et\u00a0al., 2012). These zero-shot classification results demonstrate that our approach is general enough to be applied even when the data points (images) are from a different domain relative to the classes (attributes).", "The literature on metric learning is vast (Kulis, 2012; Bellet et\u00a0al., 2013); we summarize here the work most relevant to our proposed method. Neighborhood Components Analysis (NCA) (Goldberger et\u00a0al., 2004) learns a Mahalanobis distance to maximize K-nearest-neighbor\u2019s (KNN) leave-one-out accuracy in the transformed space.Salakhutdinov and Hinton (2007) extend NCA by using a neural network to perform the transformation. Large margin nearest neighbor (LMNN) classification (Weinberger et\u00a0al., 2005) also attempts to optimize KNN accuracy but does so using a hinge loss that encourages the local neighborhood of a point to contain other points with the same label. The DNet-KNN (Min et\u00a0al., 2009) is another margin-based method that improves upon LMNN by utilizing a neural network to perform the embedding instead of a simple linear transformation. Of these, our method is most similar to the non-linear extension of NCA (Salakhutdinov and Hinton, 2007) because we use a neural network to perform the embedding and we optimize a softmax based on Euclidean distances in the transformed space, as opposed to a margin loss. A key distinction between our approach and non-linear NCA is that we form a softmax directly over classes, rather than individual points, computed from distances to each class\u2019s prototype representation. This allows each class to have a concise representation independent of the number of data points and obviates the need to store the entire support set to make predictions.", "Our approach is also similar to the nearest class mean approach (Mensink et\u00a0al., 2013), where each class is represented by the mean of its examples.This approach was developed to rapidly incorporate new classes into a classifier without retraining, however it relies on a linear embedding and was designed to handle the case where the novel classes come with a large number of examples. In contrast, our approach utilizes neural networks to non-linearly embed points and we couple this with episodic training in order to handle the few-shot scenario. Mensink et\u00a0al. attempt to extend their approach to also perform non-linear classification, but they do so by allowing classes to have multiple prototypes. They find these prototypes in a pre-processing step by using k\ud835\udc58kitalic_k-means on the input space and then perform a multi-modal variant of their linear embedding. Prototypical networks, on the other hand, learn a non-linear embedding in an end-to-end manner with no such pre-processing, producing a non-linear classifier that still only requires one prototype per class. In addition, our approach naturally generalizes to other distance functions, particularly Bregman divergences.", "Another relevant few-shot learning method is the meta-learning approach proposed in Ravi and Larochelle (2017).The key insight here is that LSTM dynamics and gradient descent can be written in effectively the same way. An LSTM can then be trained to itself train a model from a given episode, with the performance goal of generalizing well on the query points. Matching networks and prototypical networks can also be seen as forms of meta-learning, in the sense that they produce simple classifiers dynamically from new training episodes; however the core embeddings they rely on are fixed after training.The FCE extension to matching nets involves a secondary embedding that depends on the support set. However, in the few-shot scenario the amount of data is so small that a simple inductive bias seems to work well, without the need to learn a custom embedding for each episode.", "Prototypical networks are also related to the neural statistician (Edwards and Storkey, 2017) from the generative modeling literature, which extends the variational autoencoder (Kingma and Welling, 2013; Rezende et\u00a0al., 2014) to learn generative models of datasets rather than individual points. One component of the neural statistician is the \u201cstatistic network\u201d which summarizes a set of data points into a statistic vector. It does this by encoding each point within a dataset, taking a sample mean, and applying a post-processing network to obtain an approximate posterior over the statistic vector. Edwards and Storkey test their model for one-shot classification on the Omniglot dataset by considering each character to be a separate dataset and making predictions based on the class whose approximate posterior over the statistic vector has minimal KL-divergence from the posterior inferred by the test point. Like the neural statistician, we also produce a summary statistic for each class. However, ours is a discriminative model, as befits our discriminative task of few-shot classification.", "With respect to zero-shot learning, the use of embedded meta-data in prototypical networks resembles the method of Ba et\u00a0al. (2015) in that both predict the weights of a linear classifier. The DS-SJE and DA-SJE approach of Reed et\u00a0al. (2016) also learns deep multimodal embedding functions for images and class meta-data. Unlike ours, they learn using an empirical risk loss. Neither Ba et\u00a0al. (2015) nor Reed et\u00a0al. (2016) uses episodic training, which allows us to help speed up training and regularize the model.", "We have proposed a simple method called prototypical networks for few-shot learning based on the idea that we can represent each class by the mean of its examples in a representation space learned by a neural network. We train these networks to specifically perform well in the few-shot setting by using episodic training.The approach is far simpler and more efficient than recent meta-learning approaches, and produces state-of-the-art results even without sophisticated extensions developed for matching networks (although these can be applied to prototypical nets as well). We show how performance can be greatly improved by carefully considering the chosen distance metric, and by modifying the episodic learning procedure. We further demonstrate how to generalize prototypical networks to the zero-shot setting, and achieve state-of-the-art results on the CUB-200 dataset. A natural direction for future work is to utilize Bregman divergences other than squared Euclidean distance, corresponding to class-conditional distributions beyond spherical Gaussians. We conducted preliminary explorations of this, including learning a variance per dimension for each class. This did not lead to any empirical gains, suggesting that the embedding network has enough flexibility on its own without requiring additional fitted parameters per class. Overall, the simplicity and effectiveness of prototypical networks makes it a promising approach for few-shot learning.", "We would like to thank Marc Law, Sachin Ravi, Hugo Larochelle, Renjie Liao, and Oriol Vinyals for helpful discussions. This work was supported by the Samsung GRP project and the Canadian Institute for Advanced Research."], "figure_types": {"c269858a7bb34e8350f2442ccf37797856ae9bca/2-Figure1-1.png": "schematic", "c269858a7bb34e8350f2442ccf37797856ae9bca/5-Figure2-1.png": "plot", "c269858a7bb34e8350f2442ccf37797856ae9bca/6-Table1-1.png": "table", "c269858a7bb34e8350f2442ccf37797856ae9bca/6-Table2-1.png": "table", "c269858a7bb34e8350f2442ccf37797856ae9bca/7-Figure3-1.png": "plot", "c269858a7bb34e8350f2442ccf37797856ae9bca/7-Table3-1.png": "table"}}, "1703.03400": {"paper_id": "paper_75", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "arxiv_url": "https://arxiv.org/abs/1703.03400", "s2orc_url": "https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "all_figures_tables": {"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/12-Figure6-1.png": "Figure 6. Quantitative sinusoid regression results showing test-time learning curves with varying numbers of K test-time samples. Each gradient step is computed using the same K examples. Note that MAML continues to improve with additional gradient steps without overfitting to the extremely small dataset during meta-testing, and achieves a loss that is substantially lower than the baseline fine-tuning approach.", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/12-Table3-1.png": "Table 3. 5-way Omniglot Classification", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/13-Figure7-1.png": "Figure 7. A random sample of qualitative results from the sinusoid regression task.", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/2-Figure1-1.png": "Figure 1. Diagram of our model-agnostic meta-learning algorithm (MAML), which optimizes for a representation \u03b8 that can quickly adapt to new tasks.", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/6-Figure2-1.png": "Figure 2. Few-shot adaptation for the simple regression task. Left: Note that MAML is able to estimate parts of the curve where there are no datapoints, indicating that the model has learned about the periodic structure of sine waves. Right: Fine-tuning of a model pretrained on the same distribution of tasks without MAML, with a tuned step size. Due to the often contradictory outputs on the pre-training tasks, this model is unable to recover a suitable representation and fails to extrapolate from the small number of test-time samples.", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/6-Figure3-1.png": "Figure 3. Quantitative sinusoid regression results showing the learning curve at meta test-time. Note that MAML continues to improve with additional gradient steps without overfitting to the extremely small dataset during meta-testing, achieving a loss that is substantially lower than the baseline fine-tuning approach.", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/7-Table1-1.png": "Table 1. Few-shot classification on held-out Omniglot characters (top) and the MiniImagenet test set (bottom). MAML achieves results that are comparable to or outperform state-of-the-art convolutional and recurrent models. Siamese nets, matching nets, and the memory module approaches are all specific to classification, and are not directly applicable to regression or RL scenarios. The \u00b1 shows 95% confidence intervals over tasks. Note that the Omniglot results may not be strictly comparable since the train/test splits used in the prior work were not available. The MiniImagenet evaluation of baseline methods and matching networks is from Ravi & Larochelle (2017).", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/8-Figure5-1.png": "Figure 5. Reinforcement learning results for the half-cheetah and ant locomotion tasks, with the tasks shown on the far right. Each gradient step requires additional samples from the environment, unlike the supervised learning tasks. The results show that MAML can adapt to new goal velocities and directions substantially faster than conventional pretraining or random initialization, achieving good performs in just two or three gradient steps. We exclude the goal velocity, random baseline curves, since the returns are much worse (&lt; \u2212200 for cheetah and &lt; \u221225 for ant)."}, "referred_figures_tables": [["c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/2-Figure1-1.png"], ["c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/2-Figure1-1.png"]], "question_id": [2, 4], "question": ["What type of parameter would be considered a 'good' initial parameter?", "Is it true that this paper's learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters?"], "question_section": ["1. Introduction", "1. Introduction"], "question_trigger_sentence": ["The key idea underlying our method is to train the model\u2019s initial parameters such that the model has maximal performance on a new task after the parameters have been updated through one or more gradient steps computed with a small amount of data from that new task.", "From a dynamical systems standpoint, our learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters: when the sensitivity is high, small local changes to the parameters can lead to large improvements in the task loss."], "question_type": ["Shallow Question", "Shallow Question"], "evidential_info": [[{"context": "The process of training a model\u2019s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks. If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly (e.g. by primarily modifying the top layer weights in a feedforward model) can produce good results. In effect, our procedure optimizes for models that are easy and fast to fine-tune, allowing the adaptation to happen in the right space for fast learning. From a dynamical systems standpoint, our learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters: when the sensitivity is high, small local changes to the parameters can lead to large improvements in the task loss.", "rationale": "If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly (e.g. by primarily modifying the top layer weights in a feedforward model) can produce good results."}, {"context": "Our approach is also related to methods for initialization of deep networks. In computer vision, models pretrained on large-scale image classification have been shown to learn effective features for a range of problems\u00a0(Donahue et\u00a0al., 2014). In contrast, our method explicitly optimizes the model for fast adaptability, allowing it to adapt to new tasks with only a few examples.Our method can also be viewed as explicitly maximizing sensitivity of new task losses to the model parameters.A number of prior works have explored sensitivity in deep networks, often in the context of initialization\u00a0(Saxe et\u00a0al., 2014; Kirkpatrick et\u00a0al., 2016). Most of these works have considered good random initializations, though a number of papers have addressed data-dependent initializers\u00a0(Kr\u00e4henb\u00fchl et\u00a0al., 2016; Salimans & Kingma, 2016), including learned initializations\u00a0(Husken & Goerick, 2000; Maclaurin et\u00a0al., 2015). In contrast, our method explicitly trains the parameters for sensitivity on a given task distribution, allowing for extremely efficient adaptation for problems such as K-shot learning and rapid reinforcement learning in only one or a few gradient steps.", "rationale": "Our method can also be viewed as explicitly maximizing sensitivity of new task losses to the model parameters"}, {"context": "In contrast to prior work, which has sought to train recurrent neural networks that ingest entire datasets\u00a0(Santoro et\u00a0al., 2016; Duan et\u00a0al., 2016b) or feature embeddings that can be combined with nonparametric methods at test time\u00a0(Vinyals et\u00a0al., 2016; Koch, 2015), we propose a method that can learn the parameters of any standard model via meta-learning in such a way as to prepare that model for fast adaptation. The intuition behind this approach is that some internal representations are more transferrable than others. For example, a neural network might learn internal features that are broadly applicable to all tasks in p(\\mathcal{T}), rather than a single individual task. How can we encourage the emergence of such general-purpose representations? We take an explicit approach to this problem: since the model will be fine-tuned using a gradient-based learning rule on a new task, we will aim to learn a model in such a way that this gradient-based learning rule can make rapid progress on new tasks drawn from p(\\mathcal{T}), without overfitting. In effect, we will aim to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will produce large improvements on the loss function of any task drawn from p(\\mathcal{T}), when altered in the direction of the gradient of that loss (see Figure\u00a01). We make no assumption on the form of the model, other than to assume that it is parametrized by some parameter vector \\theta, and that the loss function is smooth enough in \\theta that we can use gradient-based learning techniques.", "rationale": "The intuition behind this approach is that some internal representations are more transferrable than others"}], [{"context": "The model parameters are trained by optimizing for the performance of f_{\\theta_{i}^{\\prime}} with respect to \\theta across tasks sampled from p(\\mathcal{T}).More concretely, the meta-objective is as follows:\\displaystyle\\vspace{-0.2cm}\\min_{\\theta}\\sum_{\\mathcal{T}_{i}\\sim p(\\mathcal{T})}\\mathcal{L}_{\\mathcal{T}_{i}}(f_{\\theta_{i}^{\\prime}})=\\sum_{\\mathcal{T}_{i}\\sim p(\\mathcal{T})}\\mathcal{L}_{\\mathcal{T}_{i}}(f_{\\theta-\\alpha\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}_{i}}(f_{\\theta})})Note that the meta-optimization is performed over the model parameters \\theta, whereas the objective is computed using the updated model parameters \\theta^{\\prime}.In effect, our proposed method aims to optimize the model parameters such that one or a small number of gradient steps on a new task will produce maximally effective behavior on that task.", "rationale": "Our method can also be viewed as explicitly maximizing sensitivity of new task losses to the model parameters"}, {"context": "The process of training a model\u2019s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks. If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly (e.g. by primarily modifying the top layer weights in a feedforward model) can produce good results. In effect, our procedure optimizes for models that are easy and fast to fine-tune, allowing the adaptation to happen in the right space for fast learning. From a dynamical systems standpoint, our learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters: when the sensitivity is high, small local changes to the parameters can lead to large improvements in the task loss.", "rationale": "our learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters: when the sensitivity is high, small local changes to the parameters can lead to large improvements in the task loss."}, {"context": "Our approach is also related to methods for initialization of deep networks. In computer vision, models pretrained on large-scale image classification have been shown to learn effective features for a range of problems\u00a0(Donahue et\u00a0al., 2014). In contrast, our method explicitly optimizes the model for fast adaptability, allowing it to adapt to new tasks with only a few examples.Our method can also be viewed as explicitly maximizing sensitivity of new task losses to the model parameters.A number of prior works have explored sensitivity in deep networks, often in the context of initialization\u00a0(Saxe et\u00a0al., 2014; Kirkpatrick et\u00a0al., 2016). Most of these works have considered good random initializations, though a number of papers have addressed data-dependent initializers\u00a0(Kr\u00e4henb\u00fchl et\u00a0al., 2016; Salimans & Kingma, 2016), including learned initializations\u00a0(Husken & Goerick, 2000; Maclaurin et\u00a0al., 2015). In contrast, our method explicitly trains the parameters for sensitivity on a given task distribution, allowing for extremely efficient adaptation for problems such as K-shot learning and rapid reinforcement learning in only one or a few gradient steps.", "rationale": "In effect, we will aim to find model parameters that are sensitive to changes in the task,"}, {"context": "In contrast to prior work, which has sought to train recurrent neural networks that ingest entire datasets\u00a0(Santoro et\u00a0al., 2016; Duan et\u00a0al., 2016b) or feature embeddings that can be combined with nonparametric methods at test time\u00a0(Vinyals et\u00a0al., 2016; Koch, 2015), we propose a method that can learn the parameters of any standard model via meta-learning in such a way as to prepare that model for fast adaptation. The intuition behind this approach is that some internal representations are more transferrable than others. For example, a neural network might learn internal features that are broadly applicable to all tasks in p(\\mathcal{T}), rather than a single individual task. How can we encourage the emergence of such general-purpose representations? We take an explicit approach to this problem: since the model will be fine-tuned using a gradient-based learning rule on a new task, we will aim to learn a model in such a way that this gradient-based learning rule can make rapid progress on new tasks drawn from p(\\mathcal{T}), without overfitting. In effect, we will aim to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will produce large improvements on the loss function of any task drawn from p(\\mathcal{T}), when altered in the direction of the gradient of that loss (see Figure\u00a01). We make no assumption on the form of the model, other than to assume that it is parametrized by some parameter vector \\theta, and that the loss function is smooth enough in \\theta that we can use gradient-based learning techniques.", "rationale": "In effect, our proposed method aims to optimize the model parameters such that one or a small number of gradient steps on a new task will produce maximally effective behavior on that task."}]], "composition": ["A good initial parameter is a parameter that gives good performance in many tasks even with a little fine-tuning of the parameter. This means that the loss function defined in many tasks is sensitive, and this sensitive loss leads to good updates.", "It is true. As many sentences mention, it can be seen as increasing the sensitivity of the loss function."], "Is_figure_in_evidence": [false, false], "Is_table_in_evidence": [true, true], "question_key": ["1286", "1287"], "passages": ["Learning quickly is a hallmark of human intelligence, whether it involves recognizing objects from a few examples or quickly learning new skillsafter just minutes of experience. Our artificial agents should be able to do the same, learning and adapting quickly from only a few examples, and continuing to adapt as more data becomes available. This kind of fast and flexible learning is challenging, since the agent must integrate its prior experience with a small amount of new information, while avoiding overfitting to the new data. Furthermore, the form of prior experience and new data will depend on the task. As such, for the greatest applicability, the mechanism for learning to learn (or meta-learning) should be general to the task and the form of computation required to complete the task.", "In this work, we propose a meta-learning algorithm that is general and model-agnostic, in the sense that it can be directly applied to any learning problem and model that is trained with a gradient descent procedure. Our focus is on deep neural network models, but we illustrate how our approach can easily handle different architectures and different problem settings, including classification, regression, and policy gradient reinforcement learning, with minimal modification.In meta-learning, the goal of the trained model is to quickly learn a new task from a small amount of new data, and the model istrained by the meta-learner to be able to learn on a large number of different tasks.The key idea underlying our method is totrain the model\u2019s initial parameters such that the model has maximal performance on a new task after the parameters have been updatedthrough one or more gradient steps computed with a small amount of data from that new task.Unlike prior meta-learning methods that learn an update function or learning rule\u00a0(Schmidhuber, 1987; Bengio et\u00a0al., 1992; Andrychowicz et\u00a0al., 2016; Ravi & Larochelle, 2017), our algorithm does not expand the number of learned parameters nor place constraints on the model architecture (e.g. by requiring a recurrent model\u00a0(Santoro et\u00a0al., 2016) or a Siamese network\u00a0(Koch, 2015)), and it can be readily combined with fully connected, convolutional, or recurrent neural networks. It can also be used with a variety of loss functions, including differentiable supervised losses and non-differentiable reinforcement learning objectives. ", "The process of training a model\u2019s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks. If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly (e.g. by primarily modifying the top layer weights in a feedforward model) can produce good results. In effect, our procedure optimizes for models that are easy and fast to fine-tune, allowing the adaptation to happen in the right space for fast learning. From a dynamical systems standpoint, our learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters: when the sensitivity is high, small local changes to the parameters can lead to large improvements in the task loss.", "The primary contribution of this work is a simple model- and task-agnostic algorithm for meta-learning that trains a model\u2019s parameters such that a small number of gradient updates will lead to fast learning on a new task.We demonstrate the algorithm on different model types, including fully connected and convolutional networks, and in several distinct domains, including few-shot regression, image classification, and reinforcement learning.Our evaluation shows that our meta-learning algorithm compares favorably to state-of-the-art one-shot learning methods designed specifically for supervised classification, while using fewer parameters, but that it can also be readily applied to regression and can accelerate reinforcement learning in the presence of task variability, substantially outperforming direct pretraining as initialization.", "We aim to train models that can achieve rapid adaptation, a problem setting that is often formalized as few-shot learning. In this section, we will define the problem setup and present the general form of our algorithm.", "The goal of few-shot meta-learningis to train a model that can quickly adapt to a new task using only a few datapoints and training iterations.To accomplish this, the model or learner is trained during a meta-learning phase on a set of tasks, such that the trained model can quickly adapt to new tasks using only a small number of examples or trials.In effect, the meta-learning problem treats entire tasks as training examples.In this section, we formalize this meta-learning problem setting in a general manner, including brief examples of different learning domains.We will discuss two different learning domains in detail in Section\u00a03.", "We consider a model, denoted f\ud835\udc53fitalic_f, that maps observations \ud835\udc31\ud835\udc31\\mathbf{x}bold_x to outputs \ud835\udc1a\ud835\udc1a\\mathbf{a}bold_a.During meta-learning, the modelis trained to be able to adapt to a large or infinite number of tasks.Since we would like to apply our framework to a variety of learning problems, from classification to reinforcement learning, we introduce a generic notion of a learning task below.Formally, each task \ud835\udcaf={\u2112(\ud835\udc311,\ud835\udc1a1,\u2026,\ud835\udc31H,\ud835\udc1aH),q(\ud835\udc311),q(\ud835\udc31t+1|\ud835\udc31t,\ud835\udc1at),H}\ud835\udcaf\u2112subscript\ud835\udc311subscript\ud835\udc1a1\u2026subscript\ud835\udc31\ud835\udc3bsubscript\ud835\udc1a\ud835\udc3b\ud835\udc5esubscript\ud835\udc311\ud835\udc5econditionalsubscript\ud835\udc31\ud835\udc611subscript\ud835\udc31\ud835\udc61subscript\ud835\udc1a\ud835\udc61\ud835\udc3b\\mathcal{T}=\\{\\mathcal{L}(\\mathbf{x}_{1},\\mathbf{a}_{1},\\dots,\\mathbf{x}_{H},\\mathbf{a}_{H}),q(\\mathbf{x}_{1}),q(\\mathbf{x}_{t+1}|\\mathbf{x}_{t},\\mathbf{a}_{t}),H\\}caligraphic_T = { caligraphic_L ( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , bold_x start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , bold_a start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) , italic_q ( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_q ( bold_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , italic_H }consists of a loss function \u2112\u2112\\mathcal{L}caligraphic_L, a distribution over initial observations q(\ud835\udc311)\ud835\udc5esubscript\ud835\udc311q(\\mathbf{x}_{1})italic_q ( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ), a transition distribution q(\ud835\udc31t+1|\ud835\udc31t,\ud835\udc1at)\ud835\udc5econditionalsubscript\ud835\udc31\ud835\udc611subscript\ud835\udc31\ud835\udc61subscript\ud835\udc1a\ud835\udc61q(\\mathbf{x}_{t+1}|\\mathbf{x}_{t},\\mathbf{a}_{t})italic_q ( bold_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), and an episode length H\ud835\udc3bHitalic_H. In i.i.d. supervised learning problems, the length H=1\ud835\udc3b1H\\!=\\!1italic_H = 1.The model may generate samples of length H\ud835\udc3bHitalic_H by choosing an output \ud835\udc1atsubscript\ud835\udc1a\ud835\udc61\\mathbf{a}_{t}bold_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at each time t\ud835\udc61titalic_t.The loss \u2112(\ud835\udc311,\ud835\udc1a1,\u2026,\ud835\udc31H,\ud835\udc1aH)\u2192\u211d\u2192\u2112subscript\ud835\udc311subscript\ud835\udc1a1\u2026subscript\ud835\udc31\ud835\udc3bsubscript\ud835\udc1a\ud835\udc3b\u211d\\mathcal{L}(\\mathbf{x}_{1},\\mathbf{a}_{1},\\dots,\\mathbf{x}_{H},\\mathbf{a}_{H})\\rightarrow\\mathbb{R}caligraphic_L ( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , bold_x start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , bold_a start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) \u2192 blackboard_R, provides task-specific feedback, which might be in the form of a misclassification loss or a cost function in a Markov decision process.", "In our meta-learning scenario, we consider a distribution over tasks p(\ud835\udcaf)\ud835\udc5d\ud835\udcafp(\\mathcal{T})italic_p ( caligraphic_T ) that we want our model to be able to adapt to.In the K\ud835\udc3eKitalic_K-shot learning setting, the model is trained to learn a new task \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT drawn from p(\ud835\udcaf)\ud835\udc5d\ud835\udcafp(\\mathcal{T})italic_p ( caligraphic_T ) from only K\ud835\udc3eKitalic_K samples drawn from qisubscript\ud835\udc5e\ud835\udc56q_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and feedback \u2112\ud835\udcafisubscript\u2112subscript\ud835\udcaf\ud835\udc56\\mathcal{L}_{\\mathcal{T}_{i}}caligraphic_L start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT generated by \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.During meta-training, a task \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is sampled from p(\ud835\udcaf)\ud835\udc5d\ud835\udcafp(\\mathcal{T})italic_p ( caligraphic_T ), the model is trained with K\ud835\udc3eKitalic_K samples and feedback from the corresponding loss \u2112\ud835\udcafisubscript\u2112subscript\ud835\udcaf\ud835\udc56\\mathcal{L}_{\\mathcal{T}_{i}}caligraphic_L start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT from \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and then tested on new samples from \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.The model f\ud835\udc53fitalic_f is then improved by considering how the test error on new data from qisubscript\ud835\udc5e\ud835\udc56q_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT changes with respect to the parameters. In effect, the test error on sampled tasks \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT serves as the training error of the meta-learning process.At the end of meta-training, new tasks are sampled from p(\ud835\udcaf)\ud835\udc5d\ud835\udcafp(\\mathcal{T})italic_p ( caligraphic_T ),and meta-performance is measured by the model\u2019s performance after learning from K\ud835\udc3eKitalic_K samples.Generally, tasks used for meta-testing are held out during meta-training.", "In contrast to prior work, which has sought to train recurrent neural networks that ingest entire datasets\u00a0(Santoro et\u00a0al., 2016; Duan et\u00a0al., 2016b) or feature embeddings that can be combined with nonparametric methods at test time\u00a0(Vinyals et\u00a0al., 2016; Koch, 2015), we propose a method that can learn the parameters of any standard model via meta-learning in such a way as to prepare that model for fast adaptation. The intuition behind this approach is that some internal representations are more transferrable than others. For example, a neural network might learn internal features that are broadly applicable to all tasks in p(\ud835\udcaf)\ud835\udc5d\ud835\udcafp(\\mathcal{T})italic_p ( caligraphic_T ), rather than a single individual task. How can we encourage the emergence of such general-purpose representations? We take an explicit approach to this problem: since the model will be fine-tuned using a gradient-based learning rule on a new task, we will aim to learn a model in such a way that this gradient-based learning rule can make rapid progress on new tasks drawn from p(\ud835\udcaf)\ud835\udc5d\ud835\udcafp(\\mathcal{T})italic_p ( caligraphic_T ), without overfitting. In effect, we will aim to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will produce large improvements on the loss function of any task drawn from p(\ud835\udcaf)\ud835\udc5d\ud835\udcafp(\\mathcal{T})italic_p ( caligraphic_T ), when altered in the direction of the gradient of that loss (see Figure\u00a01). We make no assumption on the form of the model, other than to assume that it is parametrized by some parameter vector \u03b8\ud835\udf03\\thetaitalic_\u03b8, and that the loss function is smooth enough in \u03b8\ud835\udf03\\thetaitalic_\u03b8 that we can use gradient-based learning techniques.", "Formally, we consider a modelrepresented by a parametrized function f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPTwith parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8.When adapting to a new task \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the model\u2019s parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 become \u03b8i\u2032superscriptsubscript\ud835\udf03\ud835\udc56\u2032\\theta_{i}^{\\prime}italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.In our method, the updated parameter vector \u03b8i\u2032superscriptsubscript\ud835\udf03\ud835\udc56\u2032\\theta_{i}^{\\prime}italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is computed using one or more gradient descent updates on task \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.For example, when using one gradient update,\u03b8i\u2032=\u03b8\u2212\u03b1\u2207\u03b8\u2112\ud835\udcafi(f\u03b8).superscriptsubscript\ud835\udf03\ud835\udc56\u2032\ud835\udf03\ud835\udefcsubscript\u2207\ud835\udf03subscript\u2112subscript\ud835\udcaf\ud835\udc56subscript\ud835\udc53\ud835\udf03\\vspace{-0.15cm}\\theta_{i}^{\\prime}=\\theta-\\alpha\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}_{i}}(f_{\\theta}).italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = italic_\u03b8 - italic_\u03b1 \u2207 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ) .The step size \u03b1\ud835\udefc\\alphaitalic_\u03b1 may be fixed as a hyperparameter or meta-learned.For simplicity of notation, we will consider one gradient update for the rest of this section, but using multiple gradient updates is a straightforward extension.", "The model parameters are trained by optimizing for the performance of f\u03b8i\u2032subscript\ud835\udc53superscriptsubscript\ud835\udf03\ud835\udc56\u2032f_{\\theta_{i}^{\\prime}}italic_f start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT with respect to \u03b8\ud835\udf03\\thetaitalic_\u03b8 across tasks sampled from p(\ud835\udcaf)\ud835\udc5d\ud835\udcafp(\\mathcal{T})italic_p ( caligraphic_T ).More concretely, the meta-objective is as follows:min\u03b8\u2211\ud835\udcafi\u223cp(\ud835\udcaf)\u2112\ud835\udcafi(f\u03b8i\u2032)=\u2211\ud835\udcafi\u223cp(\ud835\udcaf)\u2112\ud835\udcafi(f\u03b8\u2212\u03b1\u2207\u03b8\u2112\ud835\udcafi(f\u03b8))subscript\ud835\udf03subscriptsimilar-tosubscript\ud835\udcaf\ud835\udc56\ud835\udc5d\ud835\udcafsubscript\u2112subscript\ud835\udcaf\ud835\udc56subscript\ud835\udc53superscriptsubscript\ud835\udf03\ud835\udc56\u2032subscriptsimilar-tosubscript\ud835\udcaf\ud835\udc56\ud835\udc5d\ud835\udcafsubscript\u2112subscript\ud835\udcaf\ud835\udc56subscript\ud835\udc53\ud835\udf03\ud835\udefcsubscript\u2207\ud835\udf03subscript\u2112subscript\ud835\udcaf\ud835\udc56subscript\ud835\udc53\ud835\udf03\\displaystyle\\vspace{-0.2cm}\\min_{\\theta}\\sum_{\\mathcal{T}_{i}\\sim p(\\mathcal{T})}\\mathcal{L}_{\\mathcal{T}_{i}}(f_{\\theta_{i}^{\\prime}})=\\sum_{\\mathcal{T}_{i}\\sim p(\\mathcal{T})}\\mathcal{L}_{\\mathcal{T}_{i}}(f_{\\theta-\\alpha\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}_{i}}(f_{\\theta})})roman_min start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u223c italic_p ( caligraphic_T ) end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) = \u2211 start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u223c italic_p ( caligraphic_T ) end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_\u03b8 - italic_\u03b1 \u2207 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT )Note that the meta-optimization is performed over the model parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8, whereas the objective is computed using the updated model parameters \u03b8\u2032superscript\ud835\udf03\u2032\\theta^{\\prime}italic_\u03b8 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.In effect, our proposed method aims to optimize the model parameters such that one or a small number of gradient steps on a new task will produce maximally effective behavior on that task.", "The meta-optimization across tasks is performed via stochastic gradient descent (SGD), such that the model parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 are updated as follows:\u03b8\u2190\u03b8\u2212\u03b2\u2207\u03b8\u2211\ud835\udcafi\u223cp(\ud835\udcaf)\u2112\ud835\udcafi(f\u03b8i\u2032)\u2190\ud835\udf03\ud835\udf03\ud835\udefdsubscript\u2207\ud835\udf03subscriptsimilar-tosubscript\ud835\udcaf\ud835\udc56\ud835\udc5d\ud835\udcafsubscript\u2112subscript\ud835\udcaf\ud835\udc56subscript\ud835\udc53superscriptsubscript\ud835\udf03\ud835\udc56\u2032\\vspace{-0.2cm}\\theta\\leftarrow\\theta-\\beta\\nabla_{\\theta}\\sum_{\\mathcal{T}_{i}\\sim p(\\mathcal{T})}\\mathcal{L}_{\\mathcal{T}_{i}}(f_{\\theta_{i}^{\\prime}})italic_\u03b8 \u2190 italic_\u03b8 - italic_\u03b2 \u2207 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u223c italic_p ( caligraphic_T ) end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT )(1)where \u03b2\ud835\udefd\\betaitalic_\u03b2 is the meta step size. The full algorithm, in the general case, is outlined in Algorithm\u00a01.", "The MAML meta-gradient update involves a gradient through a gradient. Computationally, this requires an additional backward pass through f\ud835\udc53fitalic_f to compute Hessian-vector products, which is supported by standard deep learning libraries such as TensorFlow\u00a0(Abadi et\u00a0al., 2016). In our experiments, we also include a comparison to dropping this backward pass and using a first-order approximation, which we discuss in Section\u00a05.2.", "In this section, we discuss specific instantiations of our meta-learning algorithm for supervised learning and reinforcement learning. The domains differ in the form of loss function and in how data is generated by the task and presented to the model, but the same basic adaptation mechanism can be applied in both cases.", "Few-shot learning is well-studied in the domain of supervised tasks, where the goal is to learn a new function from only a few input/output pairs for that task, using prior data from similar tasks for meta-learning. For example, the goal might be to classify images of a Segway after seeing only one or a few examples of a Segway, with a model that has previously seen many other types of objects. Likewise, in few-shot regression, the goal is to predict the outputs of a continuous-valued function from only a few datapoints sampled from that function, after training on many functions with similar statistical properties.", "To formalize the supervised regression and classification problems in the context of the meta-learning definitions in Section\u00a02.1, we can define the horizonH=1\ud835\udc3b1H=1italic_H = 1 and drop the timestep subscript on \ud835\udc31tsubscript\ud835\udc31\ud835\udc61\\mathbf{x}_{t}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, since the model accepts a single input and produces a single output, rather than a sequence of inputs and outputs. The task \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT generates K\ud835\udc3eKitalic_K i.i.d. observations \ud835\udc31\ud835\udc31\\mathbf{x}bold_x from qisubscript\ud835\udc5e\ud835\udc56q_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and the task loss is represented by the error between the model\u2019s output for \ud835\udc31\ud835\udc31\\mathbf{x}bold_x and the corresponding target values \ud835\udc32\ud835\udc32\\mathbf{y}bold_y for that observation and task.", "Two common loss functions used for supervised classification and regression are cross-entropy and mean-squared error (MSE), which we will describe below; though, other supervised loss functions may be used as well. For regression tasks using mean-squared error, the loss takes the form:\u2112\ud835\udcafi(f\u03d5)=\u2211\ud835\udc31(j),\ud835\udc32(j)\u223c\ud835\udcafi\u2225f\u03d5(\ud835\udc31(j))\u2212\ud835\udc32(j)\u222522,subscript\u2112subscript\ud835\udcaf\ud835\udc56subscript\ud835\udc53italic-\u03d5subscriptsimilar-tosuperscript\ud835\udc31\ud835\udc57superscript\ud835\udc32\ud835\udc57subscript\ud835\udcaf\ud835\udc56superscriptsubscriptdelimited-\u2225\u2225subscript\ud835\udc53italic-\u03d5superscript\ud835\udc31\ud835\udc57superscript\ud835\udc32\ud835\udc5722\\displaystyle\\vspace{-0.2cm}\\mathcal{L}_{\\mathcal{T}_{i}}(f_{\\phi})=\\!\\!\\!\\!\\!\\!\\sum_{\\mathbf{x}^{(j)},\\mathbf{y}^{(j)}\\sim\\mathcal{T}_{i}}\\lVert f_{\\phi}(\\mathbf{x}^{(j)})-\\mathbf{y}^{(j)}\\rVert_{2}^{2},caligraphic_L start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ) = \u2211 start_POSTSUBSCRIPT bold_x start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT , bold_y start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT \u223c caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2225 italic_f start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT ) - bold_y start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,(2)where \ud835\udc31(j),\ud835\udc32(j)superscript\ud835\udc31\ud835\udc57superscript\ud835\udc32\ud835\udc57\\mathbf{x}^{(j)},\\mathbf{y}^{(j)}bold_x start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT , bold_y start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT are an input/output pair sampled from task \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. In K\ud835\udc3eKitalic_K-shot regression tasks, K\ud835\udc3eKitalic_K input/output pairs are provided for learning for each task.", "Similarly, for discrete classification tasks with a cross-entropy loss, the loss takes the form:\u2112\ud835\udcafi(f\u03d5)=\u2211\ud835\udc31(j),\ud835\udc32(j)\u223c\ud835\udcafisubscript\u2112subscript\ud835\udcaf\ud835\udc56subscript\ud835\udc53italic-\u03d5subscriptsimilar-tosuperscript\ud835\udc31\ud835\udc57superscript\ud835\udc32\ud835\udc57subscript\ud835\udcaf\ud835\udc56\\displaystyle\\mathcal{L}_{\\mathcal{T}_{i}}(f_{\\phi})=\\!\\!\\!\\!\\!\\!\\sum_{\\mathbf{x}^{(j)},\\mathbf{y}^{(j)}\\sim\\mathcal{T}_{i}}\\!\\!\\!\\!\\!\\!caligraphic_L start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ) = \u2211 start_POSTSUBSCRIPT bold_x start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT , bold_y start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT \u223c caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT\ud835\udc32(j)log\u2061f\u03d5(\ud835\udc31(j))superscript\ud835\udc32\ud835\udc57subscript\ud835\udc53italic-\u03d5superscript\ud835\udc31\ud835\udc57\\displaystyle\\mathbf{y}^{(j)}\\log f_{\\phi}(\\mathbf{x}^{(j)})bold_y start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT roman_log italic_f start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT )(3)+(1\u2212\ud835\udc32(j))log\u2061(1\u2212f\u03d5(\ud835\udc31(j)))1superscript\ud835\udc32\ud835\udc571subscript\ud835\udc53italic-\u03d5superscript\ud835\udc31\ud835\udc57\\displaystyle+(1-\\mathbf{y}^{(j)})\\log(1-f_{\\phi}(\\mathbf{x}^{(j)}))+ ( 1 - bold_y start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT ) roman_log ( 1 - italic_f start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT ) )According to the conventional terminology, K\ud835\udc3eKitalic_K-shot classification tasks use K\ud835\udc3eKitalic_K input/output pairs from each class, for a total of NK\ud835\udc41\ud835\udc3eNKitalic_N italic_K data points for N\ud835\udc41Nitalic_N-way classification. Given a distribution over tasks p(\ud835\udcafi)\ud835\udc5dsubscript\ud835\udcaf\ud835\udc56p(\\mathcal{T}_{i})italic_p ( caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), these loss functions can be directly inserted into the equations in Section\u00a02.2 to perform meta-learning, as detailed in Algorithm\u00a02.", "In reinforcement learning (RL), the goal of few-shot meta-learning is to enable an agent to quickly acquire a policy for a new test task using only a small amount of experience in the test setting. A new task might involve achieving a new goal or succeeding on a previously trained goal in a new environment. For example, an agent might learn to quickly figure out how to navigate mazes so that, when faced with a new maze, it can determine how to reliably reach the exit with only a few samples.In this section, we will discuss how MAML can be applied to meta-learning for RL.", "Each RL task \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT contains an initial state distribution qi(\ud835\udc311)subscript\ud835\udc5e\ud835\udc56subscript\ud835\udc311q_{i}(\\mathbf{x}_{1})italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) and a transition distribution qi(\ud835\udc31t+1|\ud835\udc31t,\ud835\udc1at)subscript\ud835\udc5e\ud835\udc56conditionalsubscript\ud835\udc31\ud835\udc611subscript\ud835\udc31\ud835\udc61subscript\ud835\udc1a\ud835\udc61q_{i}(\\mathbf{x}_{t+1}|\\mathbf{x}_{t},\\mathbf{a}_{t})italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), and the loss \u2112\ud835\udcafisubscript\u2112subscript\ud835\udcaf\ud835\udc56\\mathcal{L}_{\\mathcal{T}_{i}}caligraphic_L start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT corresponds to the (negative) reward function R\ud835\udc45Ritalic_R. The entire task is therefore a Markov decision process (MDP) with horizon H\ud835\udc3bHitalic_H, where the learner is allowed to query a limited number of sample trajectories for few-shot learning. Any aspect of the MDP may change across tasks in p(\ud835\udcaf)\ud835\udc5d\ud835\udcafp(\\mathcal{T})italic_p ( caligraphic_T ). The model being learned, f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT, is a policy that maps from states \ud835\udc31tsubscript\ud835\udc31\ud835\udc61\\mathbf{x}_{t}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to a distribution over actions \ud835\udc1atsubscript\ud835\udc1a\ud835\udc61\\mathbf{a}_{t}bold_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at each timestep t\u2208{1,\u2026,H}\ud835\udc611\u2026\ud835\udc3bt\\in\\{1,...,H\\}italic_t \u2208 { 1 , \u2026 , italic_H }. The loss for task \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and model f\u03d5subscript\ud835\udc53italic-\u03d5f_{\\phi}italic_f start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT takes the form\u2112\ud835\udcafi(f\u03d5)=\u2212\ud835\udd3c\ud835\udc31t,\ud835\udc1at\u223cf\u03d5,q\ud835\udcafi[\u2211t=1HRi(\ud835\udc31t,\ud835\udc1at)].subscript\u2112subscript\ud835\udcaf\ud835\udc56subscript\ud835\udc53italic-\u03d5subscript\ud835\udd3cformulae-sequencesimilar-tosubscript\ud835\udc31\ud835\udc61subscript\ud835\udc1a\ud835\udc61subscript\ud835\udc53italic-\u03d5subscript\ud835\udc5esubscript\ud835\udcaf\ud835\udc56delimited-[]superscriptsubscript\ud835\udc611\ud835\udc3bsubscript\ud835\udc45\ud835\udc56subscript\ud835\udc31\ud835\udc61subscript\ud835\udc1a\ud835\udc61\\displaystyle\\mathcal{L}_{\\mathcal{T}_{i}}(f_{\\phi})=-\\mathbb{E}_{\\mathbf{x}_{t},\\mathbf{a}_{t}\\sim f_{\\phi},q_{\\mathcal{T}_{i}}}\\left[\\sum_{t=1}^{H}R_{i}(\\mathbf{x}_{t},\\mathbf{a}_{t})\\right].caligraphic_L start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ) = - blackboard_E start_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u223c italic_f start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ \u2211 start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ] .(4)In K\ud835\udc3eKitalic_K-shot reinforcement learning, K\ud835\udc3eKitalic_K rollouts from f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT andtask \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, (\ud835\udc311,\ud835\udc1a1,\u2026\ud835\udc31H)subscript\ud835\udc311subscript\ud835\udc1a1\u2026subscript\ud835\udc31\ud835\udc3b(\\mathbf{x}_{1},\\mathbf{a}_{1},...\\mathbf{x}_{H})( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 bold_x start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ), and the corresponding rewards R(\ud835\udc31t,\ud835\udc1at)\ud835\udc45subscript\ud835\udc31\ud835\udc61subscript\ud835\udc1a\ud835\udc61R(\\mathbf{x}_{t},\\mathbf{a}_{t})italic_R ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), may be used for adaptation on a new task \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Since the expected reward is generally not differentiable due to unknown dynamics, we use policy gradient methods to estimate the gradient both for the model gradient update(s) and the meta-optimization. Since policy gradients are an on-policy algorithm, each additional gradient step during the adaptation of f\u03b8subscript\ud835\udc53\ud835\udf03f_{\\theta}italic_f start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT requires new samples from the current policy f\u03b8i\u2032subscript\ud835\udc53subscript\ud835\udf03superscript\ud835\udc56\u2032f_{\\theta_{i^{\\prime}}}italic_f start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT. We detail the algorithm in Algorithm\u00a03. This algorithm has the same structure as Algorithm\u00a02, with the principal difference being that steps 5 and 8 require sampling trajectories from the environment corresponding to task \ud835\udcafisubscript\ud835\udcaf\ud835\udc56\\mathcal{T}_{i}caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Practical implementations of this method may also use a variety of improvements recently proposed for policy gradient algorithms, including state or action-dependent baselines and trust regions\u00a0(Schulman et\u00a0al., 2015).", "The method that we propose in this paper addresses the general problem of meta-learning\u00a0(Thrun & Pratt, 1998; Schmidhuber, 1987; Naik & Mammone, 1992), which includes few-shot learning.A popular approach for meta-learning is to train a meta-learner that learns how to update the parameters of the learner\u2019s model\u00a0(Bengio et\u00a0al., 1992; Schmidhuber, 1992; Bengio et\u00a0al., 1990). This approach has been applied to learning to optimize deep networks\u00a0(Hochreiter et\u00a0al., 2001; Andrychowicz et\u00a0al., 2016; Li & Malik, 2017), as well as for learning dynamically changing recurrent networks\u00a0(Ha et\u00a0al., 2017).One recent approach learns both the weight initialization and the optimizer, for few-shot image recognition\u00a0(Ravi & Larochelle, 2017). Unlike these methods, the MAML learner\u2019s weights are updated using the gradient, rather than a learned update; our method does not introduce additional parameters for meta-learning nor require a particular learner architecture.", "Few-shot learning methods have also been developed for specific tasks such as generative modeling\u00a0(Edwards & Storkey, 2017; Rezende et\u00a0al., 2016) and image recognition\u00a0(Vinyals et\u00a0al., 2016). One successful approach for few-shot classification is to learn to compare new examples in a learned metric space using e.g. Siamese networks\u00a0(Koch, 2015) or recurrence with attention mechanisms\u00a0(Vinyals et\u00a0al., 2016; Shyam et\u00a0al., 2017; Snell et\u00a0al., 2017).These approaches have generated some of the most successful results, but are difficult to directly extend to other problems, such as reinforcement learning. Our method, in contrast, is agnostic to the form of the model and to the particular learning task.", "Another approach to meta-learning is to train memory-augmented models on many tasks, wherethe recurrent learner is trained to adapt to new tasks as it is rolled out. Such networks have been applied to few-shot image recognition\u00a0(Santoro et\u00a0al., 2016; Munkhdalai & Yu, 2017) and learning \u201cfast\u201d reinforcement learning agents\u00a0(Duan et\u00a0al., 2016b; Wang et\u00a0al., 2016).Our experiments show that our method outperforms the recurrent approach on few-shot classification. Furthermore, unlike these methods, our approach simply provides a good weight initialization and uses the same gradient descent update for both the learner and meta-update.As a result, it is straightforward to finetune the learner for additional gradient steps.", "Our approach is also related to methods for initialization of deep networks. In computer vision, models pretrained on large-scale image classification have been shown to learn effective features for a range of problems\u00a0(Donahue et\u00a0al., 2014). In contrast, our method explicitly optimizes the model for fast adaptability, allowing it to adapt to new tasks with only a few examples.Our method can also be viewed as explicitly maximizing sensitivity of new task losses to the model parameters.A number of prior works have explored sensitivity in deep networks, often in the context of initialization\u00a0(Saxe et\u00a0al., 2014; Kirkpatrick et\u00a0al., 2016). Most of these works have considered good random initializations, though a number of papers have addressed data-dependent initializers\u00a0(Kr\u00e4henb\u00fchl et\u00a0al., 2016; Salimans & Kingma, 2016), including learned initializations\u00a0(Husken & Goerick, 2000; Maclaurin et\u00a0al., 2015). In contrast, our method explicitly trains the parameters for sensitivity on a given task distribution, allowing for extremely efficient adaptation for problems such as K\ud835\udc3eKitalic_K-shot learning and rapid reinforcement learning in only one or a few gradient steps.", "The goal of our experimental evaluation is to answer the following questions: (1) Can MAML enable fast learning of new tasks? (2) Can MAML be used for meta-learning in multiple different domains, including supervised regression, classification, and reinforcement learning? (3) Can a model learned with MAML continue to improve with additional gradient updates and/or examples?", "All of the meta-learning problems that we consider require some amount of adaptation to new tasks at test-time. When possible, we compare our results to an oracle that receives the identity of the task (which is a problem-dependent representation) as an additional input, as an upper bound on the performance of the model. All of the experiments were performed using TensorFlow\u00a0(Abadi et\u00a0al., 2016), which allows for automatic differentiation through the gradient update(s) during meta-learning. The code is available online111Code for the regression and supervised experiments is at github.com/cbfinn/maml and code for the RL experiments is at github.com/cbfinn/maml_rl.", "We start with a simple regression problem that illustrates the basic principles of MAML. Each task involves regressing from the input to the output of a sine wave, where the amplitude and phase of the sinusoid are varied between tasks. Thus, p(\ud835\udcaf)\ud835\udc5d\ud835\udcafp(\\mathcal{T})italic_p ( caligraphic_T ) is continuous, where the amplitude varies within [0.1,5.0]0.15.0[0.1,5.0][ 0.1 , 5.0 ] and the phase varies within [0,\u03c0]0\ud835\udf0b[0,\\pi][ 0 , italic_\u03c0 ], and the input and output both have a dimensionality of 1111. During training and testing, datapoints \ud835\udc31\ud835\udc31\\mathbf{x}bold_x are sampled uniformly from [\u22125.0,5.0]5.05.0[-5.0,5.0][ - 5.0 , 5.0 ]. The loss is the mean-squared error between the prediction f(\ud835\udc31)\ud835\udc53\ud835\udc31f(\\mathbf{x})italic_f ( bold_x ) and true value. The regressor is a neural network model with 2222 hidden layers of size 40404040 with ReLU nonlinearities. When training with MAML, we use one gradient update with K=10\ud835\udc3e10K=10italic_K = 10 examples with a fixed step size \u03b1=0.01\ud835\udefc0.01\\alpha=0.01italic_\u03b1 = 0.01, and use Adam as the meta-optimizer\u00a0(Kingma & Ba, 2015). The baselines are likewise trained with Adam. To evaluate performance, we fine-tune a single meta-learned model on varying numbers of K\ud835\udc3eKitalic_K examples, and compare performance to two baselines: (a) pretraining on all of the tasks, which entails training a network to regress to random sinusoid functions and then, at test-time, fine-tuning with gradient descent on the K\ud835\udc3eKitalic_K provided points, using an automatically tuned step size, and (b) an oracle which receives the true amplitude and phase as input. In Appendix\u00a0C, we show comparisons to additional multi-task and adaptation methods.", "We evaluate performance by fine-tuning the model learned by MAML and the pretrained model on K={5,10,20}\ud835\udc3e51020K=\\{5,10,20\\}italic_K = { 5 , 10 , 20 } datapoints. During fine-tuning, each gradient step is computed using the same K\ud835\udc3eKitalic_K datapoints. The qualitative results, shown in Figure\u00a02 and further expanded on in Appendix\u00a0B show that the learned model is able to quickly adapt with only 5555 datapoints, shown as purple triangles, whereas the model that is pretrained using standard supervised learning on all tasks is unable to adequately adapt with so few datapoints without catastrophic overfitting. Crucially, when the K\ud835\udc3eKitalic_K datapoints are all in one half of the input range, the model trained with MAML can still infer the amplitude and phase in the other half of the range, demonstrating that the MAML trained model f\ud835\udc53fitalic_f has learned to model the periodic nature of the sine wave. Furthermore, we observe both in the qualitative and quantitative results (Figure\u00a03 and Appendix\u00a0B) that the model learned with MAML continues to improve with additional gradient steps, despite being trained for maximal performance after one gradient step. This improvement suggests that MAML optimizes the parameters such that they lie in a region that is amenable to fast adaptation and is sensitive to loss functions from p(\ud835\udcaf)\ud835\udc5d\ud835\udcafp(\\mathcal{T})italic_p ( caligraphic_T ), as discussed in Section\u00a02.2, rather than overfitting to parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 that only improve after one step.", "To evaluate MAML in comparison to prior meta-learning and few-shot learning algorithms, we applied our method to few-shot image recognition on the Omniglot\u00a0(Lake et\u00a0al., 2011) and MiniImagenet datasets. The Omniglot dataset consists of 20 instances of 1623 characters from 50 different alphabets. Each instance was drawn by a different person. The MiniImagenet dataset was proposed by\u00a0Ravi & Larochelle (2017), and involves 64 training classes, 12 validation classes, and 24 test classes. The Omniglot and MiniImagenet image recognition tasks are the most common recently used few-shot learning benchmarks\u00a0(Vinyals et\u00a0al., 2016; Santoro et\u00a0al., 2016; Ravi & Larochelle, 2017). We follow the experimental protocol proposed by\u00a0Vinyals et\u00a0al. (2016), which involves fast learning of N\ud835\udc41Nitalic_N-way classification with 1 or 5 shots. The problem of N\ud835\udc41Nitalic_N-way classification is set up as follows: select N\ud835\udc41Nitalic_N unseen classes, provide the model with K\ud835\udc3eKitalic_K different instances of each of the N\ud835\udc41Nitalic_N classes, and evaluate the model\u2019s ability to classify new instances within the N\ud835\udc41Nitalic_N classes. For Omniglot, we randomly select 1200120012001200 characters for training, irrespective of alphabet, and use the remaining for testing. The Omniglot dataset is augmented with rotations by multiples of 90909090 degrees, as proposed by\u00a0Santoro et\u00a0al. (2016).", "Our model follows the same architecture as the embedding function used by\u00a0Vinyals et\u00a0al. (2016), which has 4 modules with a 3\u00d73333\\times 33 \u00d7 3 convolutions and 64646464 filters, followed by batch normalization\u00a0(Ioffe & Szegedy, 2015), a ReLU nonlinearity, and 2\u00d72222\\times 22 \u00d7 2 max-pooling. The Omniglot images are downsampled to 28\u00d728282828\\times 2828 \u00d7 28, so the dimensionality of the last hidden layer is 64646464. As in the baseline classifier used by\u00a0Vinyals et\u00a0al. (2016), the last layer is fed into a softmax. For Omniglot, we used strided convolutions instead of max-pooling. For MiniImagenet, we used 32323232 filters per layer to reduce overfitting, as done by\u00a0(Ravi & Larochelle, 2017).In order to also provide a fair comparison against memory-augmented neural networks\u00a0(Santoro et\u00a0al., 2016) and to test the flexibility of MAML, we also provide results for a non-convolutional network. For this, we use a network with 4444 hidden layers with sizes 256256256256, 128128128128, 64646464, 64646464, each including batch normalization and ReLU nonlinearities, followed by a linear layer and softmax. For all models, the loss function is the cross-entropy error between the predicted and true class. Additional hyperparameter details are included in Appendix\u00a0A.1.", "We present the results in Table\u00a01. The convolutional model learned by MAML compares well to the state-of-the-art results on this task, narrowly outperforming the prior methods. Some of these existing methods, such as matching networks, Siamese networks, and memory models are designed with few-shot classification in mind, and are not readily applicable to domains such as reinforcement learning. Additionally, the model learned with MAML uses fewer overall parameters compared to matching networks and the meta-learner LSTM, since the algorithm does not introduce any additional parameters beyond the weights of the classifier itself. Compared to these prior methods, memory-augmented neural networks\u00a0(Santoro et\u00a0al., 2016) specifically, and recurrent meta-learning models in general, represent a more broadly applicable class of methods that, like MAML, can be used for other tasks such as reinforcement learning\u00a0(Duan et\u00a0al., 2016b; Wang et\u00a0al., 2016). However, as shown in the comparison, MAML significantly outperforms memory-augmented networks and the meta-learner LSTM on 5-way Omniglot and MiniImagenet classification, both in the 1111-shot and 5555-shot case.", "A significant computational expense in MAML comes from the use of second derivatives when backpropagating the meta-gradient through the gradient operator in the meta-objective (see Equation\u00a0(1)). On MiniImagenet, we show a comparison to a first-order approximation of MAML, where these second derivatives are omitted. Note that the resulting method still computes the meta-gradient at the post-update parameter values \u03b8i\u2032superscriptsubscript\ud835\udf03\ud835\udc56\u2032\\theta_{i}^{\\prime}italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT, which provides for effective meta-learning. Surprisingly however, the performance of this method is nearly the same as that obtained with full second derivatives, suggesting that most of the improvement in MAML comes from the gradients of the objective at the post-update parameter values, rather than the second order updates from differentiating through the gradient update. Past work has observed that ReLU neural networks are locally almost linear\u00a0(Goodfellow et\u00a0al., 2015), which suggests that second derivatives may be close to zero in most cases, partially explaining the good performance of the first-order approximation. This approximation removes the need for computing Hessian-vector products in an additional backward pass, which we found led to roughly 33%percent3333\\%33 % speed-up in network computation.", "To evaluate MAML on reinforcement learning problems, we constructed several sets of tasks based off of the simulated continuous control environments in the rllab benchmark suite\u00a0(Duan et\u00a0al., 2016a). We discuss the individual domains below. In all of the domains, the model trained by MAML is a neural network policy with two hidden layers of size 100100100100, with ReLU nonlinearities. The gradient updates are computed using vanilla policy gradient (REINFORCE)\u00a0(Williams, 1992), and we use trust-region policy optimization (TRPO) as the meta-optimizer\u00a0(Schulman et\u00a0al., 2015). In order to avoid computing third derivatives, we use finite differences to compute the Hessian-vector products for TRPO. For both learning and meta-learning updates, we use the standard linear feature baseline proposed by\u00a0Duan et\u00a0al. (2016a), which is fitted separately at each iteration for each sampled task in the batch. We compare to three baseline models: (a) pretraining one policy on all of the tasks and then fine-tuning, (b) training a policy from randomly initialized weights, and (c) an oracle policy which receives the parameters of the task as input, which for the tasks below corresponds to a goal position, goal direction, or goal velocity for the agent. The baseline models of (a) and (b) are fine-tuned with gradient descent with a manually tuned step size. Videos of the learned policies can be viewed at sites.google.com/view/maml", "2D Navigation.In our first meta-RL experiment, we study a set of tasks where a point agent must move to different goal positions in 2D, randomly chosen for each task within a unit square. The observation is the current 2D position, and actions correspond to velocity commands clipped to be in the range [\u22120.1,0.1]0.10.1[-0.1,0.1][ - 0.1 , 0.1 ]. The reward is the negative squared distance to the goal, and episodes terminate when the agent is within 0.010.010.010.01 of the goal or at the horizon of H=100\ud835\udc3b100H=100italic_H = 100. The policy was trained with MAML to maximize performance after 1111 policy gradient update using 20202020 trajectories. Additional hyperparameter settings for this problem and the following RL problems are in Appendix\u00a0A.2.In our evaluation, we compare adaptation to a new task with up to 4 gradient updates, each with 40404040 samples. The results in Figure\u00a04 show the adaptation performance of models that are initialized with MAML, conventional pretraining on the same set of tasks, random initialization, and an oracle policy that receives the goal position as input. The results show that MAML can learn a model that adapts much more quickly in a single gradient update, and furthermore continues to improve with additional updates.", "Locomotion.To study how well MAML can scale to more complex deep RL problems, we also study adaptation on high-dimensional locomotion tasks with the MuJoCo simulator \u00a0(Todorov et\u00a0al., 2012). The tasks require two simulated robots \u2013 a planar cheetah and a 3D quadruped (the \u201cant\u201d) \u2013 to run in a particular direction or at a particular velocity. In the goal velocity experiments, the reward is the negative absolute value between the current velocity of the agent and a goal, which is chosen uniformly at random between 0.00.00.00.0 and 2.02.02.02.0 for the cheetah and between 0.00.00.00.0 and 3.03.03.03.0 for the ant. In the goal direction experiments, the reward is the magnitude of the velocity in either the forward or backward direction, chosen at random for each task in p(\ud835\udcaf)\ud835\udc5d\ud835\udcafp(\\mathcal{T})italic_p ( caligraphic_T ). The horizon is H=200\ud835\udc3b200H=200italic_H = 200, with 20202020 rollouts per gradient step for all problems except the ant forward/backward task, which used 40404040 rollouts per step. The results in Figure\u00a05 show that MAML learns a model that can quickly adapt its velocity and direction with even just a single gradient update, and continues to improve with more gradient steps. The results also show that, on these challenging tasks, the MAML initialization substantially outperforms random initialization and pretraining. In fact, pretraining is in some cases worse than random initialization, a fact observed in prior RL work\u00a0(Parisotto et\u00a0al., 2016).", "We introduced a meta-learning method based on learning easily adaptable model parameters through gradient descent. Our approach has a number of benefits. It is simple and does not introduce any learned parameters for meta-learning. It can be combined with any model representation that is amenable to gradient-based training, and any differentiable objective, including classification, regression, and reinforcement learning. Lastly, since our method merely produces a weight initialization, adaptation can be performed with any amount of data and any number of gradient steps, though we demonstrate state-of-the-art results on classification with only one or five examples per class. We also show that our method can adapt an RL agent using policy gradients and a very modest amount of experience.", "Reusing knowledge from past tasks may be a crucial ingredient in making high-capacity scalable models, such as deep neural networks, amenable to fast training with small datasets. We believe that this work is one step toward a simple and general-purpose meta-learning technique that can be applied to any problem and any model. Further research in this area can make multitask initialization a standard ingredient in deep learning and reinforcement learning."], "figure_types": {"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/12-Figure6-1.png": "plot", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/12-Table3-1.png": "table", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/13-Figure7-1.png": "plot", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/2-Figure1-1.png": "schematic", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/6-Figure2-1.png": "plot", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/6-Figure3-1.png": "plot", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/7-Table1-1.png": "table", "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518/8-Figure5-1.png": "plot"}}, "1809.11096": {"paper_id": "paper_76", "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS", "arxiv_url": "https://arxiv.org/abs/1809.11096", "s2orc_url": "https://www.semanticscholar.org/paper/22aab110058ebbd198edb1f1e7b4f69fb13c0613", "all_figures_tables": {"22aab110058ebbd198edb1f1e7b4f69fb13c0613/1-Figure1-1.png": "Figure 1: Class-conditional samples generated by our model.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/12-Figure5-1.png": "Figure 5: Samples generated by our model at 256\u00d7256 resolution. Sample sheets are available here.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/12-Figure6-1.png": "Figure 6: Additional samples generated by our model at 512\u00d7512 resolution.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/13-Figure7-1.png": "Figure 7: Comparing easy classes (a) with difficult classes (b) at 512\u00d7512. Classes such as dogs which are largely textural, and common in the dataset, are far easier to model than classes involving unaligned human faces or crowds. Such classes are more dynamic and structured, and often have details to which human observers are more sensitive. The difficulty of modeling global structure is further exacerbated when producing high-resolution images, even with non-local blocks.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/13-Figure8-1.png": "Figure 8: Interpolations between z, c pairs.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/14-Figure10-1.png": "Figure 10: Nearest neighbors in VGG-16-fc7 (Simonyan & Zisserman, 2015) feature space. The generated image is in the top left.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/14-Figure9-1.png": "Figure 9: Interpolations between c with z held constant. Pose semantics are frequently maintained between endpoints (particularly in the final row). Row 2 demonstrates that grayscale is encoded in the joint z, c space, rather than in z.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/15-Figure11-1.png": "Figure 11: Nearest neighbors in ResNet-50-avgpool (He et al., 2016) feature space. The generated image is in the top left.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/15-Figure12-1.png": "Figure 12: Nearest neighbors in pixel space. The generated image is in the top left.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/16-Figure13-1.png": "Figure 13: Nearest neighbors in VGG-16-fc7 (Simonyan & Zisserman, 2015) feature space. The generated image is in the top left.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/16-Figure14-1.png": "Figure 14: Nearest neighbors in ResNet-50-avgpool (He et al., 2016) feature space. The generated image is in the top left.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/17-Figure15-1.png": "Figure 15: (a) A typical architectural layout for G; details are in the following tables. (b) A Residual Block in G. c is concatenated with a chunk of z and projected to the BatchNorm gains and biases.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/17-Table4-1.png": "Table 4: Architectures for ImageNet at 128\u00d7128 pixels. \u201cch\u201d represents the channel width multiplier in each network from Table 1.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/18-Table5-1.png": "Table 5: Architectures for ImageNet at 256\u00d7256 pixels. \u201cch\u201d represents the channel width multiplier in each network from Table 1. Relative to the 128\u00d7128 architecture, we add an additional 8 \u00b7ch ResBlock in each network at 16\u00d716 resolution, and move the non-local block in G up one stage to 128\u00d7128 resolution. Memory constraints prevent us from moving the non-local block in D.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/18-Table6-1.png": "Table 6: Architectures for ImageNet at 512\u00d7512 pixels. \u201cch\u201d represents the channel width multiplier in each network from Table 1. Relative to the 256\u00d7256 architecture, we add an additional 1 \u00b7 ch ResBlock at the 512\u00d7512 stage. Memory constraints force us to move the non-local block in both networks back to the 64\u00d764 stage as in the 128\u00d7128 pixel setting.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/20-Figure16-1.png": "Figure 16: IS vs. FID at 128\u00d7128. Scores are averaged across three random seeds.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/20-Figure17-1.png": "Figure 17: IS vs. FID at 256 and 512 pixels. Scores are averaged across three random seeds for 256.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/21-Figure18-1.png": "Figure 18: JFT-300M IS vs. FID at 256\u00d7256. We show truncation values from \u03c3 = 0 to \u03c3 = 2 (top) and from \u03c3 = 0.5 to \u03c3 = 1.5 (bottom). Each curve corresponds to a row in Table 3. The curve labeled with baseline corresponds to the first row (with orthogonal regularization and other techniques disabled), while the rest correspond to rows 2-4 \u2013 the same architecture at different capacities (Ch).", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/23-Figure19-1.png": "Figure 19: Training statistics for a typical model without special modifications. Collapse occurs after 200000 iterations.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/24-Figure20-1.png": "Figure 20: G training statistics with \u03c30 in G regularized towards 1. Collapse occurs after 125000 iterations.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/24-Figure21-1.png": "Figure 21: D training statistics with \u03c30 in G regularized towards 1. Collapse occurs after 125000 iterations.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/25-Figure22-1.png": "Figure 22: G training statistics with an R1 Gradient Penalty of strength 10 on D. This model does not collapse, but only reaches a maximum IS of 55.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/25-Figure23-1.png": "Figure 23: D training statistics with an R1 Gradient Penalty of strength 10 on D. This model does not collapse, but only reaches a maximum IS of 55.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/26-Figure24-1.png": "Figure 24: G training statistics with Dropout (keep probability 0.8) applied to the last feature layer of D. This model does not collapse, but only reaches a maximum IS of 70.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/26-Figure25-1.png": "Figure 25: D training statistics with Dropout (keep probability 0.8) applied to the last feature layer of D. This model does not collapse, but only reaches a maximum IS of 70.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/27-Figure26-1.png": "Figure 26: Additional training statistics for a typical model without special modifications. Collapse occurs after 200000 iterations.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/27-Figure27-1.png": "Figure 27: Additional training statistics with an R1 Gradient Penalty of strength 10 on D. This model does not collapse, but only reaches a maximum IS of 55.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/3-Table1-1.png": "Table 1: Fre\u0301chet Inception Distance (FID, lower is better) and Inception Score (IS, higher is better) for ablations of our proposed modifications. Batch is batch size, Param is total number of parameters, Ch. is the channel multiplier representing the number of units in each layer, Shared is using shared embeddings, Hier. is using a hierarchical latent space, Ortho. is Orthogonal Regularization, and Itr either indicates that the setting is stable to 106 iterations, or that it collapses at the given iteration. Other than rows 1-4, results are computed across 8 different random initializations.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/4-Figure2-1.png": "Figure 2: (a) The effects of increasing truncation. From left to right, threshold=2, 1.5, 1, 0.5, 0.04. (b) Saturation artifacts from applying truncation to a poorly conditioned model.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/5-Figure3-1.png": "Figure 3: A typical plot of the first singular value \u03c30 in the layers of G (a) and D (b) before Spectral Normalization. Most layers in G have well-behaved spectra, but without constraints a small subset grow throughout training and explode at collapse. D\u2019s spectra are noisier but otherwise betterbehaved. Colors from red to violet indicate increasing depth.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/7-Figure4-1.png": "Figure 4: Samples from our model with truncation threshold 0.5 (a-c) and an example of class leakage in a partially trained model (d).", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/7-Table2-1.png": "Table 2: Evaluation of models at different resolutions. We report scores without truncation (Column 3), scores at the best FID (Column 4), scores at the IS of validation data (Column 5), and scores at the max IS (Column 6). Standard deviations are computed over at least three random initializations.", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/8-Table3-1.png": "Table 3: Results on JFT-300M at 256 \u00d7 256 resolution. The FID and IS columns report these scores given by the JFT-300M-trained Inception v2 classifier with noise distributed as z \u223c N (0, I) (non-truncated). The (min FID) / IS and FID / (max IS) columns report scores at the best FID and IS from a sweep across truncated noise distributions ranging from \u03c3 = 0 to \u03c3 = 2. Images from the JFT-300M validation set have an IS of 50.88 and FID of 1.94."}, "referred_figures_tables": [["22aab110058ebbd198edb1f1e7b4f69fb13c0613/23-Figure19-1.png"]], "question_id": [9], "question": ["What is the definition of intra-class variability?"], "question_section": ["5.2 ADDITIONAL EVALUATION ON JFT-300M"], "question_trigger_sentence": ["We suspect that this is at least partially due to the intra-class variability of JFT-300M labels, as well as the relative complexity of the image distribution, which includes images with multiple objects at a variety of scales."], "question_type": ["Testing Question"], "evidential_info": [[{"context": "In Figure\u00a019 (Appendix\u00a0D), we present truncation plots for models trained on this dataset.Unlike for ImageNet, where truncation limits of \\sigma\\approx 0 tend to produce the highest fidelity scores, IS is typically maximized for our JFT-300M models when the truncation value \\sigma ranges from 0.5 to 1.We suspect that this is at least partially due to the intra-class variability of JFT-300M labels, as well as the relative complexity of the image distribution, which includes images with multiple objects at a variety of scales.Interestingly, unlike models trained on ImageNet, where training tends to collapse without heavy regularization (Section\u00a04), the models trained on JFT-300M remain stable over many hundreds of thousands of iterations.This suggests that moving beyond ImageNet to larger datasets may partially alleviate GAN stability issues.", "rationale": "We suspect that this is at least partially due to the intra-class variability of JFT-300M labels, as well as the relative complexity of the image distribution, which includes images with multiple objects at a variety of scales.Interestingly, unlike models trained on ImageNet, where training tends to collapse without heavy regularization (Section 4), the models trained on JFT-300M remain stable over many hundreds of thousands of iterations.This suggests that moving beyond ImageNet to larger datasets may partially alleviate GAN stability issues."}]], "composition": ["intra-class variability here means images with multiple objects at a variety of scales."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["1298"], "passages": ["The state of generative image modeling has advanced dramatically in recent years, with Generative Adversarial Networks (GANs, Goodfellow et\u00a0al. (2014)) at the forefront of efforts to generate high-fidelity, diverse images with models learned directly from data. GAN training is dynamic, and sensitive to nearly every aspect of its setup (from optimization parameters to model architecture), but a torrent of research has yielded empirical and theoretical insights enabling stable training in a variety of settings. Despite this progress, the current state of the art in conditional ImageNet modeling (Zhang et\u00a0al., 2018) achieves an Inception Score (Salimans et\u00a0al., 2016) of 52.5, compared to 233 for real data.", "In this work, we set out to close the gap in fidelity and variety between images generated by GANs and real-world images from the ImageNet dataset. We make the following three contributions towards this goal:", "\u2022We demonstrate that GANs benefit dramatically from scaling, and train models with two to four times as many parameters and eight times the batch size compared to prior art. We introduce two simple, general architectural changes that improve scalability, and modify a regularization scheme to improve conditioning, demonstrably boosting performance.\u2022As a side effect of our modifications, our models become amenable to the \u201ctruncation trick,\u201d a simple sampling technique that allows explicit, fine-grained control of the trade-off between sample variety and fidelity.\u2022We discover instabilities specific to large scale GANs, and characterize them empirically. Leveraging insights from this analysis, we demonstrate that a combination of novel and existing techniques can reduce these instabilities, but complete training stability can only be achieved at a dramatic cost to performance.", "Our modifications substantially improve class-conditional GANs. When trained on ImageNet at 128\u00d7\\times\u00d7128 resolution, our models (BigGANs) improve the state-of-the-art Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) from 52.52 and 18.65 to 166.5 and 7.4 respectively.We also successfully train BigGANs on ImageNet at 256\u00d7\\times\u00d7256 and 512\u00d7\\times\u00d7512 resolution, and achieve IS and FID of 232.5 and 8.1 at 256\u00d7\\times\u00d7256 and IS and FID of 241.5 and 11.5 at 512\u00d7\\times\u00d7512. Finally, we train our models on an even largerdataset \u2013 JFT-300M \u2013and demonstrate that our design choices transfer well from ImageNet. Code and weights for our pretrained generators are publicly available111https://tfhub.dev/s?q=biggan.", "A Generative Adversarial Network (GAN) involves Generator (G) and Discriminator (D) networks whose purpose, respectively, is to map random noise to samples and discriminate real and generated samples. Formally, the GAN objective, in its original form (Goodfellow et\u00a0al., 2014) involves finding a Nash equilibrium to the following two player min-max problem:", "minG\u2061maxD\u2061\ud835\udd3cx\u223cqdata(\ud835\udc99)[log\u2061D(\ud835\udc99)]+\ud835\udd3c\ud835\udc9b\u223cp(\ud835\udc9b)[log\u2061(1\u2212D(G(\ud835\udc9b)))],subscript\ud835\udc3asubscript\ud835\udc37subscript\ud835\udd3csimilar-to\ud835\udc65subscript\ud835\udc5edata\ud835\udc99delimited-[]\ud835\udc37\ud835\udc99subscript\ud835\udd3csimilar-to\ud835\udc9b\ud835\udc5d\ud835\udc9bdelimited-[]1\ud835\udc37\ud835\udc3a\ud835\udc9b\\displaystyle\\min_{G}\\max_{D}\\mathbb{E}_{x\\sim q_{\\rm data}({\\bm{x}})}[\\log D({\\bm{x}})]+\\mathbb{E}_{{\\bm{z}}\\sim p({\\bm{z}})}[\\log(1-D(G({\\bm{z}})))],roman_min start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_x \u223c italic_q start_POSTSUBSCRIPT roman_data end_POSTSUBSCRIPT ( bold_italic_x ) end_POSTSUBSCRIPT [ roman_log italic_D ( bold_italic_x ) ] + blackboard_E start_POSTSUBSCRIPT bold_italic_z \u223c italic_p ( bold_italic_z ) end_POSTSUBSCRIPT [ roman_log ( 1 - italic_D ( italic_G ( bold_italic_z ) ) ) ] ,(1)", "where \ud835\udc9b\u2208\u211ddz\ud835\udc9bsuperscript\u211dsubscript\ud835\udc51\ud835\udc67{\\bm{z}}\\in\\mathbb{R}^{d_{z}}bold_italic_z \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_POSTSUPERSCRIPT is a latent variable drawn from distribution p(\ud835\udc9b)\ud835\udc5d\ud835\udc9bp({\\bm{z}})italic_p ( bold_italic_z ) such as \ud835\udca9(0,I)\ud835\udca90\ud835\udc3c\\mathcal{N}(0,I)caligraphic_N ( 0 , italic_I ) or \ud835\udcb0[\u22121,1]\ud835\udcb011\\mathcal{U}[-1,1]caligraphic_U [ - 1 , 1 ]. When applied to images, G and D are usually convolutional neural networks (Radford et\u00a0al., 2016). Without auxiliary stabilization techniques, this training procedure is notoriously brittle, requiring finely-tuned hyperparameters and architectural choices to work at all.", "Much recent research has accordingly focused on modifications to the vanilla GAN procedure to impart stability, drawing on a growing body of empirical and theoretical insights (Nowozin et\u00a0al., 2016; S\u00f8nderby et\u00a0al., 2017; Fedus et\u00a0al., 2018). One line of work is focused on changing the objective function (Arjovsky et\u00a0al., 2017; Mao et\u00a0al., 2016; Lim & Ye, 2017; Bellemare et\u00a0al., 2017; Salimans et\u00a0al., 2018) to encourage convergence. Another line is focused on constraining D through gradient penalties (Gulrajani et\u00a0al., 2017; Kodali et\u00a0al., 2017; Mescheder et\u00a0al., 2018) or normalization (Miyato et\u00a0al., 2018), both to counteract the use of unbounded loss functions and ensure D provides gradients everywhere to G.", "Of particular relevance to our work is Spectral Normalization (Miyato et\u00a0al., 2018), which enforces Lipschitz continuity on D by normalizing its parameters with running estimates of their first singular values, inducing backwards dynamics that adaptively regularize the top singular direction. Relatedly Odena et\u00a0al. (2018) analyze the condition number of the Jacobian of G and find that performance is dependent on G\u2019s conditioning. Zhang et\u00a0al. (2018) find that employing Spectral Normalization in G improves stability, allowing for fewer D steps per iteration. We extend on these analyses to gain further insight into the pathology of GAN training.", "Other works focus on the choice of architecture, such as SA-GAN (Zhang et\u00a0al., 2018) which adds the self-attention block from (Wang et\u00a0al., 2018) to improve the ability of both G and D to model global structure. ProGAN (Karras et\u00a0al., 2018) trains high-resolution GANs in the single-class setting by training a single model across a sequence of increasing resolutions.", "In conditional GANs (Mirza & Osindero, 2014) class information can be fed into the model in various ways.In (Odena et\u00a0al., 2017) it is provided to G by concatenating a 1-hot class vector to the noise vector, and the objective is modified to encourage conditional samples to maximize the corresponding class probability predicted by an auxiliary classifier. de\u00a0Vries et\u00a0al. (2017) and Dumoulin et\u00a0al. (2017) modify the way class conditioning is passed to G by supplying it with class-conditional gains and biases in BatchNorm (Ioffe & Szegedy, 2015) layers. In Miyato & Koyama (2018), D is conditioned by using the cosine similarity between its features and a set of learned class embeddings as additional evidence for distinguishing real and generated samples, effectively encouraging generation of samples whose features match a learned class prototype.", "Objectively evaluating implicit generative models is difficult (Theis et\u00a0al., 2015). A variety of works have proposed heuristics for measuring the sample quality of models without tractable likelihoods (Salimans et\u00a0al., 2016; Heusel et\u00a0al., 2017; Bi\u0144kowski et\u00a0al., 2018; Wu et\u00a0al., 2017). Of these, the Inception Score (IS, Salimans et\u00a0al. (2016)) and Fr\u00e9chet Inception Distance (FID, Heusel et\u00a0al. (2017)) have become popular despite theirnotable flaws (Barratt & Sharma, 2018). We employ them as approximate measures of sample quality, and to enable comparison against previous work.", "In this section, we explore methods for scaling up GAN training to reap the performance benefits of larger models and larger batches. As a baseline, we employ the SA-GAN architecture of Zhang et\u00a0al. (2018), which uses the hinge loss (Lim & Ye, 2017; Tran et\u00a0al., 2017) GAN objective. We provide class information to G with class-conditional BatchNorm (Dumoulin et\u00a0al., 2017; de\u00a0Vries et\u00a0al., 2017) and to D with projection (Miyato & Koyama, 2018). The optimization settings follow Zhang et\u00a0al. (2018) (notably employing Spectral Norm in G) with the modification that we halve the learning rates and take two D steps per G step. For evaluation, we employ moving averages of G\u2019s weights following Karras et\u00a0al. (2018); Mescheder et\u00a0al. (2018); Yaz\u0131c\u0131 et\u00a0al. (2018), with a decay of 0.99990.99990.99990.9999. We use Orthogonal Initialization (Saxe et\u00a0al., 2014), whereas previous works used \ud835\udca9(0,0.02I)\ud835\udca900.02\ud835\udc3c\\mathcal{N}(0,0.02I)caligraphic_N ( 0 , 0.02 italic_I ) (Radford et\u00a0al., 2016) or Xavier initialization (Glorot & Bengio, 2010). Each model is trained on 128 to 512 cores of a Google TPUv3Pod\u00a0(Google, 2018), and computes BatchNorm statistics in G across all devices, rather than per-device as is typical. We find progressive growing (Karras et\u00a0al., 2018) unnecessary even for our 512\u00d7\\times\u00d7512 models. Additional details are in Appendix\u00a0C.", "We begin by increasing the batch size for the baseline model, and immediately find tremendous benefits in doing so. Rows 1-4 of Table\u00a01 show that simply increasing the batch size by a factor of 8 improves the state-of-the-art IS by 46%. We conjecture that this is a result of each batch covering more modes, providing better gradients for both networks. One notable side effect of this scaling is that our models reach better final performance in fewer iterations, but become unstable and undergo complete training collapse. We discuss the causes and ramifications of this in Section\u00a04. For these experiments, we report scores from checkpoints saved just before collapse.", "We then increase the width (number of channels) in each layer by 50%, approximately doubling the number of parameters in both models. This leads to a further IS improvement of 21%, which we posit is due to the increased capacity of the model relative to the complexity of the dataset.Doubling the depth did not initially lead to improvement \u2013 we addressed this later in the BigGAN-deep model, which uses a different residual block structure.", "We note that class embeddings c\ud835\udc50citalic_c used for the conditional BatchNorm layers in G contain a large number of weights. Instead of having a separate layer for each embedding\u00a0(Miyato et\u00a0al., 2018; Zhang et\u00a0al., 2018), we opt to use a shared embedding, which is linearly projected to each layer\u2019s gains and biases\u00a0(Perez et\u00a0al., 2018). This reduces computation and memory costs, and improves training speed (in number of iterations required to reach a given performance) by 37%.Next, weadd direct skip connections (skip-z\ud835\udc67zitalic_z) from the noise vector z\ud835\udc67zitalic_z to multiple layers of G rather than just the initial layer.The intuition behind this design is to allow G to use the latent space to directly influence features at different resolutions and levels of hierarchy.In BigGAN, this is accomplished by splitting z\ud835\udc67zitalic_z into one chunk per resolution, and concatenating each chunk to the conditional vector c\ud835\udc50citalic_c which gets projected to the BatchNorm gains and biases.In BigGAN-deep, we use an even simpler design, concatenating the entire z\ud835\udc67zitalic_z with the conditional vector without splitting it into chunks.Previous works (Goodfellow et\u00a0al., 2014; Denton et\u00a0al., 2015) have considered variants of this concept; our implementation is a minor modification of this design.Skip-z\ud835\udc67zitalic_z provides a modest performance improvement of around 4%, and improves training speed by a further 18%.", "Unlike models which need to backpropagate through their latents, GANs can employ an arbitrary prior p(z)\ud835\udc5d\ud835\udc67p(z)italic_p ( italic_z ), yet the vast majority of previous works have chosen to draw z\ud835\udc67zitalic_z from either \ud835\udca9(0,I)\ud835\udca90\ud835\udc3c\\mathcal{N}(0,I)caligraphic_N ( 0 , italic_I ) or \ud835\udcb0[\u22121,1]\ud835\udcb011\\mathcal{U}[-1,1]caligraphic_U [ - 1 , 1 ]. We question the optimality of this choice and explore alternatives in Appendix\u00a0E.", "Remarkably, our best results come from using a different latent distribution for sampling than was used in training. Taking a model trained with z\u223c\ud835\udca9(0,I)similar-to\ud835\udc67\ud835\udca90\ud835\udc3cz\\sim\\mathcal{N}(0,I)italic_z \u223c caligraphic_N ( 0 , italic_I ) and sampling z\ud835\udc67zitalic_z from a truncated normal (where values which fall outside a range are resampled to fall inside that range) immediately provides a boost to IS and FID. We call this the Truncation Trick: truncating a z\ud835\udc67zitalic_z vector by resampling the values with magnitude above a chosen threshold leads to improvement in individual sample quality at the cost of reduction in overall sample variety. Figure\u00a02(a) demonstrates this: as the threshold is reduced, and elements of z\ud835\udc67zitalic_z are truncated towards zero (the mode of the latent distribution), individual samples approach the mode of G\u2019s output distribution. Related observations about this trade-off were made in (Marchesi, 2016; Pieters & Wiering, 2014).", "This technique allows fine-grained, post-hoc selection of the trade-off between sample quality and variety for a given G. Notably, we can compute FID and IS for a range of thresholds, obtaining the variety-fidelity curve reminiscent of the precision-recall curve (Figure\u00a017). As IS does not penalize lack of variety in class-conditional models, reducing the truncation threshold leads to a direct increase in IS (analogous to precision). FID penalizes lack of variety (analogous to recall) but also rewards precision, so we initially see a moderate improvement in FID, but as truncation approaches zero and variety diminishes, the FID sharply drops. The distribution shift caused by sampling with different latents than those seen in training is problematic for many models. Some of our larger models are not amenable to truncation, producing saturation artifacts (Figure\u00a02(b)) when fed truncated noise. To counteract this, we seek to enforce amenability to truncation by conditioning G to be smooth, so that the full space of z\ud835\udc67zitalic_z will map to good output samples. For this, we turn to Orthogonal Regularization (Brock et\u00a0al., 2017), which directly enforces the orthogonality condition:", "R\u03b2(W)=\u03b2\u2016W\u22a4W\u2212I\u2016F2,subscript\ud835\udc45\ud835\udefd\ud835\udc4a\ud835\udefdsuperscriptsubscriptnormsuperscript\ud835\udc4atop\ud835\udc4a\ud835\udc3cF2R_{\\beta}(W)=\\beta\\|W^{\\top}W-I\\|_{\\mathrm{F}}^{2},italic_R start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT ( italic_W ) = italic_\u03b2 \u2225 italic_W start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT italic_W - italic_I \u2225 start_POSTSUBSCRIPT roman_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,(2)", "where W\ud835\udc4aWitalic_W is a weight matrix and \u03b2\ud835\udefd\\betaitalic_\u03b2 a hyperparameter. This regularization is known to often be too limiting (Miyato et\u00a0al., 2018), so we explore several variants designed to relax the constraint while still imparting the desired smoothness to our models. The version we find to work best removes the diagonal terms from the regularization, and aims to minimize the pairwise cosine similarity between filters but does not constrain their norm:", "R\u03b2(W)=\u03b2\u2016W\u22a4W\u2299(\ud835\udfcf\u2212I)\u2016F2,subscript\ud835\udc45\ud835\udefd\ud835\udc4a\ud835\udefdsuperscriptsubscriptnormdirect-productsuperscript\ud835\udc4atop\ud835\udc4a1\ud835\udc3cF2R_{\\beta}(W)=\\beta\\|W^{\\top}W\\odot(\\mathbf{1}-I)\\|_{\\mathrm{F}}^{2},italic_R start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT ( italic_W ) = italic_\u03b2 \u2225 italic_W start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT italic_W \u2299 ( bold_1 - italic_I ) \u2225 start_POSTSUBSCRIPT roman_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,(3)where \ud835\udfcf1\\mathbf{1}bold_1 denotes a matrix with all elements set to 1111.We sweep \u03b2\ud835\udefd\\betaitalic_\u03b2 values and select 10\u22124superscript10410^{-4}10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT, finding this small added penalty sufficient to improve the likelihood that our models will be amenable to truncation. Across runs in Table\u00a01, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation, compared to 60% when trained with Orthogonal Regularization.", "We find that current GAN techniques are sufficient to enable scaling to large models and distributed, large-batch training. We find that we can dramatically improve the state of the art and train models up to 512\u00d7\\times\u00d7512 resolution without need for explicit multiscale methods like Karras et\u00a0al. (2018). Despite these improvements, our models undergo training collapse, necessitating early stopping in practice. In the next two sections we investigate why settings which were stable in previous works become unstable when applied at scale.", "Much previous work has investigated GAN stability from a variety of analytical angles and on toy problems, but the instabilities we observe occur for settings which are stable at small scale, necessitating direct analysis at large scale.We monitor a range of weight, gradient, and loss statistics during training, in search of a metric which might presage the onset of training collapse, similar to (Odena et\u00a0al., 2018). We found the top three singular values\u03c30,\u03c31,\u03c32subscript\ud835\udf0e0subscript\ud835\udf0e1subscript\ud835\udf0e2\\sigma_{0},\\sigma_{1},\\sigma_{2}italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_\u03c3 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_\u03c3 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT of each weight matrix to be the most informative.They can be efficiently computed using the Alrnoldi iteration method\u00a0(Golub & der Vorst, 2000), which extends the power iteration method, used in\u00a0Miyato et\u00a0al. (2018), to estimation of additional singular vectors and values. A clear pattern emerges, as can be seen in Figure\u00a03(a) and Appendix\u00a0F: most G layers have well-behaved spectral norms, but some layers (typically the first layer in G, which is over-complete and not convolutional) are ill-behaved, with spectral norms that grow throughout training and explode at collapse.", "To ascertain if this pathology is a cause of collapse or merely a symptom, we study the effects of imposing additional conditioning on G to explicitly counteract spectral explosion. First, we directly regularize the top singular values \u03c30subscript\ud835\udf0e0\\sigma_{0}italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of each weight, either towards a fixed value \u03c3regsubscript\ud835\udf0e\ud835\udc5f\ud835\udc52\ud835\udc54\\sigma_{reg}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT or towards some ratio r\ud835\udc5fritalic_r of the second singular value, r\u22c5sg(\u03c31)\u22c5\ud835\udc5f\ud835\udc60\ud835\udc54subscript\ud835\udf0e1r\\cdot sg(\\sigma_{1})italic_r \u22c5 italic_s italic_g ( italic_\u03c3 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) (with sg\ud835\udc60\ud835\udc54sgitalic_s italic_g the stop-gradient operation to prevent the regularization from increasing \u03c31subscript\ud835\udf0e1\\sigma_{1}italic_\u03c3 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT). Alternatively, we employ a partial singular value decomposition to instead clamp \u03c30subscript\ud835\udf0e0\\sigma_{0}italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Given a weight W\ud835\udc4aWitalic_W, its first singular vectors u0subscript\ud835\udc620u_{0}italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and v0subscript\ud835\udc630v_{0}italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and \u03c3clampsubscript\ud835\udf0e\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc5a\ud835\udc5d\\sigma_{clamp}italic_\u03c3 start_POSTSUBSCRIPT italic_c italic_l italic_a italic_m italic_p end_POSTSUBSCRIPT the value to which the \u03c30subscript\ud835\udf0e0\\sigma_{0}italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT will be clamped, our weights become:W=W\u2212max\u2061(0,\u03c30\u2212\u03c3clamp)v0u0\u22a4,\ud835\udc4a\ud835\udc4a0subscript\ud835\udf0e0subscript\ud835\udf0e\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc5a\ud835\udc5dsubscript\ud835\udc630superscriptsubscript\ud835\udc620topW=W-\\max(0,\\sigma_{0}-\\sigma_{clamp})v_{0}u_{0}^{\\top},italic_W = italic_W - roman_max ( 0 , italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - italic_\u03c3 start_POSTSUBSCRIPT italic_c italic_l italic_a italic_m italic_p end_POSTSUBSCRIPT ) italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT ,(4)where \u03c3clampsubscript\ud835\udf0e\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc5a\ud835\udc5d\\sigma_{clamp}italic_\u03c3 start_POSTSUBSCRIPT italic_c italic_l italic_a italic_m italic_p end_POSTSUBSCRIPT is set to either \u03c3regsubscript\ud835\udf0e\ud835\udc5f\ud835\udc52\ud835\udc54\\sigma_{reg}italic_\u03c3 start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT or r\u22c5sg(\u03c31)\u22c5\ud835\udc5f\ud835\udc60\ud835\udc54subscript\ud835\udf0e1r\\cdot sg(\\sigma_{1})italic_r \u22c5 italic_s italic_g ( italic_\u03c3 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ). We observe that both with and without Spectral Normalization these techniques have the effect of preventing the gradual increase and explosion of either \u03c30subscript\ud835\udf0e0\\sigma_{0}italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT or \u03c30\u03c31subscript\ud835\udf0e0subscript\ud835\udf0e1\\frac{\\sigma_{0}}{\\sigma_{1}}divide start_ARG italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03c3 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG, but even though in some cases they mildly improve performance, no combination prevents training collapse. This evidence suggests that while conditioning G might improve stability, it is insufficient to ensure stability. We accordingly turn our attention to D.", "As with G, we analyze the spectra of D\u2019s weights to gain insight into its behavior, then seek to stabilize training by imposing additional constraints. Figure\u00a03(b) displays a typical plot of \u03c30subscript\ud835\udf0e0\\sigma_{0}italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT for D (with further plots in Appendix\u00a0F). Unlike G, we see that the spectra are noisy, \u03c30\u03c31subscript\ud835\udf0e0subscript\ud835\udf0e1\\frac{\\sigma_{0}}{\\sigma_{1}}divide start_ARG italic_\u03c3 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03c3 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG is well-behaved, and the singular values grow throughout training but only jump at collapse, instead of exploding.", "The spikes in D\u2019s spectra might suggest that it periodically receives very large gradients, but we observe that the Frobenius norms are smooth (Appendix\u00a0F), suggesting that this effect is primarily concentrated on the top few singular directions. We posit that this noise is a result of optimization through the adversarial training process, where G periodically produces batches which strongly perturb D . If this spectral noise is causally related to instability, a natural counter is to employ gradient penalties, which explicitly regularize changes in D\u2019s Jacobian. We explore the R1subscript\ud835\udc451R_{1}italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT zero-centered gradient penalty from Mescheder et\u00a0al. (2018):R1:=\u03b32\ud835\udd3cp\ud835\udc9f(x)[\u2016\u2207D(x)\u2016F2].assignsubscript\ud835\udc451\ud835\udefe2subscript\ud835\udd3csubscript\ud835\udc5d\ud835\udc9f\ud835\udc65delimited-[]superscriptsubscriptnorm\u2207\ud835\udc37\ud835\udc65\ud835\udc392R_{1}:=\\frac{\\gamma}{2}\\mathbb{E}_{p_{\\mathcal{D}}(x)}\\left[\\|\\nabla D(x)\\|_{F}^{2}\\right].italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT := divide start_ARG italic_\u03b3 end_ARG start_ARG 2 end_ARG blackboard_E start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( italic_x ) end_POSTSUBSCRIPT [ \u2225 \u2207 italic_D ( italic_x ) \u2225 start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .(5)", "With the default suggested \u03b3\ud835\udefe\\gammaitalic_\u03b3 strength of 10, training becomes stable and improves the smoothness and boundedness of spectra in both G and D, but performance severely degrades, resulting in a 45% reduction in IS. Reducing the penalty partially alleviates this degradation, but results in increasingly ill-behaved spectra; even with the penalty strength reduced to 1111 (the lowest strength for which sudden collapse does not occur) the IS is reduced by 20%.Repeating this experiment with various strengths of Orthogonal Regularization, DropOut (Srivastava et\u00a0al., 2014), and L2 (See Appendix\u00a0I for details), reveals similar behaviors for these regularization strategies: with high enough penalties on D, training stability can be achieved, but at a substantial cost to performance.", "We also observe that D\u2019s loss approaches zero during training, but undergoes a sharp upward jump at collapse (Appendix\u00a0F).One possible explanation for this behavior is that D is overfitting to the training set, memorizing training examples rather than learning some meaningful boundary between real and generated images. As a simple test for D\u2019s memorization (related to Gulrajani et\u00a0al. (2017)), we evaluate uncollapsed discriminators on the ImageNet training and validation sets, and measure what percentage of samples are classified as real or generated. While the training accuracy is consistently above 98%, the validation accuracy falls in the range of 50-55%, no better than random guessing (regardless of regularization strategy). This confirms that Dis indeed memorizing the training set;we deem this in line with D\u2019s role, which is not explicitly to generalize, but to distill the training data and provide a useful learning signal for G. Additional experiments and discussion are provided in Appendix\u00a0G.", "We find that stability does not come solely from G or D, but from their interaction through the adversarial training process. While the symptoms of their poor conditioning can be used to track and identify instability, ensuring reasonable conditioning proves necessary for training but insufficient to prevent eventual training collapse. It is possible to enforce stability by strongly constraining D, but doing so incurs a dramatic cost in performance. With current techniques, better final performance can be achieved by relaxing this conditioning and allowing collapse to occur at the later stages of training, by which time a model is sufficiently trained to achieve good results.", "We evaluate our models on ImageNet ILSVRC 2012\u00a0(Russakovsky et\u00a0al., 2015) at 128\u00d7\\times\u00d7128, 256\u00d7\\times\u00d7256, and 512\u00d7\\times\u00d7512 resolutions, employing the settings from Table\u00a01, row 8.The samples generated by our models are presented in Figure\u00a04, with additional samples in Appendix\u00a0A, and online222https://drive.google.com/drive/folders/1lWC6XEPD0LT5KUnPXeve_kWeY-FxH002.We report IS and FID in Table\u00a02. As our models are able to trade sample variety for quality, it is unclear how best to compare against prior art; we accordingly report values at three settings, with complete curves in Appendix\u00a0D. First, we report the FID/IS values at the truncation setting which attains the best FID. Second, we report the FID at the truncation setting for which our model\u2019s IS is the same as that attained by the real validation data, reasoning that this is a passable measure of maximum sample variety achieved while still achieving a good level of \u201cobjectness.\u201d Third, we report FID at the maximum IS achieved by each model, to demonstrate how much variety must be traded off to maximize quality. In all three cases, our models outperform the previous state-of-the-art IS and FID scores achieved by Miyato et\u00a0al. (2018) and Zhang et\u00a0al. (2018).", "In addition to the BigGAN model introduced in the first version of the paper and used in the majority of experiments (unless otherwise stated), we also present a 4x deeper model (BigGAN-deep) which uses a different configuration of residual blocks. As can be seen from Table\u00a02, BigGAN-deep substantially outperforms BigGAN across all resolutions and metrics. This confirms that our findings extend to other architectures, and that increased depth leads to improvement in sample quality.Both BigGAN and BigGAN-deep architectures are described in Appendix\u00a0B.", "Our observation that D overfits to the training set, coupled with our model\u2019s sample quality, raises the obvious question of whether or not G simply memorizes training points. To test this, we perform class-wise nearest neighbors analysis in pixel space and the feature space of pre-trained classifier networks (Appendix\u00a0A). In addition, we present both interpolations between samples and class-wise interpolations (where z\ud835\udc67zitalic_z is held constant) in Figures\u00a08 and 9. Our model convincingly interpolates between disparate samples, and the nearest neighbors for its samples are visually distinct, suggesting that our model does not simply memorize training data.", "We note that some failure modes of our partially-trained models are distinct from those previously observed. Most previous failures involve local artifacts (Odena et\u00a0al., 2016), images consisting of texture blobs instead of objects (Salimans et\u00a0al., 2016), or the canonical mode collapse. We observe class leakage, where images from one class contain properties of another, as exemplified by Figure\u00a04(d). We also find that many classes on ImageNet are more difficult than others for our model; our model is more successful at generating dogs (which make up a large portion of the dataset, and are mostly distinguished by their texture) than crowds (which comprise a small portion of the dataset and have more large-scale structure). Further discussion is available in Appendix\u00a0A.", "To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of JFT-300M\u00a0(Sun et\u00a0al., 2017).The full JFT-300M dataset contains 300M real-world images labeled with 18K categories.Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels.The resulting dataset contains 292M images \u2013 two orders of magnitude larger than ImageNet. For images with multiple labels, we sample a single label randomly and independently whenever an image is sampled.To compute IS and FID for the GANs trained on this dataset, we use an Inception v2 classifier\u00a0(Szegedy et\u00a0al., 2016) trained on this dataset.Quantitative results are presented in Table\u00a03.All models are trained with batch size 2048.We compare an ablated version of our model \u2013comparable to SA-GAN\u00a0(Zhang et\u00a0al., 2018) but with the larger batch size \u2013against a \u201cfull\u201d BigGAN model that makes uses of all of the techniques applied to obtain the best results on ImageNet (shared embedding, skip-z\ud835\udc67zitalic_z, and orthogonal regularization).Our results show that these techniques substantially improve performance even in the setting of this much larger dataset at the same model capacity (64 base channels).We further show that for a dataset of this scale, we see significant additional improvements from expanding the capacity of our models to 128 base channels, while for ImageNet GANs that additional capacity was not beneficial.", "In Figure\u00a019 (Appendix\u00a0D), we present truncation plots for models trained on this dataset.Unlike for ImageNet, where truncation limits of \u03c3\u22480\ud835\udf0e0\\sigma\\approx 0italic_\u03c3 \u2248 0 tend to produce the highest fidelity scores, IS is typically maximized for our JFT-300M models when the truncation value \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 ranges from 0.5 to 1.We suspect that this is at least partially due to the intra-class variability of JFT-300M labels, as well as the relative complexity of the image distribution, which includes images with multiple objects at a variety of scales.Interestingly, unlike models trained on ImageNet, where training tends to collapse without heavy regularization (Section\u00a04), the models trained on JFT-300M remain stable over many hundreds of thousands of iterations.This suggests that moving beyond ImageNet to larger datasets may partially alleviate GAN stability issues.", "The improvement over the baseline GAN model that we achieve on this datasetwithout changes to the underlying models or training and regularization techniques (beyond expanded capacity) demonstrates thatour findings extend from ImageNetto datasets with scale and complexity thus far unprecedented for generative models of images.", "We have demonstrated that Generative Adversarial Networks trained to model natural images of multiple categories highly benefit from scaling up, both in terms of fidelity and variety of the generated samples. As a result, our models set a new level of performance among ImageNet GAN models, improving on the state of the art by a large margin.We have also presented an analysis of the training behavior of large scale GANs, characterized their stability in terms of the singular values of their weights, and discussed the interplay between stability and performance.", "We would like to thankKai Arulkumaran, Matthias Bauer, Peter Buchlovsky, Jeffrey Defauw, Sander Dieleman, Ian Goodfellow, Ariel Gordon, Karol Gregor, Dominik Grewe, Chris Jones, Jacob Menick, Augustus Odena, Suman Ravuri, Ali Razavi, Mihaela Rosca, and Jeff Stanway."], "figure_types": {"22aab110058ebbd198edb1f1e7b4f69fb13c0613/1-Figure1-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/12-Figure5-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/12-Figure6-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/13-Figure7-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/13-Figure8-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/14-Figure10-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/14-Figure9-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/15-Figure11-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/15-Figure12-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/16-Figure13-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/16-Figure14-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/17-Figure15-1.png": "schematic", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/17-Table4-1.png": "schematic", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/18-Table5-1.png": "schematic", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/18-Table6-1.png": "schematic", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/20-Figure16-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/20-Figure17-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/21-Figure18-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/23-Figure19-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/24-Figure20-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/24-Figure21-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/25-Figure22-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/25-Figure23-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/26-Figure24-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/26-Figure25-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/27-Figure26-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/27-Figure27-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/3-Table1-1.png": "table", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/4-Figure2-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/5-Figure3-1.png": "plot", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/7-Figure4-1.png": "photograph(s)", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/7-Table2-1.png": "table", "22aab110058ebbd198edb1f1e7b4f69fb13c0613/8-Table3-1.png": "table"}}, "1602.02697": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "arxiv_url": "https://arxiv.org/abs/1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "all_figures_tables": {"53b047e503f4c24602f376a774d653f7ed56c024/10-Figure11-1.png": "Figure 11: Label predictions matched between the substitutes (DNN and LR) and their target oracles on test data.", "53b047e503f4c24602f376a774d653f7ed56c024/10-Table2-1.png": "Table 2: Impact of our refinements, Periodic Step Size (PSS) and Reservoir Sampling (RS), on the percentage of label predictions matched between the substitutes and their target classifiers on test data after \u03c1 = 9 substitute iterations.", "53b047e503f4c24602f376a774d653f7ed56c024/10-Table3-1.png": "Table 3: Misclassification rates (%) of the Amazon and Google oracles on adversarial samples produced with DNN and LR substitutes after \u03c1 = 3, 6 epochs. The 2nd column is the number of queries during substitute training. Last row uses a periodic step size and reservoir sampling.", "53b047e503f4c24602f376a774d653f7ed56c024/11-Table4-1.png": "Table 4: Evaluation of adversarial training: the columns indicate the input variation parameter used to inject adversarial examples during training and to compute the attacks, the attack success rate when examples crafted on the (O)racle are deployed against the (O)racle, the attack success rate when examples crafted on the (S)ubstitute are deployed against the (S)ubstitute, and the attack success rate when examples crafted on the (S)ubstitute are deployed against the (O)racle. .", "53b047e503f4c24602f376a774d653f7ed56c024/12-Figure12-1.png": "Figure 12: Evaluation of defensive distillation: Percentage of adversarial examples crafted using the Goodfellow algorithm at varying \u03b5 misclassified by the oracle. T is the temperature of distillation [10]. Curves marked by (direct) indicate baseline attacks computed on the oracle, all other curves where computed using a substitute, as described in Section 4. Despite distillation preventing the attack on the oracle directly, using a substitute allows us to evade it.", "53b047e503f4c24602f376a774d653f7ed56c024/13-Figure13-1.png": "Figure 13: DNN architectures: ID: reference used in the paper, In: input dimension, Out: output dimension, CM: convolutional layer with 2x2 kernels followed by max-pooling with kernel 2x2, RL: rectified linear layer except for 200s where sigmoid units are used, S: softmax layer.", "53b047e503f4c24602f376a774d653f7ed56c024/14-Figure14-1.png": "Figure 14: Frequencies of cost gradient sign matrix components equal between substitute A and the oracle at substitute training epochs \u03c1 \u2208 {0, 3, 6} (three on the right), compared to a pair of random sign matrices (first image).", "53b047e503f4c24602f376a774d653f7ed56c024/14-Figure15-1.png": "Figure 15: Frequencies of cost gradient sign matrix components equal between substitute A and the oracle", "53b047e503f4c24602f376a774d653f7ed56c024/2-Figure1-1.png": "Figure 1: DNN Classifier: the model processes an image of a handwritten digit and outputs the probility of it being in one of the N = 10 classes for digits 0 to 9 (from [10]).", "53b047e503f4c24602f376a774d653f7ed56c024/3-Figure2-1.png": "Figure 2: Adversarial samples (misclassified) in the bottom row are created from the legitimate samples [7, 13] in the top row. The DNN outputs are identified below the samples.", "53b047e503f4c24602f376a774d653f7ed56c024/5-Figure3-1.png": "Figure 3: Training of the substitute DNN F : the attacker (1) collects an initial substitute training set S0 and (2) selects an architecture F . Using oracle O\u0303, the attacker (3) labels S0 and (4) trains substitute F . After (5) Jacobian-based dataset augmentation, steps (3) through (5) are repeated for several substitute epochs \u03c1.", "53b047e503f4c24602f376a774d653f7ed56c024/6-Figure4-1.png": "Figure 4: Substitute DNN Accuracies: each column corresponds to an initial substitute training set: 150 MNIST test samples, and handcrafted digits. Accuracy is reported on the unused 9,850 MNIST test samples.", "53b047e503f4c24602f376a774d653f7ed56c024/6-Figure5-1.png": "Figure 5: Success Rate and Transferability of Adversarial Samples for the MetaMind attacks: performed using MNIST-based and handcrafted substitutes: each bar corresponds to a different perturbation input variation.", "53b047e503f4c24602f376a774d653f7ed56c024/7-Figure6-1.png": "Figure 6: MetaMind Oracle Confusion Matrices for different input variations \u03b5. Cell (x, y) indicates the share of digit y instances classified by the oracle as digit x.", "53b047e503f4c24602f376a774d653f7ed56c024/7-Figure7-1.png": "Figure 7: Success Rate and Transferability of Adversarial Samples crafted on the GTRSRB dataset: each bar corresponds to a different input variation.", "53b047e503f4c24602f376a774d653f7ed56c024/8-Table1-1.png": "Table 1: Substitute Accuracy at \u03c1 = 2 and \u03c1 = 6 substitute epochs and Transferability of Adversarial Samples: for \u03b5 = 0.4 after \u03c1 = 6 substitute epochs.", "53b047e503f4c24602f376a774d653f7ed56c024/9-Figure10-1.png": "Figure 10: Impact of the input variation \u03b5 in the Papernot algorithm on the success rate and adversarial sample transferability computed for \u03b5 \u2208 {0.5, 0.7, 1} on DNNs from Table 1 with distortion \u03a5 = 39.80%.", "53b047e503f4c24602f376a774d653f7ed56c024/9-Figure8-1.png": "Figure 8: Impact of input variation \u03b5 in the Goodfellow crafting algorithm on the transferability of adversarial samples: for architectures from Table 1.", "53b047e503f4c24602f376a774d653f7ed56c024/9-Figure9-1.png": "Figure 9: Impact of the maximum distortion \u03a5 in the Papernot algorithm on success rate and transferability of adversarial samples: increasing \u03a5 yields higher transferability rates across DNNs."}, "referred_figures_tables": [["53b047e503f4c24602f376a774d653f7ed56c024/9-Figure8-1.png", "53b047e503f4c24602f376a774d653f7ed56c024/8-Table1-1.png", "53b047e503f4c24602f376a774d653f7ed56c024/8-Table1-1.png"]], "question_id": [0], "question": ["How the architecture is chosen"], "question_section": ["sec 4.1"], "question_trigger_sentence": ["(The adversary selects an architecture to be trained as the substitute F)"], "question_type": ["Deep/complex question"], "evidential_info": [[{"context": "Substitute Architecture: This factor is not the mostlimiting as the adversary must at least have some partial knowledge of theoracle input (e.g., images, text) andexpected output (e.g., classification). The adversary can thus usean architecture adapted to the input-output relation. For instance, aconvolutional neural network is suitable for image classification. Furthermore,we show in Section\u00a06 that the type, number, and size of layers usedin the substitute DNN have relatively littleimpact on the success of the attack. Adversaries can also consider performingan architecture exploration and train several substitute models beforeselecting the one yielding the highest attack success.", "rationale": "The adversary (attacking part) must at least have some partial knowledge of the input (e.g., images, text) and expected output (e.g., classification). The adversary then selects an appropriate architecture adapted to the input-output relation. For instance, if the task is image classification, a convolutional neural network is the best choice. Furthermore, in Section 6 is proven that the type, number, and size of layers used in the substitute DNN have relatively little impact on the success of the attack, so these attributes do not determine the architecture."}, {"context": "Substitute DNN Training Algorithm: We now describe the five-step training procedure outlined in Algorithm\u00a01:\u2022Initial Collection (1): The adversary collects a very small set S_{0} of inputs representative of the input domain. For instance, if the targeted oracle O classifies handwritten digits, the adversary collects 10 images of each digit 0 through 9. We show in Section\u00a05 that this set does not necessarily have to come from the distribution from which the targeted oracle was trained.\u2022Architecture Selection (2): The adversary selects an architecture to be trained as the substitute F. Again, this can be done using high-level knowledge of the classification task performed by the oracle (e.g., convolutional networks are appropriate for vision)\u2022Substitute Training: The adversary iteratively trains moreaccurate substitute DNNs F_{\\rho} by repeating the following for \u03c1\u22080..\u03c1m\u2062a\u2062x\\rho\\in 0..\\rho_{max}italic_\u03c1 \u2208 0 . . italic_\u03c1 start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT:\u2013Labeling (3): By querying for the labels \\tilde{O}(\\vec{x}) output by oracle O, the adversary labels each sample \\vec{x}\\in S_{\\rho} in its initial substitute training set S_{\\rho}.\u2013Training (4): The adversary trains the architecture chosen at step (2) using substitute training set S_{\\rho} in conjunction with classical training techniques.\u2013Augmentation (5): The adversary applies our augmentation technique on the initial substitute training set S_{\\rho} to produce a larger substitute training set S_{\\rho+1} with more synthetic training points. This new training set better represents the model\u2019s decision boundaries. The adversary repeats steps (3) and (4) with the augmented set S_{\\rho+1}.Step (3) is repeated several times to increase the substitute DNN\u2019s accuracy and the similarity of its decision boundaries with the oracle. We introduce the term substitute training epoch, indexed with \\rho, to refer to each iteration performed. This leads to this formalization of the Jacobian-based Dataset Augmentation performed at step (5) of our substitute training algorithm to find more synthetic training points:S_{\\rho+1}=\\{\\vec{x}+\\lambda\\cdot\\operatorname{sgn}(J_{F}[\\tilde{O}(\\vec{x})]):\\vec{x}\\in S_{\\rho}\\}\\cup S_{\\rho}(4)where \\lambda is a parameter of the augmentation: it defines the size of the step taken in the sensitive direction identified by the Jacobian matrix to augment the set S_{\\rho} into S_{\\rho+1}.", "rationale": "They train substitute DNNs A and F to M (see Table 13) of different architectures. The substitute architectures differ by type, number, and size of layers. In order to compare different architectures the measure the accuracy, as well as the adversarial sample transferability. The most important transferability drop follows from removing all convolutional layers. Changing the hidden layer activation function from rectified linear to a sigmoid does not impact transferability significantly."}, {"context": "Choosing an Architecture: We train substitute DNNs A and F to M (cf.Table\u00a013) using 150 samples from the MNIST test set as the substitute training set. During eachof the 6 substitute training epochs, the DNN is trained for 5 epochsfrom scratch. Between epochs, synthetic data is added to the training set using Jacobian-baseddataset augmentations with step \\lambda=0.1. The substitutearchitectures differ from the oracle\u2019s by the type, number, and sizeof layers. In Table\u00a01,we report the accuracy of each architecture after 2 and 6 substitute training epochs, as well as the adversarialsample transferability after 6 epochs. Adversarial samples are crafted using the Goodfellow algorithm with an inputvariation of \\varepsilon=0.4 (which we justify later). The last column ofTable\u00a01shows that the choice of architecture has a limitedimpact on adversarial sample transferability, and therefore on the attacksuccess. The most important transferability drop follows from removing allconvolutional layers. Changing the hidden layer activation function fromrectified linear to a sigmoid does not impact transferability significantly.", "rationale": "In Figure 8, architecture A outperforms all others as it is a copy of the oracle\u2019s and acts as a baseline. Other architectures have asymptotic transferability rates ranging between 72.24%and 80.21%, confirming that the substitute architecture choice has a limited impact on transferability. Increasing the value of epsilon variable above 0.4 yields little improvement in transferability and should be avoided to guarantee indistinguishability of adversarial samples to humans."}, {"context": "Goodfellow\u2019s algorithm: Recall from Equation\u00a05 the perturbation computed in the Goodfellow attack.Its only parameter is the variation \\varepsilon added inthe direction of the gradient sign. We use the same architecture set asbefore to quantify the impact of \\varepsilon onadversarial sample transferability.In Figure\u00a08, architecture A outperformsall others: it is a copy of the oracle\u2019s and acts as a baseline. Otherarchitectures have asymptotic transferability rates ranging between 72.24\\%and 80.21\\%, confirming that the substitute architecture choice hasa limited impact on transferability. Increasing the value of \\varepsilon above0.4 yields little improvement in transferability and should be avoidedto guarantee indistinguishability of adversarial samples to humans.", "rationale": "The adversary (attacking part) selects an architecture using high-level knowledge of the classification task that is performed (e.g., convolutional networks are appropriate for vision)"}]], "composition": ["The adversary (attacking part) must at least have some partial knowledge of the input (e.g., images, text) and expected output (e.g., classification) in order to select the architecture of the attacking system. The adversary selects an appropriate architecture adapted to the input-output relation. For instance, if the task is image classification or machine visioon, a convolutional neural network is the best choice. The parameters of the system (Deep Neural Network), like training epochs, number of layers , nodes etc., have relatively little impact on the success of the attack, so they do not determine the architecture."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["1299"], "passages": ["A classifier is a ML model that learns a mapping between inputs and a set of classes. For instance, a malware detector is a classifier taking executables as inputs and assigning them to the benign or malware class. Efforts in the security\u00a0[5, 2, 9, 18] and machine learning\u00a0[14, 4] communities exposed the vulnerability of classifiers to integrityattacks.Such attacks are often instantiated by adversarial examples: legitimate inputs altered by adding small,often imperceptible,perturbations to force a learned classifier to misclassify theresulting adversarial inputs, while remaining correctly classified by a human observer.To illustrate, consider the following images,potentially consumed by an autonomous vehicle\u00a0[13]:", "", "To humans, these images appear to be the same: our biological classifiers(vision) identify each image as a stop sign.The image on the left\u00a0[13] is indeed an ordinary image ofa stop sign.We produced the image on the right by adding a precise perturbation thatforces a particular DNN to classify it as a yield sign, as described in Section\u00a05.2.Here, an adversary could potentially use the altered image to cause a car without failsafes to behavedangerously.This attack would require modifyingthe image used internally by the car through transformations of the physical traffic sign. Related works showed the feasibility of such physical transformations for a state-of-the-art vision classifier\u00a0[6] and face recognition model\u00a0[11].It is thus conceivable that physical adversarial traffic signs could be generated by maliciouslymodifying the sign itself, e.g., with stickers or paint.", "In this paper, we introduce the first demonstration that black-box attacks against DNN classifiers are practical for real-world adversaries with no knowledge about the model. We assume the adversary (a) has no information about the structure or parameters of the DNN, and (b) does not have access to any large training dataset.The adversary\u2019s only capability is to observe labels assigned by the DNN forchosen inputs, in a manner analog to a cryptographic oracle.", "Our novel attack strategy is to train a local substitute DNN with a syntheticdataset: the inputs are synthetic and generatedby the adversary, while the outputs are labels assigned by thetarget DNN and observed by the adversary. Adversarialexamples are crafted using the substitute parameters, which are known tous.They are not only misclassified by the substitute butalso by the target DNN, because both models have similar decision boundaries.", "This is a considerable departure from previous work, which evaluatedperturbations required to craft adversarial examples using either:(a) detailed knowledge of the DNN architecture andparameters\u00a0[2, 4, 9, 14],or (b) an independently collected training set to fit an auxiliary model\u00a0[2, 4, 14].This limited their applicability to strong adversariescapable ofgaining insider knowledge of the targeted ML model, orcollecting large labeled training sets. We release assumption (a) by learning a substitute: it gives us the benefit of having full access to the model and apply previous adversarial example crafting methods.We release assumption (b) by replacing the independently collected training set with a synthetic dataset constructed by the adversary with synthetic inputs and labeled by observing the target DNN\u2019s output.", "Our threat model thus corresponds to the real-world scenario of users interacting withclassifiers hosted remotely by a third-party keeping the modelinternals secret. In fact, we instantiate our attackagainst classifiers automatically trained by MetaMind, Amazon, andGoogle.We areable to access them only after training is completed.Thus, we provide the first correctly blinded experiments concerningadversarial examples as a security risk.", "We show that our black-box attack is applicable to many remote systems taking decisions based on ML, because it combines three key properties: (a) the capabilities required are limited to observing output class labels, (b) the number of labels queried is limited, and (c) the approach applies and scales to different ML classifier types (see Section\u00a07), in addition to state-of-the-art DNNs. In contrast, previous work failed to simultaneously provide all of these three keyproperties\u00a0[4, 14, 12, 15, 18].Our contributions are:\u2022We introduce in Section\u00a04 an attackagainst black-box DNN classifiers. It crafts adversarial examples without knowledge of the classifier training data or model. To do so, a synthetic dataset is constructed by the adversary to train a substitute for the targeted DNN classifier.\u2022In Section\u00a05, we instantiate the attack against a remote DNN classifier hosted by MetaMind. The DNN misclassifies 84.24%percent84.2484.24\\%84.24 %of the adversarial inputs crafted.\u2022The attack is calibrated inSection\u00a06 to (a) reduce the number of queries made to the target model and (b)maximizemisclassification of adversarial examples. \u2022We generalize the attack to other ML classifiers like logistic regression. In Section\u00a07, we target models hosted by Amazon and Google. They misclassify adversarial examples at rates of 96.19%percent96.1996.19\\%96.19 % and 88.94%percent88.9488.94\\%88.94 %.\u2022Section\u00a08 shows that our attack evades defenses proposed in the literature because the substitute trained by the adversary is unaffected by defenses deployed on the targeted oracle model to reduce its vulnerability.\u2022In Appendix B, we provide an intuition of why adversarial examples crafted with the substitute also mislead target models by empirically observing that substitutes havegradients correlated to the target\u2019s.Disclosure: We disclosed our attacks to MetaMind, Amazon, and Google. Note that no damage was caused as we demonstrated control of models created for our own account.", "We provide preliminaries of deep learning to enableunderstanding of our threat model and attack. We refer readersinterested to the more detailed presentation in\u00a0[3].", "A deep neural network (DNN), as illustrated inFigure\u00a01, is a ML technique that uses ahierarchical composition of n\ud835\udc5bnitalic_n parametric functions to model an inputx\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG. Each functionfisubscript\ud835\udc53\ud835\udc56f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for i\u22081..ni\\in 1..nitalic_i \u2208 1 . . italic_n is modeled using a layer of neurons, which areelementary computing units applying an activation functionto the previous layer\u2019s weighted representation of the input to generate a newrepresentation. Each layer is parameterized by a weight vector\u03b8isubscript\ud835\udf03\ud835\udc56\\theta_{i}italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (we omit the vector notation) impacting each neuron\u2019s activation. Such weights holdthe knowledge of a DNN model F\ud835\udc39Fitalic_F and are evaluated during itstraining phase, as detailed below. Thus, a DNN defines and computes:F(x\u2192)=fn(\u03b8n,fn\u22121(\u03b8n\u22121,\u00a0\u2026\u00a0f2(\u03b82,f1(\u03b81,x\u2192))))\ud835\udc39\u2192\ud835\udc65subscript\ud835\udc53\ud835\udc5bsubscript\ud835\udf03\ud835\udc5bsubscript\ud835\udc53\ud835\udc5b1subscript\ud835\udf03\ud835\udc5b1\u00a0\u2026\u00a0subscript\ud835\udc532subscript\ud835\udf032subscript\ud835\udc531subscript\ud835\udf031\u2192\ud835\udc65F(\\vec{x})=f_{n}\\left(\\theta_{n},f_{n-1}\\left(\\theta_{n-1},\\text{ ... }f_{2}\\left(\\theta_{2},f_{1}\\left(\\theta_{1},\\vec{x}\\right)\\right)\\right)\\right)italic_F ( over\u2192 start_ARG italic_x end_ARG ) = italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_\u03b8 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT ( italic_\u03b8 start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , \u2026 italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_\u03b8 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_\u03b8 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , over\u2192 start_ARG italic_x end_ARG ) ) ) )(1)", "The training phase of a DNN F\ud835\udc39Fitalic_F learns values for itsparameters \u03b8F={\u03b81,\u2026,\u03b8n}subscript\ud835\udf03\ud835\udc39subscript\ud835\udf031\u2026subscript\ud835\udf03\ud835\udc5b\\theta_{F}=\\{\\theta_{1},...,\\theta_{n}\\}italic_\u03b8 start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = { italic_\u03b8 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_\u03b8 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. We focus on classification tasks, where thegoal is to assign inputs a label among a predefined set of labels. The DNN is given a large set of known input-output pairs(x\u2192,y\u2192)\u2192\ud835\udc65\u2192\ud835\udc66(\\vec{x},\\vec{y})( over\u2192 start_ARG italic_x end_ARG , over\u2192 start_ARG italic_y end_ARG ) and it adjusts weight parameters to reduce a costquantifying the prediction error between the prediction F(x\u2192)\ud835\udc39\u2192\ud835\udc65F(\\vec{x})italic_F ( over\u2192 start_ARG italic_x end_ARG ) andthe correct output y\u2192\u2192\ud835\udc66\\vec{y}over\u2192 start_ARG italic_y end_ARG. The adjustment is typically performed usingtechniques derived from the backpropagationalgorithm. Briefly, such techniquessuccessively propagate error gradients with respect to network parameters fromthe network\u2019s output layer to its input layer.", "During the test phase, the DNN is deployed with a fixed set ofparameters \u03b8Fsubscript\ud835\udf03\ud835\udc39\\theta_{F}italic_\u03b8 start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT to make predictions on inputs unseen duringtraining. We consider classifiers: the DNN produces a probability vectorF(x\u2192)\ud835\udc39\u2192\ud835\udc65F(\\vec{x})italic_F ( over\u2192 start_ARG italic_x end_ARG ) encoding its belief of input x\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG being in each of theclasses (cf. Figure\u00a01). The weight parameters\u03b8Fsubscript\ud835\udf03\ud835\udc39\\theta_{F}italic_\u03b8 start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT hold the model knowledge acquired by training. Ideally,the model shouldgeneralize and make accurate predictions for inputs outside of the domainexplored during training. However, attacks manipulating DNN inputs with adversarialexamples showed this is not the case in practice\u00a0[4, 9, 14].", "A taxonomy of adversaries against DNN classifiers is found in\u00a0[9].In our work, the adversaryseeks to force a classifier to misclassify inputs in any class different fromtheir correct class. To achieve this, we consider a weak adversarywith access to the DNN output only. The adversary has no knowledge of the architecturalchoices made to design the DNN, which include the number, type, and size oflayers, nor of the training data used to learn the DNN\u2019s parameters. Such attacks are referred to as black box, where adversaries need not know internal details of asystem to compromise it.", "Targeted Model: Weconsider attackers targeting a multi-class DNN classifier. Itoutputs probability vectors, where each vector componentencodes the DNN\u2019s belief of the input being part of one of the predefinedclasses. We consider the ongoing example ofa DNN classifying images, as shown inFigure\u00a01. Such DNNs can be used to classifyhandwritten digits into classes associated with digits from 0 to 9, imagesof objects in a fixed number of categories, or images of traffic signs intoclasses identifying its type (STOP, yield, \u2026).", "Adversarial Capabilities: The oracle O\ud835\udc42Oitalic_O is the targeted DNN. Its name refers to the only capability of the adversary:accessing the label O~(x\u2192)~\ud835\udc42\u2192\ud835\udc65\\tilde{O}(\\vec{x})over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG ) forany input x\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG by querying oracle O\ud835\udc42Oitalic_O. Theoutput label O~(x\u2192)~\ud835\udc42\u2192\ud835\udc65\\tilde{O}(\\vec{x})over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG ) isthe index of the class assigned the largest probability by the DNN:O~(x\u2192)=arg\u2061maxj\u22080..N\u22121\u2061Oj(x\u2192)\\tilde{O}(\\vec{x})=\\arg\\max_{j\\in 0..N-1}O_{j}(\\vec{x})over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG ) = roman_arg roman_max start_POSTSUBSCRIPT italic_j \u2208 0 . . italic_N - 1 end_POSTSUBSCRIPT italic_O start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( over\u2192 start_ARG italic_x end_ARG )(2)where Oj(x\u2192)subscript\ud835\udc42\ud835\udc57\u2192\ud835\udc65O_{j}(\\vec{x})italic_O start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( over\u2192 start_ARG italic_x end_ARG ) is the j\ud835\udc57jitalic_j-th component of the probability vectorO(x\u2192)\ud835\udc42\u2192\ud835\udc65O(\\vec{x})italic_O ( over\u2192 start_ARG italic_x end_ARG ) output by DNN O\ud835\udc42Oitalic_O. Distinguishing betweenlabels and probabilities makes adversariesrealistic (they more often have access to labels than probabilities) but weaker: labels encode less informationabout the model\u2019s learned behavior. Accessinglabels O~normal-~\ud835\udc42\\tilde{O}over~ start_ARG italic_O end_ARG produced by the DNN O\ud835\udc42Oitalic_O is the only capability assumed in ourthreat model.We do not haveaccess to the oracle internals or training data.", "Adversarial Goal: We want to produce a minimally altered version of any input x\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG, namedadversarial sample, and denoted x*\u2192\u2192superscript\ud835\udc65\\vec{x^{*}}over\u2192 start_ARG italic_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_ARG, misclassified by oracle O\ud835\udc42Oitalic_O: O~(x*\u2192)\u2260O~(x\u2192)~\ud835\udc42\u2192superscript\ud835\udc65~\ud835\udc42\u2192\ud835\udc65\\tilde{O}(\\vec{x^{*}})\\neq\\tilde{O}(\\vec{x})over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_ARG ) \u2260 over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG ). This corresponds to an attack on theoracle\u2019s output integrity. Adversarial samples solve the followingoptimization problem:x*\u2192=x\u2192+arg\u2061min\u2061{z\u2192:O~(x\u2192+z\u2192)\u2260O~(x\u2192)}=x\u2192+\u03b4x\u2192\u2192superscript\ud835\udc65\u2192\ud835\udc65:\u2192\ud835\udc67~\ud835\udc42\u2192\ud835\udc65\u2192\ud835\udc67~\ud835\udc42\u2192\ud835\udc65\u2192\ud835\udc65subscript\ud835\udeff\u2192\ud835\udc65\\vec{x^{*}}=\\vec{x}+\\arg\\min\\{\\vec{z}:\\tilde{O}(\\vec{x}+\\vec{z})\\neq\\tilde{O}(\\vec{x})\\}=\\vec{x}+\\delta_{\\vec{x}}over\u2192 start_ARG italic_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_ARG = over\u2192 start_ARG italic_x end_ARG + roman_arg roman_min { over\u2192 start_ARG italic_z end_ARG : over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG + over\u2192 start_ARG italic_z end_ARG ) \u2260 over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG ) } = over\u2192 start_ARG italic_x end_ARG + italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT(3)Examples of adversarial samples can be found inFigure\u00a02. The first row contains legitimate samplesand the second corresponding adversarial samples that aremisclassified. This misclassification must be achieved by adding a minimalperturbation \u03b4x\u2192\ud835\udeff\u2192\ud835\udc65\\delta\\vec{x}italic_\u03b4 over\u2192 start_ARG italic_x end_ARG so as to evade human detection. Even with totalknowledge of the architecture used to train model O\ud835\udc42Oitalic_O and itsparameters resulting from training, finding such a minimal perturbation is nottrivial, as properties of DNNs preclude the optimizationproblem from being linear or convex. This is exacerbated by our threatmodel: removing knowledge of model O\ud835\udc42Oitalic_O\u2019s architecture and training data makes it harder to find a perturbation such that O~(x\u2192+\u03b4x\u2192)\u2260O~(x\u2192)~\ud835\udc42\u2192\ud835\udc65\ud835\udeff\u2192\ud835\udc65~\ud835\udc42\u2192\ud835\udc65\\tilde{O}(\\vec{x}+\\delta\\vec{x})\\neq\\tilde{O}(\\vec{x})over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG + italic_\u03b4 over\u2192 start_ARG italic_x end_ARG ) \u2260 over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG ) holds.", "In Appendix C, we give a presentation of attacks conducted in related threat models\u2014with stronger assumptions.", "We introduce our black-box attack.As stated in Section\u00a03, the adversary wants to craftinputs misclassified by the ML model using the solecapability of accessing the label O~(x\u2192)~\ud835\udc42\u2192\ud835\udc65\\tilde{O}(\\vec{x})over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG ) assigned by classifier for any choseninput x\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG. The strategy is to learn asubstitute for the target model using a synthetic dataset generated by the adversary and labeled by observing the oracle output. Then, adversarial examples are crafted using this substitute. We expect the target DNN to misclassify them due to transferabilitybetween architectures\u00a0[14, 4]", "To understand the difficulty of conducting the attack under this threat model,recall Equation\u00a03 formalizing the adversarial goal offinding a minimal perturbation that forcesthe targeted oracle to misclassify. A closed form solution cannot be found when the target is a non-convex ML model: e.g., a DNN.The basis for most adversarialattacks\u00a0[4, 9, 14]is to approximate its solutionusing gradient-based optimization on functions defined by a DNN.Because evaluating these functions and their gradients requires knowledgeof the DNN architecture and parameters, such an attack is not possibleunder our black-box scenario.It was shown that adversaries with accessto an independently collected labeled training set from the same population distributionthan the oracle could train a model with a different architecture and use it as a substitute\u00a0[14]:adversarial examples designed to manipulate the substituteare often misclassified by the targeted model.However, many modern machine learning systems require large andexpensive training sets for training. For instance,we consider models trained with several tens of thousandsof labeled examples. This makes attacks basedon this paradigm unfeasible for adversaries without large labeled datasets.", "In this paper, we show black-box attacks can be accomplished ata much lower cost, without labeling an independent trainingset.In our approach, to enable the adversary to train a substitute model without a real labeled dataset, we use the target DNN as an oracle to construct a synthetic dataset. The inputs are synthetically generated and the outputs are labels observed from the oracle.Using this synthetic dataset,the attacker builds an approximation F\ud835\udc39Fitalic_F of the model O\ud835\udc42Oitalic_Olearned by the oracle. This substitute network F\ud835\udc39Fitalic_F is then used to craft adversarial samples misclassifiedby F\ud835\udc39Fitalic_FIndeed, with its full knowledge of the substitute DNN F\ud835\udc39Fitalic_F parameters, the adversary can use one of the previouslydescribed attacks\u00a0[4, 9] to craftadversarial samples misclassified by F\ud835\udc39Fitalic_F. As long as the transferabilityproperty holds between F\ud835\udc39Fitalic_F and O\ud835\udc42Oitalic_O, adversarial samples crafted for F\ud835\udc39Fitalic_F willalso be misclassified by O\ud835\udc42Oitalic_O. This leads us to propose the followingstrategy:1.Substitute Model Training: the attacker queries the oracle with synthetic inputs selected by a Jacobian-based heuristic to build a model F\ud835\udc39Fitalic_F approximating the oracle model O\ud835\udc42Oitalic_O\u2019s decision boundaries.2.Adversarial Sample Crafting: the attacker uses substitute network F\ud835\udc39Fitalic_F to craft adversarial samples, which are then misclassified by oracle O\ud835\udc42Oitalic_O due to the transferability of adversarial samples.", "Training a substitute model F\ud835\udc39Fitalic_F approximating oracle O\ud835\udc42Oitalic_O is challenging because we must: (1) select an architecture for our substitute without knowledge of thetargeted oracle\u2019s architecture, and (2) limit the number of queriesmade to the oracle in order to ensure that the approach is tractable. Ourapproach, illustrated in Figure\u00a03, overcomes these challenges mainly by introducing a synthetic data generation technique, the Jacobian-based DatasetAugmentation. We emphasize that this technique is notdesigned to maximize the substitute DNN\u2019s accuracy but rather ensure that itapproximates the oracle\u2019s decision boundaries with few label queries.", "Substitute Architecture: This factor is not the mostlimiting as the adversary must at least have some partial knowledge of theoracle input (e.g., images, text) andexpected output (e.g., classification). The adversary can thus usean architecture adapted to the input-output relation. For instance, aconvolutional neural network is suitable for image classification. Furthermore,we show in Section\u00a06 that the type, number, and size of layers usedin the substitute DNN have relatively littleimpact on the success of the attack. Adversaries can also consider performingan architecture exploration and train several substitute models beforeselecting the one yielding the highest attack success.", "Generating a Synthetic Dataset: To better understandthe need for synthetic data, note that we could potentially make an infinite number of queriesto obtain the oracle\u2019s output O(x\u2192)\ud835\udc42\u2192\ud835\udc65O(\\vec{x})italic_O ( over\u2192 start_ARG italic_x end_ARG ) for any input x\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG belonging tothe input domain. This would provide us with a copy of the oracle.However, this is simply not tractable: consider a DNN with M\ud835\udc40Mitalic_M inputcomponents, each taking discrete values among a set of K\ud835\udc3eKitalic_K possible values, thenumber of possible inputs to be queried is KMsuperscript\ud835\udc3e\ud835\udc40K^{M}italic_K start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT. The intractability is evenmore apparent for inputs in the continuous domain. Furthermore, making a largenumber of queries renders the adversarial behavior easy to detect.", "A natural alternative is to resort to randomly selecting additionalpoints to be queried. For instance, we tried using Gaussian noise to selectpoints on which to train substitutes. However, the resulting modelswere not able to learn by querying the oracle. This is likely due tonoise not being representative of the input distribution.To address this issue, we thus introduce a heuristic efficientlyexploring the input domain and, as shown inSections\u00a05 and\u00a06, drastically limits thenumber of oracle queries. Furthermore, our technique also ensures that thesubstitute DNN is an approximation of the targeted DNN i.e. itlearns similar decision boundaries.", "The heuristic used to generate synthetic training inputs is based on identifying directions in which the model\u2019s output isvarying, around an initial set of training points. Such directionsintuitively require more input-output pairsto capture the output variations of the target DNN O\ud835\udc42Oitalic_O. Therefore, to get asubstitute DNN accurately approximating the oracle\u2019s decision boundaries,the heuristic prioritizes these samples when querying the oracle for labels. These directions are identified with the substitute DNN\u2019s Jacobian matrix JFsubscript\ud835\udc3d\ud835\udc39J_{F}italic_J start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT, which isevaluated at several input points x\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG (how these points are chosen isdescribed below). Precisely, the adversary evaluates the sign of the Jacobian matrix dimension corresponding to the label assigned to input x\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG by the oracle:sgn\u2061(JF(x\u2192)[O~(x\u2192)])sgnsubscript\ud835\udc3d\ud835\udc39\u2192\ud835\udc65delimited-[]~\ud835\udc42\u2192\ud835\udc65\\operatorname{sgn}\\left(J_{F}(\\vec{x})[\\tilde{O}(\\vec{x})]\\right)roman_sgn ( italic_J start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ( over\u2192 start_ARG italic_x end_ARG ) [ over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG ) ] ). To obtain a new synthetic training point, a term \u03bb\u22c5sgn\u2061(JF(x\u2192)[O~(x\u2192)])\u22c5\ud835\udf06sgnsubscript\ud835\udc3d\ud835\udc39\u2192\ud835\udc65delimited-[]~\ud835\udc42\u2192\ud835\udc65\\lambda\\cdot\\operatorname{sgn}\\left(J_{F}(\\vec{x})[\\tilde{O}(\\vec{x})]\\right)italic_\u03bb \u22c5 roman_sgn ( italic_J start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ( over\u2192 start_ARG italic_x end_ARG ) [ over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG ) ] ) is added to the original point x\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG. We name this technique Jacobian-basedDataset Augmentation. We base our substitute training algorithm on the idea ofiteratively refining the model in directions identified using the Jacobian.", "Substitute DNN Training Algorithm: We now describe the five-step training procedure outlined in Algorithm\u00a01:\u2022Initial Collection (1): The adversary collects a very small set S0subscript\ud835\udc460S_{0}italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of inputs representative of the input domain. For instance, if the targeted oracle O\ud835\udc42Oitalic_O classifies handwritten digits, the adversary collects 10 images of each digit 0 through 9. We show in Section\u00a05 that this set does not necessarily have to come from the distribution from which the targeted oracle was trained.\u2022Architecture Selection (2): The adversary selects an architecture to be trained as the substitute F\ud835\udc39Fitalic_F. Again, this can be done using high-level knowledge of the classification task performed by the oracle (e.g., convolutional networks are appropriate for vision)\u2022Substitute Training: The adversary iteratively trains moreaccurate substitute DNNs F\u03c1subscript\ud835\udc39\ud835\udf0cF_{\\rho}italic_F start_POSTSUBSCRIPT italic_\u03c1 end_POSTSUBSCRIPT by repeating the following for \u03c1\u22080..\u03c1max\\rho\\in 0..\\rho_{max}italic_\u03c1 \u2208 0 . . italic_\u03c1 start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT:\u2013Labeling (3): By querying for the labels O~(x\u2192)~\ud835\udc42\u2192\ud835\udc65\\tilde{O}(\\vec{x})over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG ) output by oracle O\ud835\udc42Oitalic_O, the adversary labels each sample x\u2192\u2208S\u03c1\u2192\ud835\udc65subscript\ud835\udc46\ud835\udf0c\\vec{x}\\in S_{\\rho}over\u2192 start_ARG italic_x end_ARG \u2208 italic_S start_POSTSUBSCRIPT italic_\u03c1 end_POSTSUBSCRIPT in its initial substitute training set S\u03c1subscript\ud835\udc46\ud835\udf0cS_{\\rho}italic_S start_POSTSUBSCRIPT italic_\u03c1 end_POSTSUBSCRIPT.\u2013Training (4): The adversary trains the architecture chosen at step (2) using substitute training set S\u03c1subscript\ud835\udc46\ud835\udf0cS_{\\rho}italic_S start_POSTSUBSCRIPT italic_\u03c1 end_POSTSUBSCRIPT in conjunction with classical training techniques.\u2013Augmentation (5): The adversary applies our augmentation technique on the initial substitute training set S\u03c1subscript\ud835\udc46\ud835\udf0cS_{\\rho}italic_S start_POSTSUBSCRIPT italic_\u03c1 end_POSTSUBSCRIPT to produce a larger substitute training set S\u03c1+1subscript\ud835\udc46\ud835\udf0c1S_{\\rho+1}italic_S start_POSTSUBSCRIPT italic_\u03c1 + 1 end_POSTSUBSCRIPT with more synthetic training points. This new training set better represents the model\u2019s decision boundaries. The adversary repeats steps (3) and (4) with the augmented set S\u03c1+1subscript\ud835\udc46\ud835\udf0c1S_{\\rho+1}italic_S start_POSTSUBSCRIPT italic_\u03c1 + 1 end_POSTSUBSCRIPT.Step (3) is repeated several times to increase the substitute DNN\u2019s accuracy and the similarity of its decision boundaries with the oracle. We introduce the term substitute training epoch, indexed with \u03c1\ud835\udf0c\\rhoitalic_\u03c1, to refer to each iteration performed. This leads to this formalization of the Jacobian-based Dataset Augmentation performed at step (5) of our substitute training algorithm to find more synthetic training points:S\u03c1+1={x\u2192+\u03bb\u22c5sgn\u2061(JF[O~(x\u2192)]):x\u2192\u2208S\u03c1}\u222aS\u03c1subscript\ud835\udc46\ud835\udf0c1conditional-set\u2192\ud835\udc65\u22c5\ud835\udf06sgnsubscript\ud835\udc3d\ud835\udc39delimited-[]~\ud835\udc42\u2192\ud835\udc65\u2192\ud835\udc65subscript\ud835\udc46\ud835\udf0csubscript\ud835\udc46\ud835\udf0cS_{\\rho+1}=\\{\\vec{x}+\\lambda\\cdot\\operatorname{sgn}(J_{F}[\\tilde{O}(\\vec{x})]):\\vec{x}\\in S_{\\rho}\\}\\cup S_{\\rho}italic_S start_POSTSUBSCRIPT italic_\u03c1 + 1 end_POSTSUBSCRIPT = { over\u2192 start_ARG italic_x end_ARG + italic_\u03bb \u22c5 roman_sgn ( italic_J start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT [ over~ start_ARG italic_O end_ARG ( over\u2192 start_ARG italic_x end_ARG ) ] ) : over\u2192 start_ARG italic_x end_ARG \u2208 italic_S start_POSTSUBSCRIPT italic_\u03c1 end_POSTSUBSCRIPT } \u222a italic_S start_POSTSUBSCRIPT italic_\u03c1 end_POSTSUBSCRIPT(4)where \u03bb\ud835\udf06\\lambdaitalic_\u03bb is a parameter of the augmentation: it defines the size of the step taken in the sensitive direction identified by the Jacobian matrix to augment the set S\u03c1subscript\ud835\udc46\ud835\udf0cS_{\\rho}italic_S start_POSTSUBSCRIPT italic_\u03c1 end_POSTSUBSCRIPT into S\u03c1+1subscript\ud835\udc46\ud835\udf0c1S_{\\rho+1}italic_S start_POSTSUBSCRIPT italic_\u03c1 + 1 end_POSTSUBSCRIPT.", "Once the adversary trained a substitute DNN, it uses it to craft adversarial samples. This is performed by implementing twopreviously introduced approaches described in\u00a0[4, 9]. We provide an overview of the two approaches, namely the Goodfellow et al. algorithm and thePapernot et al. algorithm. Both techniques share a similar intuition ofevaluating the model\u2019s sensitivity to input modifications in order to select asmall perturbation achieving the misclassification goal111Our attack can be implemented with other adversarial example algorithms. We focus on these two in our evaluation..", "Goodfellow et al. algorithm: This algorithm is also known as thefast gradient sign method\u00a0[4].Given a model F\ud835\udc39Fitalic_F with an associatedcost function c(F,x\u2192,y)\ud835\udc50\ud835\udc39\u2192\ud835\udc65\ud835\udc66c(F,\\vec{x},y)italic_c ( italic_F , over\u2192 start_ARG italic_x end_ARG , italic_y ), the adversary crafts an adversarial samplex*\u2192=x\u2192+\u03b4x\u2192\u2192superscript\ud835\udc65\u2192\ud835\udc65subscript\ud835\udeff\u2192\ud835\udc65\\vec{x^{*}}=\\vec{x}+\\delta_{\\vec{x}}over\u2192 start_ARG italic_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_ARG = over\u2192 start_ARG italic_x end_ARG + italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT for a given legitimate sample x\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG bycomputing the following perturbation:\u03b4x\u2192=\u03b5sgn\u2061(\u2207x\u2192c(F,x\u2192,y))subscript\ud835\udeff\u2192\ud835\udc65\ud835\udf00sgnsubscript\u2207\u2192\ud835\udc65\ud835\udc50\ud835\udc39\u2192\ud835\udc65\ud835\udc66\\delta_{\\vec{x}}=\\varepsilon\\operatorname{sgn}(\\nabla_{\\vec{x}}c(F,\\vec{x},y))italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT = italic_\u03b5 roman_sgn ( \u2207 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT italic_c ( italic_F , over\u2192 start_ARG italic_x end_ARG , italic_y ) )(5)where perturbation sgn\u2061(\u2207x\u2192c(F,x\u2192,y))sgnsubscript\u2207\u2192\ud835\udc65\ud835\udc50\ud835\udc39\u2192\ud835\udc65\ud835\udc66\\operatorname{sgn}(\\nabla_{\\vec{x}}c(F,\\vec{x},y))roman_sgn ( \u2207 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT italic_c ( italic_F , over\u2192 start_ARG italic_x end_ARG , italic_y ) ) is the sign of themodel\u2019s cost function222As described here, the method causes simple misclassification.It has been extended to achieve chosen target classes.gradient.The cost gradient is computed with respect tox\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG using sample x\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG and label y\ud835\udc66yitalic_y as inputs. The value of theinput variation parameter \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 factoring the sign matrix controls theperturbation\u2019s amplitude. Increasing its value increases the likelihood ofx*\u2192\u2192superscript\ud835\udc65\\vec{x^{*}}over\u2192 start_ARG italic_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_ARG being misclassified by model F\ud835\udc39Fitalic_F but on the contrary makesadversarial samples easier to detect by humans. InSection\u00a06, we evaluate the impact of parameter \u03b5\ud835\udf00\\varepsilonitalic_\u03b5on the successfulness of our attack. ", "Papernot et al. algorithm: This algorithm is suitable forsource-target misclassification attacks where adversaries seek to take samplesfrom any legitimate source class to any chosen targetclass\u00a0[9]. Misclassification attacks are a specialcase of source-target misclassifications, where the target class canbe any class different from the legitimate source class. Given model F\ud835\udc39Fitalic_F, theadversary crafts an adversarial sample x*\u2192=x\u2192+\u03b4x\u2192\u2192superscript\ud835\udc65\u2192\ud835\udc65subscript\ud835\udeff\u2192\ud835\udc65\\vec{x^{*}}=\\vec{x}+\\delta_{\\vec{x}}over\u2192 start_ARG italic_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_ARG = over\u2192 start_ARG italic_x end_ARG + italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPTfor a given legitimate sample x\u2192\u2192\ud835\udc65\\vec{x}over\u2192 start_ARG italic_x end_ARG by adding a perturbation\u03b4x\u2192subscript\ud835\udeff\u2192\ud835\udc65\\delta_{\\vec{x}}italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT to a subset of the input components x\u2192isubscript\u2192\ud835\udc65\ud835\udc56\\vec{x}_{i}over\u2192 start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.", "To choose input components forming perturbation \u03b4x\u2192subscript\ud835\udeff\u2192\ud835\udc65\\delta_{\\vec{x}}italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT,components are sorted by decreasing adversarial saliency value. Theadversarial saliency value S(x\u2192,t)[i]\ud835\udc46\u2192\ud835\udc65\ud835\udc61delimited-[]\ud835\udc56S(\\vec{x},t)[i]italic_S ( over\u2192 start_ARG italic_x end_ARG , italic_t ) [ italic_i ] of component i\ud835\udc56iitalic_i for anadversarial target class t\ud835\udc61titalic_t is defined as:S(x\u2192,t)[i]={0\u00a0if\u00a0\u2202Ft\u2202x\u2192i(x\u2192)<0\u00a0or\u00a0\u2211j\u2260t\u2202Fj\u2202x\u2192i(x\u2192)>0\u2202Ft\u2202x\u2192i(x\u2192)|\u2211j\u2260t\u2202Fj\u2202x\u2192i(x\u2192)|\u00a0otherwise\ud835\udc46\u2192\ud835\udc65\ud835\udc61delimited-[]\ud835\udc56cases0\u00a0if\u00a0subscript\ud835\udc39\ud835\udc61subscript\u2192\ud835\udc65\ud835\udc56\u2192\ud835\udc65expectation0\u00a0or\u00a0subscript\ud835\udc57\ud835\udc61subscript\ud835\udc39\ud835\udc57subscript\u2192\ud835\udc65\ud835\udc56\u2192\ud835\udc650subscript\ud835\udc39\ud835\udc61subscript\u2192\ud835\udc65\ud835\udc56\u2192\ud835\udc65subscript\ud835\udc57\ud835\udc61subscript\ud835\udc39\ud835\udc57subscript\u2192\ud835\udc65\ud835\udc56\u2192\ud835\udc65\u00a0otherwiseS(\\vec{x},t)[i]=\\left\\{\\begin{array}[]{c}0\\mbox{ if }\\frac{\\partial F_{t}}{\\partial\\vec{x}_{i}}(\\vec{x})<0\\mbox{ or }\\sum_{j\\neq t}\\frac{\\partial F_{j}}{\\partial\\vec{x}_{i}}(\\vec{x})>0\\\\\\frac{\\partial F_{t}}{\\partial\\vec{x}_{i}}(\\vec{x})\\left|\\sum_{j\\neq t}\\frac{\\partial F_{j}}{\\partial\\vec{x}_{i}}(\\vec{x})\\right|\\mbox{ otherwise}\\end{array}\\right.italic_S ( over\u2192 start_ARG italic_x end_ARG , italic_t ) [ italic_i ] = { start_ARRAY start_ROW start_CELL 0 if divide start_ARG \u2202 italic_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG \u2202 over\u2192 start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ( over\u2192 start_ARG italic_x end_ARG ) < 0 or \u2211 start_POSTSUBSCRIPT italic_j \u2260 italic_t end_POSTSUBSCRIPT divide start_ARG \u2202 italic_F start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG start_ARG \u2202 over\u2192 start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ( over\u2192 start_ARG italic_x end_ARG ) > 0 end_CELL end_ROW start_ROW start_CELL divide start_ARG \u2202 italic_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG \u2202 over\u2192 start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ( over\u2192 start_ARG italic_x end_ARG ) | \u2211 start_POSTSUBSCRIPT italic_j \u2260 italic_t end_POSTSUBSCRIPT divide start_ARG \u2202 italic_F start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG start_ARG \u2202 over\u2192 start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ( over\u2192 start_ARG italic_x end_ARG ) | otherwise end_CELL end_ROW end_ARRAY(6)where matrix JF=[\u2202Fj\u2202x\u2192i]ijsubscript\ud835\udc3d\ud835\udc39subscriptdelimited-[]subscript\ud835\udc39\ud835\udc57subscript\u2192\ud835\udc65\ud835\udc56\ud835\udc56\ud835\udc57J_{F}=\\left[\\frac{\\partial F_{j}}{\\partial\\vec{x}_{i}}\\right]_{ij}italic_J start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = [ divide start_ARG \u2202 italic_F start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG start_ARG \u2202 over\u2192 start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ] start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT isthe model\u2019s Jacobian matrix. Input components i\ud835\udc56iitalic_i are added to perturbation\u03b4x\u2192subscript\ud835\udeff\u2192\ud835\udc65\\delta_{\\vec{x}}italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT in order of decreasing adversarial saliency valueS(x\u2192,t)[i]\ud835\udc46\u2192\ud835\udc65\ud835\udc61delimited-[]\ud835\udc56S(\\vec{x},t)[i]italic_S ( over\u2192 start_ARG italic_x end_ARG , italic_t ) [ italic_i ] until the resulting adversarial sample x*\u2192=x\u2192+\u03b4x\u2192\u2192superscript\ud835\udc65\u2192\ud835\udc65subscript\ud835\udeff\u2192\ud835\udc65\\vec{x^{*}}=\\vec{x}+\\delta_{\\vec{x}}over\u2192 start_ARG italic_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_ARG = over\u2192 start_ARG italic_x end_ARG + italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT is misclassified by F\ud835\udc39Fitalic_F. The perturbation introducedfor each selected input component can vary: greater perturbationreduce the number of components perturbed to achieve misclassification.", "Each algorithm has its benefits and drawbacks. The Goodfellow algorithm iswell suited for fast crafting of many adversarial sampleswith relatively large perturbations thus potentially easier to detect.The Papernot algorithm reducesperturbations at the expense of a greater computing cost.", "We validate our attack against remote and local classifiers. We first apply it to target a DNN remotelyprovided by MetaMind, through their API333The API can beaccessed online at www.metamind.io that allows a user to train classifiers usingdeep learning. The API returns labels produced by the DNN for any given inputbut does not provide access to the DNN. This corresponds to theoracle described in our threat model. We show that:\u2022An adversaryusing our attack can reliably force the DNN trainedusing MetaMind on MNIST\u00a0[7] to misclassify84.24%percent84.2484.24\\%84.24 % of adversarial examples crafted with a perturbation not affecting human recognition.\u2022A second oracle trained locally with the German TrafficSigns Recognition Benchmark (GTSRB)\u00a0[13],can beforced to misclassify more than 64.24%percent64.2464.24\\%64.24 % of altered inputs without affecting human recognition.", "Description of the Oracle: We used the MNIST handwritten digit dataset to train the DNN\u00a0[7]. It comprises 60,0006000060,00060 , 000 training and 10,0001000010,00010 , 000 test images of handwritten digits. The task associated with the dataset isto identify the digit corresponding to each image. Each 28282828x28282828grayscale sample is encoded as a vector of pixel intensities in theinterval [0,1]01[0,1][ 0 , 1 ] and obtained by reading the image pixel matrix row-wise.", "We registered for an API key on MetaMind\u2019s website, which gave us access tothree functionalities: dataset upload, automated model training, and modelprediction querying. We uploaded the 50,0005000050,00050 , 000 samples included in the MNISTtraining set to MetaMind and then used the API to train a classifieron the dataset. We emphasize that training is automated: we have noaccess to the training algorithm, model architecture, or model parameters. Allwe are given is the accuracy of the resulting model, computed by MetaMind usinga validation set created by isolating 10%percent1010\\%10 % of the training samples. Details can be found on MetaMind\u2019swebsite.", "Training took 36 hours to return a classifierwith a 94.97%percent94.9794.97\\%94.97 % accuracy. This performance cannot beimproved as we cannot access or modify the model\u2019s specifications andtraining algorithm. Once training is completed, we could access the modelpredictions, for any input of our choice, through the API. Predictions take theform of a class label.Thiscorresponds to the threat model described inSection\u00a03.", "Initial Substitute Training Sets: First, the adversary collects an initial substitute trainingset. We describe two such sets used to attack the MetaMindoracle:", "\u2022MNIST subset: This initial substitute trainingset is made of 150150150150 samples from the MNIST test set.They differ from those used by the oracle for training as test and trainingsets are distinct. We assume adversaries can collect such a limited sample setunder the threat model described in Section\u00a03 withminimal knowledge of the oracle task: here, handwrittendigit classification.\u2022Handcrafted set: To ensure our resultsdo not stem from similarities between the MNIST test and training sets, wealso consider a handcrafted initial substitute training set. We handcrafted 100100100100 samples by handwriting 10101010 digits for eachclass between 00 and 9999 with alaptop trackpad. We then adapted them to the MNIST format of 28282828x28282828grayscale pixels. Some are shown below.", "Substitute DNN Training: The adversary uses the initial substitute training sets and the oracle to trainsubsitute DNNs. Our substitute architecture A, a standard forimage classification, is describedin Table\u00a013 (cf. appendix).The substitute DNN is trained on ourmachine for 6666 substitute epochs.Duringeach of these 6666 epochs, the model is trained for 10101010 epochsfrom scratch with a learning rate of 10\u22122superscript10210^{-2}10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT and momentum of 0.90.90.90.9. Betweensubstitute epochs, we perform a Jacobian-based dataset augmentation with a stepsize of \u03bb=0.1\ud835\udf060.1\\lambda=0.1italic_\u03bb = 0.1 to generate additional synthetic training data, which we label using the MetaMind oracle.", "The accuracy of the two substitute DNNs is reported inFigure\u00a04. It is computed with the MNISTtest set (minus the 150150150150 samples used in the first initial substitute trainingset). The adversary does not have access to this full test set: wesolely use it to analyze our results. The two substituteDNNs respectively achieve a 81.20%percent81.2081.20\\%81.20 % and 67.00%percent67.0067.00\\%67.00 % accuracy on the MNIST test set after 6666 substitute training epochs. These accuracies fallshort of current state-of-the-art accuracies on this task. However, the adversary has access to a limited number ofsamples (in this case 6,400=100\u00d7266400100superscript266,400=100\\times 2^{6}6 , 400 = 100 \u00d7 2 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT instead of 50,0005000050,00050 , 000 forstate-of-the-art models). Furthermore, the adversarial goal is to craftadversarial samples misclassified by the oracle. Instead of learning asubstitute DNN with optimal accuracy, the adversary is interested inlearning a substitute capable of mimicking the oracle decisionboundaries.", "Adversarial Sample Crafting: Using the substitute DNNs, we then craft adversarial samples using Goodfellow\u2019s algorithm. We decided to use the 10,0001000010,00010 , 000samples from the MNIST test set as our legitimate samples.444Again,adversaries do not need access to the dataset and can use any legitimate sampleof their choice to craft adversarial samples. We use it in order toshow that expected inputs can be misclassified on a large scale. We evaluatesample crafting using two metrics: success rate andtransferability. The success rate is the proportion ofadversarial samples misclassified by the substitute DNN.Our goal is to verify whether these samples are also misclassified bythe oracle or not. Therefore, the transferability of adversarial samplesrefers to the oracle misclassification rate of adversarial samples craftedusing the substitute DNN.", "Figure\u00a05 details both metricsfor each substitute DNN and for several values of the input variation\u03b5\ud835\udf00\\varepsilonitalic_\u03b5 (cf. Equation\u00a05). Transferability reaches 84.24%percent84.2484.24\\%84.24 % for the first substituteDNN and 78.72%percent78.7278.72\\%78.72 % for the second, with input variations of\u03b5=0.3\ud835\udf000.3\\varepsilon=0.3italic_\u03b5 = 0.3. Our attack strategy is thus effectively able to severelydamage the output integrity of the MetaMind oracle. Using thesubstitute training set handcrafted by the adversary\u00a0limits the transferability of adversarial sampleswhen compared to the substitute set extracted from MNIST data,for all input variations except \u03b5=0.2\ud835\udf000.2\\varepsilon=0.2italic_\u03b5 = 0.2.Yet, the transferability of both substitutes is similar, corroborating that ourattack can be executed without access to any of the oracle\u2019s training data.", "To analyze the labels assigned by the MetaMind oracle, we plot confusionmatrices for adversarial samples crafted using the first substitute DNN with4444 values of \u03b5\ud835\udf00\\varepsilonitalic_\u03b5. InFigure\u00a06, rates on the diagonal indicatethe proportion of samples correctly classified by the oracle for each of the10101010 classes. Off-diagonal values are the proportion of samplesmisclassified in a wrong class. For instance, cell (8,3)83(8,3)( 8 , 3 ) in the third matrixindicates that 89%percent8989\\%89 % instances of a 3333 are classified as a 8888 by the oraclewhen perturbed with an input variation of \u03b5=0.25\ud835\udf000.25\\varepsilon=0.25italic_\u03b5 = 0.25.Confusion matrices converge to most samples being classified as4444s and 8888s as \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 increases. This could be due to DNNs more easily classifying inputs inthese classes\u00a0[9].", "We now validate our attack on a different dataset, usingan oracle trained locally to recognize traffic signs on the GTSRBdataset. The attack achieves higher transferability rates at lower distortionscompared to the MNIST oracle.", "Oracle Description: The GTSRB dataset is an image collectionconsisting of 43 traffic signs\u00a0[13]. Images vary in size and areRGB-encoded. To simplify, we resize images to 32323232x32323232pixels, recenter them by subtracting the mean component, and rescalethem by factoring their standard deviations out. We keep 35,0003500035,00035 , 000 images forour training set and 4,00040004,0004 , 000 for our validation set (out of the 39,2093920939,20939 , 209available), and 10,0001000010,00010 , 000 for our test set (out of 12,6301263012,63012 , 630). We train theoracle on our machine, using the DNN B fromTable\u00a013 (cf. appendix), for 50505050 epochs with a learning rateof 10\u22122superscript10210^{-2}10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT and a momentum of 0.90.90.90.9 (both decayed by 0.50.50.50.5 every 10101010epochs).", "Substitute DNN Training: The adversary uses twoinitial substitute training sets extracted from the GTSRB test set. Thefirst includes the first 1,00010001,0001 , 000 samples and the second thefirst 500500500500. The number of initial samples is higher than forMNIST substitutes as inputs have a higher dimensionality. We trainthree substitute architectures C, D, and E (cf.Table\u00a013) using the oracle for 6666 substitutetraining epochs with a Jacobian-based dataset augmentation parameter of\u03bb=0.1\ud835\udf060.1\\lambda=0.1italic_\u03bb = 0.1. Substitute C and E where trained with the 1,00010001,0001 , 000 sampleinitial substitute training set and achieve a 71.42%percent71.4271.42\\%71.42 % accuracy. Substitute Dwas trained with the initial set of 500500500500 samples. Its accuracy of 60.12%percent60.1260.12\\%60.12 % islower than C and E.", "Adversarial Crafting: We use Goodfellow\u2019s algorithmwith \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 between 0.010.010.010.01 and 0.50.50.50.5 to craftadversarial samples from the test set. Resultsare shown in Figure\u00a07. Adversarial samplescrafted with variations \u03b5<0.3\ud835\udf000.3\\varepsilon<0.3italic_\u03b5 < 0.3 are more transferable thanthose crafted with the same \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 for MNIST models. This is likely due to the higherinput dimensionality\u20143,07230723,0723 , 072 components instead of 784784784784\u2014which means almost 4444 timesmore perturbation is applied with the same \u03b5\ud835\udf00\\varepsilonitalic_\u03b5.Nevertheless, with success rates higher than 98.98%percent98.9898.98\\%98.98 % and transferabilityrates ranging from 64.24%percent64.2464.24\\%64.24 % to 69.03%percent69.0369.03\\%69.03 % for \u03b5=0.3\ud835\udf000.3\\varepsilon=0.3italic_\u03b5 = 0.3, which is hardto distinguish for humans, the attack is successful. Thetransferability of adversarial samples crafted using substitute DNN D iscomparable or higher than corresponding samples for DNNs C and E,despite being lessaccurate (trained with less samples). This emphasizes that there is no strong correlation between substitute accuracy and transferability.", "Having shown in Section\u00a05 that an adversary can force anMNIST oracle from MetaMind, and a GTSRB oracle trained locally, tomisclassify inputs,we now perform a parameter space exploration of both attack steps\u2013thesubstitute DNN training and the adversarial sample crafting. We explore the following questions: \u201c(1) How can substitutetraining be fine-tuned to improve adversarial sample transferability?\u201d and (2)\u201cFor each adversarial sample crafting strategies, which parameters optimizetransferability?\u201d. We found that:\u2022In Section\u00a06.1, we show that the choice of substitute DNN architecture (number of layers, size,activation function, type) has a limited impact on adversarial sampletransferability. Increasing the number of epochs, after the substitute DNN hasreached an asymptotic accuracy, does not improve adversarial sampletransferability.\u2022At comparable input perturbation magnitude, the Goodfellow and Papernotalgorithms have similar transferability rates (see Section\u00a06.2).In this section, we use an oracle trained locally to limit querying of theMetaMind API. We train architecture A (cf. Table\u00a013)for 50505050 epochs with a learning parameter 10\u22122superscript10210^{-2}10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT and a momentum 0.90.90.90.9 (bothdecayed by 0.50.50.50.5 every 10101010 epochs).", "We first seek to quantify the impact of substitute training algorithm parameters on adversarial sample transferability and introduce a refinement to reduce oracle querying.", "Choosing an Architecture: We train substitute DNNs A and F to M (cf.Table\u00a013) using 150150150150 samples from the MNIST test set as the substitute training set. During eachof the 6666 substitute training epochs, the DNN is trained for 5555 epochsfrom scratch. Between epochs, synthetic data is added to the training set using Jacobian-baseddataset augmentations with step \u03bb=0.1\ud835\udf060.1\\lambda=0.1italic_\u03bb = 0.1. The substitutearchitectures differ from the oracle\u2019s by the type, number, and sizeof layers. In Table\u00a01,we report the accuracy of each architecture after 2222 and 6666 substitute training epochs, as well as the adversarialsample transferability after 6 epochs. Adversarial samples are crafted using the Goodfellow algorithm with an inputvariation of \u03b5=0.4\ud835\udf000.4\\varepsilon=0.4italic_\u03b5 = 0.4 (which we justify later). The last column ofTable\u00a01shows that the choice of architecture has a limitedimpact on adversarial sample transferability, and therefore on the attacksuccess. The most important transferability drop follows from removing allconvolutional layers. Changing the hidden layer activation function fromrectified linear to a sigmoid does not impact transferability significantly.", "Choosing the number of substitute epochs: Anothertunable parameter is the number ofepochs for which substitute DNNs are trained. Intuitively, one would hypothesize thatthe longer we train the substitute, the more samples labeled using theoracle are included in the substitute training set, thus the higher thetransferability of adversarial samples will be. This intuition is confirmedonly partially by our experiments on substitute DNN A. We find that forfor input variations \u03b5\u22640.3\ud835\udf000.3\\varepsilon\\leq 0.3italic_\u03b5 \u2264 0.3, the transferability is slightlyimproved by a rate between +3%percent3+3\\%+ 3 % to +9%percent9+9\\%+ 9 %, but for variations\u03b5\u22650.4\ud835\udf000.4\\varepsilon\\geq 0.4italic_\u03b5 \u2265 0.4, the transferability is slightly degraded by less than1%percent11\\%1 %.", "Setting the step size: We trainedsubstitute A using different Jacobian-baseddataset augmentation step sizes \u03bb\ud835\udf06\\lambdaitalic_\u03bb. Increasing or decreasing the step size (from \u03bb=0.1\ud835\udf060.1\\lambda=0.1italic_\u03bb = 0.1 used in therest of this paper) does not modify the substitute accuracy by more than 3%percent33\\%3 %. Larger step sizes decrease convergence stability while smaller values yield slower convergence.However, increasing step size \u03bb\ud835\udf06\\lambdaitalic_\u03bb negativelyimpacts adversarial sample transferability : for instance with a step sizeof 0.30.30.30.3 compared to 0.10.10.10.1, the transferability rate for \u03b5=0.25\ud835\udf000.25\\varepsilon=0.25italic_\u03b5 = 0.25is 10.82%percent10.8210.82\\%10.82 % instead of 22.35%percent22.3522.35\\%22.35 % and for \u03b5=0.5\ud835\udf000.5\\varepsilon=0.5italic_\u03b5 = 0.5, 82.07%percent82.0782.07\\%82.07 %instead of 85.22%percent85.2285.22\\%85.22 %.", "However, having the step size periodically alternating between positive and negativevalues improves the quality of the oracle approximation made by the substitute. This could be explained by the fact that after a few substitute epochs, synthetic inputs are outside of the input domain and are thus clipped to produce an acceptable input.We introduce an iteration period \u03c4\ud835\udf0f\\tauitalic_\u03c4after which the step size is multiplied by \u221211-1- 1. Thus, the step size\u03bb\ud835\udf06\\lambdaitalic_\u03bb is now replaced by:\u03bb\u03c1=\u03bb\u22c5(\u22121)\u230a\u03c1\u03c4\u230bsubscript\ud835\udf06\ud835\udf0c\u22c5\ud835\udf06superscript1\ud835\udf0c\ud835\udf0f\\lambda_{\\rho}=\\lambda\\cdot(-1)^{\\left\\lfloor\\frac{\\rho}{\\tau}\\right\\rfloor}italic_\u03bb start_POSTSUBSCRIPT italic_\u03c1 end_POSTSUBSCRIPT = italic_\u03bb \u22c5 ( - 1 ) start_POSTSUPERSCRIPT \u230a divide start_ARG italic_\u03c1 end_ARG start_ARG italic_\u03c4 end_ARG \u230b end_POSTSUPERSCRIPT(7)where \u03c4\ud835\udf0f\\tauitalic_\u03c4 is set to be the number of epochs after which the Jacobian-baseddataset augmentation does not lead any substantial improvement in thesubstitute. A grid search can also be performed to find an optimal value forthe period \u03c4\ud835\udf0f\\tauitalic_\u03c4. We also experimented with a decreasing grid step amplitude\u03bb\ud835\udf06\\lambdaitalic_\u03bb, but did not find that it yielded substantial improvements.", "Reducing Oracle Querying: We apply reservoir sampling\u00a0[16]to reduce the number of queries made to the oracle. This is usefulwhen learning substitutes in realistic environments, or when interacting with paid APIs, where the number of label queries an adversary can make withoutexceeding a quota or being detected by a defender islimited. Reservoirsampling is a technique that randomly select \u03ba\ud835\udf05\\kappaitalic_\u03ba samples from a list of samples.Thetotal number of samples in the list can be both very large and unknown.We use it to select \u03ba\ud835\udf05\\kappaitalic_\u03ba new inputsbefore a Jacobian-based dataset augmentation. This preventsthe exponential growth of queries made to the oracle at each augmentation. At iterations \u03c1>\u03c3\ud835\udf0c\ud835\udf0e\\rho>\\sigmaitalic_\u03c1 > italic_\u03c3 (the first \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 iterations areperformed normally), when considering the previous set S\u03c1\u22121subscript\ud835\udc46\ud835\udf0c1S_{\\rho-1}italic_S start_POSTSUBSCRIPT italic_\u03c1 - 1 end_POSTSUBSCRIPT ofsubstitute training inputs, we select \u03ba\ud835\udf05\\kappaitalic_\u03ba inputs from S\u03c1\u22121subscript\ud835\udc46\ud835\udf0c1S_{\\rho-1}italic_S start_POSTSUBSCRIPT italic_\u03c1 - 1 end_POSTSUBSCRIPT to beaugmented in S\u03c1subscript\ud835\udc46\ud835\udf0cS_{\\rho}italic_S start_POSTSUBSCRIPT italic_\u03c1 end_POSTSUBSCRIPT. Using reservoirsampling ensures that each input in S\u03c1\u22121subscript\ud835\udc46\ud835\udf0c1S_{\\rho-1}italic_S start_POSTSUBSCRIPT italic_\u03c1 - 1 end_POSTSUBSCRIPT has an equal probability1|S\u03c1\u22121|1subscript\ud835\udc46\ud835\udf0c1\\frac{1}{\\left|S_{\\rho-1}\\right|}divide start_ARG 1 end_ARG start_ARG | italic_S start_POSTSUBSCRIPT italic_\u03c1 - 1 end_POSTSUBSCRIPT | end_ARG to be augmented in S\u03c1subscript\ud835\udc46\ud835\udf0cS_{\\rho}italic_S start_POSTSUBSCRIPT italic_\u03c1 end_POSTSUBSCRIPT. The numberof queries made to the oracle is reduced from n\u22c52\u03c1\u22c5\ud835\udc5bsuperscript2\ud835\udf0cn\\cdot 2^{\\rho}italic_n \u22c5 2 start_POSTSUPERSCRIPT italic_\u03c1 end_POSTSUPERSCRIPT for the vanillaJacobian-based augmentation to n\u22c52\u03c3+\u03ba\u22c5(\u03c1\u2212\u03c3)\u22c5\ud835\udc5bsuperscript2\ud835\udf0e\u22c5\ud835\udf05\ud835\udf0c\ud835\udf0en\\cdot 2^{\\sigma}+\\kappa\\cdot(\\rho-\\sigma)italic_n \u22c5 2 start_POSTSUPERSCRIPT italic_\u03c3 end_POSTSUPERSCRIPT + italic_\u03ba \u22c5 ( italic_\u03c1 - italic_\u03c3 ) with reservoir sampling. In Section\u00a07, we show that using reservoir sampling to reducethe number of synthetic training inputs does not significantly degrade the substitute accuracy.", "We compare the transferability ofadversarial samples produced by each algorithm introducedpreviously\u00a0[4, 9], to elect the strongest technique under our threat model.", "Goodfellow\u2019s algorithm: Recall from Equation\u00a05 the perturbation computed in the Goodfellow attack.Its only parameter is the variation \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 added inthe direction of the gradient sign. We use the same architecture set asbefore to quantify the impact of \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 onadversarial sample transferability.In Figure\u00a08, architecture A outperformsall others: it is a copy of the oracle\u2019s and acts as a baseline. Otherarchitectures have asymptotic transferability rates ranging between 72.24%percent72.2472.24\\%72.24 %and 80.21%percent80.2180.21\\%80.21 %, confirming that the substitute architecture choice hasa limited impact on transferability. Increasing the value of \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 above0.40.40.40.4 yields little improvement in transferability and should be avoidedto guarantee indistinguishability of adversarial samples to humans.", "Papernot\u2019s algorithm: This algorithm is fine-tuned bytwo parameters: the maximum distortion \u03a5\u03a5\\Upsilonroman_\u03a5 and the inputvariation \u03b5\ud835\udf00\\varepsilonitalic_\u03b5. The maximum distortion555In\u00a0[9], the algorithm stopped perturbing when the input reached the target class. Here, we forcethe algorithm to continue perturbing until it changed\u03a5\u03a5\\Upsilonroman_\u03a5 input components. defines the number of input components that are altered inperturbation \u03b4x\u2192subscript\ud835\udeff\u2192\ud835\udc65\\delta_{\\vec{x}}italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT. The input variation,similarly to the Goodfellow algorithm, controls the amount of change induced toaltered input components.", "We first evaluate the impact of the maximum distortion \u03a5\u03a5\\Upsilonroman_\u03a5 onadversarial sample transferability. For now, components selected to beperturbed are increased by \u03b5=1\ud835\udf001\\varepsilon=1italic_\u03b5 = 1. Intuitively,increasing the maximum distortion makes adversarial samples moretransferable. Higher distortionsincrease the misclassification confidence of the substitute DNN, andalso increases the likelihood of the oracle misclassifying the same sample. These results are reported inFigure\u00a09. Increasing distortion \u03a5\u03a5\\Upsilonroman_\u03a5 from7.14%percent7.147.14\\%7.14 % to 28.57%percent28.5728.57\\%28.57 % improves transferability: at a 7.14%percent7.147.14\\%7.14 % distortion, theaverage transferability across all architectures is 14.70%percent14.7014.70\\%14.70 %whereas at a 28.57%percent28.5728.57\\%28.57 % distortion, the average transferability is at 55.53%percent55.5355.53\\%55.53 %.", "We now quantify the impact of the variation\u03b5\ud835\udf00\\varepsilonitalic_\u03b5 introduced to each input component selected in\u03b4x\u2192subscript\ud835\udeff\u2192\ud835\udc65\\delta_{\\vec{x}}italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT. We find that reducing the input variationfrom 1111 to 0.70.70.70.7significantly degrades adversarial sample transferability,approximatively by a factor of 2 (cf. Figure\u00a010). This is explained by the fixeddistortion parameter \u03a5\u03a5\\Upsilonroman_\u03a5, which prevents the crafting algorithmfrom increasing the number of components altered to compensate for the reduced effectiveness yielded by the smaller \u03b5\ud835\udf00\\varepsilonitalic_\u03b5.", "Comparing Crafting Algorithms: To compare the two crafting strategies and their differing perturbation styles fairly,we compare their success rate given a fixed L1 norm of the introduced perturbation \u03b4x\u2192subscript\ud835\udeff\u2192\ud835\udc65\\delta_{\\vec{x}}italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT, which can be defined as:\u2016\u03b4x\u2192\u20161=\u03b5\u22c5\u2016\u03b4x\u2192\u20160subscriptnormsubscript\ud835\udeff\u2192\ud835\udc651\u22c5\ud835\udf00subscriptnormsubscript\ud835\udeff\u2192\ud835\udc650\\|\\delta_{\\vec{x}}\\|_{1}=\\varepsilon\\cdot\\|\\delta_{\\vec{x}}\\|_{0}\u2225 italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_\u03b5 \u22c5 \u2225 italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT(8)where \u2016\u03b4x\u2192\u20160subscriptnormsubscript\ud835\udeff\u2192\ud835\udc650\\|\\delta_{\\vec{x}}\\|_{0}\u2225 italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is the number of input componentsselected in the perturbation \u03b4x\u2192subscript\ud835\udeff\u2192\ud835\udc65\\delta_{\\vec{x}}italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT,and \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 the inputvariation introduced to each component perturbed.For the Goodfellow algorithm, we alwayshave \u2016\u03b4x\u2192\u20160=1subscriptnormsubscript\ud835\udeff\u2192\ud835\udc6501\\|\\delta_{\\vec{x}}\\|_{0}=1\u2225 italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 1, whereas for the Papernot algorithm, values varyfor both \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 and \u2016\u03b4x\u2192\u20160subscriptnormsubscript\ud835\udeff\u2192\ud835\udc650\\|\\delta_{\\vec{x}}\\|_{0}\u2225 italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. For instance,\u2016\u03b4x\u2192\u20161=0.4subscriptnormsubscript\ud835\udeff\u2192\ud835\udc6510.4\\|\\delta_{\\vec{x}}\\|_{1}=0.4\u2225 italic_\u03b4 start_POSTSUBSCRIPT over\u2192 start_ARG italic_x end_ARG end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.4 corresponds to aGoodfellow algorithm with \u03b5=0.4\ud835\udf000.4\\varepsilon=0.4italic_\u03b5 = 0.4 and a Papernot algorithm with\u03b5=1\ud835\udf001\\varepsilon=1italic_\u03b5 = 1 and \u03a5=40%\u03a5percent40\\Upsilon=40\\%roman_\u03a5 = 40 %. Corresponding transferabilityrates can be found inTable\u00a01 andFigure\u00a09 for our running set of architectures.Performances are comparable with some DNNs performing better with one algorithm and others with the other.Thus, the choice of algorithm depends on acceptable perturbations:e.g., all features perturbed a little vs. few features perturbed a lot.Indeed, the Goodfellow algorithm gives more control on \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 while thePapernot algorithm gives more control on \u03a5\u03a5\\Upsilonroman_\u03a5.", "So far, all substitutes and oracles considered were learned with DNNs.However, no part of the attack limits its applicability to other ML techniques.For instance, we show that the attack generalizes to non-differentiable target oracles like decision trees.As pointed out by Equation\u00a04, the only limitationis placed on the substitute: it must model a differentiablefunction\u2014to allow for synthetic data to be generated with its Jacobian matrix.We show below that:\u2022Substitutes can also be learned with logistic regression.\u2022The attack generalizes to additional ML models by:(1) learning substitutes of 4 classifier types(logistic regression, SVM, decision tree, nearest neighbors) in addition to DNNs,and (2) targeting remote models hosted by Amazon Web Services and Google CloudPrediction with success rates of 96.19%percent96.1996.19\\%96.19 % and 88.94%percent88.9488.94\\%88.94 % after 800800800800 queriesto train the substitute.", "We here show that our approach generalizes to ML models that are not DNNs.Indeed, we learn substitutes for 4 representative types of ML classifiers inaddition to DNNs: logistic regression (LR), support vector machines (SVM),decision trees (DT), and nearest neighbor (kNN). All of these classifiers aretrained on MNIST, with no feature engineering (i.e. directly on raw pixel values) as done in Section\u00a05.", "Whereas we previously trained all of our substitutes using DNNs only, we now useboth DNNs and LR as substitute models. The Jacobian-based dataset augmentationdescribed in the context of DNNs is easily adapted to logistic regression: thelater is analog to the softmax layer frequently used by the former whenoutputting probability vectors. We use 100100100100 samples from the MNIST test set asthe initial substitute training set and use the two refinements introduced inSection\u00a06: a periodic step size and reservoirsampling.", "Figure\u00a011(a) and\u00a011(b) plot for each iteration \u03c1\ud835\udf0c\\rhoitalic_\u03c1 theshare of samples on which the substitute DNNs and LRs agree with predictions made bythe oracle they are approximating. This proportion is estimated bycomparing labels assigned to the test set by the substitutes andoracles before each iteration \u03c1\ud835\udf0c\\rhoitalic_\u03c1 of the Jacobian-based datasetaugmentation.All substitutes are able toapproximate the corresponding oracle at rates higher between 77%percent7777\\%77 % and 83%percent8383\\%83 % after \u03c1=10\ud835\udf0c10\\rho=10italic_\u03c1 = 10 iterations (to the exception of the decision tree oracle, which could be due to its non-continuity).LR substitute accuracies aregenerally lower than those of DNN substitutes, except when targeting the LR and SVM oracles where LR substitutes outperform DNN ones. However, LR substitutes are computationally more efficient and reach their asymptotic match rate faster,after \u03c1=3\ud835\udf0c3\\rho=3italic_\u03c1 = 3 iterations, corresponding to 800800800800 oraclequeries.", "Table\u00a02 quantifies the impact of refinements introduced in Section\u00a06 on results reported in Figure\u00a011(a) and\u00a011(b).The periodic step size (PSS) increases the oracle approximation accuracy ofsubstitutes. After\u03c1=9\ud835\udf0c9\\rho=9italic_\u03c1 = 9 epochs, a substitute DNN trained with PSSmatches 89.28%percent89.2889.28\\%89.28 % of the DNN oracle labels, whereas the vanilla substituteDNN matches only 78.01%percent78.0178.01\\%78.01 %.Similarly, the LR substitute with PSS matches 84.01%percent84.0184.01\\%84.01 % of the LR oracle labels while the vanilla substitute matched 72.00%percent72.0072.00\\%72.00 %. Using reservoir sampling (RS) reduces oracle querying. Forinstance, 10101010 iterations with RS (\u03c3=3\ud835\udf0e3\\sigma=3italic_\u03c3 = 3 and \u03ba=400\ud835\udf05400\\kappa=400italic_\u03ba = 400) make 100\u22c523+400(10\u22123)=3,600\u22c5100superscript234001033600100\\cdot 2^{3}+400(10-3)=3,600100 \u22c5 2 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT + 400 ( 10 - 3 ) = 3 , 600 queries to the oracle instead of102,400102400102,400102 , 400 without RS. This decreases the substitute accuracy,but when combined with PSS it remains superior to the vanilla substitutes. For instance, the vanilla substitute matched 7,80178017,8017 , 801 of the DNN oracle labels,the PSS one 8,92889288,9288 , 928, and the PSS with RS one 8,29082908,2908 , 290. Simarly,the vanilla LR substitute matched 71.56%percent71.5671.56\\%71.56 % of the SVM oracle labels, the PSS one 82.19%percent82.1982.19\\%82.19 %, and the PSS with RS 79.20%percent79.2079.20\\%79.20 %.", "Amazon oracle: To train a classifier on Amazon MachineLearning,666https://aws.amazon.com/machine-learning, we uploaded a CSV version of the MNISTdataset to a S3 bucket.We then loaded thedata, selected the multi-class model type, and keept default configuration settings. The process took a few minutes and produced a classifier achieving a92.17%percent92.1792.17\\%92.17 % test set accuracy. We cannot improve the accuracy due to the automated nature of training. We then activate real-time predictions to query the model for labels from our machine with the provided API.Although probabilities are returned, we discard them and retainonly the most likely label\u2014as stated in our threat model(Section\u00a03).", "Google oracle: The procedureto train a classifier on Google\u2019s Cloud Prediction API777https://cloud.google.com/prediction/ is similar to Amazon\u2019s. Weupload the CSV file with the MNISTtraining data to Google Cloud Storage.We then train a model using the Prediction API.The only property we canspecify is the expected multi-class nature of our model.We then evaluate the resultingmodel on the MNIST test set.The API reportsan accuracy of 92%percent9292\\%92 % on this test set for the model trained.", "Substitute Training: By augmenting an initial training set of 100100100100 test set samples, wetrain a DNN and LR substitute for each of the two oracles. We measure success as the rate of adversarialsamples misclassified by the corresponding oracle, among the 10,0001000010,00010 , 000 produced from the test set using the fast gradient sign method with parameter \u03b5=0.3\ud835\udf000.3\\varepsilon=0.3italic_\u03b5 = 0.3. These rates, computed after \u03c1\u2208{3,6}\ud835\udf0c36\\rho\\in\\{3,6\\}italic_\u03c1 \u2208 { 3 , 6 } dataset augmentation iterations, are reported in Table\u00a03. Results reported in the last row use both a periodic step size and reservoir sampling (hence the reduced number of queries made to train the substitute).", "Experimental Results: With a 96.19%percent96.1996.19\\%96.19 % misclassificationrate for a perturbation \u03b5=0.3\ud835\udf000.3\\varepsilon=0.3italic_\u03b5 = 0.3 crafted using a LR substitutetrained with 800800800800 oracle queries, the model hosted by Amazon is easilymisled. The model trained by Google is somewhat morerobust to adversarial samples, butis still vulnerable to a large proportion of samples: 88.94%percent88.9488.94\\%88.94 % of adversarialsamples produced in the same conditions are misclassified. A careful read of the documentation indicated that the model trained by Amazon is a multinomial logistic regression.888docs.aws.amazon.com/machine-learningAs pointed out in\u00a0[4], shallow models like logistic regressionare unable to cope with adversarial samples and learn robust classifiers. This explains why the attack is very successful and the LRsubstitute performs better than the DNN substitute. We were however not able to find the ML technique Google uses.", "The last row of Table\u00a03 shows how combining periodic step sizes withreservoir sampling allow us to reduce querying of both oracles duringsubstitute training, while crafting adversarial samples withhigher transferability to the target classifier.Indeed, querying is reduced by a factor larger than 3333 from 6,40064006,4006 , 400 to 2,00020002,0002 , 000 queries,while misclassification decreases only from 96.78%percent96.7896.78\\%96.78 % to 95.68%percent95.6895.68\\%95.68 % for the Amazon DNN substitute.It is still larger thanthe rate of 87.44%percent87.4487.44\\%87.44 % achieved after 800800800800 queries by the substitute learned without the refinements.Similarly, themisclassification rate of the Google LR substitute is 97.72%percent97.7297.72\\%97.72 %\u2014compared to 92.05%percent92.0592.05\\%92.05 % with the original method after \u03c1=6\ud835\udf0c6\\rho=6italic_\u03c1 = 6 epochs, confirming the result.", "The two types of defense strategies are: (1) reactive where one seeks todetect adversarial examples, and (2) proactive where one makes the modelitself more robust. Our attack is not more easily detectablethan a classic adversarial example attack. Indeed, oracle queries may bedistributed among a set of colluding users, and as such remain hard to detect. The defender may increase the attacker\u2019s cost by training models withhigher input dimensionality or modeling complexity, as ourexperimental results indicate that these two factors increase the number of queries requiredto train substitutes. In the following, we thus only analyze our attack in the face of defenses that seek to make the(oracle) model robust.", "Many potential defense mechanisms fall into a category we call gradientmasking. These techniques construct a model that does not have usefulgradients, e.g., by using a nearest neighbor classifier instead of a DNN. Suchmethods make it difficult to construct an adversarial example directly, due tothe absence of a gradient, but are often still vulnerable to the adversarialexamples that affect a smooth version of the same model. Previously, it has beenshown that nearest neighbor was vulnerable to attacks based on transferringadversarial examples from smoothed nearestneighbors[4].", "We show a more general flaw in the category of gradient masking.Even if the defender attempts to prevent attacks by not publishingthe directions in which the model is sensitive, these directionscan be discovered by other means, in which case thesame attack can still succeed.We show that the black-box attack based on transfer from a substitute modelovercomes gradient masking defenses. No fully effective defense mechanism is known, but we study the two with thegreatest empirical success so far:adversarial training\u00a0[4, 14], anddefensive distillation for DNNs\u00a0[10].", "Adversarial training:It was shown that injecting adversarial examples throughout training increasesthe robustness of significantly descriptive models, such as DNNs\u00a0[4, 14, 17].We implemented an approximation of this defense using the Google Prediction API.Since the API does not support the generation of adversarial examplesat every step of training, as a correct implementation of adversarial training woulddo, we instead inject a large amount of adversarial examples infrequently.After training in this way, the model has a misclassification rate of 8.75%percent8.758.75\\%8.75 % onthe unperturbed test set,but the adversarial misclassification rate rises to 100%percent100100\\%100 % when \u03c1=6\ud835\udf0c6\\rho=6italic_\u03c1 = 6.To evaluate this defense strategy using a correct implementation, we resortto training the oracle locally, using our own codebase that includes support forgenerating adversarial examples at each step.After each training batch, we compute and train on adversarial examplesgenerated with the fast gradient sign method before starting training on the next batch of theoriginal training data.Results are given in Table\u00a04.We observe that for \u03b5=0.15\ud835\udf000.15\\varepsilon=0.15italic_\u03b5 = 0.15, the defense can be evaded using theblack-box attack with adversarial examples crafted on the substitute andmisclassified by the oracle at rates up to 71.25%percent71.2571.25\\%71.25 %.However, for \u03b5=0.3\ud835\udf000.3\\varepsilon=0.3italic_\u03b5 = 0.3, the black-box attack is not effective anymore.Therefore, making a machine learning model robust to small and infinitesimalperturbations of its inputs is an example of gradient masking and canbe evaded using our substitute-based black-box approach.However, making the model robust to larger and finite perturbations preventsthe black-box attack.To confirm this hypothesis, we now show that defensive distillation, whichmakes the model robust to infinitesimal perturbations, can be evaded by theblack-box approach.", "Defensive distillation:Due to space constraints, we refer readers to [10] fora detailed presentation of defensivedistillation, which is an alternative defense.Because the remotely hosted APIs we study here do not implement defensive distillation or provideprimitives that could be used to implement it,we are forced to evaluate this defense on a locally trained oracle.Therefore, we train a distilled model as described in\u00a0[10] to act as our MNIST oracle.", "We train several variants of the DNN architecture A at differentdistillation temperatures T=5,10,100\ud835\udc47510100T=5,10,100italic_T = 5 , 10 , 100.For each of them, we measure the success of the fast gradient sign attack(i.e., the Goodfellow et al. algorithm) directly performed on the distilled oracle\u2014as a baseline corresponding to awhite-box attack\u2014and using a substitute DNN trained with synthetic data asdescribed throughout the present paper.The results are reported in Figure\u00a012 for different values of the input variation parameter \u03b5\ud835\udf00\\varepsilonitalic_\u03b5 on the horizontal axis. We find that defensive distillation defends against the fast gradient sign method when the attack is performed directly on the distilled model, i.e. in white-box settings. However, in black-box settings using the attack introduced in the present paper, the fast gradient sign method is found to be successful regardless of the distillation temperature used by the oracle. We hypothesize that this is due to the way distillation defends against the attack: it reduces the gradients in local neighborhoods of training points. However, our substitute model is not distilled, and as such possesses the gradients required for the fast gradient sign method to be successful when computing adversarial examples.", "Defenses which make models robust in a small neighborhood of the training manifold perform gradient masking: they smooth the decision surface and reduce gradients used by adversarial crafting in small neighborhoods. However, using a substitute and our black-box approach evades these defenses, as the substitute model is not trained to be robust to the said small perturbations. We conclude that defending against finite perturbations is a more promising avenue for future work than defending against infinitesimal perturbations. ", "We introduced an attack, based on a novel substitute training algorithm using synthetic data generation, tocraft adversarial examples misclassified by black-box DNNs. Our work is asignificant step towards relaxing strong assumptions about adversarialcapabilities made by previous attacks.We assumed only that the adversary is capable of observing labelsassigned by the model to inputs of its choice.We validated our attack design bytargeting a remote DNN served by MetaMind, forcing it to misclassify84.24%percent84.2484.24\\%84.24 % of our adversarial samples. We also conducted an extensivecalibration of our algorithm and generalized it to other ML models by instantiating it against classifiers hosted by Amazon and Google, with success rates of 96.19%percent96.1996.19\\%96.19 % and 88.94%percent88.9488.94\\%88.94 %. Our attack evades a category of defenses, which we call gradient masking, previously proposed to increase resilience to adversarial examples. Finally, we provided an intuition foradversarial sample transferability across DNNs in Appendix B.", "Nicolas Papernot is supportedby a Google PhD Fellowship in Security.Research was also supported in part by the Army Research Laboratory,under Cooperative Agreement Number W911NF-13-2-0045 (ARL Cyber SecurityCRA), and the Army Research Office under grant W911NF-13-1-0421.The views and conclusions contained in this document are those of theauthors and should not be interpreted as representing the official policies,either expressed or implied, of the Army Research Laboratory or the U.S.Government. The U.S.\u00a0Government is authorized to reproduce and distributereprints for government purposes notwithstanding any copyright notation hereon."], "figure_types": {"53b047e503f4c24602f376a774d653f7ed56c024/10-Figure11-1.png": "plot", "53b047e503f4c24602f376a774d653f7ed56c024/10-Table2-1.png": "table", "53b047e503f4c24602f376a774d653f7ed56c024/10-Table3-1.png": "table", "53b047e503f4c24602f376a774d653f7ed56c024/11-Table4-1.png": "table", "53b047e503f4c24602f376a774d653f7ed56c024/12-Figure12-1.png": "plot", "53b047e503f4c24602f376a774d653f7ed56c024/13-Figure13-1.png": "table", "53b047e503f4c24602f376a774d653f7ed56c024/14-Figure14-1.png": "plot", "53b047e503f4c24602f376a774d653f7ed56c024/14-Figure15-1.png": "other", "53b047e503f4c24602f376a774d653f7ed56c024/2-Figure1-1.png": "schematic", "53b047e503f4c24602f376a774d653f7ed56c024/3-Figure2-1.png": "photograph(s)", "53b047e503f4c24602f376a774d653f7ed56c024/5-Figure3-1.png": "schematic", "53b047e503f4c24602f376a774d653f7ed56c024/6-Figure4-1.png": "table", "53b047e503f4c24602f376a774d653f7ed56c024/6-Figure5-1.png": "plot", "53b047e503f4c24602f376a774d653f7ed56c024/7-Figure6-1.png": "plot", "53b047e503f4c24602f376a774d653f7ed56c024/7-Figure7-1.png": "plot", "53b047e503f4c24602f376a774d653f7ed56c024/8-Table1-1.png": "table", "53b047e503f4c24602f376a774d653f7ed56c024/9-Figure10-1.png": "plot", "53b047e503f4c24602f376a774d653f7ed56c024/9-Figure8-1.png": "plot", "53b047e503f4c24602f376a774d653f7ed56c024/9-Figure9-1.png": "plot"}}, "1612.08242": {"paper_id": "paper_83", "title": "YOLO9000: Better, Faster, Stronger", "arxiv_url": "https://arxiv.org/abs/1612.08242", "s2orc_url": "https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6", "all_figures_tables": {"7d39d69b23424446f0400ef603b2e3e22d0309d6/1-Figure1-1.png": "Figure 1: YOLO9000. YOLO9000 can detect a wide variety of object classes in real-time.", "7d39d69b23424446f0400ef603b2e3e22d0309d6/3-Figure2-1.png": "Figure 2: Clustering box dimensions on VOC and COCO. We run k-means clustering on the dimensions of bounding boxes to get good priors for our model. The left image shows the average IOU we get with various choices for k. k = 5 gives a good tradeoff for recall vs. complexity of the model. The right image shows the relative centroids for VOC and COCO. COCO has greater variation in size than VOC.", "7d39d69b23424446f0400ef603b2e3e22d0309d6/3-Table1-1.png": "Table 1: Average IOU of boxes to closest priors on VOC 2007.", "7d39d69b23424446f0400ef603b2e3e22d0309d6/4-Figure3-1.png": "Figure 3: Bounding boxes with dimension priors and location prediction. We predict the width and height of the box as offsets from cluster centroids. We predict the center coordinates of the box relative to the location of filter application using a sigmoid function.", "7d39d69b23424446f0400ef603b2e3e22d0309d6/4-Figure4-1.png": "Figure 4: Accuracy and speed on VOC 2007.", "7d39d69b23424446f0400ef603b2e3e22d0309d6/5-Table2-1.png": "Table 2: The path from YOLO to YOLOv2. Most of the listed design decisions lead to significant increases in mAP. Two exceptions are switching to a fully convolutional network with anchor boxes and using the new network. Switching to the anchor box style approach increased recall without changing mAP while using the new network cut computation by 33%.", "7d39d69b23424446f0400ef603b2e3e22d0309d6/5-Table3-1.png": "Table 3: Detection frameworks on PASCAL VOC 2007.", "7d39d69b23424446f0400ef603b2e3e22d0309d6/6-Table4-1.png": "Table 4: PASCAL VOC2012 test detection results. YOLOv2 performs on par with state-of-the-art detectors like Faster R-CNN with ResNet and SSD512 and is 2\u2212 10\u00d7 faster.", "7d39d69b23424446f0400ef603b2e3e22d0309d6/6-Table5-1.png": "Table 5: Results on COCO test-dev2015. Table adapted from [11]", "7d39d69b23424446f0400ef603b2e3e22d0309d6/6-Table6-1.png": "Table 6: Darknet-19.", "7d39d69b23424446f0400ef603b2e3e22d0309d6/7-Figure5-1.png": "Figure 5: Prediction on ImageNet vs WordTree. Most ImageNet models use one large softmax to predict a probability distribution. Using WordTree we perform multiple softmax operations over co-hyponyms.", "7d39d69b23424446f0400ef603b2e3e22d0309d6/8-Figure6-1.png": "Figure 6: Combining datasets using WordTree hierarchy. Using the WordNet concept graph we build a hierarchical tree of visual concepts. Then we can merge datasets together by mapping the classes in the dataset to synsets in the tree. This is a simplified view of WordTree for illustration purposes.", "7d39d69b23424446f0400ef603b2e3e22d0309d6/8-Table7-1.png": "Table 7: YOLO9000 Best and Worst Classes on ImageNet."}, "referred_figures_tables": [["7d39d69b23424446f0400ef603b2e3e22d0309d6/3-Figure2-1.png", "7d39d69b23424446f0400ef603b2e3e22d0309d6/3-Table1-1.png"], ["7d39d69b23424446f0400ef603b2e3e22d0309d6/7-Figure5-1.png"]], "question_id": [7, 15], "question": ["How does the graph of the average IOU vs. number of clusters imply the claim that k = 5 is the optimal choice for the complexity/recall tradeoff?", "Are the softmax values of different sets of co-hyponyms compared?"], "question_section": ["Better", "Stronger"], "question_trigger_sentence": ["We choose k = 5 as a good tradeoff between model complexity and high recall.", " Using WordTree we perform multiple softmax operations over co-hyponyms."], "question_type": ["Deep/complex question", "Shallow question"], "evidential_info": [[{"context": "Instead of choosing priors by hand, we run k-means clustering on the training set bounding boxes to automatically find good priors. If we use standard k-means with Euclidean distance larger boxes generate more error than smaller boxes. However, what we really want are priors that lead to good IOU scores, which is independent of the size of the box. Thus for our distance metric we use:", "rationale": "k-means is run for various values of k and average IOU with closest centroid is plotted. Paper choose k=5 as a good tradeoff between model complexity and high recall."}, {"context": "We run k-means for various values of k and plot the average IOU with closest centroid, see Figure 2. We choose k=5 as a good tradeoff between model complexity and high recall. The cluster centroids are significantly different than hand-picked anchor boxes. There are fewer short, wide boxes and more tall, thin boxes.", "rationale": "We compare the average IOU to closest prior of our clustering strategy and the hand-picked anchor boxes in Table 1. At only 5 priors the centroids perform similarly to 9 anchor boxes with an average IOU of 61.0 compared to 60.9.."}, {"context": "We compare the average IOU to closest prior of our clustering strategy and the hand-picked anchor boxes in Table 1. At only 5 priors the centroids perform similarly to 9 anchor boxes with an average IOU of 61.0 compared to 60.9. If we use 9 centroids we see a much higher average IOU. This indicates that using k-means to generate our bounding box starts the model off with a better representation and makes the task easier to learn.", "rationale": "Instead of choosing priors by hand, we run k-means clustering on the training set bounding boxes to automatically find good priors"}], [{"context": "Most approaches to classification use a softmax layer across all the possible categories to compute the final probability distribution. Using a softmax assumes the classes are mutually exclusive. This presents problems for combining datasets, for example you would not want to combine ImageNet and COCO using this model because the classes \u201cNorfolk terrier\u201d and \u201cdog\u201d are not mutually exclusive.", "rationale": "Most approaches to classification use a softmax layer across all the possible categories to compute the final probability distribution. Using a softmax assumes the classes are mutually exclusive. This presents problems for combining datasets, for example you would not want to combine ImageNet and COCO using this model because the classes \u201cNorfolk terrier\u201d and \u201cdog\u201d are not mutually exclusive."}, {"context": "The final result is WordTree, a hierarchical model of visual concepts. To perform classification with WordTree we predict conditional probabilities at every node for the probability of each hyponym of that synset given that synset. For example, at the \u201cterrier\u201d node we predict:", "rationale": "To compute the conditional probabilities proposed model predicts a vector of 1369 values and computes the softmax over all sysnsets that are hyponyms of the same concept."}, {"context": "To validate this approach we train the Darknet-19 model on WordTree built using the 1000 class ImageNet. To build WordTree1k we add in all of the intermediate nodes which expands the label space from 1000 to 1369. During training we propagate ground truth labels up the tree so that if an image is labelled as a \u201cNorfolk terrier\u201d it also gets labelled as a \u201cdog\u201d and a \u201cmammal\u201d, etc. To compute the conditional probabilities our model predicts a vector of 1369 values and we compute the softmax over all sysnsets that are hyponyms of the same concept, see Figure 5.", "rationale": "To perform classification with WordTree paper predicts conditional probabilities at every node for the probability of each hyponym of that synset given that synset. For example, at the \u201cterrier\u201d node we predict:"}]], "composition": ["A graph is shown between average IOU vs. number of clusters. Number of anchar boxes are then hand-picked by comparing the average IOU closest to the prior. K=5 is choosen because At only 5 priors the centroids perform similarly to 9 anchor boxes with an average IOU of 61.0 compared to 60.9.", "Classification approaches use a softmax layer across all categories to predict the final probability of all classes. This technique would fail for models which combine datasets having similar classes. To overcome the proposed model also use a softmax over all sysnsets that are co-hyponyms.  Hence the final probability is computed by conditional probabilities at every node for the probability of each hyponym of that synset given that synset."], "Is_figure_in_evidence": [true, true], "Is_table_in_evidence": [true, false], "question_key": ["1311", "1317"], "passages": ["General purpose object detection should be fast, accurate, and able to recognize a wide variety of objects. Since the introduction of neural networks, detection frameworks have become increasingly fast and accurate. However, most detection methods are still constrained to a small set of objects.", "Current object detection datasets are limited compared to datasets for other tasks like classification and tagging. The most common detection datasets contain thousands to hundreds of thousands of images with dozens to hundreds of tags [3] [10] [2]. Classification datasets have millions of images with tens or hundreds of thousands of categories [20] [2].", "We would like detection to scale to level of object classification. However, labelling images for detection is far more expensive than labelling for classification or tagging (tags are often user-supplied for free). Thus we are unlikely to see detection datasets on the same scale as classification datasets in the near future.", "We propose a new method to harness the large amount of classification data we already have and use it to expand the scope of current detection systems. Our method uses a hierarchical view of object classification that allows us to combine distinct datasets together.", "We also propose a joint training algorithm that allows us to train object detectors on both detection and classification data. Our method leverages labeled detection images to learn to precisely localize objects while it uses classification images to increase its vocabulary and robustness.", "Using this method we train YOLO9000, a real-time object detector that can detect over 9000 different object categories. First we improve upon the base YOLO detection system to produce YOLOv2, a state-of-the-art, real-time detector. Then we use our dataset combination method and joint training algorithm to train a model on more than 9000 classes from ImageNet as well as detection data from COCO.", "All of our code and pre-trained models are available online at http://pjreddie.com/yolo9000/.", "YOLO suffers from a variety of shortcomings relative to state-of-the-art detection systems. Error analysis of YOLO compared to Fast R-CNN shows that YOLO makes a significant number of localization errors. Furthermore, YOLO has relatively low recall compared to region proposal-based methods. Thus we focus mainly on improving recall and localization while maintaining classification accuracy.", "Computer vision generally trends towards larger, deeper networks [6] [18] [17]. Better performance often hinges on training larger networks or ensembling multiple models together. However, with YOLOv2 we want a more accurate detector that is still fast. Instead of scaling up our network, we simplify the network and then make the representation easier to learn. We pool a variety of ideas from past work with our own novel concepts to improve YOLO\u2019s performance. A summary of results can be found in Table 2.", "Batch Normalization. Batch normalization leads to significant improvements in convergence while eliminating the need for other forms of regularization [7]. By adding batch normalization on all of the convolutional layers in YOLO we get more than 2% improvement in mAP. Batch normalization also helps regularize the model. With batch normalization we can remove dropout from the model without overfitting.", "High Resolution Classifier. All state-of-the-art detection methods use classifier pre-trained on ImageNet [16]. Starting with AlexNet most classifiers operate on input images smaller than 256\u00d7256256256256\\times 256256 \u00d7 256 [8]. The original YOLO trains the classifier network at 224\u00d7224224224224\\times 224224 \u00d7 224 and increases the resolution to 448448448448 for detection. This means the network has to simultaneously switch to learning object detection and adjust to the new input resolution.", "For YOLOv2 we first fine tune the classification network at the full 448\u00d7448448448448\\times 448448 \u00d7 448 resolution for 10 epochs on ImageNet. This gives the network time to adjust its filters to work better on higher resolution input. We then fine tune the resulting network on detection. This high resolution classification network gives us an increase of almost 4% mAP.", "Convolutional With Anchor Boxes. YOLO predicts the coordinates of bounding boxes directly using fully connected layers on top of the convolutional feature extractor. Instead of predicting coordinates directly Faster R-CNN predicts bounding boxes using hand-picked priors [15]. Using only convolutional layers the region proposal network (RPN) in Faster R-CNN predicts offsets and confidences for anchor boxes. Since the prediction layer is convolutional, the RPN predicts these offsets at every location in a feature map. Predicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learn.", "We remove the fully connected layers from YOLO and use anchor boxes to predict bounding boxes. First we eliminate one pooling layer to make the output of the network\u2019s convolutional layers higher resolution. We also shrink the network to operate on 416416416416 input images instead of 448\u00d7448448448448\\times 448448 \u00d7 448. We do this because we want an odd number of locations in our feature map so there is a single center cell. Objects, especially large objects, tend to occupy the center of the image so it\u2019s good to have a single location right at the center to predict these objects instead of four locations that are all nearby. YOLO\u2019s convolutional layers downsample the image by a factor of 32 so by using an input image of 416416416416 we get an output feature map of 13\u00d713131313\\times 1313 \u00d7 13.", "When we move to anchor boxes we also decouple the class prediction mechanism from the spatial location and instead predict class and objectness for every anchor box. Following YOLO, the objectness prediction still predicts the IOU of the ground truth and the proposed box and the class predictions predict the conditional probability of that class given that there is an object.", "Using anchor boxes we get a small decrease in accuracy. YOLO only predicts 98 boxes per image but with anchor boxes our model predicts more than a thousand. Without anchor boxes our intermediate model gets 69.569.569.569.5 mAP with a recall of 81%percent8181\\%81 %. With anchor boxes our model gets 69.269.269.269.2 mAP with a recall of 88%percent8888\\%88 %. Even though the mAP decreases, the increase in recall means that our model has more room to improve.", "Dimension Clusters. We encounter two issues with anchor boxes when using them with YOLO. The first is that the box dimensions are hand picked. The network can learn to adjust the boxes appropriately but if we pick better priors for the network to start with we can make it easier for the network to learn to predict good detections.", "Instead of choosing priors by hand, we run k-means clustering on the training set bounding boxes to automatically find good priors. If we use standard k-means with Euclidean distance larger boxes generate more error than smaller boxes. However, what we really want are priors that lead to good IOU scores, which is independent of the size of the box. Thus for our distance metric we use:", "d(box,centroid)=1\u2212IOU(box,centroid)\ud835\udc51boxcentroid1IOUboxcentroidd(\\text{box},\\text{centroid})=1-\\text{IOU}(\\text{box},\\text{centroid})italic_d ( box , centroid ) = 1 - IOU ( box , centroid )", "We run k-means for various values of k\ud835\udc58kitalic_k and plot the average IOU with closest centroid, see Figure 2. We choose k=5\ud835\udc585k=5italic_k = 5 as a good tradeoff between model complexity and high recall. The cluster centroids are significantly different than hand-picked anchor boxes. There are fewer short, wide boxes and more tall, thin boxes.", "We compare the average IOU to closest prior of our clustering strategy and the hand-picked anchor boxes in Table 1. At only 5 priors the centroids perform similarly to 9 anchor boxes with an average IOU of 61.0 compared to 60.9. If we use 9 centroids we see a much higher average IOU. This indicates that using k-means to generate our bounding box starts the model off with a better representation and makes the task easier to learn.", "Direct location prediction. When using anchor boxes with YOLO we encounter a second issue: model instability, especially during early iterations. Most of the instability comes from predicting the (x,y)\ud835\udc65\ud835\udc66(x,y)( italic_x , italic_y ) locations for the box. In region proposal networks the network predicts values txsubscript\ud835\udc61\ud835\udc65t_{x}italic_t start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT and tysubscript\ud835\udc61\ud835\udc66t_{y}italic_t start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT and the (x,y)\ud835\udc65\ud835\udc66(x,y)( italic_x , italic_y ) center coordinates are calculated as:", "x\ud835\udc65\\displaystyle xitalic_x=(tx*wa)\u2212xaabsentsubscript\ud835\udc61\ud835\udc65subscript\ud835\udc64\ud835\udc4esubscript\ud835\udc65\ud835\udc4e\\displaystyle=(t_{x}*w_{a})-x_{a}= ( italic_t start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT * italic_w start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) - italic_x start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPTy\ud835\udc66\\displaystyle yitalic_y=(ty*ha)\u2212yaabsentsubscript\ud835\udc61\ud835\udc66subscript\u210e\ud835\udc4esubscript\ud835\udc66\ud835\udc4e\\displaystyle=(t_{y}*h_{a})-y_{a}= ( italic_t start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT * italic_h start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) - italic_y start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT", "For example, a prediction of tx=1subscript\ud835\udc61\ud835\udc651t_{x}=1italic_t start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = 1 would shift the box to the right by the width of the anchor box, a prediction of tx=\u22121subscript\ud835\udc61\ud835\udc651t_{x}=-1italic_t start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = - 1 would shift it to the left by the same amount.", "This formulation is unconstrained so any anchor box can end up at any point in the image, regardless of what location predicted the box. With random initialization the model takes a long time to stabilize to predicting sensible offsets.", "Instead of predicting offsets we follow the approach of YOLO and predict location coordinates relative to the location of the grid cell. This bounds the ground truth to fall between 00 and 1111. We use a logistic activation to constrain the network\u2019s predictions to fall in this range.", "The network predicts 5 bounding boxes at each cell in the output feature map. The network predicts 5 coordinates for each bounding box, txsubscript\ud835\udc61\ud835\udc65t_{x}italic_t start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT, tysubscript\ud835\udc61\ud835\udc66t_{y}italic_t start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT, twsubscript\ud835\udc61\ud835\udc64t_{w}italic_t start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT, thsubscript\ud835\udc61\u210et_{h}italic_t start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT, and tosubscript\ud835\udc61\ud835\udc5ct_{o}italic_t start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT. If the cell is offset from the top left corner of the image by (cx,cy)subscript\ud835\udc50\ud835\udc65subscript\ud835\udc50\ud835\udc66(c_{x},c_{y})( italic_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) and the bounding box prior has width and height pwsubscript\ud835\udc5d\ud835\udc64p_{w}italic_p start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT, phsubscript\ud835\udc5d\u210ep_{h}italic_p start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT, then the predictions correspond to:", "bxsubscript\ud835\udc4f\ud835\udc65\\displaystyle b_{x}italic_b start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT=\u03c3(tx)+cxabsent\ud835\udf0esubscript\ud835\udc61\ud835\udc65subscript\ud835\udc50\ud835\udc65\\displaystyle=\\sigma(t_{x})+c_{x}= italic_\u03c3 ( italic_t start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) + italic_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPTbysubscript\ud835\udc4f\ud835\udc66\\displaystyle b_{y}italic_b start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT=\u03c3(ty)+cyabsent\ud835\udf0esubscript\ud835\udc61\ud835\udc66subscript\ud835\udc50\ud835\udc66\\displaystyle=\\sigma(t_{y})+c_{y}= italic_\u03c3 ( italic_t start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) + italic_c start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPTbwsubscript\ud835\udc4f\ud835\udc64\\displaystyle b_{w}italic_b start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT=pwetwabsentsubscript\ud835\udc5d\ud835\udc64superscript\ud835\udc52subscript\ud835\udc61\ud835\udc64\\displaystyle=p_{w}e^{t_{w}}= italic_p start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT end_POSTSUPERSCRIPTbhsubscript\ud835\udc4f\u210e\\displaystyle b_{h}italic_b start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT=phethabsentsubscript\ud835\udc5d\u210esuperscript\ud835\udc52subscript\ud835\udc61\u210e\\displaystyle=p_{h}e^{t_{h}}= italic_p start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUPERSCRIPTPr(object)*IOU(b,object)\ud835\udc43\ud835\udc5fobject\ud835\udc3c\ud835\udc42\ud835\udc48\ud835\udc4fobject\\displaystyle Pr(\\text{object})*IOU(b,\\text{object})italic_P italic_r ( object ) * italic_I italic_O italic_U ( italic_b , object )=\u03c3(to)absent\ud835\udf0esubscript\ud835\udc61\ud835\udc5c\\displaystyle=\\sigma(t_{o})= italic_\u03c3 ( italic_t start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT )", "Since we constrain the location prediction the parametrization is easier to learn, making the network more stable. Using dimension clusters along with directly predicting the bounding box center location improves YOLO by almost 5% over the version with anchor boxes.", "Fine-Grained Features.This modified YOLO predicts detections on a 13\u00d713131313\\times 1313 \u00d7 13 feature map. While this is sufficient for large objects, it may benefit from finer grained features for localizing smaller objects. Faster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions. We take a different approach, simply adding a passthrough layer that brings features from an earlier layer at 26\u00d726262626\\times 2626 \u00d7 26 resolution.", "The passthrough layer concatenates the higher resolution features with the low resolution features by stacking adjacent features into different channels instead of spatial locations, similar to the identity mappings in ResNet. This turns the 26\u00d726\u00d7512262651226\\times 26\\times 51226 \u00d7 26 \u00d7 512 feature map into a 13\u00d713\u00d720481313204813\\times 13\\times 204813 \u00d7 13 \u00d7 2048 feature map, which can be concatenated with the original features. Our detector runs on top of this expanded feature map so that it has access to fine grained features. This gives a modest 1% performance increase.", "Multi-Scale Training. The original YOLO uses an input resolution of 448\u00d7448448448448\\times 448448 \u00d7 448. With the addition of anchor boxes we changed the resolution to 416\u00d7416416416416\\times 416416 \u00d7 416. However, since our model only uses convolutional and pooling layers it can be resized on the fly. We want YOLOv2 to be robust to running on images of different sizes so we train this into the model.", "Instead of fixing the input image size we change the network every few iterations. Every 10 batches our network randomly chooses a new image dimension size. Since our model downsamples by a factor of 32, we pull from the following multiples of 32: {320,352,\u2026,608}320352\u2026608\\{320,352,...,608\\}{ 320 , 352 , \u2026 , 608 }. Thus the smallest option is 320\u00d7320320320320\\times 320320 \u00d7 320 and the largest is 608\u00d7608608608608\\times 608608 \u00d7 608. We resize the network to that dimension and continue training.", "This regime forces the network to learn to predict well across a variety of input dimensions. This means the same network can predict detections at different resolutions. The network runs faster at smaller sizes so YOLOv2 offers an easy tradeoff between speed and accuracy.", "At low resolutions YOLOv2 operates as a cheap, fairly accurate detector. At 288\u00d7288288288288\\times 288288 \u00d7 288 it runs at more than 90 FPS with mAP almost as good as Fast R-CNN. This makes it ideal for smaller GPUs, high framerate video, or multiple video streams.", "At high resolution YOLOv2 is a state-of-the-art detector with 78.6 mAP on VOC 2007 while still operating above real-time speeds. See Table 3 for a comparison of YOLOv2 with other frameworks on VOC 2007. Figure 4", "Further Experiments. We train YOLOv2 for detection on VOC 2012. Table 4 shows the comparative performance of YOLOv2 versus other state-of-the-art detection systems. YOLOv2 achieves 73.4 mAP while running far faster than competing methods. We also train on COCO and compare to other methods in Table 5. On the VOC metric (IOU = .5) YOLOv2 gets 44.0 mAP, comparable to SSD and Faster R-CNN.", "We want detection to be accurate but we also want it to be fast. Most applications for detection, like robotics or self-driving cars, rely on low latency predictions. In order to maximize performance we design YOLOv2 to be fast from the ground up.", "Most detection frameworks rely on VGG-16 as the base feature extractor [17]. VGG-16 is a powerful, accurate classification network but it is needlessly complex. The convolutional layers of VGG-16 require 30.69 billion floating point operations for a single pass over a single image at 224\u00d7224224224224\\times 224224 \u00d7 224 resolution.", "The YOLO framework uses a custom network based on the Googlenet architecture [19]. This network is faster than VGG-16, only using 8.52 billion operations for a forward pass. However, it\u2019s accuracy is slightly worse than VGG-16. For single-crop, top-5 accuracy at 224\u00d7224224224224\\times 224224 \u00d7 224, YOLO\u2019s custom model gets 88.0% ImageNet compared to 90.0% for VGG-16.", "Darknet-19. We propose a new classification model to be used as the base of YOLOv2. Our model builds off of prior work on network design as well as common knowledge in the field. Similar to the VGG models we use mostly 3\u00d73333\\times 33 \u00d7 3 filters and double the number of channels after every pooling step [17]. Following the work on Network in Network (NIN) we use global average pooling to make predictions as well as 1\u00d71111\\times 11 \u00d7 1 filters to compress the feature representation between 3\u00d73333\\times 33 \u00d7 3 convolutions [9]. We use batch normalization to stabilize training, speed up convergence, and regularize the model [7].", "Our final model, called Darknet-19, has 19 convolutional layers and 5 maxpooling layers. For a full description see Table 6. Darknet-19 only requires 5.58 billion operations to process an image yet achieves 72.9%percent72.972.9\\%72.9 % top-1 accuracy and 91.2%percent91.291.2\\%91.2 % top-5 accuracy on ImageNet.", "Training for classification. We train the network on the standard ImageNet 1000 class classification dataset for 160 epochs using stochastic gradient descent with a starting learning rate of 0.10.10.10.1, polynomial rate decay with a power of 4444, weight decay of 0.00050.00050.00050.0005 and momentum of 0.90.90.90.9 using the Darknet neural network framework [13]. During training we use standard data augmentation tricks including random crops, rotations, and hue, saturation, and exposure shifts.", "As discussed above, after our initial training on images at 224\u00d7224224224224\\times 224224 \u00d7 224 we fine tune our network at a larger size, 448448448448. For this fine tuning we train with the above parameters but for only 10 epochs and starting at a learning rate of 10\u22123superscript10310^{-3}10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT. At this higher resolution our network achieves a top-1 accuracy of 76.5%percent76.576.5\\%76.5 % and a top-5 accuracy of 93.3%percent93.393.3\\%93.3 %.", "Training for detection. We modify this network for detection by removing the last convolutional layer and instead adding on three 3\u00d73333\\times 33 \u00d7 3 convolutional layers with 1024102410241024 filters each followed by a final 1\u00d71111\\times 11 \u00d7 1 convolutional layer with the number of outputs we need for detection. For VOC we predict 5 boxes with 5 coordinates each and 20 classes per box so 125 filters. We also add a passthrough layer from the final 3\u00d73\u00d7512335123\\times 3\\times 5123 \u00d7 3 \u00d7 512 layer to the second to last convolutional layer so that our model can use fine grain features.", "We train the network for 160 epochs with a starting learning rate of 10\u22123superscript10310^{-3}10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT, dividing it by 10 at 60 and 90 epochs. We use a weight decay of 0.00050.00050.00050.0005 and momentum of 0.90.90.90.9. We use a similar data augmentation to YOLO and SSD with random crops, color shifting, etc. We use the same training strategy on COCO and VOC.", "We propose a mechanism for jointly training on classification and detection data. Our method uses images labelled for detection to learn detection-specific information like bounding box coordinate prediction and objectness as well as how to classify common objects. It uses images with only class labels to expand the number of categories it can detect.", "During training we mix images from both detection and classification datasets. When our network sees an image labelled for detection we can backpropagate based on the full YOLOv2 loss function. When it sees a classification image we only backpropagate loss from the classification-specific parts of the architecture.", "This approach presents a few challenges. Detection datasets have only common objects and general labels, like \u201cdog\u201d or \u201cboat\u201d. Classification datasets have a much wider and deeper range of labels. ImageNet has more than a hundred breeds of dog, including \u201cNorfolk terrier\u201d, \u201cYorkshire terrier\u201d, and \u201cBedlington terrier\u201d. If we want to train on both datasets we need a coherent way to merge these labels.", "Most approaches to classification use a softmax layer across all the possible categories to compute the final probability distribution. Using a softmax assumes the classes are mutually exclusive. This presents problems for combining datasets, for example you would not want to combine ImageNet and COCO using this model because the classes \u201cNorfolk terrier\u201d and \u201cdog\u201d are not mutually exclusive.", "We could instead use a multi-label model to combine the datasets which does not assume mutual exclusion. This approach ignores all the structure we do know about the data, for example that all of the COCO classes are mutually exclusive.", "Hierarchical classification. ImageNet labels are pulled from WordNet, a language database that structures concepts and how they relate [12]. In WordNet, \u201cNorfolk terrier\u201d and \u201cYorkshire terrier\u201d are both hyponyms of \u201cterrier\u201d which is a type of \u201chunting dog\u201d, which is a type of \u201cdog\u201d, which is a \u201ccanine\u201d, etc. Most approaches to classification assume a flat structure to the labels however for combining datasets, structure is exactly what we need.", "WordNet is structured as a directed graph, not a tree, because language is complex. For example a \u201cdog\u201d is both a type of \u201ccanine\u201d and a type of \u201cdomestic animal\u201d which are both synsets in WordNet. Instead of using the full graph structure, we simplify the problem by building a hierarchical tree from the concepts in ImageNet.", "To build this tree we examine the visual nouns in ImageNet and look at their paths through the WordNet graph to the root node, in this case \u201cphysical object\u201d. Many synsets only have one path through the graph so first we add all of those paths to our tree. Then we iteratively examine the concepts we have left and add the paths that grow the tree by as little as possible. So if a concept has two paths to the root and one path would add three edges to our tree and the other would only add one edge, we choose the shorter path.", "The final result is WordTree, a hierarchical model of visual concepts. To perform classification with WordTree we predict conditional probabilities at every node for the probability of each hyponym of that synset given that synset. For example, at the \u201cterrier\u201d node we predict:", "Pr(Norfolk terrier\\displaystyle Pr(\\text{Norfolk terrier}italic_P italic_r ( Norfolk terrier|terrier)\\displaystyle|\\text{terrier})| terrier )Pr(Yorkshire terrier\\displaystyle Pr(\\text{Yorkshire terrier}italic_P italic_r ( Yorkshire terrier|terrier)\\displaystyle|\\text{terrier})| terrier )Pr(Bedlington terrier\\displaystyle Pr(\\text{Bedlington terrier}italic_P italic_r ( Bedlington terrier|terrier)\\displaystyle|\\text{terrier})| terrier )\u2026\u2026\\displaystyle...\u2026", "If we want to compute the absolute probability for a particular node we simply follow the path through the tree to the root node and multiply to conditional probabilities. So if we want to know if a picture is of a Norfolk terrier we compute:", "Pr(Norfolk terrier)\ud835\udc43\ud835\udc5fNorfolk terrier\\displaystyle Pr(\\text{Norfolk terrier})italic_P italic_r ( Norfolk terrier )=Pr(Norfolk terrier|terrier)absent\ud835\udc43\ud835\udc5fconditionalNorfolk terrierterrier\\displaystyle=Pr(\\text{Norfolk terrier}|\\text{terrier})= italic_P italic_r ( Norfolk terrier | terrier )*Pr(terrier\\displaystyle*Pr(\\text{terrier}* italic_P italic_r ( terrier|hunting dog)\\displaystyle|\\text{hunting dog})| hunting dog )*\u2026absent\u2026\\displaystyle*\\ldots* \u2026*\\displaystyle***Pr(mammal\\displaystyle*Pr(\\text{mammal}* italic_P italic_r ( mammal|Pr(animal)\\displaystyle|Pr(\\text{animal})| italic_P italic_r ( animal )*Pr(animal\\displaystyle*Pr(\\text{animal}* italic_P italic_r ( animal|physical object)\\displaystyle|\\text{physical object})| physical object )", "For classification purposes we assume that the the image contains an object: Pr(physical object)=1\ud835\udc43\ud835\udc5fphysical object1Pr(\\text{physical object})=1italic_P italic_r ( physical object ) = 1.", "To validate this approach we train the Darknet-19 model on WordTree built using the 1000 class ImageNet. To build WordTree1k we add in all of the intermediate nodes which expands the label space from 1000 to 1369. During training we propagate ground truth labels up the tree so that if an image is labelled as a \u201cNorfolk terrier\u201d it also gets labelled as a \u201cdog\u201d and a \u201cmammal\u201d, etc. To compute the conditional probabilities our model predicts a vector of 1369 values and we compute the softmax over all sysnsets that are hyponyms of the same concept, see Figure 5.", "Using the same training parameters as before, our hierarchical Darknet-19 achieves 71.9%percent71.971.9\\%71.9 % top-1 accuracy and 90.4%percent90.490.4\\%90.4 % top-5 accuracy. Despite adding 369 additional concepts and having our network predict a tree structure our accuracy only drops marginally. Performing classification in this manner also has some benefits. Performance degrades gracefully on new or unknown object categories. For example, if the network sees a picture of a dog but is uncertain what type of dog it is, it will still predict \u201cdog\u201d with high confidence but have lower confidences spread out among the hyponyms.", "This formulation also works for detection. Now, instead of assuming every image has an object, we use YOLOv2\u2019s objectness predictor to give us the value of Pr(physical object)\ud835\udc43\ud835\udc5fphysical objectPr(\\text{physical object})italic_P italic_r ( physical object ). The detector predicts a bounding box and the tree of probabilities. We traverse the tree down, taking the highest confidence path at every split until we reach some threshold and we predict that object class.", "Dataset combination with WordTree. We can use WordTree to combine multiple datasets together in a sensible fashion. We simply map the categories in the datasets to synsets in the tree. Figure 6 shows an example of using WordTree to combine the labels from ImageNet and COCO. WordNet is extremely diverse so we can use this technique with most datasets.", "Joint classification and detection. Now that we can combine datasets using WordTree we can train our joint model on classification and detection. We want to train an extremely large scale detector so we create our combined dataset using the COCO detection dataset and the top 9000 classes from the full ImageNet release. We also need to evaluate our method so we add in any classes from the ImageNet detection challenge that were not already included. The corresponding WordTree for this dataset has 9418 classes. ImageNet is a much larger dataset so we balance the dataset by oversampling COCO so that ImageNet is only larger by a factor of 4:1.", "Using this dataset we train YOLO9000. We use the base YOLOv2 architecture but only 3 priors instead of 5 to limit the output size. When our network sees a detection image we backpropagate loss as normal. For classification loss, we only backpropagate loss at or above the corresponding level of the label. For example, if the label is \u201cdog\u201d we do assign any error to predictions further down in the tree, \u201cGerman Shepherd\u201d versus \u201cGolden Retriever\u201d, because we do not have that information.", "When it sees a classification image we only backpropagate classification loss. To do this we simply find the bounding box that predicts the highest probability for that class and we compute the loss on just its predicted tree. We also assume that the predicted box overlaps what would be the ground truth label by at least .3.3.3.3 IOU and we backpropagate objectness loss based on this assumption.", "Using this joint training, YOLO9000 learns to find objects in images using the detection data in COCO and it learns to classify a wide variety of these objects using data from ImageNet.", "We evaluate YOLO9000 on the ImageNet detection task. The detection task for ImageNet shares on 44 object categories with COCO which means that YOLO9000 has only seen classification data for the majority of the test images, not detection data. YOLO9000 gets 19.7 mAP overall with 16.0 mAP on the disjoint 156 object classes that it has never seen any labelled detection data for. This mAP is higher than results achieved by DPM but YOLO9000 is trained on different datasets with only partial supervision [4]. It also is simultaneously detecting 9000 other object categories, all in real-time.", "When we analyze YOLO9000\u2019s performance on ImageNet we see it learns new species of animals well but struggles with learning categories like clothing and equipment. New animals are easier to learn because the objectness predictions generalize well from the animals in COCO. Conversely, COCO does not have bounding box label for any type of clothing, only for person, so YOLO9000 struggles to model categories like \u201csunglasses\u201d or \u201cswimming trunks\u201d.", "We introduce YOLOv2 and YOLO9000, real-time detection systems. YOLOv2 is state-of-the-art and faster than other detection systems across a variety of detection datasets. Furthermore, it can be run at a variety of image sizes to provide a smooth tradeoff between speed and accuracy.", "YOLO9000 is a real-time framework for detection more than 9000 object categories by jointly optimizing detection and classification. We use WordTree to combine data from various sources and our joint optimization technique to train simultaneously on ImageNet and COCO. YOLO9000 is a strong step towards closing the dataset size gap between detection and classification.", "Many of our techniques generalize outside of object detection. Our WordTree representation of ImageNet offers a richer, more detailed output space for image classification. Dataset combination using hierarchical classification would be useful in the classification and segmentation domains. Training techniques like multi-scale training could provide benefit across a variety of visual tasks.", "For future work we hope to use similar techniques for weakly supervised image segmentation. We also plan to improve our detection results using more powerful matching strategies for assigning weak labels to classification data during training. Computer vision is blessed with an enormous amount of labelled data. We will continue looking for ways to bring different sources and structures of data together to make stronger models of the visual world."], "figure_types": {"7d39d69b23424446f0400ef603b2e3e22d0309d6/1-Figure1-1.png": "photograph(s)", "7d39d69b23424446f0400ef603b2e3e22d0309d6/3-Figure2-1.png": "plot", "7d39d69b23424446f0400ef603b2e3e22d0309d6/3-Table1-1.png": "table", "7d39d69b23424446f0400ef603b2e3e22d0309d6/4-Figure3-1.png": "schematic", "7d39d69b23424446f0400ef603b2e3e22d0309d6/4-Figure4-1.png": "plot", "7d39d69b23424446f0400ef603b2e3e22d0309d6/5-Table2-1.png": "table", "7d39d69b23424446f0400ef603b2e3e22d0309d6/5-Table3-1.png": "table", "7d39d69b23424446f0400ef603b2e3e22d0309d6/6-Table4-1.png": "table", "7d39d69b23424446f0400ef603b2e3e22d0309d6/6-Table5-1.png": "table", "7d39d69b23424446f0400ef603b2e3e22d0309d6/6-Table6-1.png": "table", "7d39d69b23424446f0400ef603b2e3e22d0309d6/7-Figure5-1.png": "schematic", "7d39d69b23424446f0400ef603b2e3e22d0309d6/8-Figure6-1.png": "schematic", "7d39d69b23424446f0400ef603b2e3e22d0309d6/8-Table7-1.png": "table"}}, "1610.06475": {"paper_id": "paper_85", "title": "ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras", "arxiv_url": "https://arxiv.org/abs/1610.06475", "s2orc_url": "https://www.semanticscholar.org/paper/877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6", "all_figures_tables": {"877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/2-Figure1-1.png": "Fig. 1. ORB-SLAM2 processes stereo and RGB-D inputs to estimate camera trajectory and build a map of the environment. The system is able to close loops, relocalize, and reuse its map in real-time on standard CPUs with high accuracy and robustness.", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/4-Figure2-1.png": "Fig. 2. ORB-SLAM2 is composed of three main parallel threads: tracking, local mapping and loop closing, which can create a fourth thread to perform full BA after a loop closure. The tracking thread pre-processes the stereo or RGB-D input so that the rest of the system operates independently of the input sensor. Although it is not shown in this figure, ORB-SLAM2 also works with a monocular input as in [1].", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/6-Figure3-1.png": "Fig. 3. Tracked points in KITTI 01 [2]. Green points have a depth less than 40 times the stereo baseline, while blue points are further away. In this kind of sequences it is important to insert keyframes often enough so that the amount of close points allows for accurate translation estimation. Far points contribute to estimate orientation but provide weak information for translation and scale.", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/6-Figure4-1.png": "Fig. 4. Estimated trajectory (black) and ground-truth (red) in KITTI 00, 01, 05 and 07.", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/6-TableI-1.png": "TABLE I COMPARISON OF ACCURACY IN THE KITTI DATASET.", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/7-Figure5-1.png": "Fig. 5. Estimated trajectory (black) and ground-truth (red) in KITTI 08. Left: monocular ORB-SLAM [1], right: ORB-SLAM2 (stereo). Monocular ORBSLAM suffers from severe scale drift in this sequence, especially at the turns. In contrast the proposed stereo version is able to estimate the true scale of the trajectory and map without scale drift.", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/7-Figure6-1.png": "Fig. 6. Estimated trajectory (black) and groundtruth (red) in EuRoC V1 02 medium, V2 02 medium, MH 03 medium and MH 05 difficutlt.", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/7-TableII-1.png": "TABLE II EUROC DATASET. COMPARISON OF TRANSLATION RMSE (m).", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/8-Figure7-1.png": "Fig. 7. Dense pointcloud reconstructions from estimated keyframe poses and sensor depth maps in TUM RGB-D fr3 office, fr1 room, fr2 desk and fr3 nst.", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/8-TableIII-1.png": "TABLE III TUM RGB-D DATASET. COMPARISON OF TRANSLATION RMSE (m).", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/9-TableIV-1.png": "TABLE IV TIMING RESULTS OF EACH THREAD IN MILISECONDS (MEAN \u00b1 2 STD. DEVIATIONS)."}, "referred_figures_tables": [["877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/4-Figure2-1.png"], ["877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/2-Figure1-1.png"], ["877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/4-Figure2-1.png"], ["877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/6-TableI-1.png"]], "question_id": [3, 4, 5, 1], "question": ["Is the difference between ORB-SLAM and ORB-SLAM2 that ORB-SLAM only supports monocular cameras?", "What is fusion?", "What information from the input images do ORB features extract?", "What metrics are used for the evaluation of SLAM systems?"], "question_section": ["Introduction", "Introduction", "ORB-SLAM2", "Abstract"], "question_trigger_sentence": ["In this paper we build on our monocular ORB-SLAM [1] and propose ORB-SLAM2 with the following contributions:\n\u2022 The first open-source1 SLAM system for monocular, stereo and RGB-D cameras, including loop closing, relocalization and map reuse.\n\u2022 Our RGB-D results show that by using Bundle Adjustment (BA) we achieve more accuracy than state-of-the-art methods based on ICP or photometric and depth error minimization.\n\u2022 By using close and far stereo points and monocular observations our stereo results are more accurate than the state-of-the-art direct stereo SLAM.\n\u2022 A lightweight localization mode that can effectively reuse the map with mapping disabled.", "Note that our SLAM does not perform any fusion like KinectFusion [4] or similar, but the good definition indicates the accuracy of the keyframe poses. ", "The system uses the same ORB features [17] for tracking, mapping and place recognition tasks. These features are robust to rotation and scale and present a good invariance to camera auto-gain and auto-exposure, and illumination changes.", "The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution."], "question_type": ["Shallow question", "Testing question", "Shallow question", "Shallow question"], "evidential_info": [[{"context": "ORB-SLAM2 for stereo and RGB-D cameras is built on our monocular feature-based ORB-SLAM [1], whose main components are summarized here for reader convenience.A general overview of the system is shown in Fig. 2. The system has three main parallel threads: 1) the tracking to localize the camera with every frame by findingfeature matches to the local map and minimizing the reprojection error applying motion-only BA, 2) the local mapping to manage the local map and optimize it, performing local BA,3) the loop closing to detect large loops and correct the accumulated drift by performing a pose-graph optimization. This thread launches a fourth thread to perform full BA afterthe pose-graph optimization, to compute the optimal structure and motion solution.", "rationale": "In this paper authors build on our monocular ORB-SLAM [1] and propose ORB-SLAM2"}, {"context": "In this paper we build on our monocular ORB-SLAM [1] and propose ORB-SLAM2 with the following contributions:\u2022The first open-source111https://github.com/raulmur/ORB_SLAM2 SLAM system for monocular, stereo and RGB-D cameras, including loop closing, relocalization and map reuse.\u2022Our RGB-D results show that by using Bundle Adjustment (BA) we achieve more accuracy than state-of-the-art methods based on ICP or photometric and depth error minimization.\u2022By using close and far stereo points and monocular observations our stereo results are more accurate than the state-of-the-art direct stereo SLAM.\u2022A lightweight localization mode that can effectively reuse the map with mapping disabled.", "rationale": "ORB-SLAM2 for stereo and RGB-D cameras is built on monocular feature-based ORB-SLAM."}], [{"context": "Fig. 1 shows examples of ORB-SLAM2 output from stereo and RGB-D inputs. The stereo case shows the final trajectory and sparse reconstruction of the sequence 00 fromthe KITTI dataset [2]. This is an urban sequence with multiple loop closures that ORB-SLAM2 was able to successfully detect. The RGB-D case shows the keyframe poses estimatedin sequence fr1_room from the TUM RGB-D Dataset [3], and a dense pointcloud, rendered by backprojecting sensor depth maps from the estimated keyframe poses. Note that our SLAMdoes not perform any fusion like KinectFusion [4] or similar, but the gooddefinition indicates the accuracy of the keyframe poses. More examples are shown on the attached video.", "rationale": "One of the earliest and most famed RGB-D SLAM systems was the KinectFusion of Newcombe et al. [4]. This method fused all depth data from the sensor into a volumetricdense model that is used to track the camera pose using ICP."}, {"context": "One of the earliest and most famed RGB-D SLAM systems was the KinectFusion of Newcombe et al. [4]. This method fused all depth data from the sensor into a volumetricdense model that is used to track the camera pose using ICP. This system was limited to small workspaces due to its volumetric representation and the lack of loop closing.Kintinuous by Whelan et al. [12] was able to operate in large environments by using a rolling cyclical buffer and included loop closing using place recognitionand pose graph optimization.", "rationale": "Proposed SLAM does not perform any fusion like KinectFusion"}], [{"context": "The system uses the same ORB features [17] for tracking, mapping and place recognition tasks. These features are robust to rotation and scale and present a good invarianceto camera auto-gain and auto-exposure, and illumination changes. Moreover they are fast to extract and match allowing for real-time operation and show good precision/recallperformance in bag-of-word place recognition [18].", "rationale": "Proposed system uses the same ORB features [17] for tracking, mapping and place recognition tasks. These features are robust to rotation and scale and present a good invariance to camera auto-gain and auto-exposure, and illumination changes. Moreover, they are fast to extract and match allowing for real-time operation and show good precision/recall performance in bag-of-word place recognition [18]."}, {"context": "ORB-SLAM2 as a feature-based method pre-processes the input to extract features at salient keypoint locations, as shown in Fig. 2b. The inputimages are then discarded and all system operationsare based on these features, so that the system is independent of the sensor being stereo or RGB-D.Our system handles monocular and stereo keypoints, which are further classified as close or far.", "rationale": "ORB-SLAM2 as a feature-based method pre-processes the input to extract features at salient keypoint locations."}, {"context": "Stereo keypoints are defined by three coordinates \\mathbf{x}_{\\mathrm{s}}=\\left(u_{L},v_{L},u_{R}\\right), being (u_{L},v_{L}) the coordinates on the left image and u_{R} the horizontalcoordinate in the right image. For stereo cameras, we extract ORB in both images and for every left ORB we search for a match in the right image. This can be done very efficiently assumingstereo rectified images, so that epipolar lines are horizontal. We then generate the stereo keypoint with the coordinates of the left ORB and the horizontal coordinate of the rightmatch, which is subpixel refined by patch correlation. For RGB-D cameras, we extract ORB features onthe RGB image and, as proposed by Strasdat et al. [8], for each feature with coordinates\\left(u_{L},v_{L}\\right) we transform its depth value d into a virtual right coordinate:u_{R}=u_{L}-\\frac{f_{x}b}{d}(1)where f_{x} is the horizontal focal length and b is the baseline between thestructured light projector and the infrared camera, which we approximate to 8cm for Kinectand Asus Xtion. The uncertainty of the depth sensor is represented by the uncertainty of thevirtual right coordinate. In this way, features from stereo and RGB-D input are handled equallyby the rest of the system.", "rationale": "For stereo cameras, we extract ORB in both images and for every left ORB we search for a match in the right image."}], [{"context": "The KITTI dataset [2] contains stereo sequences recorded from a car in urban and highway environments. The stereo sensor has a \u223c54cm baseline and works at 10Hz with a resolution after rectification of 1240 \u00d7 376 pixels. Sequences 00, 02, 05, 06, 07 and 09 contain loops. Our ORB-SLAM2 detects all loops and is able to reuse its map afterwards, except for sequence 09 where the loop happens in very few frames at the end of the sequence. Table I shows results in the 11 training sequences, which have public ground-truth, compared to the state-of-the-art Stereo LSD-SLAM [11], to our knowledge the only stereo SLAM showing detailed results for all sequences. We use two different metrics, the absolute translation RMSE tabs proposed in [3], and the average relative translation trel and rotation rrel errors proposed in [2].", "rationale": "The KITTI dataset [2] contains stereo sequences recorded from a car in urban and highway environments. The stereo sensor has a \u223c54cm baseline and works at 10Hz with a resolution after rectification of 1240 \u00d7 376 pixels. Sequences 00, 02, 05, 06, 07 and 09 contain loops. Our ORB-SLAM2 detects all loops and is able to reuse its map afterwards, except for sequence 09 where the loop happens in very few frames at the end of the sequence. Table I shows results in the 11 training sequences, which have public ground-truth, compared to the state-of-the-art Stereo LSD-SLAM [11], to our knowledge the only stereo SLAM showing detailed results for all sequences. We use two different metrics, the absolute translation RMSE tabs proposed in [3], and the average relative translation trel and rotation rrel errors proposed in [2]."}]], "composition": ["ORB-SLAM2 for stereo and RGB-D cameras is built on monocular feature-based ORB-SLAM. This shows that ORB-SLAM only supports monocular cameras as compared with ORB-SLAM.", "Fusion is used in KinectFusion method in which all depth data from the sensor is fused into a volumetric dense model which is then used to track to camera pose.", "ORB features are extracted at salient keypoints in both view of image. For every left ORB image a matching feature can be found at right image. ORB extract such features from images which are robust to rotation and scale and present a good invariance to camera auto-gain and auto-exposure, and illumination changes.", "for evaluation of SLAM systems two different metrics, the absolute translation RMSE tabs proposed in [3], and the average relative translation trel and rotation rrel errors are used."], "Is_figure_in_evidence": [true, true, true, false], "Is_table_in_evidence": [false, false, false, true], "question_key": ["1345", "1346", "1347", "1350"], "passages": ["Simultaneous Localization and Mapping (SLAM) has been a hot research topic in the last two decades in the Computer Vision and Robotics communities, and has recentlyattracted the attention of high-technological companies. SLAM techniques build a map of an unknown environment and localize the sensor in the map with a strong focus on real-timeoperation. Among the different sensor modalities, cameras are cheap and provide rich information of the environment that allows for robust and accurate place recognition.Therefore Visual SLAM solutions, where the main sensor is a camera, are of major interest nowadays.Place recognition is a key module of a SLAM system to close loops (i.e. detect when the sensor returns to a mapped area and correct the accumulated error in exploration)and to relocalize the camera after a tracking failure, due to occlusion or aggressive motion, or at system re-initialization.", "Visual SLAM can be performed by using just a monocular camera, which is the cheapest and smallest sensor setup.However as depth is not observable from just one camera, the scale of the map andestimated trajectory is unknown. In addition the system bootstrapping require multi-view or filtering techniques to produce an initial map as it cannot be triangulated from the veryfirst frame. Last but not least, monocular SLAM suffers from scale drift and may fail if performing pure rotations in exploration. By using a stereo oran RGB-D camera all these issues are solved and allows for the most reliable Visual SLAM solutions.", "In this paper we build on our monocular ORB-SLAM [1] and propose ORB-SLAM2 with the following contributions:\u2022The first open-source111https://github.com/raulmur/ORB_SLAM2 SLAM system for monocular, stereo and RGB-D cameras, including loop closing, relocalization and map reuse.\u2022Our RGB-D results show that by using Bundle Adjustment (BA) we achieve more accuracy than state-of-the-art methods based on ICP or photometric and depth error minimization.\u2022By using close and far stereo points and monocular observations our stereo results are more accurate than the state-of-the-art direct stereo SLAM.\u2022A lightweight localization mode that can effectively reuse the map with mapping disabled.", "Fig. 1 shows examples of ORB-SLAM2 output from stereo and RGB-D inputs. The stereo case shows the final trajectory and sparse reconstruction of the sequence 00 fromthe KITTI dataset [2]. This is an urban sequence with multiple loop closures that ORB-SLAM2 was able to successfully detect. The RGB-D case shows the keyframe poses estimatedin sequence fr1_room from the TUM RGB-D Dataset [3], and a dense pointcloud, rendered by backprojecting sensor depth maps from the estimated keyframe poses. Note that our SLAMdoes not perform any fusion like KinectFusion [4] or similar, but the gooddefinition indicates the accuracy of the keyframe poses. More examples are shown on the attached video.", "In the rest of the paper, we discuss related work in Section II, we describe our system in Section III, then present the evaluation results in Section IV andend with conclusions in Section V.", "In this section we discuss related work on stereo and RGB-D SLAM. Our discussion, as well as the evaluation in Section IV is focused only on SLAM approaches.", "A remarkable early stereo SLAM system was the work of Paz et al. [5]. Based on Conditionally Independent Divide and Conquer EKF-SLAM it was ableto operate in larger environments than other approaches at that time. Most importantly, it was the first stereo SLAM exploiting both close and far points(i.e. points whose depth cannot be reliably estimated due to little disparity in the stereo camera), using an inverse depth parametrization [6] for the latter.They empirically showed that points can be reliably triangulated if their depth is less than \u223c40similar-toabsent40{\\sim}40\u223c 40 times the stereo baseline. In this work we follow this strategy of treating in a differentway close and far points, as explained in Section III-A.", "Most modern stereo SLAM systems are keyframe-based [7] and perform BA optimization in a local area to achieve scalability. The work of Strasdat et al. [8]performs a joint optimization of BA (point-pose constraints) in an inner window of keyframes and pose-graph (pose-pose constraints) in an outer window. By limiting thesize of these windows the method achieves constant time complexity, at the expense of not guaranteeing global consistency. The RSLAM of Mei et al. [9] uses a relativerepresentationof landmarks and poses and performs relative BA in an active area which can be constrained for constant-time. RSLAM is able to close loops which allow to expandactive areas at both sides of a loop, but global consistency is not enforced. The recent S-PTAM by Pire et al. [10] performs local BA, however itlacks large loop closing.Similar to these approaches we perform BA in a local set of keyframes so that the complexity is independent of the map size and we can operate in large environments.However our goal is to build a globally consistent map. When closing a loop, our system aligns first both sides, similar to RSLAM, so that the tracking is able to continue localizingusing the old map and then performs a pose-graph optimization that minimizes the drift accumulated in the loop, followed by full BA.", "The recent Stereo LSD-SLAM of Engel et al. [11] is a semi-dense direct approach that minimizes photometric error in image regions with highgradient. Not relying on features, the method is expected to be more robust to motion blur or poorly-textured environments. However as a direct method its performancecan be severely degraded by unmodeled effects like rolling shutter or non-lambertian reflectance.", "One of the earliest and most famed RGB-D SLAM systems was the KinectFusion of Newcombe et al. [4]. This method fused all depth data from the sensor into a volumetricdense model that is used to track the camera pose using ICP. This system was limited to small workspaces due to its volumetric representation and the lack of loop closing.Kintinuous by Whelan et al. [12] was able to operate in large environments by using a rolling cyclical buffer and included loop closing using place recognitionand pose graph optimization.", "Probably the first popular open-source system was the RGB-D SLAM of Endres et al. [13]. This is a feature-based system, whose front-end computes frame-to-frame motionby feature matching and ICP. The back-end performs pose-graph optimization with loop closure constraints from a heuristic search. Similarly the back-end of DVO-SLAM by Kerl et al. [14]optimizes a pose-graph where keyframe-to-keyframe constraints are computed from a visual odometry that minimizes both photometric and depth error. DVO-SLAM also searches forloop candidates in a heuristic fashion over all previous frames, instead of relying on place recognition.", "The recent ElasticFusion of Whelan et al. [15] builds a surfel-based map of the environment. This is a map-centric approach that forget poses and performsloop closing applying a non-rigid deformation to the map, instead of a standard pose-graph optimization. The detailed reconstruction and localization accuracy of this systemis impressive, but the current implementation is limited to room-size maps as the complexity scales with the number of surfels in the map.", "As proposed by Strasdat et al. [8] our ORB-SLAM2 uses depth information to synthesize a stereo coordinate for extracted features on the image. This way our system isagnostic of the input being stereo or RGB-D. Differently to all above methods our back-end is based on bundle adjustment and builds a globally consistent sparse reconstruction.Therefore our method is lightweight and works with standard CPUs. Our goal is long-term and globally consistent localization instead of building the most detailed dense reconstruction.However from the highly accurate keyframe poses one could fuse depth maps and get accurate reconstruction on-the-fly in a local area or post-process the depth maps from all keyframesafter a full BA and get an accurate 3D model of the whole scene.", "ORB-SLAM2 for stereo and RGB-D cameras is built on our monocular feature-based ORB-SLAM [1], whose main components are summarized here for reader convenience.A general overview of the system is shown in Fig. 2. The system has three main parallel threads: 1) the tracking to localize the camera with every frame by findingfeature matches to the local map and minimizing the reprojection error applying motion-only BA, 2) the local mapping to manage the local map and optimize it, performing local BA,3) the loop closing to detect large loops and correct the accumulated drift by performing a pose-graph optimization. This thread launches a fourth thread to perform full BA afterthe pose-graph optimization, to compute the optimal structure and motion solution.", "The system has embedded a Place Recognition module based on DBoW2 [16] for relocalization, in case of tracking failure (e.g. an occlusion) or for reinitialization in an alreadymapped scene,and for loop detection. The system maintains a covisibiliy graph [8] that links any two keyframes observing common points and a minimum spanning tree connecting all keyframes.These graph structures allow to retrieve local windows of keyframes, so that tracking and local mapping operate locally, allowing to work on large environments, and serve asstructure for the pose-graph optimization performed when closing a loop.", "The system uses the same ORB features [17] for tracking, mapping and place recognition tasks. These features are robust to rotation and scale and present a good invarianceto camera auto-gain and auto-exposure, and illumination changes. Moreover they are fast to extract and match allowing for real-time operation and show good precision/recallperformance in bag-of-word place recognition [18].", "In the rest of this section we present how stereo/depth information is exploited and which elements of the system are affected. For a detailed description of each system block, werefer the reader to our monocular publication [1].", "ORB-SLAM2 as a feature-based method pre-processes the input to extract features at salient keypoint locations, as shown in Fig. 2b. The inputimages are then discarded and all system operationsare based on these features, so that the system is independent of the sensor being stereo or RGB-D.Our system handles monocular and stereo keypoints, which are further classified as close or far.", "Stereo keypoints are defined by three coordinates \ud835\udc31s=(uL,vL,uR)subscript\ud835\udc31ssubscript\ud835\udc62\ud835\udc3fsubscript\ud835\udc63\ud835\udc3fsubscript\ud835\udc62\ud835\udc45\\mathbf{x}_{\\mathrm{s}}=\\left(u_{L},v_{L},u_{R}\\right)bold_x start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT = ( italic_u start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ), being (uL,vL)subscript\ud835\udc62\ud835\udc3fsubscript\ud835\udc63\ud835\udc3f(u_{L},v_{L})( italic_u start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ) the coordinates on the left image and uRsubscript\ud835\udc62\ud835\udc45u_{R}italic_u start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT the horizontalcoordinate in the right image. For stereo cameras, we extract ORB in both images and for every left ORB we search for a match in the right image. This can be done very efficiently assumingstereo rectified images, so that epipolar lines are horizontal. We then generate the stereo keypoint with the coordinates of the left ORB and the horizontal coordinate of the rightmatch, which is subpixel refined by patch correlation. For RGB-D cameras, we extract ORB features onthe RGB image and, as proposed by Strasdat et al. [8], for each feature with coordinates(uL,vL)subscript\ud835\udc62\ud835\udc3fsubscript\ud835\udc63\ud835\udc3f\\left(u_{L},v_{L}\\right)( italic_u start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ) we transform its depth value d\ud835\udc51ditalic_d into a virtual right coordinate:uR=uL\u2212fxbdsubscript\ud835\udc62\ud835\udc45subscript\ud835\udc62\ud835\udc3fsubscript\ud835\udc53\ud835\udc65\ud835\udc4f\ud835\udc51u_{R}=u_{L}-\\frac{f_{x}b}{d}italic_u start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT = italic_u start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT - divide start_ARG italic_f start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_b end_ARG start_ARG italic_d end_ARG(1)where fxsubscript\ud835\udc53\ud835\udc65f_{x}italic_f start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT is the horizontal focal length and b\ud835\udc4fbitalic_b is the baseline between thestructured light projector and the infrared camera, which we approximate to 8cm for Kinectand Asus Xtion. The uncertainty of the depth sensor is represented by the uncertainty of thevirtual right coordinate. In this way, features from stereo and RGB-D input are handled equallyby the rest of the system.", "A stereo keypoint is classified as close if its associated depth is less than 40 times the stereo/RGB-D baseline, as suggested in [5], otherwise it isclassified as far. Close keypoints can be safely triangulated from one frame as depth is accurately estimated and provide scale, translation and rotation information.On the other hand far points provide accurate rotation information but weaker scale and translation information. We triangulate far points when they are supported by multiple views.", "Monocular keypoints are defined by two coordinates \ud835\udc31m=(uL,vL)subscript\ud835\udc31msubscript\ud835\udc62\ud835\udc3fsubscript\ud835\udc63\ud835\udc3f\\mathbf{x}_{\\mathrm{m}}=\\left(u_{L},v_{L}\\right)bold_x start_POSTSUBSCRIPT roman_m end_POSTSUBSCRIPT = ( italic_u start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ) on the left image and correspond to all those ORB for which a stereo matchcould not be found or that have an invalid depth value in the RGB-D case. These points are only triangulated from multiple views and do not provide scale information, but contributeto the rotation and translation estimation.", "One of the main benefits of using stereo or RGB-D cameras is that, by having depth information from just one frame, we do not need a specific structure from motion initializationas in the monocular case. At system startup we create a keyframe with the first frame, set its pose to the origin, and create an initial map from all stereo keypoints.", "Our system performs BA to optimize the camera pose in the tracking thread (motion-only BA), to optimize a local window of keyframes and points in the local mapping thread (local BA),and after a loop closure to optimize all keyframes and points (full BA). We use the Levenberg\u2013Marquardt method implemented in g2o [19].", "Motion-only BA optimizes the camera orientation \ud835\udc11\u2208SO(3)\ud835\udc11\ud835\udc46\ud835\udc423\\mathbf{R}\\in SO(3)bold_R \u2208 italic_S italic_O ( 3 ) and position \ud835\udc2d\u2208\u211d3\ud835\udc2dsuperscript\u211d3\\mathbf{t}\\in\\mathbb{R}^{3}bold_t \u2208 blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT, minimizing the reprojectionerror between matched 3D points \ud835\udc17i\u2208\u211d3superscript\ud835\udc17\ud835\udc56superscript\u211d3\\mathbf{X}^{i}\\in\\mathbb{R}^{3}bold_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT in world coordinatesand keypoints \ud835\udc31(\u22c5)isubscriptsuperscript\ud835\udc31\ud835\udc56\u22c5\\mathbf{x}^{i}_{\\mathrm{(\\cdot)}}bold_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( \u22c5 ) end_POSTSUBSCRIPT, either monocular \ud835\udc31mi\u2208\u211d2subscriptsuperscript\ud835\udc31\ud835\udc56msuperscript\u211d2\\mathbf{x}^{i}_{\\mathrm{m}}\\in\\mathbb{R}^{2}bold_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_m end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT or stereo \ud835\udc31si\u2208\u211d3subscriptsuperscript\ud835\udc31\ud835\udc56ssuperscript\u211d3\\mathbf{x}^{i}_{\\mathrm{s}}\\in\\mathbb{R}^{3}bold_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT, with i\u2208\ud835\udcb3\ud835\udc56\ud835\udcb3i\\in\\mathcal{X}italic_i \u2208 caligraphic_Xthe set of all matches:", "{\ud835\udc11,\ud835\udc2d}=argmin\ud835\udc11,\ud835\udc2d\u2211i\u2208\ud835\udcb3\u03c1(\u2016\ud835\udc31(\u22c5)i\u2212\u03c0(\u22c5)(\ud835\udc11\ud835\udc17i+\ud835\udc2d)\u2016\u03a32)\ud835\udc11\ud835\udc2dsubscriptargmin\ud835\udc11\ud835\udc2dsubscript\ud835\udc56\ud835\udcb3\ud835\udf0csubscriptsuperscriptnormsubscriptsuperscript\ud835\udc31\ud835\udc56\u22c5subscript\ud835\udf0b\u22c5superscript\ud835\udc11\ud835\udc17\ud835\udc56\ud835\udc2d2\u03a3\\{\\mathbf{R},\\mathbf{t}\\}=\\operatorname*{argmin}_{\\mathbf{R},\\mathbf{t}}\\sum_{i\\in\\mathcal{X}}\\rho\\left(\\left\\|\\mathbf{x}^{i}_{\\mathrm{(\\cdot)}}-\\pi_{\\mathrm{(\\cdot)}}\\left(\\mathbf{R}\\mathbf{X}^{i}+\\mathbf{t}\\right)\\right\\|^{2}_{\\Sigma}\\right){ bold_R , bold_t } = roman_argmin start_POSTSUBSCRIPT bold_R , bold_t end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_i \u2208 caligraphic_X end_POSTSUBSCRIPT italic_\u03c1 ( \u2225 bold_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( \u22c5 ) end_POSTSUBSCRIPT - italic_\u03c0 start_POSTSUBSCRIPT ( \u22c5 ) end_POSTSUBSCRIPT ( bold_RX start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT + bold_t ) \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_\u03a3 end_POSTSUBSCRIPT )(2)where \u03c1\ud835\udf0c\\rhoitalic_\u03c1 is the robust Huber cost function and \u03a3\u03a3\\Sigmaroman_\u03a3 the covariance matrix associated to the scale of the keypoint. The projection functions \u03c0(\u22c5)subscript\ud835\udf0b\u22c5\\pi_{(\\cdot)}italic_\u03c0 start_POSTSUBSCRIPT ( \u22c5 ) end_POSTSUBSCRIPT,monocular \u03c0msubscript\ud835\udf0bm\\pi_{\\mathrm{m}}italic_\u03c0 start_POSTSUBSCRIPT roman_m end_POSTSUBSCRIPT and rectified stereo \u03c0ssubscript\ud835\udf0bs\\pi_{\\mathrm{s}}italic_\u03c0 start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT,are defined as follows:\u03c0m([XYZ])=[fxXZ+cxfyYZ+cy],\u03c0s([XYZ])=[fxXZ+cxfyYZ+cyfxX\u2212bZ+cx]formulae-sequencesubscript\ud835\udf0bmmatrix\ud835\udc4b\ud835\udc4c\ud835\udc4dmatrixsubscript\ud835\udc53\ud835\udc65\ud835\udc4b\ud835\udc4dsubscript\ud835\udc50\ud835\udc65subscript\ud835\udc53\ud835\udc66\ud835\udc4c\ud835\udc4dsubscript\ud835\udc50\ud835\udc66subscript\ud835\udf0bsmatrix\ud835\udc4b\ud835\udc4c\ud835\udc4dmatrixsubscript\ud835\udc53\ud835\udc65\ud835\udc4b\ud835\udc4dsubscript\ud835\udc50\ud835\udc65subscript\ud835\udc53\ud835\udc66\ud835\udc4c\ud835\udc4dsubscript\ud835\udc50\ud835\udc66subscript\ud835\udc53\ud835\udc65\ud835\udc4b\ud835\udc4f\ud835\udc4dsubscript\ud835\udc50\ud835\udc65\\pi_{\\mathrm{m}}\\left(\\begin{bmatrix}X\\\\Y\\\\Z\\end{bmatrix}\\right)=\\begin{bmatrix}f_{x}\\frac{X}{Z}+c_{x}\\\\[1.99997pt]f_{y}\\frac{Y}{Z}+c_{y}\\end{bmatrix},\\pi_{\\mathrm{s}}\\left(\\begin{bmatrix}X\\\\Y\\\\Z\\end{bmatrix}\\right)=\\begin{bmatrix}f_{x}\\frac{X}{Z}+c_{x}\\\\[1.99997pt]f_{y}\\frac{Y}{Z}+c_{y}\\\\[1.99997pt]f_{x}\\frac{X-b}{Z}+c_{x}\\end{bmatrix}italic_\u03c0 start_POSTSUBSCRIPT roman_m end_POSTSUBSCRIPT ( [ start_ARG start_ROW start_CELL italic_X end_CELL end_ROW start_ROW start_CELL italic_Y end_CELL end_ROW start_ROW start_CELL italic_Z end_CELL end_ROW end_ARG ] ) = [ start_ARG start_ROW start_CELL italic_f start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT divide start_ARG italic_X end_ARG start_ARG italic_Z end_ARG + italic_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_f start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT divide start_ARG italic_Y end_ARG start_ARG italic_Z end_ARG + italic_c start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] , italic_\u03c0 start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT ( [ start_ARG start_ROW start_CELL italic_X end_CELL end_ROW start_ROW start_CELL italic_Y end_CELL end_ROW start_ROW start_CELL italic_Z end_CELL end_ROW end_ARG ] ) = [ start_ARG start_ROW start_CELL italic_f start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT divide start_ARG italic_X end_ARG start_ARG italic_Z end_ARG + italic_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_f start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT divide start_ARG italic_Y end_ARG start_ARG italic_Z end_ARG + italic_c start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_f start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT divide start_ARG italic_X - italic_b end_ARG start_ARG italic_Z end_ARG + italic_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ](3)where (fx,fy)subscript\ud835\udc53\ud835\udc65subscript\ud835\udc53\ud835\udc66(f_{x},f_{y})( italic_f start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) is the focal length, (cx,cy)subscript\ud835\udc50\ud835\udc65subscript\ud835\udc50\ud835\udc66(c_{x},c_{y})( italic_c start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) is the principal point and b\ud835\udc4fbitalic_b the baseline, all known from calibration.", "Local BA optimizes a set of covisible keyframes \ud835\udca6Lsubscript\ud835\udca6\ud835\udc3f\\mathcal{K}_{L}caligraphic_K start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT and all points seen in those keyframes \ud835\udcabLsubscript\ud835\udcab\ud835\udc3f\\mathcal{P}_{L}caligraphic_P start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT.All other keyframes \ud835\udca6Fsubscript\ud835\udca6\ud835\udc39\\mathcal{K}_{F}caligraphic_K start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT, not in \ud835\udca6Lsubscript\ud835\udca6\ud835\udc3f\\mathcal{K}_{L}caligraphic_K start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT, observing points in \ud835\udcabLsubscript\ud835\udcab\ud835\udc3f\\mathcal{P}_{L}caligraphic_P start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPTcontribute to the cost function but remain fixed in the optimization. Defining \ud835\udcb3ksubscript\ud835\udcb3\ud835\udc58\\mathcal{X}_{k}caligraphic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT as the set of matches between pointsin \ud835\udcabLsubscript\ud835\udcab\ud835\udc3f\\mathcal{P}_{L}caligraphic_P start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT and keypoints in a keyframe k\ud835\udc58kitalic_k, the optimization problem is the following:{\ud835\udc17i,\ud835\udc11l,\ud835\udc2dl|i\u2208\ud835\udcabL,l\u2208\ud835\udca6L}=argmin\ud835\udc17i,\ud835\udc11l,\ud835\udc2dl\u2211k\u2208\ud835\udca6L\u222a\ud835\udca6F\u2211j\u2208\ud835\udcb3k\u03c1(Ekj)Ekj=\u2016\ud835\udc31(\u22c5)j\u2212\u03c0(\u22c5)(\ud835\udc11k\ud835\udc17j+\ud835\udc2dk)\u2016\u03a32conditional-setsuperscript\ud835\udc17\ud835\udc56subscript\ud835\udc11\ud835\udc59subscript\ud835\udc2d\ud835\udc59formulae-sequence\ud835\udc56subscript\ud835\udcab\ud835\udc3f\ud835\udc59subscript\ud835\udca6\ud835\udc3fsubscriptargminsuperscript\ud835\udc17\ud835\udc56subscript\ud835\udc11\ud835\udc59subscript\ud835\udc2d\ud835\udc59subscript\ud835\udc58subscript\ud835\udca6\ud835\udc3fsubscript\ud835\udca6\ud835\udc39subscript\ud835\udc57subscript\ud835\udcb3\ud835\udc58\ud835\udf0csubscript\ud835\udc38\ud835\udc58\ud835\udc57subscript\ud835\udc38\ud835\udc58\ud835\udc57subscriptsuperscriptdelimited-\u2225\u2225subscriptsuperscript\ud835\udc31\ud835\udc57\u22c5subscript\ud835\udf0b\u22c5subscript\ud835\udc11\ud835\udc58superscript\ud835\udc17\ud835\udc57subscript\ud835\udc2d\ud835\udc582\u03a3\\begin{gathered}\\{\\mathbf{X}^{i},\\mathbf{R}_{l},\\mathbf{t}_{l}|i\\in\\mathcal{P}_{L},l\\in\\mathcal{K}_{L}\\}=\\operatorname*{argmin}_{\\mathbf{X}^{i},\\mathbf{R}_{l},\\mathbf{t}_{l}}\\sum_{k\\in\\mathcal{K}_{L}\\cup\\mathcal{K}_{F}}\\sum_{j\\in\\mathcal{X}_{k}}\\rho\\left(E_{kj}\\right)\\\\E_{kj}=\\left\\|\\mathbf{x}^{j}_{\\mathrm{(\\cdot)}}-\\pi_{\\mathrm{(\\cdot)}}\\left(\\mathbf{R}_{k}\\mathbf{X}^{j}+\\mathbf{t}_{k}\\right)\\right\\|^{2}_{\\Sigma}\\end{gathered}start_ROW start_CELL { bold_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_R start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , bold_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | italic_i \u2208 caligraphic_P start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT , italic_l \u2208 caligraphic_K start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT } = roman_argmin start_POSTSUBSCRIPT bold_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_R start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , bold_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_k \u2208 caligraphic_K start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT \u222a caligraphic_K start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_j \u2208 caligraphic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_\u03c1 ( italic_E start_POSTSUBSCRIPT italic_k italic_j end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL italic_E start_POSTSUBSCRIPT italic_k italic_j end_POSTSUBSCRIPT = \u2225 bold_x start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( \u22c5 ) end_POSTSUBSCRIPT - italic_\u03c0 start_POSTSUBSCRIPT ( \u22c5 ) end_POSTSUBSCRIPT ( bold_R start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_X start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT + bold_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_\u03a3 end_POSTSUBSCRIPT end_CELL end_ROW(4)", "Full BA is the specific case of local BA, where all keyframes and points in the map are optimized, except the origin keyframe that is fixed to eliminate the gauge freedom.", "Loop closing is performed in two steps, firstly a loop has to be detected and validated, and secondly the loop is corrected optimizing a pose-graph.In contrast to monocular ORB-SLAM, where scale drift may occur [20], the stereo/depth information makes scale observable and the geometric validation andpose-graph optimization no longer require dealing with scale drift and are based on rigid body transformations instead of similarities.", "In ORB-SLAM2 we have incorporated a full BA optimization after the pose-graph to achieve the optimal solution. This optimization might be very costly and therefore weperform it in a separate thread, allowing the system to continue creating map and detecting loops. However this brings the challenge of merging the bundle adjustmentoutput with the current state of the map. If a new loop is detected while the optimization is running, we abort the optimizationand proceed to close the loop, which will launch the full BA optimization again. When the full BA finishes,we need to merge the updated subset of keyframes and points optimized by the full BA, with the non-updated keyframes and pointsthat where inserted while the optimization was running. This is done by propagating the correction of updated keyframes (i.e. the transformation from the non-optimized to the optimized pose)to non-updated keyframes through the spanning tree. Non-updated points are transformed according to the correction applied to their reference keyframe.", "ORB-SLAM2 follows the policy introduced in monocular ORB-SLAM of inserting keyframes very often and culling redundant ones afterwards. The distinction between close and far stereopoints allows us to introduce a new condition for keyframe insertion, which can be critical in challenging environments where a big part of the scene is far from the stereo sensor,as shown in Fig. 3. In such environment we need to have a sufficient amount of close points to accurately estimate translation, therefore if the number of trackedclose points drops below \u03c4tsubscript\ud835\udf0f\ud835\udc61\\tau_{t}italic_\u03c4 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and the frame could create at least \u03c4csubscript\ud835\udf0f\ud835\udc50\\tau_{c}italic_\u03c4 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT new close stereo points, the system will insert a new keyframe. We empiricallyfound that \u03c4t=100subscript\ud835\udf0f\ud835\udc61100\\tau_{t}=100italic_\u03c4 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 100 and \u03c4c=70subscript\ud835\udf0f\ud835\udc5070\\tau_{c}=70italic_\u03c4 start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = 70 works well in all our experiments.", "We incorporate a Localization Mode which can be useful for lightweight long-term localization in well mapped areas, as long as there are not significant changes in the environment. In this mode the local mapping and loop closing threads are deactivatedand the camera is continuously localized by the tracking using relocalization if needed. In this mode the tracking leverages visual odometry matches and matches to map points.Visual odometry matches are matches between ORB in the current frame and 3D points created in the previous frame from the stereo/depth information. These matches make the localizationrobust to unmapped regions, but drift can be accumulated. Map point matches ensure drift-free localization to the existing map.This mode is demonstrated in the accompanying video.", "We have evaluated ORB-SLAM2 in three popular datasets and compared to other state-of-the-art SLAM systems, using always the results published by the original authors and standard evaluation metrics in the literature.We have run ORB-SLAM2 in an Intel Core i7-4790 desktop computer with 16Gb RAM. In order to account for the non-deterministic nature of the multi-threading system,we run each sequence 5 times and show median results for the accuracy of the estimated trajectory.Our open-source implementation includes the calibration and instructions to run the system in all these datasets.", "The KITTI dataset [2] contains stereo sequences recorded from a car in urban and highway environments. The stereo sensor has a \u223c54cmsimilar-toabsent54cm{\\sim}54\\textrm{cm}\u223c 54 cm baseline and works at 10Hz with a resolutionafter rectification of 1240\u00d737612403761240\\times 3761240 \u00d7 376 pixels. Sequences 00, 02, 05, 06, 07 and 09 contain loops. Our ORB-SLAM2 detects all loops and is able to reuse its map afterwards, except for sequence 09where the loop happens in very few frames at the end of the sequence. Table I shows results in the 11 training sequences, which have public ground-truth, compared to the state-of-the-artStereo LSD-SLAM [11], to our knowledge the only stereo SLAM showing detailed results for all sequences.We use two different metrics, the absolute translation RMSE tabssubscript\ud835\udc61\ud835\udc4e\ud835\udc4f\ud835\udc60t_{abs}italic_t start_POSTSUBSCRIPT italic_a italic_b italic_s end_POSTSUBSCRIPT proposed in [3], and the average relative translation trelsubscript\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc59t_{rel}italic_t start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT and rotation rrelsubscript\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc59r_{rel}italic_r start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT errors proposed in [2].Our system outperforms Stereo LSD-SLAM in most sequences, and achieves in general a relative error lower than 1%. The sequence 01, see Fig. 3, is the only highway sequence in thetraining set and the translation error is slightly worse. Translation is harder to estimate in this sequence because very few close points can be tracked, due tohigh speed and low frame-rate. However orientation can be accurately estimated, achieving an error of 0.21 degrees per 100 meters, as there are many far point that can be long tracked.Fig. 4 shows some examples of estimated trajectories.", "Compared to the monocular results presented in [1], the proposed stereo version is able to process the sequence 01 where the monocular system failed.In this highway sequence, see Fig. 3, close points are in view only for a few frames. The ability of the stereo version to create points from just one stereo keyframeinstead of the delayed initialization of the monocular, consisting on finding matches between two keyframes, is critical in this sequence not to lose tracking.Moreover the stereo system estimates the map and trajectory with metric scale and does not suffer from scale drift, as seen in Fig. 5.", "The recent EuRoC dataset [21] contains 11 stereo sequences recorded from a micro aerial vehicle (MAV) flying around two different rooms and a large industrial environment.The stereo sensor has a \u223c11cmsimilar-toabsent11cm{{\\sim}11}\\textrm{cm}\u223c 11 cm baseline and provides WVGA images at 20Hz. The sequences are classified as easy , medium and difficult depending onMAV\u2019s speed, illumination and scene texture. In all sequences the MAV revisits the environment and ORB-SLAM2 is able to reuse its map, closing loops when necessary.Table II shows absolute translation RMSE of ORB-SLAM2 for all sequences, comparing to Stereo LSD-SLAM, for the results provided in [11].ORB-SLAM2 achieves a localization precision of a few centimeters and is more accurate than Stereo LSD-SLAM. Our tracking get lost in some parts ofV2_03_difficult due to severe motion blur. As shown in [22], this sequence can be processed using IMU information. Fig. 6 shows examples of computed trajectories compared to the ground-truth.", "The TUM RGB-D dataset [3] contains indoors sequences from RGB-D sensors grouped in several categories to evaluate object reconstruction and SLAM/odometry methods underdifferent texture, illumination and structure conditions. We show results in a subset of sequences where most RGB-D methods are usually evaluated. In Table III we compare our accuracy to the following state-of-the-art methods:ElasticFusion [15], Kintinuous [12], DVO-SLAM [14] and RGB-D SLAM [13]. Our method is the only one based on bundle adjustment and outperformsthe other approaches in most sequences. As we already noticed for RGB-D SLAM results in [1], depthmaps for freiburg2 sequences have a 4%percent\\%% scale bias, probably comingfrom miscalibration, that we have compensated in our runs and could partly explain our significantly better results.Fig. 7 shows the point clouds that result from backprojecting the sensor depth maps from the computed keyframe posesin four sequences.The good definition and the straight contours of desks and posters prove the high accuracy localization of our approach.", "In order to complete the evaluation of the proposed system, we present in Table IV timing results in three sequences with different image resolutionsand sensors. The mean and two standard deviation ranges are shown for each thread task. As these sequences contain one single loop, the full BA and sometasks of the loop closing thread are executed just once and only a single time measurement is reported.The average tracking time per frame is below the inverse ofthe camera frame-rate for each sequence, meaning that our system is able to work in real-time. As ORB extraction in stereo images is parallelized, it can be seen that extracting1000 ORB features in the stereo WVGA images of V2_02 is similar to extracting the same amountof features in the single VGA image channel of fr3_office.", "The number of keyframes in the loop is shown as reference for the times relatedto loop closing. While the loop in KITTI 07 contains more keyframes, the covisibility graph built for the indoor fr3_office is denser and therefore the loop fusion, pose-graph optimizationand full BA tasks are more expensive. The higher density of the covisibility graph makes the local map contain more keyframes and points and therefore local map tracking and local BA arealso more expensive.", "We have presented a full SLAM system for monocular, stereoand RGB-D sensors, able to perform relocalization, loop closing andreuse its map in real-time on standard CPUs. We focus on buildingglobally consistent maps for reliable and long-term localization in awide range of environments as demonstrated in the experiments.The proposed localization mode with the relocalization capability of the system yields a very robust, zero-drift, and ligthweight localization method for known environments. This mode can beuseful for certain applications, such as tracking the user viewpointin virtual reality in a well-mapped space.", "The comparison to the state-of-the-art shows that ORB-SLAM2 achieves in most casesthe highest accuracy. In the KITTI visual odometry benchmark ORB-SLAM2 is currentlythe best stereo SLAM solution. Crucially, compared with the stereo visual odometrymethods that have flourished in recent years, ORB-SLAM2 achieves zero-drift localizationin already mapped areas.", "Surprisingly our RGB-D results demonstrate that if the most accuratecamera localization is desired, bundle adjustment performs betterthan direct methods or ICP, with the additional advantage of beingless computationally expensive, not requiring GPU processing tooperate in real-time.", "We have released the source code of our system,with examples and instructions so that it can be easily used byother researchers.ORB-SLAM2 is to the best of our knowledge the first open-source visual SLAM systemthat can work either with monocular, stereo and RGB-D inputs. Moreover our source code containsan example of an augmented reality application222https://youtu.be/kPwy8yA4CKMusing a monocular camera to show thepotential of our solution.", "Future extensions might include, to name some examples, non-overlapping multi-camera, fisheye or omnidirectionalcameras support, large scale dense fusion, cooperative mapping or increased motion blur robustness."], "figure_types": {"877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/2-Figure1-1.png": "photograph(s)", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/4-Figure2-1.png": "schematic", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/6-Figure3-1.png": "photograph(s)", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/6-Figure4-1.png": "plot", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/6-TableI-1.png": "table", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/7-Figure5-1.png": "plot", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/7-Figure6-1.png": "plot", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/7-TableII-1.png": "table", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/8-Figure7-1.png": "photograph(s)", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/8-TableIII-1.png": "table", "877a0ce8bc0457837e7c43dd0eb99e11ba4d84d6/9-TableIV-1.png": "table"}}, "1707.01083": {"paper_id": "paper_91", "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices", "arxiv_url": "https://arxiv.org/abs/1707.01083", "s2orc_url": "https://www.semanticscholar.org/paper/9da734397acd7ff7c557960c62fb1b400b27bd89", "all_figures_tables": {"9da734397acd7ff7c557960c62fb1b400b27bd89/2-Figure1-1.png": "Figure 1. Channel shuffle with two stacked group convolutions. GConv stands for group convolution. a) two stacked convolution layers with the same number of groups. Each output channel only relates to the input channels within the group. No cross talk; b) input and output channels are fully related when GConv2 takes data from different groups after GConv1; c) an equivalent implementation to b) using channel shuffle.", "9da734397acd7ff7c557960c62fb1b400b27bd89/3-Figure2-1.png": "Figure 2. ShuffleNet Units. a) bottleneck unit [9] with depthwise convolution (DWConv) [3, 12]; b) ShuffleNet unit with pointwise group convolution (GConv) and channel shuffle; c) ShuffleNet unit with stride = 2.", "9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table1-1.png": "Table 1. ShuffleNet architecture. The complexity is evaluated with FLOPs, i.e. the number of floating-point multiplication-adds. Note that for Stage 2, we do not apply group convolution on the first pointwise layer because the number of input channels is relatively small.", "9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table2-1.png": "Table 2. Classification error vs. number of groups g (smaller number represents better performance)", "9da734397acd7ff7c557960c62fb1b400b27bd89/5-Table3-1.png": "Table 3. ShuffleNet with/without channel shuffle (smaller number represents better performance)", "9da734397acd7ff7c557960c62fb1b400b27bd89/6-Table4-1.png": "Table 4. Classification error vs. various structures (%, smaller number represents better performance). We do not report VGG-like structure on smaller networks because the accuracy is significantly worse.", "9da734397acd7ff7c557960c62fb1b400b27bd89/6-Table5-1.png": "Table 5. ShuffleNet vs. MobileNet [12] on ImageNet Classification", "9da734397acd7ff7c557960c62fb1b400b27bd89/7-Table6-1.png": "Table 6. Complexity comparison. *Implemented by BVLC (https://github.com/BVLC/caffe/tree/master/models/bvlc googlenet)", "9da734397acd7ff7c557960c62fb1b400b27bd89/7-Table7-1.png": "Table 7. Object detection results on MS COCO (larger numbers represents better performance). For MobileNets we compare two results: 1) COCO detection scores reported by [12]; 2) finetuning from our reimplemented MobileNets, whose training and finetuning settings are exactly the same as that for ShuffleNets.", "9da734397acd7ff7c557960c62fb1b400b27bd89/7-Table8-1.png": "Table 8. Actual inference time on mobile device (smaller number represents better performance). The platform is based on a single Qualcomm Snapdragon 820 processor. All results are evaluated with single thread."}, "referred_figures_tables": [["9da734397acd7ff7c557960c62fb1b400b27bd89/2-Figure1-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/2-Figure1-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table1-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table2-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/5-Table3-1.png"], ["9da734397acd7ff7c557960c62fb1b400b27bd89/2-Figure1-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/2-Figure1-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/2-Figure1-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table2-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/5-Table3-1.png"], ["9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table1-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table2-1.png"], ["9da734397acd7ff7c557960c62fb1b400b27bd89/3-Figure2-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/3-Figure2-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/3-Figure2-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/3-Figure2-1.png"], ["9da734397acd7ff7c557960c62fb1b400b27bd89/2-Figure1-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table1-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table2-1.png"], ["9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table2-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/6-Table4-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table1-1.png"], ["9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table1-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table2-1.png", "9da734397acd7ff7c557960c62fb1b400b27bd89/7-Table8-1.png"]], "question_id": [12, 13, 15, 17, 2, 6, 19], "question": ["How channel shuffle operation works for two groups?", "Why it is possible to say that multiple group convolutional layers works efficiently without weakening representation?", "How is complexity calculated given scale factor of the ShuffleNet model? Given scale factor 0.25 and complexity of ShuffleNet 1x is 140 MFLOPS", "What is the activation function for a ShuffleNet Unit?", "What are the side effects of group convolution?\n", "How ShuffleNet allowed more feature maps for a given computational complexity?", "What will be the effect in performance if group numbers for convolution is increased? "], "question_section": ["Approach", "Approach", "Approach", "Approach", "Introduction", "Introduction", "Approach"], "question_trigger_sentence": ["This can be efficiently and elegantly im- plemented by a channel shuffle operation (Fig 1 (c)): sup- pose a convolutional layer with g groups whose output has g \u00d7 n channels; we first reshape the output channel dimen- sion into (g, n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups.", "Channel shuffle operation makes it possible to build more powerful structures with multiple group convolutional layers.", "then \u201dShuffleNet s\u00d7\u201d means scaling the number of filters in ShuffleNet 1\u00d7 by s times thus overall complexity will be roughly s2 times of ShuffleNet 1\u00d7.", "The usage of batch normalization (BN) [15] and nonlinearity is similar to [9, 40], except that we do not use ReLU after depthwise convolution as suggested by [3]. --> [can be seen in the figure that although it was not used after depthwise convolution, the ReLu activation was used after channel concatenation]", "To overcome the side effects brought by group con- volutions, we come up with a novel channel shuffle opera- tion to help the information flowing across feature channels.", "Compared with popular struc- tures like [30, 9, 40], for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks.", "\nIn ShuffleNet units, group number g controls the connec- tion sparsity of pointwise convolutions. Table 1 explores different group numbers and we adapt the output chan- nels to ensure overall computation cost roughly unchanged (\u223c140 MFLOPs). Obviously, larger group numbers result in more output channels (thus more convolutional filters) for a given complexity constraint, which helps to encode more information, though it might also lead to degradation for an individual convolutional filter due to limited corresponding input channels. In Sec 4.1.1 we will study the impact of this number subject to different computational constrains."], "question_type": ["Shallow question", "Deep/complex question", "Testing question", "Shallow question", "Shallow question", "Deep/complex question", "Deep/complex question"], "evidential_info": [[{"context": "If we allow group convolution to obtain input data from different groups (as shown in Fig\u00a01 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig\u00a01 (c)): suppose a convolutional layer with g groups whose output has g\\times n channels; we first reshape the output channel dimension into (g,n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training.", "rationale": "Channel shuffle seems to consistently boost the performance of the model when the number of groups is 3 and 8. Also, the performance boost is more significant in the case of 8 groups."}, {"context": "In ShuffleNet units, group number g controls the connection sparsity of pointwise convolutions. Table\u00a01 explores different group numbers and we adapt the output channels to ensure overall computation cost roughly unchanged (\\sim140 MFLOPs). Obviously, larger group numbers result in more output channels (thus more convolutional filters) for a given complexity constraint, which helps to encode more information, though it might also lead to degradation for an individual convolutional filter due to limited corresponding input channels. In Sec\u00a04.1.1 we will study the impact of this number subject to different computational constrains.", "rationale": "We relate the input and output channels for group convolutions by first dividing each group into subgroups and shuffling subgroups so each newly formed group will obtain a subgroup from every other group."}, {"context": "Table 2 also shows that for some models (e.g. ShuffleNet 0.5\\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models.", "rationale": "Larger number of groups gives larger output channels for a given computational complexity. It may result in better information encoding, but also may damage each convolution filter as the number of input channels decrease."}, {"context": "The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. Table\u00a03 compares the performance of ShuffleNet structures (group number is set to 3 or 8 for instance) with/without channel shuffle. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g=8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.", "rationale": "In classification errors, overall having more groups tends to give better results, and it amplifies as the models get smaller. However, if there are too many groups, the classification error may drop, possibly due to individual convolution filters getting too few input channels."}], [{"context": "If we allow group convolution to obtain input data from different groups (as shown in Fig\u00a01 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig\u00a01 (c)): suppose a convolutional layer with g groups whose output has g\\times n channels; we first reshape the output channel dimension into (g,n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training.", "rationale": "When stacking multiple group convolutions, the information flow between channel groups is blocked and the representative capability is reduced."}, {"context": "From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\\times and 0.25\\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.", "rationale": "Allowing groups to obtain information from other groups by shuffling channels in-between group convolutions helps avoid the side effect."}, {"context": "Table 2 also shows that for some models (e.g. ShuffleNet 0.5\\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models.", "rationale": "The authors empirically show that models with group convolutions perform better than the models without group convolutions. Also, as the computational complexity decreases, the effect seems to amplify, possibly due to an increase in output channels after group convolutions."}, {"context": "The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. Table\u00a03 compares the performance of ShuffleNet structures (group number is set to 3 or 8 for instance) with/without channel shuffle. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g=8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.", "rationale": "The AlexNet, ResNeXt, Xception, and MobileNet have already shown that using group convolutions may result in state-of-the-art performance."}, {"context": "The concept of group convolution, which was first introduced in AlexNet\u00a0[21] for distributing the model over two GPUs, has been well demonstrated its effectiveness in ResNeXt\u00a0[40]. Depthwise separable convolution proposed in Xception\u00a0[3] generalizes the ideas of separable convolutions in Inception series\u00a0[34, 32]. Recently, MobileNet\u00a0[12] utilizes the depthwise separable convolutions and gains state-of-the-art results among lightweight models. Our work generalizes group convolution and depthwise separable convolution in a novel form.", "rationale": "The experiments show that having too many groups might decrease the performance of the model, as a large number of groups mean each convolution filter will get a smaller number of input channels."}, {"context": "Modern convolutional neural networks\u00a0[30, 33, 34, 32, 9, 10] usually consist of repeated building blocks with the same structure. Among them, state-of-the-art networks such as Xception\u00a0[3] and ResNeXt\u00a0[40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, we notice that both designs do not fully take the 1\\times 1 convolutions (also called pointwise convolutions in \u00a0[12]) into account, which require considerable complexity. For example, in ResNeXt\u00a0[40] only 3\\times 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in \u00a0[40]). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy.", "rationale": "Xception and ResNeXt achieved state-of-the-art results by finding an appropriate trade-off between representation ability and computational cost by using group convolutions."}, {"context": "To address the issue, a straightforward solution is to apply channel sparse connections, for example group convolutions, also on 1\\times 1 layers. By ensuring that each convolution operates only on the corresponding input channel group, group convolution significantly reduces computation cost. However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels. Fig\u00a01 (a) illustrates a situation of two stacked group convolution layers. It is clear that outputs from a certain group only relate to the inputs within the group. This property blocks information flow between channel groups and weakens representation.", "rationale": "The evaluations show that using channel shuffle to relate input and output channels in group convolutions consistently boosts the performance of the model. Especially, when the number of groups increases."}], [{"context": "To customize the network to a desired complexity, we can simply apply a scale factor s on the number of channels. For example, we denote the networks in Table\u00a01 as \u201dShuffleNet 1\\times\u201d, then \u201dShuffleNet s\u00d7s\\timesitalic_s \u00d7\u201d means scaling the number of filters in ShuffleNet 1\\times by s times thus overall complexity will be roughly s^{2} times of ShuffleNet 1\\times.", "rationale": "\"ShuffleNet s x\" means scaling the number of channels in ShuffleNet 1x by the factor of s. It also means that the overall complexity will be s^2 multiplied by the complexity of ShuffleNet 1x."}, {"context": "Table 2. Classification error vs. number of groups g (smaller number represents better performance)", "rationale": "ShuffleNet 0.25x will have the complexity of 13 MFLOPs."}], [{"context": "Taking advantage of the channel shuffle operation, we propose a novel ShuffleNet unit specially designed for small networks. We start from the design principle of bottleneck unit\u00a0[9] in Fig\u00a02 (a). It is a residual block. In its residual branch, for the 3\\times 3 layer, we apply a computational economical 3\\times 3 depthwise convolution\u00a0[3] on the bottleneck feature map. Then, we replace the first 1\\times 1 layer with pointwise group convolution followed by a channel shuffle operation, to form a ShuffleNet unit, as shown in Fig\u00a02 (b). The purpose of the second pointwise group convolution is to recover the channel dimension to match the shortcut path. For simplicity, we do not apply an extra channel shuffle operation after the second pointwise layer as it results in comparable scores. The usage of batch normalization (BN)\u00a0[15] and nonlinearity is similar to \u00a0[9, 40], except that we do not use ReLU after depthwise convolution as suggested by \u00a0[3]. As for the case where ShuffleNet is applied with stride, we simply make two modifications (see Fig\u00a02 (c)): (i) add a 3\\times 3 average pooling on the shortcut path; (ii) replace the element-wise addition with channel concatenation, which makes it easy to enlarge channel dimension with little extra computation cost.", "rationale": "The use of non-linear activation functions and batch normalization is similar to [9, 40], however, the ReLU is not used after depthwise convolution following [3]."}, {"context": "Figure 2. ShuffleNet Units. a) bottleneck unit [9] with depthwise convolution (DWConv) [3, 12]; b) ShuffleNet unit with pointwise group", "rationale": "As can be seen in the figure, the ReLU is used only after the first 1x1 group convolution and after the channel concatenating shortcut branch and the residual branch in the ShuffleNet unit."}], [{"context": "In ShuffleNet units, group number g controls the connection sparsity of pointwise convolutions. Table\u00a01 explores different group numbers and we adapt the output channels to ensure overall computation cost roughly unchanged (\\sim140 MFLOPs). Obviously, larger group numbers result in more output channels (thus more convolutional filters) for a given complexity constraint, which helps to encode more information, though it might also lead to degradation for an individual convolutional filter due to limited corresponding input channels. In Sec\u00a04.1.1 we will study the impact of this number subject to different computational constrains.", "rationale": "Stacking up several group convolutions may result in a blocked flow of information between channel groups and weakened representation since convolution occurs only within the same group."}, {"context": "Table 2 also shows that for some models (e.g. ShuffleNet 0.5\\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models.", "rationale": "The individual convolution filters of every single group may suffer due to a reduced number of input channels, thus damaging the representative capability."}, {"context": "To address the issue, a straightforward solution is to apply channel sparse connections, for example group convolutions, also on 1\\times 1 layers. By ensuring that each convolution operates only on the corresponding input channel group, group convolution significantly reduces computation cost. However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels. Fig\u00a01 (a) illustrates a situation of two stacked group convolution layers. It is clear that outputs from a certain group only relate to the inputs within the group. This property blocks information flow between channel groups and weakens representation.", "rationale": "While group convolutions allow more output channels increasing the encoded information, individual filters suffer as the number of input channels is decreased."}], [{"context": "We notice that state-of-the-art basic architectures such as Xception\u00a0[3] and ResNeXt\u00a0[40] become less efficient in extremely small networks because of the costly dense 1\\times 1 convolutions. We propose using pointwise group convolutions to reduce computation complexity of 1\\times 1 convolutions. To overcome the side effects brought by group convolutions, we come up with a novel channel shuffle operation to help the information flowing across feature channels. Based on the two techniques, we build a highly efficient architecture called ShuffleNet. Compared with popular structures like \u00a0[30, 9, 40], for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks.", "rationale": "Due to pointwise group convolution, ShuffleNet has less complexity for the same computational budget. Given the input size of cxhxw, bottleneck channels m, and group size of g, ShuffleNet requires hw(2cm/g+9m) FLOPs, while ResNeXt needs hw(2cm+9m^{2}/g) FLOPs, and  ResNet requires hw(2cm+9m^{2}) FLOPs."}, {"context": "Thanks to pointwise group convolution with channel shuffle, all components in ShuffleNet unit can be computed efficiently. Compared with ResNet\u00a0[9] (bottleneck design) and ResNeXt\u00a0[40], our structure has less complexity under the same settings. For example, given the input size c\\times h\\times w and the bottleneck channels m, ResNet unit requires hw(2cm+9m^{2}) FLOPs and ResNeXt has hw(2cm+9m^{2}/g) FLOPs, while our ShuffleNet unit requires only hw(2cm/g+9m) FLOPs, where g means the number of groups for convolutions. In other words, given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information.", "rationale": "Empirically, for the computation budget of under 38 MFLOPs, ShuffleNet has 576 output channels in Stage 4, while other models have almost 2 times smaller numbers."}, {"context": "From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\\times and 0.25\\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.", "rationale": "For small models, the increase in group numbers result in consistently increased classification score. As the authors suggest, it can be due to increased feature maps."}, {"context": "Table 2 also shows that for some models (e.g. ShuffleNet 0.5\\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models.", "rationale": "Increased feature maps help to encode more information in small models for scarce computational budgets."}, {"context": "We use exactly the same settings to train these models. Results are shown in Table\u00a04. Our ShuffleNet models outperform most others by a significant margin under different complexities. Interestingly, we find an empirical relationship between feature map channels and classification accuracy. For example, under the complexity of 38 MFLOPs, output channels of Stage 4 (see Table\u00a01) for VGG-like, ResNet, ResNeXt, Xception-like, ShuffleNet models are 50, 192, 192, 288, 576 respectively, which is consistent with the increase of accuracy. Since the efficient design of ShuffleNet, we can use more channels for a given computation budget, thus usually resulting in better performance.", "rationale": "Increasing feature maps seem to have a better effect on performance as the model becomes smaller."}], [{"context": "In ShuffleNet units, group number g controls the connection sparsity of pointwise convolutions. Table\u00a01 explores different group numbers and we adapt the output channels to ensure overall computation cost roughly unchanged (\\sim140 MFLOPs). Obviously, larger group numbers result in more output channels (thus more convolutional filters) for a given complexity constraint, which helps to encode more information, though it might also lead to degradation for an individual convolutional filter due to limited corresponding input channels. In Sec\u00a04.1.1 we will study the impact of this number subject to different computational constrains.", "rationale": "Under 140 MFLOPs, the increase in the number of groups may lead to an increase in performance due to more output channels (i.e better information encoding). On the other hand, it leads to a decrease in the number of input channels per individual convolution filter, thus resulting in poor representative capability."}, {"context": "From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\\times and 0.25\\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.", "rationale": "For the ShuffleNet 0.5x having 8 groups (g = 8) leads to saturation or a drop in classification score, perhaps due to the smaller number of input channels per individual convolution filter. However, as the model gets smaller having more groups consistently gives better results, which means having wider feature maps for tiny models is more beneficial."}, {"context": "Table 2 also shows that for some models (e.g. ShuffleNet 0.5\\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models.", "rationale": "The ShuffleNet with group numbers more than 1 (g > 1) consistently gives better results compared to having a single group (g = 1). Perhaps, the gain results from wider feature maps that ShuffleNet with more groups allows."}, {"context": "Finally, we evaluate the actual inference speed of ShuffleNet models on a mobile device with an ARM platform. Though ShuffleNets with larger group numbers (e.g. g=4 or g=8) usually have better performance, we find it less efficient in our current implementation. Empirically g=3 usually has a proper trade-off between accuracy and actual inference time. As shown in Table\u00a08, three input resolutions are exploited for the test. Due to memory access and other overheads, we find every 4\\times theoretical complexity reduction usually results in \\sim2.6\\times actual speedup in our implementation. Nevertheless, compared with AlexNet\u00a0[21] our ShuffleNet 0.5\\times model still achieves \\sim13\\times actual speedup under comparable classification accuracy (the theoretical speedup is 18\\times), which is much faster than previous AlexNet-level models or speedup approaches such as \u00a0[14, 16, 22, 42, 43, 38].", "rationale": "In the authors' implementation, the ShuffleNet with a large number of groups (i.e g > 3), seems to be less efficient on mobile devices with ARM platform. The 3 groups' variants seem to have the best tradeoff among all possible variants."}]], "composition": ["In the case of channel shuffle operation for two groups, each group is divided into two and shuffled so each new group has a subgroup from both old groups. For example, |A|B| -> |aa|bb| -> |ab|ab|. In terms of performance, two groups seem to work consistently better than the single group case and consistently worse than having more than 2 groups.", "It is clearly stated in the paper that having group convolutions is a trade-off between representative capability and the computational cost of the model. The ShuffleNet allows stacking multiple group convolutions with an appropriate number of groups because of channel shuffle and it is empirically shown in the paper. However, it is also noted that having too many groups might sometimes damage the performance. Thus, multiple group convolutions work efficiently only when the number of groups is chosen carefully and channel shuffle is used.", "As it is shown in Table 2, the complexity of ShufflNet 0.25x will be 13 MFLOPs.", "The use of activation functions in the ShuffleNet unit happens only after the first 1x1 group convolution and the last concatenation of shortcut and residual paths, following the suggestions of referenced papers [3, 9, 40]. And the only non-linear activation function that is used is ReLU.", "The side effects of group convolutions are: blocked flow of information between channel groups when multiple group convolutions are combined; and damaged individual convolution filters for each group due to decreased number of input channels.", "The ShuffleNet uses pointwise group convolution with channel shuffling, thus design-wise it has less complexity (requires hw(2cm/g+9m) FLOPs). This means it allows wider feature maps for a given computational budget. And the effect seems to increase the performance better as the model gets smaller.", "For ShuffleNet, having more than 1 group seems to show consistently better results for all complexities. As the model gets smaller, the performance gain seems to increase more as the number of groups increases. However, for larger models, a too large number of groups led to saturation or a drop in classification error, possibly due to reduced representative capabilities."], "Is_figure_in_evidence": [false, false, false, true, false, false, false], "Is_table_in_evidence": [true, true, true, false, true, true, true], "question_key": ["1411", "1412", "1414", "1416", "1419", "1423", "1428"], "passages": ["Building deeper and larger convolutional neural networks (CNNs) is a primary trend for solving major visual recognition tasks\u00a0[21, 9, 33, 5, 28, 24]. The most accurate CNNs usually have hundreds of layers and thousands of channels\u00a0[9, 34, 32, 40], thus requiring computation at billions of FLOPs. This report examines the opposite extreme: pursuing the best accuracy in very limited computational budgets at tens or hundreds of MFLOPs, focusing on common mobile platforms such as drones, robots, and smartphones. Note that many existing works\u00a0[16, 22, 43, 42, 38, 27] focus on pruning, compressing, or low-bit representing a \u201cbasic\u201d network architecture. Here we aim to explore a highly efficient basic architecture specially designed for our desired computing ranges.", "We notice that state-of-the-art basic architectures such as Xception\u00a0[3] and ResNeXt\u00a0[40] become less efficient in extremely small networks because of the costly dense 1\u00d71111\\times 11 \u00d7 1 convolutions. We propose using pointwise group convolutions to reduce computation complexity of 1\u00d71111\\times 11 \u00d7 1 convolutions. To overcome the side effects brought by group convolutions, we come up with a novel channel shuffle operation to help the information flowing across feature channels. Based on the two techniques, we build a highly efficient architecture called ShuffleNet. Compared with popular structures like \u00a0[30, 9, 40], for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks.", "We evaluate our models on the challenging ImageNet classification\u00a0[4, 29] and MS COCO object detection\u00a0[23] tasks. A series of controlled experiments shows the effectiveness of our design principles and the better performance over other structures. Compared with the state-of-the-art architecture MobileNet\u00a0[12], ShuffleNet achieves superior performance by a significant margin, e.g. absolute 7.8% lower ImageNet top-1 error at level of 40 MFLOPs.", "We also examine the speedup on real hardware, i.e. an off-the-shelf ARM-based computing core. The ShuffleNet model achieves \u223csimilar-to\\sim\u223c13\u00d7\\times\u00d7 actual speedup (theoretical speedup is 18\u00d7\\times\u00d7) over AlexNet\u00a0[21] while maintaining comparable accuracy.", "The last few years have seen the success of deep neural networks in computer vision tasks\u00a0[21, 36, 28], in which model designs play an important role. The increasing needs of running high quality deep neural networks on embedded devices encourage the study on efficient model designs\u00a0[8].For example, GoogLeNet\u00a0[33] increases the depth of networks with much lower complexity compared to simply stacking convolution layers. SqueezeNet\u00a0[14] reduces parameters and computation significantly while maintaining accuracy. ResNet\u00a0[9, 10] utilizes the efficient bottleneck structure to achieve impressive performance.SENet\u00a0[13] introduces an architectural unit that boosts performance at slight computation cost.Concurrent with us, a very recent work\u00a0[46] employs reinforcement learning and model search to explore efficient model designs. The proposed mobile NASNet model achieves comparable performance with our counterpart ShuffleNet model (26.0% @ 564 MFLOPs vs. 26.3% @ 524 MFLOPs for ImageNet classification error). But\u00a0[46] do not report results on extremely tiny models (e.g. complexity less than 150 MFLOPs), nor evaluate the actual inference time on mobile devices.", "The concept of group convolution, which was first introduced in AlexNet\u00a0[21] for distributing the model over two GPUs, has been well demonstrated its effectiveness in ResNeXt\u00a0[40]. Depthwise separable convolution proposed in Xception\u00a0[3] generalizes the ideas of separable convolutions in Inception series\u00a0[34, 32]. Recently, MobileNet\u00a0[12] utilizes the depthwise separable convolutions and gains state-of-the-art results among lightweight models. Our work generalizes group convolution and depthwise separable convolution in a novel form.", "To the best of our knowledge, the idea of channel shuffle operation is rarely mentioned in previous work on efficient model design, although CNN library cuda-convnet\u00a0[20] supports \u201crandom sparse convolution\u201d layer, which is equivalent to random channel shuffle followed by a group convolutional layer. Such \u201crandom shuffle\u201d operation has different purpose and been seldom exploited later. Very recently, another concurrent work \u00a0[41] also adopt this idea for a two-stage convolution. However, \u00a0[41] did not specially investigate the effectiveness of channel shuffle itself and its usage in tiny model design.", "This direction aims to accelerate inference while preserving accuracy of a pre-trained model.Pruning network connections\u00a0[6, 7] or channels\u00a0[38] reducesredundant connections in a pre-trained model while maintaining performance.Quantization\u00a0[31, 27, 39, 45, 44] andfactorization\u00a0[22, 16, 18, 37] are proposed inliterature to reduce redundancy in calculations to speed up inference.Without modifying the parameters, optimized convolution algorithms implemented by FFT\u00a0[25, 35] and other methods\u00a0[2] decrease time consumption in practice.Distilling\u00a0[11] transfers knowledge fromlarge models into small ones, which makes training small models easier.", "Modern convolutional neural networks\u00a0[30, 33, 34, 32, 9, 10] usually consist of repeated building blocks with the same structure. Among them, state-of-the-art networks such as Xception\u00a0[3] and ResNeXt\u00a0[40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, we notice that both designs do not fully take the 1\u00d71111\\times 11 \u00d7 1 convolutions (also called pointwise convolutions in \u00a0[12]) into account, which require considerable complexity. For example, in ResNeXt\u00a0[40] only 3\u00d73333\\times 33 \u00d7 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in \u00a0[40]). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy.", "To address the issue, a straightforward solution is to apply channel sparse connections, for example group convolutions, also on 1\u00d71111\\times 11 \u00d7 1 layers. By ensuring that each convolution operates only on the corresponding input channel group, group convolution significantly reduces computation cost. However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels. Fig\u00a01 (a) illustrates a situation of two stacked group convolution layers. It is clear that outputs from a certain group only relate to the inputs within the group. This property blocks information flow between channel groups and weakens representation.", "If we allow group convolution to obtain input data from different groups (as shown in Fig\u00a01 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig\u00a01 (c)): suppose a convolutional layer with g\ud835\udc54gitalic_g groups whose output has g\u00d7n\ud835\udc54\ud835\udc5bg\\times nitalic_g \u00d7 italic_n channels; we first reshape the output channel dimension into (g,n)\ud835\udc54\ud835\udc5b(g,n)( italic_g , italic_n ), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training.", "Channel shuffle operation makes it possible to build more powerful structures with multiple group convolutional layers. In the next subsection we will introduce an efficient network unit with channel shuffle and group convolution.", "Taking advantage of the channel shuffle operation, we propose a novel ShuffleNet unit specially designed for small networks. We start from the design principle of bottleneck unit\u00a0[9] in Fig\u00a02 (a). It is a residual block. In its residual branch, for the 3\u00d73333\\times 33 \u00d7 3 layer, we apply a computational economical 3\u00d73333\\times 33 \u00d7 3 depthwise convolution\u00a0[3] on the bottleneck feature map. Then, we replace the first 1\u00d71111\\times 11 \u00d7 1 layer with pointwise group convolution followed by a channel shuffle operation, to form a ShuffleNet unit, as shown in Fig\u00a02 (b). The purpose of the second pointwise group convolution is to recover the channel dimension to match the shortcut path. For simplicity, we do not apply an extra channel shuffle operation after the second pointwise layer as it results in comparable scores. The usage of batch normalization (BN)\u00a0[15] and nonlinearity is similar to \u00a0[9, 40], except that we do not use ReLU after depthwise convolution as suggested by \u00a0[3]. As for the case where ShuffleNet is applied with stride, we simply make two modifications (see Fig\u00a02 (c)): (i) add a 3\u00d73333\\times 33 \u00d7 3 average pooling on the shortcut path; (ii) replace the element-wise addition with channel concatenation, which makes it easy to enlarge channel dimension with little extra computation cost.", "Thanks to pointwise group convolution with channel shuffle, all components in ShuffleNet unit can be computed efficiently. Compared with ResNet\u00a0[9] (bottleneck design) and ResNeXt\u00a0[40], our structure has less complexity under the same settings. For example, given the input size c\u00d7h\u00d7w\ud835\udc50\u210e\ud835\udc64c\\times h\\times witalic_c \u00d7 italic_h \u00d7 italic_w and the bottleneck channels m\ud835\udc5amitalic_m, ResNet unit requires hw(2cm+9m2)\u210e\ud835\udc642\ud835\udc50\ud835\udc5a9superscript\ud835\udc5a2hw(2cm+9m^{2})italic_h italic_w ( 2 italic_c italic_m + 9 italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) FLOPs and ResNeXt has hw(2cm+9m2/g)\u210e\ud835\udc642\ud835\udc50\ud835\udc5a9superscript\ud835\udc5a2\ud835\udc54hw(2cm+9m^{2}/g)italic_h italic_w ( 2 italic_c italic_m + 9 italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_g ) FLOPs, while our ShuffleNet unit requires only hw(2cm/g+9m)\u210e\ud835\udc642\ud835\udc50\ud835\udc5a\ud835\udc549\ud835\udc5ahw(2cm/g+9m)italic_h italic_w ( 2 italic_c italic_m / italic_g + 9 italic_m ) FLOPs, where g\ud835\udc54gitalic_g means the number of groups for convolutions. In other words, given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information.", "In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on low-power mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations. Such drawback is also referred in \u00a0[3], which has a runtime library based on TensorFlow\u00a0[1]. In ShuffleNet units, we intentionally use depthwise convolution only on bottleneck in order to prevent overhead as much as possible.", "Built on ShuffleNet units, we present the overall ShuffleNet architecture in Table 1. The proposed network is mainly composed of a stack of ShuffleNet units grouped into three stages. The first building block in each stage is applied with stride = 2. Other hyper-parameters within a stage stay the same, and for the next stage the output channels are doubled. Similar to \u00a0[9], we set the number of bottleneck channels to 1/4 of the output channels for each ShuffleNet unit. Our intent is to provide a reference design as simple as possible, although we find that further hyper-parameter tunning might generate better results.", "In ShuffleNet units, group number g\ud835\udc54gitalic_g controls the connection sparsity of pointwise convolutions. Table\u00a01 explores different group numbers and we adapt the output channels to ensure overall computation cost roughly unchanged (\u223csimilar-to\\sim\u223c140 MFLOPs). Obviously, larger group numbers result in more output channels (thus more convolutional filters) for a given complexity constraint, which helps to encode more information, though it might also lead to degradation for an individual convolutional filter due to limited corresponding input channels. In Sec\u00a04.1.1 we will study the impact of this number subject to different computational constrains.", "To customize the network to a desired complexity, we can simply apply a scale factor s\ud835\udc60sitalic_s on the number of channels. For example, we denote the networks in Table\u00a01 as \u201dShuffleNet 1\u00d7\\times\u00d7\u201d, then \u201dShuffleNet s\u00d7s\\timesitalic_s \u00d7\u201d means scaling the number of filters in ShuffleNet 1\u00d7\\times\u00d7 by s\ud835\udc60sitalic_s times thus overall complexity will be roughly s2superscript\ud835\udc602s^{2}italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT times of ShuffleNet 1\u00d7\\times\u00d7.", "We mainly evaluate our models on the ImageNet 2012 classification dataset\u00a0[29, 4]. We follow most of the training settings and hyper-parameters used in \u00a0[40], with two exceptions: (i) we set the weight decay to 4e-5 instead of 1e-4 and use linear-decay learning rate policy (decreased from 0.5 to 0); (ii) we use slightly less aggressive scale augmentation for data preprocessing. Similar modifications are also referenced in \u00a0[12] because such small networks usually suffer from underfitting rather than overfitting. It takes 1 or 2 days to train a model for 3\u00d71053superscript1053\\times 10^{5}3 \u00d7 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT iterations on 4 GPUs, whose batch size is set to 1024. To benchmark, we compare single crop top-1 performance on ImageNet validation set, i.e. cropping 224\u00d7224224224224\\times 224224 \u00d7 224 center view from 256\u00d7256\\times256 \u00d7 input image and evaluating classification accuracy. We use exactly the same settings for all models to ensure fair comparisons.", "The core idea of ShuffleNet lies in pointwise group convolution and channel shuffle operation. In this subsection we evaluate them respectively.", "To evaluate the importance of pointwise group convolutions, we compare ShuffleNet models of the same complexity whose numbers of groups range from 1 to 8. If the group number equals 1, no pointwise group convolution is involved and then the ShuffleNet unit becomes an \u201dXception-like\u201d\u00a0[3] structure. For better understanding, we also scale the width of the networks to 3 different complexities and compare their classification performance respectively. Results are shown in Table\u00a02.", "From the results, we see that models with group convolutions (g>1\ud835\udc541g>1italic_g > 1) consistently perform better than the counterparts without pointwise group convolutions (g=1\ud835\udc541g=1italic_g = 1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\u00d7\\times\u00d7 the best entry (g=8\ud835\udc548g=8italic_g = 8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\u00d7\\times\u00d7 and 0.25\u00d7\\times\u00d7 the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.", "Table 2 also shows that for some models (e.g. ShuffleNet 0.5\u00d7\\times\u00d7) when group numbers become relatively large (e.g. g=8\ud835\udc548g=8italic_g = 8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\u00d7\\times\u00d7 larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models.", "The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. Table\u00a03 compares the performance of ShuffleNet structures (group number is set to 3 or 8 for instance) with/without channel shuffle. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g=8\ud835\udc548g=8italic_g = 8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.", "Recent leading convolutional units in VGG\u00a0[30], ResNet\u00a0[9], GoogleNet\u00a0[33], ResNeXt\u00a0[40] and Xception\u00a0[3] have pursued state-of-the-art results with large models (e.g. \u22651absent1\\geq 1\u2265 1GFLOPs), but do not fully explore low-complexity conditions. In this section we survey a variety of building blocks and make comparisons with ShuffleNet under the same complexity constraint.", "For fair comparison, we use the overall network architecture as shown in Table\u00a01. We replace the ShuffleNet units in Stage 2-4 with other structures, then adapt the number of channels to ensure the complexity remains unchanged. The structures we explored include:", "\u2022VGG-like. Following the design principle of VGG net\u00a0[30], we use a two-layer 3\u00d7\\times\u00d73 convolutions as the basic building block. Different from \u00a0[30], we add a Batch Normalization layer\u00a0[15] after each of the convolutions to make end-to-end training easier.\u2022ResNet. We adopt the \u201dbottleneck\u201d design in our experiment, which has been demonstrated more efficient in \u00a0[9] . Same as \u00a0[9], the bottleneck ratio111In the bottleneck-like units (like ResNet, ResNeXt or ShuffleNet) bottleneck ratio implies the ratio of bottleneck channels to output channels. For example, bottleneck ratio = 1:4:141:41 : 4 means the output feature map is 4 times the width of the bottleneck feature map.  is also 1:4:141:41 : 4.\u2022Xception-like. The original structure proposed in \u00a0[3] involves fancy designs or hyper-parameters for different stages, which we find difficult for fair comparison on small models. Instead, we remove the pointwise group convolutions and channel shuffle operation from ShuffleNet (also equivalent to ShuffleNet with g=1\ud835\udc541g=1italic_g = 1). The derived structure shares the same idea of \u201cdepthwise separable convolution\u201d as in \u00a0[3], which is called an Xception-like structure here.\u2022ResNeXt. We use the settings of cardinality =16absent16=16= 16 and bottleneck ratio =1:2:absent12=1:2= 1 : 2 as suggested in \u00a0[40]. We also explore other settings, e.g. bottleneck ratio =1:4:absent14=1:4= 1 : 4, and get similar results.", "We use exactly the same settings to train these models. Results are shown in Table\u00a04. Our ShuffleNet models outperform most others by a significant margin under different complexities. Interestingly, we find an empirical relationship between feature map channels and classification accuracy. For example, under the complexity of 38 MFLOPs, output channels of Stage 4 (see Table\u00a01) for VGG-like, ResNet, ResNeXt, Xception-like, ShuffleNet models are 50, 192, 192, 288, 576 respectively, which is consistent with the increase of accuracy. Since the efficient design of ShuffleNet, we can use more channels for a given computation budget, thus usually resulting in better performance.", "Note that the above comparisons do not include GoogleNet or Inception series\u00a0[33, 34, 32]. We find it nontrivial to generate such Inception structures to small networks because the original design of Inception module involves too many hyper-parameters. As a reference, the first GoogleNet version\u00a0[33] has 31.3% top-1 error at the cost of 1.5 GFLOPs (See Table\u00a06). More sophisticated Inception versions\u00a0[34, 32] are more accurate, however, involve significantly increased complexity. Recently, Kim et al. propose a lightweight network structure named PVANET\u00a0[19] which adopts Inception units. Our reimplemented PVANET (with 224\u00d7\\times\u00d7224 input size) has 29.7% classification error with a computation complexity of 557 MFLOPs, while our ShuffleNet 2x model (g=3\ud835\udc543g=3italic_g = 3) gets 26.3% with 524 MFLOPs (see Table\u00a06).", "Recently Howard et al. have proposed MobileNets\u00a0[12] which mainly focus on efficient network architecture for mobile devices. MobileNet takes the idea of depthwise separable convolution from \u00a0[3] and achieves state-of-the-art results on small models.", "Table\u00a05 compares classification scores under a variety of complexity levels. It is clear that our ShuffleNet models are superior to MobileNet for all the complexities. Though our ShuffleNet network is specially designed for small models (<150absent150<150< 150 MFLOPs), we find it is still better than MobileNet for higher computation cost, e.g. 3.1% more accurate than MobileNet 1\u00d7\\times\u00d7 at the cost of 500 MFLOPs. For smaller networks (\u223csimilar-to\\sim\u223c40 MFLOPs) ShuffleNet surpasses MobileNet by 7.8%. Note that our ShuffleNet architecture contains 50 layers while MobileNet only has 28 layers. For better understanding, we also try ShuffleNet on a 26-layer architecture by removing half of the blocks in Stage 2-4 (see \u201dShuffleNet 0.5\u00d7\\times\u00d7 shallow (g=3\ud835\udc543g=3italic_g = 3)\u201d in Table\u00a05). Results show that the shallower model is still significantly better than the corresponding MobileNet, which implies that the effectiveness of ShuffleNet mainly results from its efficient structure, not the depth.", "Table\u00a06 compares our ShuffleNet with a few popular models. Results show that with similar accuracy ShuffleNet is much more efficient than others. For example, ShuffleNet 0.5\u00d7\\times\u00d7 is theoretically 18\u00d7\\times\u00d7 faster than AlexNet\u00a0[21] with comparable classification score. We will evaluate the actual running time in Sec 4.5.", "It is also worth noting that the simple architecture design makes it easy to equip ShuffeNets with the latest advances such as [13, 26]. For example, in [13] the authors propose Squeeze-and-Excitation (SE) blocks which achieve state-of-the-art results on large ImageNet models. We find SE modules also take effect in combination with the backbone ShuffleNets, for instance, boosting the top-1 error of ShuffleNet 2\u00d7\\times\u00d7 to 24.7% (shown in Table\u00a05). Interestingly, though negligible increase of theoretical complexity, we find ShuffleNets with SE modules are usually 25\u223c40%similar-to25percent4025\\sim 40\\%25 \u223c 40 % slower than the \u201craw\u201d ShuffleNets on mobile devices, which implies that actual speedup evaluation is critical on low-cost architecture design. In Sec\u00a04.5 we will make further discussion.", "To evaluate the generalization ability for transfer learning, we test our ShuffleNet model on the task of MS COCO object detection\u00a0[23]. We adopt Faster-RCNN\u00a0[28] as the detection framework and use the publicly released Caffe code\u00a0[28, 17] for training with default settings. Similar to \u00a0[12], the models are trained on the COCO train+val dataset excluding 5000 minival images and we conduct testing on the minival set. Table\u00a07 shows the comparison of results trained and evaluated on two input resolutions. Comparing ShuffleNet 2\u00d7\\times\u00d7 with MobileNet whose complexity are comparable (524 vs. 569 MFLOPs), our ShuffleNet 2\u00d7\\times\u00d7 surpasses MobileNet by a significant margin on both resolutions; our ShuffleNet 1\u00d7\\times\u00d7 also achieves comparable results with MobileNet on 600\u00d7\\times\u00d7 resolution, but has \u223csimilar-to\\sim\u223c4\u00d7\\times\u00d7 complexity reduction. We conjecture that this significant gain is partly due to ShuffleNet\u2019s simple design of architecture without bells and whistles.", "Finally, we evaluate the actual inference speed of ShuffleNet models on a mobile device with an ARM platform. Though ShuffleNets with larger group numbers (e.g. g=4\ud835\udc544g=4italic_g = 4 or g=8\ud835\udc548g=8italic_g = 8) usually have better performance, we find it less efficient in our current implementation. Empirically g=3\ud835\udc543g=3italic_g = 3 usually has a proper trade-off between accuracy and actual inference time. As shown in Table\u00a08, three input resolutions are exploited for the test. Due to memory access and other overheads, we find every 4\u00d7\\times\u00d7 theoretical complexity reduction usually results in \u223csimilar-to\\sim\u223c2.6\u00d7\\times\u00d7 actual speedup in our implementation. Nevertheless, compared with AlexNet\u00a0[21] our ShuffleNet 0.5\u00d7\\times\u00d7 model still achieves \u223csimilar-to\\sim\u223c13\u00d7\\times\u00d7 actual speedup under comparable classification accuracy (the theoretical speedup is 18\u00d7\\times\u00d7), which is much faster than previous AlexNet-level models or speedup approaches such as \u00a0[14, 16, 22, 42, 43, 38]."], "figure_types": {"9da734397acd7ff7c557960c62fb1b400b27bd89/2-Figure1-1.png": "schematic", "9da734397acd7ff7c557960c62fb1b400b27bd89/3-Figure2-1.png": "schematic", "9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table1-1.png": "table", "9da734397acd7ff7c557960c62fb1b400b27bd89/4-Table2-1.png": "table", "9da734397acd7ff7c557960c62fb1b400b27bd89/5-Table3-1.png": "table", "9da734397acd7ff7c557960c62fb1b400b27bd89/6-Table4-1.png": "table", "9da734397acd7ff7c557960c62fb1b400b27bd89/6-Table5-1.png": "table", "9da734397acd7ff7c557960c62fb1b400b27bd89/7-Table6-1.png": "table", "9da734397acd7ff7c557960c62fb1b400b27bd89/7-Table7-1.png": "table", "9da734397acd7ff7c557960c62fb1b400b27bd89/7-Table8-1.png": "table"}}, "1707.07012": {"paper_id": "paper_93", "title": "Learning Transferable Architectures for Scalable Image Recognition", "arxiv_url": "https://arxiv.org/abs/1707.07012", "s2orc_url": "https://www.semanticscholar.org/paper/d0611891b9e8a7c5731146097b6f201578f47b2f", "all_figures_tables": {"d0611891b9e8a7c5731146097b6f201578f47b2f/12-Figure8-1.png": "Figure 8. Example detections of best performing NASNet-A featurization with Faster-RCNN trained on COCO dataset. Top and middle images courtesy of http://wikipedia.org. Bottom image courtesy of Jonathan Huang", "d0611891b9e8a7c5731146097b6f201578f47b2f/13-Figure10-1.png": "Figure 10. Architecture of NASNet-C convolutional cell with B = 4 blocks identified with CIFAR-10. The input (white) is the hidden state from previous activations (or input image). The output (pink) is the result of a concatenation operation across all resulting branches. Each convolutional cell is the result of B blocks. A single block corresponds to two primitive operations (yellow) and a combination operation (green).", "d0611891b9e8a7c5731146097b6f201578f47b2f/13-Figure9-1.png": "Figure 9. Architecture of NASNet-B convolutional cell with B = 4 blocks identified with CIFAR-10. The input (white) is the hidden state from previous activations (or input image). Each convolutional cell is the result of B blocks. A single block is corresponds to two primitive operations (yellow) and a combination operation (green). As do we not concatenate the output hidden states, each output hidden state is used as a hidden state in the future layers. Each cell takes in 4 hidden states and thus needs to also create 4 output hidden states. Each output hidden state is therefore labeled with 0, 1, 2, 3 to represent the next four layers in that order.", "d0611891b9e8a7c5731146097b6f201578f47b2f/2-Figure1-1.png": "Figure 1. Overview of Neural Architecture Search [44]. A controller RNN predicts architecture A from a search space with probability p. A child network with architecture A is trained to convergence achieving accuracy R. Scale the gradients of p by R to update the RNN controller.", "d0611891b9e8a7c5731146097b6f201578f47b2f/3-Figure2-1.png": "Figure 2. Scalable architecture for image classification consists of two repeated motifs termed Normal Cell and Reduction Cell. This diagram highlights the model architecture for CIFAR-10 and ImageNet. The choice for the number of times the Normal Cells that gets stacked between reduction cells, N , can vary in our experiments.", "d0611891b9e8a7c5731146097b6f201578f47b2f/4-Figure3-1.png": "Figure 3. Controller model architecture for recursively constructing one block of a convolutional cell. Each block requires selecting 5 discrete parameters, each of which corresponds to the output of a softmax layer. Example constructed block shown on right. A convolutional cell contains B blocks, hence the controller contains 5B softmax layers for predicting the architecture of a convolutional cell. In our experiments, the number of blocks B is 5.", "d0611891b9e8a7c5731146097b6f201578f47b2f/5-Figure4-1.png": "Figure 4. Architecture of the best convolutional cells (NASNet-A) with B = 5 blocks identified with CIFAR-10 . The input (white) is the hidden state from previous activations (or input image). The output (pink) is the result of a concatenation operation across all resulting branches. Each convolutional cell is the result of B blocks. A single block is corresponds to two primitive operations (yellow) and a combination operation (green). Note that colors correspond to operations in Figure 3.", "d0611891b9e8a7c5731146097b6f201578f47b2f/5-Table1-1.png": "Table 1. Performance of Neural Architecture Search and other state-of-the-art models on CIFAR-10.", "d0611891b9e8a7c5731146097b6f201578f47b2f/6-Figure5-1.png": "Figure 5. Accuracy versus computational demand (left) and number of parameters (right) across top performing published CNN architectures on ImageNet 2012 ILSVRC challenge prediction task. Computational demand is measured in the number of floating-point multiplyadd operations to process a single image. Black circles indicate previously published work and red squares highlight our proposed models.", "d0611891b9e8a7c5731146097b6f201578f47b2f/6-Table2-1.png": "Table 2. Performance of architecture search and other published state-of-the-art models on ImageNet classification. Mult-Adds indicate the number of composite multiply-accumulate operations for a single image. Note that the composite multiple-accumulate operations are calculated for the image size reported in the table. Model size for [15] calculated from open-source implementation.", "d0611891b9e8a7c5731146097b6f201578f47b2f/6-Table3-1.png": "Table 3. Performance on ImageNet classification on a subset of models operating in a constrained computational setting, i.e., &lt; 1.5B multiply-accumulate operations per image. All models employ 224x224 images.", "d0611891b9e8a7c5731146097b6f201578f47b2f/7-Figure6-1.png": "Figure 6. Example detections showing improvements of object detection over previous state-of-the-art model for Faster-RCNN with Inception-ResNet-v2 featurization [18] (top) and NASNet-A featurization (bottom).", "d0611891b9e8a7c5731146097b6f201578f47b2f/7-Table4-1.png": "Table 4. Object detection performance on COCO on mini-val and test-dev datasets across a variety of image featurizations. All results employ Faster-RCNN object detection framework [28] from a single crop of an image. Top rows highlight mobile-optimized image featurizations, while bottom rows indicate computationally heavy image featurizations geared towards achieving best results. All mini-val results employ the same 8K subset of validation images in [18].", "d0611891b9e8a7c5731146097b6f201578f47b2f/8-Figure7-1.png": "Figure 7. Measuring the efficiency of random search (RS) to reinforcement learning (RL) for learning neural architectures. The x-axis measures the total number of model architectures sampled, and the y-axis is the validation performance after 20 epochs on CIFAR-10 training. Each pair of curves measures the mean accuracy across top ranking models identified by each algorithm."}, "referred_figures_tables": [["d0611891b9e8a7c5731146097b6f201578f47b2f/7-Figure6-1.png"], ["d0611891b9e8a7c5731146097b6f201578f47b2f/6-Table3-1.png"], ["d0611891b9e8a7c5731146097b6f201578f47b2f/5-Figure4-1.png"], ["d0611891b9e8a7c5731146097b6f201578f47b2f/13-Figure10-1.png", "d0611891b9e8a7c5731146097b6f201578f47b2f/7-Table4-1.png"], ["d0611891b9e8a7c5731146097b6f201578f47b2f/3-Figure2-1.png", "d0611891b9e8a7c5731146097b6f201578f47b2f/5-Table1-1.png"], ["d0611891b9e8a7c5731146097b6f201578f47b2f/3-Figure2-1.png", "d0611891b9e8a7c5731146097b6f201578f47b2f/8-Figure7-1.png", "d0611891b9e8a7c5731146097b6f201578f47b2f/4-Figure3-1.png"]], "question_id": [5, 6, 8, 14, 11, 18], "question": ["Is random search (RS) more efficient that reinforcement learning (RL) for learning neural architectures?", "Does NASNets perform better than MobileNet, ShuffleNet under resource-constraint setting?", "What are the networks that were constructed from the best three searches?\n", "What are the approaches that led to improved accuracy with lesser parameters for NASNets compared to Inception, ResNet and PolyNet?", "Why cutout data augmentation improve NASNet-A model error rate?", "How is normal cell different from reduction cell for NASNets?"], "question_section": ["Experiments and Results", "Experiments and Results", "Experiments and Results", "Experiments and Results", "Experiments and Results", "Experiments and Results"], "question_trigger_sentence": ["Comparing the efficiency of random search (RS) to re- inforcement learning (RL) for learning neural architectures. The x-axis measures the total number of model architectures sampled, and the y-axis is the validation performance on CIFAR-10 after 20 epochs of training.", "Finally, we test how well the best convolutional cells may perform in a resource-constrained setting, e.g., mobile devices (Table 3). In these settings, the number of float- ing point operations is severely constrained and predictive performance must be weighed against latency requirements on a device with limited computational resources.", "We call the three networks constructed from the best three searches NASNet- A, NASNet-B and NASNet-C.", "In the first set of experiments, we train several image clas- sification systems operating on 299x299 or 331x331 reso- lution images with different experiments scaled in compu- tational demand to create models that are roughly on par in computational cost with Inception-v2 [29], Inception-v3 [60] and PolyNet [69]. We show that this family of mod- els achieve state-of-the-art performance with fewer floating point operations and parameters than comparable architec- tures.", "As can be seen from the Table, a large NASNet-A model with cutout data augmentation [12] achieves a state-of-the-art error rate of 2.40% (averaged across 5 runs), which is slightly better than the previous best record of 2.56% by [12]. The best single run from our model achieves 2.19% error rate.", "Figure 4 shows a diagram of the top performing Normal Cell and Reduction Cell. Note the prevalence of separable convolutions and the number of branches compared with competing architectures "], "question_type": ["Shallow question", "Testing question", "Shallow question", "Testing question", "Deep/complex question", "Testing question"], "evidential_info": [[{"context": "Figure 6 shows the performance of reinforcement learning (RL) and random search (RS) as more model architectures are sampled. Note that the best model identified with RL is significantly better than the best model found by RS by over 1% as measured by on CIFAR-10. Additionally, RL finds an entire range of models that are of superior quality to random search. We observe this in the mean performance of the top-5 and top-25 models identified in RL versus RS.We take these results to indicate that although RS may provide a viable search strategy, RL finds better architectures in the NASNet search space.", "rationale": "Figure 6 shows the performance of reinforcement learning (RL) and random search (RS) as more model architectures are sampled. Note that the best model identified with RL is significantly better than the best model found by RS by over 1% as measured by on CIFAR-10."}], [{"context": "Finally, we test how well the best convolutional cells may perform in a resource-constrained setting, e.g., mobile devices (Table 3). In these settings, the number of floating point operations is severely constrained and predictive performance must be weighed against latency requirements on a device with limited computational resources. MobileNet [24] and ShuffleNet [70] provide state-of-the-art results obtaining 70.6% and 70.9\\% accuracy, respectively on 224x224 images using \\sim550M multliply-add operations. An architecture constructed from the best convolutional cells achieves superior predictive performance (74.0% accuracy) surpassing previous models but with comparable computational demand. In summary, we find that the learned convolutional cells are flexible across model scales achieving state-of-the-art performance across almost 2 orders of magnitude in computational budget.", "rationale": "MobileNet [24] and ShuffleNet [70] provide state-of-the-art results obtaining 70.6% and 70.9\\% accuracy, respectively on 224x224 images using \\sim550M multliply-add operations. An architecture constructed from the best convolutional cells achieves superior predictive performance (74.0% accuracy) surpassing previous models but with comparable computational demand."}], [{"context": "Figure 4 shows a diagram of the top performing Normal Cell and Reduction Cell. Note the prevalence of separable convolutions and the number of branches compared with competing architectures [53, 59, 20, 60, 58]. Subsequent experiments focus on this convolutional cell architecture, although we examine the efficacy of other, top-ranked convolutional cells in ImageNet experiments (described in Appendix B) and report their results as well. We call the three networks constructed from the best three searches NASNet-A, NASNet-B and NASNet-C.", "rationale": "We call the three networks constructed from the best three searches NASNet-A, NASNet-B and NASNet-C."}], [{"context": "For the mobile-optimized network, our resulting system achieves a mAP of 29.6% \u2013 exceeding previous mobile-optimized networks that employ Faster-RCNN by over 5.0% (Table 4). For the best NASNet network, our resulting network operating on images of the same spatial resolution (800 \\times 800) achieves mAP = 40.7%, exceeding equivalent object detection systems based off lesser performing image featurization (i.e. Inception-ResNet-v2) by 4.0% [28, 52] (see Appendix for example detections on images and side-by-side comparisons). Finally, increasing the spatial resolution of the input image results in the best reported, single model result for object detection of 43.1%, surpassing the best previous best by over 4.0% [37].222A primary advance in the best reported object detection system is the introduction of a novel loss [37]. Pairing this loss with NASNet-A image featurization may lead to even further performance gains. Additionally, performance gains are achievable through ensembling multiple inferences across multiple model instances and image crops (e.g., [28]). These results provide further evidence that NASNet provides superior, generic image features that may be transferred across other computer vision tasks. Figure\u00a010 and Figure\u00a011 in Appendix\u00a0C show four examples of object detection results produced by NASNet-A with the Faster-RCNN framework.", "rationale": "A primary advance in the best reported object detection system is the introduction of a novel loss [37]. Pairing this loss with NASNet-A image featurization may lead to even further performance gains. Additionally, performance gains are achievable through ensembling multiple inferences across multiple model instances and image crops (e.g., [28]). These results provide further evidence that NASNet provides superior, generic image features that may be transferred across other computer vision tasks."}], [{"context": "For the task of image classification with CIFAR-10, we set N=4 or 6 (Figure 2). The test accuracies of the best architectures are reported in Table\u00a01 along with other state-of-the-art models. As can be seen from the Table, a large NASNet-A model with cutout data augmentation\u00a0[12] achieves a state-of-the-art error rate of 2.40% (averaged across 5 runs), which is slightly better than the previous best record of 2.56% by [12]. The best single run from our model achieves 2.19% error rate.", "rationale": "As can be seen from the Table, a large NASNet-A model with cutout data augmentation [12] achieves a state-of-the-art error rate of 2.40% (averaged across 5 runs), which is slightly better than the previous best record of 2.56% by [12]."}], [{"context": "In our approach, the overall architectures of the convolutional nets are manually predetermined. They are composed of convolutional cells repeated many times where each convolutional cell has the same architecture, but different weights. To easily build scalable architectures for images of any size, we need two types of convolutional cells to serve two main functions when taking in a feature map as input: (1) convolutional cells that return a feature map of the same dimension, and (2) convolutional cells that return a feature map where the feature map height and width is reduced by a factor of two. We name the first type and second type of convolutional cells Normal Cell and Reduction Cell respectively. For the Reduction Cell, we make the initial operation applied to the cell\u2019s inputs have a stride of two to reduce the height and width. All of our operations that we consider for building our convolutional cells have an option of striding.", "rationale": "The Reduction and Normal Cell could have the same architecture, but we empirically found it beneficial to learn two separate architectures.We use a common heuristic to double the number of filters in the output whenever the spatial activation size is reduced in order to maintain roughly constant hidden state dimension [32, 53]."}, {"context": "Figure\u00a02 shows our placement of Normal and Reduction Cells for CIFAR-10 and ImageNet. Note on ImageNet we have more Reduction Cells, since the incoming image size is 299x299 compared to 32x32 for CIFAR. The Reduction and Normal Cell could have the same architecture, but we empirically found it beneficial to learn two separate architectures.We use a common heuristic to double the number of filters in the output whenever the spatial activation size is reduced in order to maintain roughly constant hidden state dimension [32, 53].Importantly,much like Inception and ResNet models [59, 20, 60, 58],we consider the number of motif repetitions N and the number of initial convolutional filters as free parameters that we tailor to the scale of an image classification problem.", "rationale": "To allow the controller RNN to predict both Normal Cell and Reduction Cell, we simply make the controller have 2\\times 5B predictions in total, where the first 5B predictions are for the Normal Cell and the second 5B predictions are for the Reduction Cell."}, {"context": "What varies in the convolutional nets is the structures of the Normal and Reduction Cells, which are searched by the controller RNN.The structures of the cells can be searched within a search space defined as follows (see Appendix, Figure 7 for schematic). In our search space, each cell receives as input two initial hidden states h_{i} and h_{i-1} which are the outputs of two cells in previous two lower layers or the input image. The controller RNN recursively predicts the rest of the structure of the convolutional cell, given these two initial hidden states (Figure\u00a03). The predictions of the controller for each cell are grouped into B blocks, where each block has 5 prediction steps made by 5 distinct softmax classifiers corresponding to discrete choices of the elements of a block:", "rationale": "We name the first type and second type of convolutional cells Normal Cell and Reduction Cell respectively. For the Reduction Cell, we make the initial operation applied to the cell\u2019s inputs have a stride of two to reduce the height and width."}, {"context": "To allow the controller RNN to predict both Normal Cell and Reduction Cell, we simply make the controller have 2\\times 5B predictions in total, where the first 5B predictions are for the Normal Cell and the second 5B predictions are for the Reduction Cell.", "rationale": "What varies in the convolutional nets is the structures of the Normal and Reduction Cells, which are searched by the controller RNN."}]], "composition": ["No, Reninforcement Learning is more efficient than Random Search for learning neural architectures.", "From the above evidential sentence, it is obvious that NASNets with 74% accuracy perform better than MobileNet and ShuffleNet with 70.6% and 70.9% accuracies respectively.", "The networks constructed from the best three searches are NASNet-A, NASNet-B and NASNet-C.", "Ensembling multiple inferences across multiple model instances and image crops led to improved accuracy with lesser parameters for NASNets compared to Inception, ResNet and PolyNet.", "From the above evidential paragraph, we can see that the cutout data augmentation achieves a state-of-the-art error rate of 2.40% which is better than the previous record. But, why it improves the performance cannot be answered in this paper.", "We learn two separate architectures for reduction and normal cells. During prediction,  the first 5B predictions are for the Normal Cell and the second 5B predictions are for the Reduction Cell. For Reduction cell authors make the initial operation applied to the cell\u2019s inputs have a stride of two to reduce the height and width which is not done for Normal cell."], "Is_figure_in_evidence": [true, false, true, false, true, true], "Is_table_in_evidence": [false, true, false, true, false, false], "question_key": ["1434", "1435", "1437", "1441", "1448", "1450"], "passages": ["Developing neural network image classification models often requires significant architecture engineering. Starting from the seminal work of\u00a0[32] on using convolutional architectures [17, 34] for ImageNet\u00a0[11] classification, successive advancements through architecture engineering have achieved impressive results\u00a0[53, 59, 20, 60, 58, 68].", "In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset.Our approach is inspired by the recently proposed Neural Architecture Search (NAS) framework\u00a0[71], which uses a reinforcement learning search method to optimize architecture configurations. Applying NAS, or any other search methods, directly to a large dataset, such as the ImageNet dataset, is however computationally expensive. We therefore propose to search for a good architecture on a proxy dataset, for example the smaller CIFAR-10 dataset, and then transfer the learned architecture to ImageNet. We achieve this transferrability by designing a search space (which we call \u201cthe NASNet search space\u201d) so that the complexity of the architecture is independent of the depth of the network and the size of input images. More concretely, all convolutional networks in our search space are composed of convolutional layers (or \u201ccells\u201d) with identical structure but different weights. Searching for the best convolutional architectures is therefore reduced to searching for the best cell structure. Searching for the best cell structure has two main benefits: it is much faster than searching for an entire network architecture and the cell itself is more likely to generalize to other problems.In our experiments, this approach significantly accelerates the search for the best architectures using CIFAR-10 by a factor of 7\u00d77\\times7 \u00d7 and learns architectures that successfully transfer to ImageNet.", "Our main result is that the best architecture found on CIFAR-10, called NASNet,achieves state-of-the-art accuracy when transferred to ImageNet classification without much modification. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5. This result amounts to a 1.2% improvement in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is also state-of-the-art.", "Additionally, by simply varying the number of the convolutional cells and number of filters in the convolutional cells, we can create different versions of NASNets with different computational demands. Thanks to this property of the cells, we can generate a family of models that achieve accuracies superior to all human-invented models at equivalent or smaller computational budgets [60, 29]. Notably, the smallest version of NASNet achieves 74.0% top-1 accuracy on ImageNet, which is 3.1% better than previously engineered architectures targeted towards mobile and embedded vision tasks [24, 70].", "Finally, we show that the image features learned by NASNets are generically useful and transfer to other computer vision problems. In our experiments, the features learned by NASNets from ImageNet classification can be combined with the Faster-RCNN framework\u00a0[47] to achieve state-of-the-art on COCO object detection task for both the largest as well as mobile-optimized models. Our largest NASNet model achieves 43.1% mAP, which is 4% better than previous state-of-the-art. ", "The proposed method is related to previous work in hyperparameter optimization\u00a0[44, 4, 5, 54, 55, 6, 40] \u2013 especially recent approaches in designing architectures such as Neural Fabrics\u00a0[48], DiffRNN\u00a0[41], MetaQNN\u00a0[3] and DeepArchitect\u00a0[43]. A more flexible class of methods for designing architecture is evolutionary algorithms [65, 16, 57, 30, 46, 42, 67], yet they have not had as much success at large scale.Xie and Yuille [67] also transferred learned architectures from CIFAR-10 to ImageNet but performance of these models (top-1 accuracy 72.1%) are notably below previous state-of-the-art (Table 2).", "The concept of having one neural network interact with a second neural network to aid the learning process, or learning to learn or meta-learning [23, 49] has attracted much attention in recent years\u00a0[1, 62, 14, 19, 35, 45, 15]. Most of these approaches have not been scaled to large problems like ImageNet. An exception is the recent work focused on learning an optimizer for ImageNet classification that achieved notable improvements [64].", "The design of our search space took much inspiration from LSTMs\u00a0[22], and Neural Architecture Search Cell\u00a0[71]. The modular structure of the convolutional cell is also related to previous methods on ImageNet such as VGG [53], Inception\u00a0[59, 60, 58], ResNet/ResNext\u00a0[20, 68], and Xception/MobileNet [9, 24].", "Our work makes use of search methods to find good convolutional architectures on a dataset of interest. The main search method we use in this work is the Neural Architecture Search (NAS) framework proposed by\u00a0[71]. In NAS, a controller recurrent neural network (RNN) samples child networks with different architectures. The child networks are trained to convergence to obtain some accuracy on a held-out validation set. The resulting accuracies are used to update the controller so that the controller will generate better architectures over time. The controller weights are updated with policy gradient (see Figure\u00a01). ", "The main contribution of this work is the design of a novel search space, such that the best architecture found on the CIFAR-10 dataset would scale to larger, higher-resolution image datasets across a range of computational settings.We name this search space the NASNet search space as it gives rise to NASNet, the best architecture found in our experiments. One inspiration for the NASNet search space is the realization that architecture engineering with CNNs often identifiesrepeated motifs consisting of combinations of convolutionalfilter banks, nonlinearities and a prudent selection ofconnections to achieve state-of-the-art results (such as the repeated modules present in the Inception and ResNet models [59, 20, 60, 58]). These observations suggest that it may be possible for the controller RNN to predict a generic convolutional cell expressed in terms of these motifs. This cell can then be stacked in series to handle inputs of arbitrary spatial dimensions and filter depth.", "In our approach, the overall architectures of the convolutional nets are manually predetermined. They are composed of convolutional cells repeated many times where each convolutional cell has the same architecture, but different weights. To easily build scalable architectures for images of any size, we need two types of convolutional cells to serve two main functions when taking in a feature map as input: (1) convolutional cells that return a feature map of the same dimension, and (2) convolutional cells that return a feature map where the feature map height and width is reduced by a factor of two. We name the first type and second type of convolutional cells Normal Cell and Reduction Cell respectively. For the Reduction Cell, we make the initial operation applied to the cell\u2019s inputs have a stride of two to reduce the height and width. All of our operations that we consider for building our convolutional cells have an option of striding.", "Figure\u00a02 shows our placement of Normal and Reduction Cells for CIFAR-10 and ImageNet. Note on ImageNet we have more Reduction Cells, since the incoming image size is 299x299 compared to 32x32 for CIFAR. The Reduction and Normal Cell could have the same architecture, but we empirically found it beneficial to learn two separate architectures.We use a common heuristic to double the number of filters in the output whenever the spatial activation size is reduced in order to maintain roughly constant hidden state dimension [32, 53].Importantly,much like Inception and ResNet models [59, 20, 60, 58],we consider the number of motif repetitions N\ud835\udc41Nitalic_N and the number of initial convolutional filters as free parameters that we tailor to the scale of an image classification problem.", "What varies in the convolutional nets is the structures of the Normal and Reduction Cells, which are searched by the controller RNN.The structures of the cells can be searched within a search space defined as follows (see Appendix, Figure 7 for schematic). In our search space, each cell receives as input two initial hidden states hisubscript\u210e\ud835\udc56h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and hi\u22121subscript\u210e\ud835\udc561h_{i-1}italic_h start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT which are the outputs of two cells in previous two lower layers or the input image. The controller RNN recursively predicts the rest of the structure of the convolutional cell, given these two initial hidden states (Figure\u00a03). The predictions of the controller for each cell are grouped into B\ud835\udc35Bitalic_B blocks, where each block has 5 prediction steps made by 5 distinct softmax classifiers corresponding to discrete choices of the elements of a block:", "Step 1.Select a hidden state from hi,hi\u22121subscript\u210e\ud835\udc56subscript\u210e\ud835\udc561h_{i},h_{i-1}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT or from the set of hidden states created in previous blocks.Step 2.Select a second hidden state from the same options as in Step 1.Step 3.Select an operation to apply to the hidden state selected in Step 1.Step 4.Select an operation to apply to the hidden state selected in Step 2.Step 5.Select a method to combine the outputs of Step 3 and 4 to create a new hidden state.The algorithm appends the newly-created hidden state to the set of existing hidden states as a potential input in subsequent blocks. The controller RNN repeats the above 5 prediction steps B\ud835\udc35Bitalic_B times corresponding to the B\ud835\udc35Bitalic_B blocks in a convolutional cell. In our experiments, selecting B=5\ud835\udc355B=5italic_B = 5 provides good results, although we have not exhaustively searched this space due to computational limitations.", "In steps 3 and 4, the controller RNN selects an operation to apply to the hidden states. We collected the following set of operations based on their prevalence in the CNN literature:", "In step 5 the controller RNN selects a method to combine the two hidden states, either (1) element-wise addition between two hidden states or (2) concatenation between two hidden states along the filter dimension.Finally, all of the unused hidden states generated in the convolutional cell are concatenated together in depth to provide the final cell output.", "To allow the controller RNN to predict both Normal Cell and Reduction Cell, we simply make the controller have 2\u00d75B25\ud835\udc352\\times 5B2 \u00d7 5 italic_B predictions in total, where the first 5B5\ud835\udc355B5 italic_B predictions are for the Normal Cell and the second 5B5\ud835\udc355B5 italic_B predictions are for the Reduction Cell.", "Finally, our work makes use of the reinforcement learning proposal in NAS\u00a0[71]; however, it is also possible to use random search to search for architectures in the NASNet search space. In random search, instead of sampling the decisions from the softmax classifiers in the controller RNN, we can sample the decisions from the uniform distribution. In our experiments, we find that random search is slightly worse than reinforcement learning on the CIFAR-10 dataset. Although there is value in using reinforcement learning, the gap is smaller than what is found in the original work of\u00a0[71]. This result suggests that 1) the NASNet search space is well-constructed such that random search can perform reasonably well and 2) random search is a difficult baseline to beat. We will compare reinforcement learning against random search in Section\u00a04.4.", "In this section, we describe our experiments with the method described above to learn convolutional cells. In summary, all architecture searches are performed using the CIFAR-10 classification task [31]. The controller RNN was trained using Proximal Policy Optimization (PPO) [51] by employing a global workqueue system for generating a pool of child networks controlled by the RNN. In our experiments, the pool of workers in the workqueue consisted of 500 GPUs.", "The result of this search process over 4 days yields several candidate convolutional cells. We note that this search procedure is almost 7\u00d77\\times7 \u00d7 faster than previous approaches [71] that took 28 days.111In particular, we note that previous architecture search [71] used 800 GPUs for 28 days resulting in 22,400 GPU-hours. The method in this paper uses 500 GPUs across 4 days resulting in 2,000 GPU-hours. The former effort used Nvidia K40 GPUs, whereas the current efforts used faster NVidia P100s. Discounting the fact that the we use faster hardware, we estimate that the current procedure is roughly about 7\u00d77\\times7 \u00d7 more efficient. Additionally, we demonstrate below that the resulting architecture is superior in accuracy.", "Figure 4 shows a diagram of the top performing Normal Cell and Reduction Cell. Note the prevalence of separable convolutions and the number of branches compared with competing architectures [53, 59, 20, 60, 58]. Subsequent experiments focus on this convolutional cell architecture, although we examine the efficacy of other, top-ranked convolutional cells in ImageNet experiments (described in Appendix B) and report their results as well. We call the three networks constructed from the best three searches NASNet-A, NASNet-B and NASNet-C.", "We demonstrate the utility of the convolutional cells by employing this learned architecture on CIFAR-10 and a family of ImageNet classification tasks. The latter family of tasks is explored across a few orders of magnitude in computational budget.After having learned the convolutional cells, several hyper-parameters may be explored to build a final network for a given task: (1) the number of cell repeats N\ud835\udc41Nitalic_N and (2) the number of filters in the initial convolutional cell. After selecting the number of initial filters, we use a common heuristic to double the number of filters whenever the stride is 2.Finally, we define a simple notation, e.g., 4444 @ 64646464, to indicate these two parameters in all networks, where 4444 and 64646464 indicate the number of cell repeats and the number of filters in the penultimate layer of the network, respectively.", "For complete details of of the architecture learning algorithm and the controller system, please refer to Appendix A. Importantly, when training NASNets, we discovered ScheduledDropPath, a modified version of DropPath\u00a0[33], to be an effective regularization method for NASNet. In DropPath\u00a0[33], each path in the cell is stochastically dropped with some fixed probability during training. In our modified version, ScheduledDropPath, each path in the cell is dropped out with a probability that is linearly increased over the course of training. We find that DropPath does not work well for NASNets, while ScheduledDropPath significantly improves the final performance of NASNets in both CIFAR and ImageNet experiments.", "For the task of image classification with CIFAR-10, we set N=4\ud835\udc414N=4italic_N = 4 or 6 (Figure 2). The test accuracies of the best architectures are reported in Table\u00a01 along with other state-of-the-art models. As can be seen from the Table, a large NASNet-A model with cutout data augmentation\u00a0[12] achieves a state-of-the-art error rate of 2.40% (averaged across 5 runs), which is slightly better than the previous best record of 2.56% by [12]. The best single run from our model achieves 2.19% error rate.", "We performed several sets of experiments on ImageNet with the best convolutional cells learned from CIFAR-10.We emphasize that we merely transfer the architectures from CIFAR-10 but train all ImageNet models weights from scratch.", "Results are summarized in Table 2 and 3 and Figure 5. In the first set of experiments, we train several image classification systems operating on 299x299 or 331x331 resolution images with different experiments scaled in computational demand to create models that are roughly on par in computational cost with Inception-v2 [29], Inception-v3 [60] and PolyNet [69].We show that this family of models achieve state-of-the-art performance with fewer floating point operations and parameters than comparable architectures. Second, we demonstrate that by adjusting the scale of the model we can achieve state-of-the-art performance at smaller computational budgets, exceeding streamlined CNNs hand-designed for this operating regime [24, 70].", "Note we do not have residual connections between convolutional cells as the models learn skip connections on their own. We empirically found manually inserting residual connections between cells to not help performance.Our training setup on ImageNet is similar to [60], but please see Appendix A for details.", "Table\u00a02 shows that the convolutional cells discovered with CIFAR-10 generalize well to ImageNet problems.In particular, each model based on the convolutional cells exceeds the predictive performance of the corresponding hand-designed model.Importantly, the largest model achieves a new state-of-the-art performance for ImageNet (82.7%) based on single, non-ensembled predictions, surpassing previous best published result by \u223csimilar-to\\sim\u223c1.2% [8]. Among the unpublished works, our model is on par with the best reported result of 82.7%\u00a0[25], while having significantly fewer floating point operations.Figure 5 shows a complete summary of our results in comparison with other published results. Note the family of models based on convolutional cells provides an envelope over a broad class of human-invented architectures.", "Finally, we test how well the best convolutional cells may perform in a resource-constrained setting, e.g., mobile devices (Table 3). In these settings, the number of floating point operations is severely constrained and predictive performance must be weighed against latency requirements on a device with limited computational resources. MobileNet [24] and ShuffleNet [70] provide state-of-the-art results obtaining 70.6% and 70.9%percent\\%% accuracy, respectively on 224x224 images using \u223csimilar-to\\sim\u223c550M multliply-add operations. An architecture constructed from the best convolutional cells achieves superior predictive performance (74.0% accuracy) surpassing previous models but with comparable computational demand. In summary, we find that the learned convolutional cells are flexible across model scales achieving state-of-the-art performance across almost 2 orders of magnitude in computational budget.", "Image classification networks provide generic image features that may be transferred to other computer vision problems [13]. One of the most important problems is the spatial localization of objects within an image. To further validate the performance of the family of NASNet-A networks, we test whether object detection systems derived from NASNet-A lead to improvements in object detection [28].", "To address this question, we plug in the family of NASNet-A networks pretrained on ImageNet into the Faster-RCNN object detection pipeline [47] using an open-source software platform [28]. We retrain the resulting object detection pipeline on the combined COCO training plus validation dataset excluding 8,000 mini-validation images.We perform single model evaluation using 300-500 RPN proposals per image. In other words, we onlypass a single image through a single network. We evaluate the model on the COCO mini-val [28] and test-dev dataset and report the mean average precision (mAP) as computed with the standard COCO metric library [38]. We perform a simple search over learning rate schedules to identify the best possible model. Finally, we examine the behavior of two object detection systems employing the best performing NASNet-A image featurization (NASNet-A, 6666 @ 4032403240324032) as well as the image featurization geared towards mobile platforms (NASNet-A, 4444 @ 1056105610561056).", "For the mobile-optimized network, our resulting system achieves a mAP of 29.6% \u2013 exceeding previous mobile-optimized networks that employ Faster-RCNN by over 5.0% (Table 4). For the best NASNet network, our resulting network operating on images of the same spatial resolution (800 \u00d7\\times\u00d7 800) achieves mAP = 40.7%, exceeding equivalent object detection systems based off lesser performing image featurization (i.e. Inception-ResNet-v2) by 4.0% [28, 52] (see Appendix for example detections on images and side-by-side comparisons). Finally, increasing the spatial resolution of the input image results in the best reported, single model result for object detection of 43.1%, surpassing the best previous best by over 4.0% [37].222A primary advance in the best reported object detection system is the introduction of a novel loss [37]. Pairing this loss with NASNet-A image featurization may lead to even further performance gains. Additionally, performance gains are achievable through ensembling multiple inferences across multiple model instances and image crops (e.g., [28]). These results provide further evidence that NASNet provides superior, generic image features that may be transferred across other computer vision tasks. Figure\u00a010 and Figure\u00a011 in Appendix\u00a0C show four examples of object detection results produced by NASNet-A with the Faster-RCNN framework.", "Though what search method to use is not the focus of the paper, an open question is how effective is the reinforcement learning search method. In this section, we study the effectiveness of reinforcement learning for architecture search on the CIFAR-10 image classification problem and compare it to brute-force random search (considered to be a very strong baseline for black-box optimization\u00a0[5]) given an equivalent amount of computational resources.", "Figure 6 shows the performance of reinforcement learning (RL) and random search (RS) as more model architectures are sampled. Note that the best model identified with RL is significantly better than the best model found by RS by over 1% as measured by on CIFAR-10. Additionally, RL finds an entire range of models that are of superior quality to random search. We observe this in the mean performance of the top-5 and top-25 models identified in RL versus RS.We take these results to indicate that although RS may provide a viable search strategy, RL finds better architectures in the NASNet search space.", "In this work, we demonstrate how to learn scalable, convolutional cells from data that transfer to multiple image classification tasks. The learned architecture is quite flexible as it may be scaled in terms of computational cost and parameters to easily address a variety of problems. In all cases, the accuracy of the resulting model exceeds all human-designed models \u2013 ranging from models designed for mobile applications to computationally-heavy models designed to achieve the most accurate results.", "The key insight in our approach is to design a search space that decouples the complexity of an architecture from the depth of a network. This resulting search space permits identifying good architectures on a small dataset (i.e., CIFAR-10) and transferring the learned architecture to image classifications across a range of data and computational scales.", "The resulting architectures approach or exceed state-of-the-art performance in both CIFAR-10 and ImageNet datasets with less computational demand than human-designed architectures [60, 29, 69]. The ImageNet results are particularly important because many state-of-the-art computer vision problems (e.g., object detection [28], face detection [50], image localization [63]) derive image features or architectures from ImageNet classification models. For instance, we find that image features obtained from ImageNet used in combination with the Faster-RCNN framework achieves state-of-the-art object detection results. Finally, we demonstrate that we can use the resulting learned architecture to perform ImageNet classification with reduced computational budgets that outperform streamlined architectures targeted to mobile and embedded platforms [24, 70]."], "figure_types": {"d0611891b9e8a7c5731146097b6f201578f47b2f/12-Figure8-1.png": "photograph(s)", "d0611891b9e8a7c5731146097b6f201578f47b2f/13-Figure10-1.png": "schematic", "d0611891b9e8a7c5731146097b6f201578f47b2f/13-Figure9-1.png": "schematic", "d0611891b9e8a7c5731146097b6f201578f47b2f/2-Figure1-1.png": "schematic", "d0611891b9e8a7c5731146097b6f201578f47b2f/3-Figure2-1.png": "schematic", "d0611891b9e8a7c5731146097b6f201578f47b2f/4-Figure3-1.png": "schematic", "d0611891b9e8a7c5731146097b6f201578f47b2f/5-Figure4-1.png": "schematic", "d0611891b9e8a7c5731146097b6f201578f47b2f/5-Table1-1.png": "table", "d0611891b9e8a7c5731146097b6f201578f47b2f/6-Figure5-1.png": "plot", "d0611891b9e8a7c5731146097b6f201578f47b2f/6-Table2-1.png": "table", "d0611891b9e8a7c5731146097b6f201578f47b2f/6-Table3-1.png": "table", "d0611891b9e8a7c5731146097b6f201578f47b2f/7-Figure6-1.png": "photograph(s)", "d0611891b9e8a7c5731146097b6f201578f47b2f/7-Table4-1.png": "table", "d0611891b9e8a7c5731146097b6f201578f47b2f/8-Figure7-1.png": "plot"}}, "1708.02002": {"paper_id": "paper_95", "title": "Focal Loss for Dense Object Detection", "arxiv_url": "https://arxiv.org/abs/1708.02002", "s2orc_url": "https://www.semanticscholar.org/paper/1a857da1a8ce47b2aa185b91b5cb215ddef24de7", "all_figures_tables": {"1a857da1a8ce47b2aa185b91b5cb215ddef24de7/5-Figure3-1.png": "Figure 3. The one-stage RetinaNet network architecture uses a Feature Pyramid Network (FPN) [19] backbone on top of a feedforward ResNet architecture [15] (a) to generate a rich, multi-scale convolutional feature pyramid (b). To this backbone RetinaNet attaches two subnetworks, one for classifying anchor boxes (c) and one for regressing from anchor boxes to ground-truth object boxes (d). The network design is intentionally simple, which enables this work to focus on a novel focal loss function that eliminates the accuracy gap between our one-stage detector and state-of-the-art two-stage detectors like Faster R-CNN with FPN [19] while running at faster speeds.", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/6-Table1-1.png": "Table 1. Ablation experiments for RetinaNet and Focal Loss (FL). All models are trained on trainval35k and tested on minival unless noted. If not specified, default values are: \u03b3 = 2; anchors for 3 scales and 3 aspect ratios; ResNet-50-FPN backbone; and a 600 pixel train and test image scale. (a) RetinaNet with \u03b1-balanced CE achieves at most 31.1 AP. (b) In contrast, using FL with the same exact network gives a 2.9 AP gain and is fairly robust to exact \u03b3/\u03b1 settings. (c) Using 2-3 scale and 3 aspect ratio anchors yields good results after which point performance saturates. (d) FL outperforms the best variants of online hard example mining (OHEM) [30, 21] by over 3 points AP. (e) Accuracy/Speed trade-off of RetinaNet on test-dev for various network depths and image scales (see also Figure 2).", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/7-Figure4-1.png": "Figure 4. Cumulative distribution functions of the normalized loss for positive and negative samples for different values of \u03b3 for a converged model. The effect of changing \u03b3 on the distribution of the loss for positive examples is minor. For negatives, however, increasing \u03b3 heavily concentrates the loss on hard examples, focusing nearly all attention away from easy negatives.", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/8-Table2-1.png": "Table 2. Object detection single-model results (bounding box AP), vs. state-of-the-art on COCO test-dev. We show results for our RetinaNet-101-800 model, trained with scale jitter and for 1.5\u00d7 longer than the same model from Table 1e. Our model achieves top results, outperforming both one-stage and two-stage models. For a detailed breakdown of speed versus accuracy see Table 1e and Figure 2.", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/9-Figure5-1.png": "Figure 5. Focal loss variants compared to the cross entropy as a function of xt = yx. Both the original FL and alternate variant FL\u2217 reduce the relative loss for well-classified examples (xt &gt; 0).", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/9-Figure6-1.png": "Figure 6. Derivates of the loss functions from Figure 5 w.r.t. x.", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/9-Figure7-1.png": "Figure 7. Effectiveness of FL\u2217 with various settings \u03b3 and \u03b2. The plots are color coded such that effective settings are shown in blue.", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/9-Table3-1.png": "Table 3. Results of FL and FL\u2217 versus CE for select settings."}, "referred_figures_tables": [["1a857da1a8ce47b2aa185b91b5cb215ddef24de7/6-Table1-1.png"], ["1a857da1a8ce47b2aa185b91b5cb215ddef24de7/6-Table1-1.png"]], "question_id": [7, 19], "question": ["What are the other loss functions experimented by the authors'? ", "Why the normalization wasn't done taken all anchors into account?"], "question_section": ["Related Work", "Experiments"], "question_trigger_sentence": ["Robust Estimation: There has been much interest in de- signing robust loss functions (e.g., Huber loss [", "The total focal loss of an image is computed as the sum of the focal loss over all \u223c100k anchors, normalized by the number of anchors assigned to a ground-truth box."], "question_type": ["deep/complex question", "Deep/complex question"], "evidential_info": [[{"context": "There has been much interest in designing robust loss functions (e.g., Huber loss [13]) that reduce the contribution of outliers by down-weighting the loss of examples with large errors (hard examples). In contrast, rather than addressing outliers, our focal loss is designed to address class imbalance by down-weighting inliers (easy examples) such that their contribution to the total loss is small even if their number is large. In other words, the focal loss performs the opposite role of a robust loss: it focuses training on a sparse set of hard examples.", "rationale": "Finally, in early experiments, we attempted to train with the hinge loss [13] on p_{\\textrm{t}}, which sets loss to 0 above a certain value of p_{\\textrm{t}}."}, {"context": "The CE loss can be seen as the blue (top) curve in Figure\u00a01. One notable property of this loss, which can be easily seen in its plot, is that even examples that are easily classified (p_{\\textrm{t}}\\gg.5) incur a loss with non-trivial magnitude. When summed over a large number of easy examples, these small loss values can overwhelm the rare class.", "rationale": "The loss function is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases, see Figure 1."}, {"context": "In practice we use an \\alpha-balanced variant of the focal loss:\\textrm{FL}(p_{\\textrm{t}})=-\\alpha_{\\textrm{t}}(1-p_{\\textrm{t}})^{\\gamma}\\log(p_{\\textrm{t}}).(5)We adopt this form in our experiments as it yields slightly improved accuracy over the non-\\alpha-balanced form. Finally, we note that the implementation of the loss layer combines the sigmoid operation for computing p with the loss computation, resulting in greater numerical stability.", "rationale": "Our next attempt to improve learning involved using the \\alpha-balanced CE loss described in \u00a73.1."}, {"context": "Our next attempt to improve learning involved using the \\alpha-balanced CE loss described in \u00a73.1. Results for various \\alpha are shown in Table\u00a01a. Setting \\alpha=.75 gives a gain of 0.9 points AP.", "rationale": "In practice we use an \\alpha-balanced variant of the focal loss:\\textrm{FL}(p_{\\textrm{t}})=-\\alpha_{\\textrm{t}}(1-p_{\\textrm{t}})^{\\gamma}\\log(p_{\\textrm{t}})."}, {"context": "Finally, in early experiments, we attempted to train with the hinge loss [13] on p_{\\textrm{t}}, which sets loss to 0 above a certain value of p_{\\textrm{t}}. However, this was unstable and we did not manage to obtain meaningful results. Results exploring alternate loss functions are in the appendix.", "rationale": "There has been much interest in designing robust loss functions (e.g., Huber loss [13]) that reduce the contribution of outliers by down-weighting the loss of examples with large errors (hard examples)."}, {"context": "In this paper, we propose a new loss function that acts as a more effective alternative to previous approaches for dealing with class imbalance. The loss function is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases, see Figure\u00a01. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. Experiments show that our proposed Focal Loss enables us to train a high-accuracy, one-stage detector that significantly outperforms the alternatives of training with the sampling heuristics or hard example mining, the previous state-of-the-art techniques for training one-stage detectors. Finally, we note that the exact form of the focal loss is not crucial, and we show other instantiations can achieve similar results.", "rationale": "The CE loss can be seen as the blue (top) curve in Figure 1. One notable property of this loss, which can be easily seen in its plot, is that even examples that are easily classified (p_{\\textrm{t}}\\gg.5) incur a loss with non-trivial magnitude."}], [{"context": "We use the focal loss introduced in this work as the loss on the output of the classification subnet. As we will show in \u00a75, we find that \\gamma=2 works well in practice and the RetinaNet is relatively robust to \\gamma\\in[0.5,5]. We emphasize that when training RetinaNet, the focal loss is applied to all \\scriptstyle\\sim100k anchors in each sampled image. This stands in contrast to common practice of using heuristic sampling (RPN) or hard example mining (OHEM, SSD) to select a small set of anchors (e.g., 256) for each minibatch. The total focal loss of an image is computed as the sum of the focal loss over all \\scriptstyle\\sim100k anchors, normalized by the number of anchors assigned to a ground-truth box. We perform the normalization by the number of assigned anchors, not total anchors, since the vast majority of anchors are easy negatives and receive negligible loss values under the focal loss. Finally we note that \\alpha, the weight assigned to the rare class, also has a stable range, but it interacts with \\gamma making it necessary to select the two together (see Tables\u00a01a and 1b). In general \\alpha should be decreased slightly as \\gamma is increased (for \\gamma=2, \\alpha=0.25 works best).", "rationale": "We perform the normalization by the number of assigned anchors, not total anchors, since the vast majority of anchors are easy negatives and receive negligible loss values under the focal loss."}]], "composition": ["The main loss function used by authors is The Focal Loss. Besides this, the other loss functions experimented on are: 1) Hinge Loss 2) Dynamically scaled cross entropy loss 3) \\alpha-balanced CE loss 4) \\alpha-balanced variant of the focal loss 5) Huber loss 6) The CE loss", "The normalisation is not done by taking all anchors into account because vast majority of anchors are easy negatives and receive negligible loss values under the focal loss."], "Is_figure_in_evidence": [false, false], "Is_table_in_evidence": [true, true], "question_key": ["1470", "1480"], "passages": ["Current state-of-the-art object detectors are based on a two-stage, proposal-driven mechanism. As popularized in the R-CNN framework [11], the first stage generates a sparse set of candidate object locations and the second stage classifies each candidate location as one of the foreground classes or as background using a convolutional neural network. Through a sequence of advances [10, 28, 20, 14], this two-stage framework consistently achieves top accuracy on the challenging COCO benchmark [21].", "Despite the success of two-stage detectors, a natural question to ask is: could a simple one-stage detector achieve similar accuracy? One stage detectors are applied over a regular, dense sampling of object locations, scales, and aspect ratios. Recent work on one-stage detectors, such as YOLO [26, 27] and SSD [22, 9], demonstrates promising results, yielding faster detectors with accuracy within 10-40% relative to state-of-the-art two-stage methods.", "This paper pushes the envelop further: we present a one-stage object detector that, for the first time, matches the state-of-the-art COCO AP of more complex two-stage detectors, such as the Feature Pyramid Network (FPN) [20] or Mask R-CNN [14] variants of Faster R-CNN [28]. To achieve this result, we identify class imbalance during training as the main obstacle impeding one-stage detector from achieving state-of-the-art accuracy and propose a new loss function that eliminates this barrier.", "Class imbalance is addressed in R-CNN-like detectors by a two-stage cascade and sampling heuristics. The proposal stage (e.g., Selective Search [35], EdgeBoxes [39], DeepMask [24, 25], RPN [28]) rapidly narrows down the number of candidate object locations to a small number (e.g., 1-2k), filtering out most background samples. In the second classification stage, sampling heuristics, such as a fixed foreground-to-background ratio (1:3), or online hard example mining (OHEM) [31], are performed to maintain a manageable balance between foreground and background.", "In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. In practice this often amounts to enumerating \u223csimilar-to\\scriptstyle\\sim\u223c100k locations that densely cover spatial positions, scales, and aspect ratios. While similar sampling heuristics may also be applied, they are inefficient as the training procedure is still dominated by easily classified background examples. This inefficiency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping [33, 29] or hard example mining [37, 8, 31].", "In this paper, we propose a new loss function that acts as a more effective alternative to previous approaches for dealing with class imbalance. The loss function is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases, see Figure\u00a01. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. Experiments show that our proposed Focal Loss enables us to train a high-accuracy, one-stage detector that significantly outperforms the alternatives of training with the sampling heuristics or hard example mining, the previous state-of-the-art techniques for training one-stage detectors. Finally, we note that the exact form of the focal loss is not crucial, and we show other instantiations can achieve similar results.", "To demonstrate the effectiveness of the proposed focal loss, we design a simple one-stage object detector called RetinaNet, named for its dense sampling of object locations in an input image. Its design features an efficient in-network feature pyramid and use of anchor boxes. It draws on a variety of recent ideas from [22, 6, 28, 20]. RetinaNet is efficient and accurate; our best model, based on a ResNet-101-FPN backbone, achieves a COCO test-dev AP of 39.1 while running at 5 fps, surpassing the previously best published single-model results from both one and two-stage detectors, see Figure\u00a02.", "The sliding-window paradigm, in which a classifier is applied on a dense image grid, has a long and rich history. One of the earliest successes is the classic work of LeCun et al. who applied convolutional neural networks to handwritten digit recognition [19, 36]. Viola and Jones [37] used boosted object detectors for face detection, leading to widespread adoption of such models. The introduction of HOG [4] and integral channel features [5] gave rise to effective methods for pedestrian detection. DPMs [8] helped extend dense detectors to more general object categories and had top results on PASCAL [7] for many years. While the sliding-window approach was the leading detection paradigm in classic computer vision, with the resurgence of deep learning [18], two-stage detectors, described next, quickly came to dominate object detection.", "The dominant paradigm in modern object detection is based on a two-stage approach. As pioneered in the Selective Search work [35], the first stage generates a sparse set of candidate proposals that should contain all objects while filtering out the majority of negative locations, and the second stage classifies the proposals into foreground classes / background. R-CNN [11] upgraded the second-stage classifier to a convolutional network yielding large gains in accuracy and ushering in the modern era of object detection. R-CNN was improved over the years, both in terms of speed [15, 10] and by using learned object proposals [6, 24, 28]. Region Proposal Networks (RPN) integrated proposal generation with the second-stage classifier into a single convolution network, forming the Faster R-CNN framework [28]. Numerous extensions to this framework have been proposed, e.g. [20, 31, 32, 16, 14].", "OverFeat [30] was one of the first modern one-stage object detector based on deep networks. More recently SSD [22, 9] and YOLO [26, 27] have renewed interest in one-stage methods. These detectors have been tuned for speed but their accuracy trails that of two-stage methods. SSD has a 10-20% lower AP, while YOLO focuses on an even more extreme speed/accuracy trade-off. See Figure\u00a02. Recent work showed that two-stage detectors can be made fast simply by reducing input image resolution and the number of proposals, but one-stage methods trailed in accuracy even with a larger compute budget [17]. In contrast, the aim of this work is to understand if one-stage detectors can match or surpass the accuracy of two-stage detectors while running at similar or faster speeds.", "The design of our RetinaNet detector shares many similarities with previous dense detectors, in particular the concept of \u2018anchors\u2019 introduced by RPN [28] and use of features pyramids as in SSD [22] and FPN [20]. We emphasize that our simple detector achieves top results not based on innovations in network design but due to our novel loss.", "Both classic one-stage object detection methods, like boosted detectors [37, 5] and DPMs [8], and more recent methods, like SSD [22], face a large class imbalance during training. These detectors evaluate 104superscript10410^{4}10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT-105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT candidate locations per image but only a few locations contain objects. This imbalance causes two problems: (1) training is inefficient as most locations are easy negatives that contribute no useful learning signal; (2) en masse, the easy negatives can overwhelm training and lead to degenerate models. A common solution is to perform some form of hard negative mining [33, 37, 8, 31, 22] that samples hard examples during training or more complex sampling/reweighing schemes [2]. In contrast, we show that our proposed focal loss naturally handles the class imbalance faced by a one-stage detector and allows us to efficiently train on all examples without sampling and without easy negatives overwhelming the loss and computed gradients.", "There has been much interest in designing robust loss functions (e.g., Huber loss [13]) that reduce the contribution of outliers by down-weighting the loss of examples with large errors (hard examples). In contrast, rather than addressing outliers, our focal loss is designed to address class imbalance by down-weighting inliers (easy examples) such that their contribution to the total loss is small even if their number is large. In other words, the focal loss performs the opposite role of a robust loss: it focuses training on a sparse set of hard examples.", "The Focal Loss is designed to address the one-stage object detection scenario in which there is an extreme imbalance between foreground and background classes during training (e.g., 1:1000). We introduce the focal loss starting from the cross entropy (CE) loss for binary classification111Extending the focal loss to the multi-class case is straightforward and works well; for simplicity we focus on the binary loss in this work.:CE(p,y)={\u2212log\u2061(p)if\u00a0y=1\u2212log\u2061(1\u2212p)otherwise.CE\ud835\udc5d\ud835\udc66cases\ud835\udc5dif\u00a0y=11\ud835\udc5dotherwise.\\textrm{CE}(p,y)=\\begin{cases}-\\log(p)&\\text{if $y=1$}\\\\-\\log(1-p)&\\text{otherwise.}\\end{cases}CE ( italic_p , italic_y ) = { start_ROW start_CELL - roman_log ( italic_p ) end_CELL start_CELL if italic_y = 1 end_CELL end_ROW start_ROW start_CELL - roman_log ( 1 - italic_p ) end_CELL start_CELL otherwise. end_CELL end_ROW(1)In the above y\u2208{\u00b11}\ud835\udc66plus-or-minus1y\\in\\{\\pm 1\\}italic_y \u2208 { \u00b1 1 } specifies the ground-truth class and p\u2208[0,1]\ud835\udc5d01p\\in[0,1]italic_p \u2208 [ 0 , 1 ] is the model\u2019s estimated probability for the class with label y=1\ud835\udc661y=1italic_y = 1. For notational convenience, we define ptsubscript\ud835\udc5dtp_{\\textrm{t}}italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT:pt={pif\u00a0y=11\u2212potherwise,subscript\ud835\udc5dtcases\ud835\udc5dif\u00a0y=11\ud835\udc5dotherwise,p_{\\textrm{t}}=\\begin{cases}p&\\text{if $y=1$}\\\\1-p&\\text{otherwise,}\\end{cases}italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT = { start_ROW start_CELL italic_p end_CELL start_CELL if italic_y = 1 end_CELL end_ROW start_ROW start_CELL 1 - italic_p end_CELL start_CELL otherwise, end_CELL end_ROW(2)and rewrite CE(p,y)=CE(pt)=\u2212log\u2061(pt)CE\ud835\udc5d\ud835\udc66CEsubscript\ud835\udc5dtsubscript\ud835\udc5dt\\textrm{CE}(p,y)=\\textrm{CE}(p_{\\textrm{t}})=-\\log(p_{\\textrm{t}})CE ( italic_p , italic_y ) = CE ( italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT ) = - roman_log ( italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT ).", "The CE loss can be seen as the blue (top) curve in Figure\u00a01. One notable property of this loss, which can be easily seen in its plot, is that even examples that are easily classified (pt\u226b.5much-greater-thansubscript\ud835\udc5dt.5p_{\\textrm{t}}\\gg.5italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT \u226b .5) incur a loss with non-trivial magnitude. When summed over a large number of easy examples, these small loss values can overwhelm the rare class.", "A common method for addressing class imbalance is to introduce a weighting factor \u03b1\u2208[0,1]\ud835\udefc01\\alpha\\in[0,1]italic_\u03b1 \u2208 [ 0 , 1 ] for class 1111 and 1\u2212\u03b11\ud835\udefc1-\\alpha1 - italic_\u03b1 for class \u221211-1- 1. In practice \u03b1\ud835\udefc\\alphaitalic_\u03b1 may be set by inverse class frequency or treated as a hyperparameter to set by cross validation. For notational convenience, we define \u03b1tsubscript\ud835\udefct\\alpha_{\\textrm{t}}italic_\u03b1 start_POSTSUBSCRIPT t end_POSTSUBSCRIPT analogously to how we defined ptsubscript\ud835\udc5dtp_{\\textrm{t}}italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT. We write the \u03b1\ud835\udefc\\alphaitalic_\u03b1-balanced CE loss as:CE(pt)=\u2212\u03b1tlog\u2061(pt).CEsubscript\ud835\udc5dtsubscript\ud835\udefctsubscript\ud835\udc5dt\\textrm{CE}(p_{\\textrm{t}})=-\\alpha_{\\textrm{t}}\\log(p_{\\textrm{t}}).CE ( italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT ) = - italic_\u03b1 start_POSTSUBSCRIPT t end_POSTSUBSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT ) .(3)This loss is a simple extension to CE that we consider as an experimental baseline for our proposed focal loss.", "As our experiments will show, the large class imbalance encountered during training of dense detectors overwhelms the cross entropy loss. Easily classified negatives comprise the majority of the loss and dominate the gradient. While \u03b1\ud835\udefc\\alphaitalic_\u03b1 balances the importance of positive/negative examples, it does not differentiate between easy/hard examples. Instead, we propose to reshape the loss function to down-weight easy examples and thus focus training on hard negatives.", "More formally, we propose to add a modulating factor (1\u2212pt)\u03b3superscript1subscript\ud835\udc5dt\ud835\udefe(1-p_{\\textrm{t}})^{\\gamma}( 1 - italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_\u03b3 end_POSTSUPERSCRIPT to the cross entropy loss, with tunable focusing parameter \u03b3\u22650\ud835\udefe0\\gamma\\geq 0italic_\u03b3 \u2265 0. We define the focal loss as:FL(pt)=\u2212(1\u2212pt)\u03b3log\u2061(pt).FLsubscript\ud835\udc5dtsuperscript1subscript\ud835\udc5dt\ud835\udefesubscript\ud835\udc5dt\\textrm{FL}(p_{\\textrm{t}})=-(1-p_{\\textrm{t}})^{\\gamma}\\log(p_{\\textrm{t}}).FL ( italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT ) = - ( 1 - italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_\u03b3 end_POSTSUPERSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT ) .(4)", "The focal loss is visualized for several values of \u03b3\u2208[0,5]\ud835\udefe05\\gamma\\in[0,5]italic_\u03b3 \u2208 [ 0 , 5 ] in Figure\u00a01. We note two properties of the focal loss. (1) When an example is misclassified and ptsubscript\ud835\udc5dtp_{\\textrm{t}}italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT is small, the modulating factor is near 1111 and the loss is unaffected. As pt\u21921\u2192subscript\ud835\udc5dt1p_{\\textrm{t}}\\rightarrow 1italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT \u2192 1, the factor goes to 0 and the loss for well-classified examples is down-weighted. (2) The focusing parameter \u03b3\ud835\udefe\\gammaitalic_\u03b3 smoothly adjusts the rate at which easy examples are down-weighted. When \u03b3=0\ud835\udefe0\\gamma=0italic_\u03b3 = 0, FL is equivalent to CE, and as \u03b3\ud835\udefe\\gammaitalic_\u03b3 is increased the effect of the modulating factor is likewise increased (we found \u03b3=2\ud835\udefe2\\gamma=2italic_\u03b3 = 2 to work best in our experiments).", "Intuitively, the modulating factor reduces the loss contribution from easy examples and extends the range in which an example receives low loss. For instance, with \u03b3=2\ud835\udefe2\\gamma=2italic_\u03b3 = 2, an example classified with pt=0.9subscript\ud835\udc5dt0.9p_{\\textrm{t}}=0.9italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT = 0.9 would have 100\u00d7100\\times100 \u00d7 lower loss compared with CE and with pt\u22480.968subscript\ud835\udc5dt0.968p_{\\textrm{t}}\\approx 0.968italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT \u2248 0.968 it would have 1000\u00d71000\\times1000 \u00d7 lower loss. This in turn increases the importance of correcting misclassified examples (whose loss is scaled down by at most 4\u00d74\\times4 \u00d7 for pt\u2264.5subscript\ud835\udc5dt.5p_{\\textrm{t}}\\leq.5italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT \u2264 .5 and \u03b3=2\ud835\udefe2\\gamma=2italic_\u03b3 = 2).", "In practice we use an \u03b1\ud835\udefc\\alphaitalic_\u03b1-balanced variant of the focal loss:FL(pt)=\u2212\u03b1t(1\u2212pt)\u03b3log\u2061(pt).FLsubscript\ud835\udc5dtsubscript\ud835\udefctsuperscript1subscript\ud835\udc5dt\ud835\udefesubscript\ud835\udc5dt\\textrm{FL}(p_{\\textrm{t}})=-\\alpha_{\\textrm{t}}(1-p_{\\textrm{t}})^{\\gamma}\\log(p_{\\textrm{t}}).FL ( italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT ) = - italic_\u03b1 start_POSTSUBSCRIPT t end_POSTSUBSCRIPT ( 1 - italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_\u03b3 end_POSTSUPERSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT ) .(5)We adopt this form in our experiments as it yields slightly improved accuracy over the non-\u03b1\ud835\udefc\\alphaitalic_\u03b1-balanced form. Finally, we note that the implementation of the loss layer combines the sigmoid operation for computing p\ud835\udc5dpitalic_p with the loss computation, resulting in greater numerical stability.", "While in our main experimental results we use the focal loss definition above, its precise form is not crucial. In the appendix we consider other instantiations of the focal loss and demonstrate that these can be equally effective.", "Binary classification models are by default initialized to have equal probability of outputting either y=\u22121\ud835\udc661y=-1italic_y = - 1 or 1111. Under such an initialization, in the presence of class imbalance, the loss due to the frequent class can dominate total loss and cause instability in early training. To counter this, we introduce the concept of a \u2018prior\u2019 for the value of p\ud835\udc5dpitalic_p estimated by the model for the rare class (foreground) at the start of training. We denote the prior by \u03c0\ud835\udf0b\\piitalic_\u03c0 and set it so that the model\u2019s estimated p\ud835\udc5dpitalic_p for examples of the rare class is low, e.g. 0.010.010.010.01. We note that this is a change in model initialization (see \u00a74.1) and not of the loss function. We found this to improve training stability for both the cross entropy and focal loss in the case of heavy class imbalance.", "Two-stage detectors are often trained with the cross entropy loss without use of \u03b1\ud835\udefc\\alphaitalic_\u03b1-balancing or our proposed loss. Instead, they address class imbalance through two mechanisms: (1) a two-stage cascade and (2) biased minibatch sampling. The first cascade stage is an object proposal mechanism [35, 24, 28] that reduces the nearly infinite set of possible object locations down to one or two thousand. Importantly, the selected proposals are not random, but are likely to correspond to true object locations, which removes the vast majority of easy negatives. When training the second stage, biased sampling is typically used to construct minibatches that contain, for instance, a 1:3 ratio of positive to negative examples. This ratio is like an implicit \u03b1\ud835\udefc\\alphaitalic_\u03b1-balancing factor that is implemented via sampling. Our proposed focal loss is designed to address these mechanisms in a one-stage detection system directly via the loss function.", "RetinaNet is a single, unified network composed of a backbone network and two task-specific subnetworks. The backbone is responsible for computing a convolutional feature map over an entire input image and is an off-the-self convolutional network. The first subnet performs convolutional object classification on the backbone\u2019s output; the second subnet performs convolutional bounding box regression. The two subnetworks feature a simple design that we propose specifically for one-stage, dense detection, see Figure\u00a03. While there are many possible choices for the details of these components, most design parameters are not particularly sensitive to exact values as shown in the experiments. We describe each component of RetinaNet next.", "We adopt the Feature Pyramid Network (FPN) from [20] as the backbone network for RetinaNet. In brief, FPN augments a standard convolutional network with a top-down pathway and lateral connections so the network efficiently constructs a rich, multi-scale feature pyramid from a single resolution input image, see Figure\u00a03(a)-(b). Each level of the pyramid can be used for detecting objects at a different scale. FPN improves multi-scale predictions from fully convolutional networks (FCN) [23], as shown by its gains for RPN [28] and DeepMask-style proposals [24], as well at two-stage detectors such as Fast R-CNN [10] or Mask R-CNN [14].", "Following [20], we build FPN on top of the ResNet architecture [16]. We construct a pyramid with levels P3subscript\ud835\udc433P_{3}italic_P start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT through P7subscript\ud835\udc437P_{7}italic_P start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT, where l\ud835\udc59litalic_l indicates pyramid level (Plsubscript\ud835\udc43\ud835\udc59P_{l}italic_P start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT has resolution 2lsuperscript2\ud835\udc592^{l}2 start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT lower than the input). As in [20] all pyramid levels have C=256\ud835\udc36256C=256italic_C = 256 channels. Details of the pyramid generally follow [20] with a few modest differences.222RetinaNet uses feature pyramid levels P3subscript\ud835\udc433P_{3}italic_P start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT to P7subscript\ud835\udc437P_{7}italic_P start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT, where P3subscript\ud835\udc433P_{3}italic_P start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT to P5subscript\ud835\udc435P_{5}italic_P start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT are computed from the output of the corresponding ResNet residual stage (C3subscript\ud835\udc363C_{3}italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT through C5subscript\ud835\udc365C_{5}italic_C start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT) using top-down and lateral connections just as in [20], P6subscript\ud835\udc436P_{6}italic_P start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT is obtained via a 3\u00d7\\times\u00d73 stride-2 conv on C5subscript\ud835\udc365C_{5}italic_C start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT, and P7subscript\ud835\udc437P_{7}italic_P start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT is computed by applying ReLU followed by a 3\u00d7\\times\u00d73 stride-2 conv on P6subscript\ud835\udc436P_{6}italic_P start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT. This differs slightly from [20]: (1) we don\u2019t use the high-resolution pyramid level P2subscript\ud835\udc432P_{2}italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT for computational reasons, (2) P6subscript\ud835\udc436P_{6}italic_P start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT is computed by strided convolution instead of downsampling, and (3) we include P7subscript\ud835\udc437P_{7}italic_P start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT to improve large object detection. These minor modifications improve speed while maintaining accuracy. While many design choices are not crucial, we emphasize the use of the FPN backbone is; preliminary experiments using features from only the final ResNet layer yielded low AP.", "We use translation-invariant anchor boxes similar to those in the RPN variant in [20]. The anchors have areas of 322superscript32232^{2}32 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT to 5122superscript5122512^{2}512 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT on pyramid levels P3subscript\ud835\udc433P_{3}italic_P start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT to P7subscript\ud835\udc437P_{7}italic_P start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT, respectively. As in [20], at each pyramid level we use anchors at three aspect ratios {1\\{1{ 1:2,22,2 , 1111:1111, 2222:1}1\\}1 }. For denser scale coverage than in [20], at each level we add anchors of sizes {20superscript202^{0}2 start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT, 21/3superscript2132^{1/3}2 start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT, 22/3superscript2232^{2/3}2 start_POSTSUPERSCRIPT 2 / 3 end_POSTSUPERSCRIPT} of the original set of 3 aspect ratio anchors. This improve AP in our setting. In total there are A=9\ud835\udc349A=9italic_A = 9 anchors per level and across levels they cover the scale range 32 - 813 pixels with respect to the network\u2019s input image.", "Each anchor is assigned a length K\ud835\udc3eKitalic_K one-hot vector of classification targets, where K\ud835\udc3eKitalic_K is the number of object classes, and a 4-vector of box regression targets. We use the assignment rule from RPN [28] but modified for multi-class detection and with adjusted thresholds. Specifically, anchors are assigned to ground-truth object boxes using an intersection-over-union (IoU) threshold of 0.5; and to background if their IoU is in [0, 0.4). As each anchor is assigned to at most one object box, we set the corresponding entry in its length K\ud835\udc3eKitalic_K label vector to 1111 and all other entries to 00. If an anchor is unassigned, which may happen with overlap in [0.4, 0.5), it is ignored during training. Box regression targets are computed as the offset between each anchor and its assigned object box, or omitted if there is no assignment.", "The classification subnet predicts the probability of object presence at each spatial position for each of the A\ud835\udc34Aitalic_A anchors and K\ud835\udc3eKitalic_K object classes. This subnet is a small FCN attached to each FPN level; parameters of this subnet are shared across all pyramid levels. Its design is simple. Taking an input feature map with C\ud835\udc36Citalic_C channels from a given pyramid level, the subnet applies four 3\u00d7\\times\u00d73 conv layers, each with C\ud835\udc36Citalic_C filters and each followed by ReLU activations, followed by a 3\u00d7\\times\u00d73 conv layer with KA\ud835\udc3e\ud835\udc34KAitalic_K italic_A filters. Finally sigmoid activations are attached to output the KA\ud835\udc3e\ud835\udc34KAitalic_K italic_A binary predictions per spatial location, see Figure\u00a03 (c). We use C=256\ud835\udc36256C=256italic_C = 256 and A=9\ud835\udc349A=9italic_A = 9 in most experiments.", "In contrast to RPN [28], our object classification subnet is deeper, uses only 3\u00d7\\times\u00d73 convs, and does not share parameters with the box regression subnet (described next). We found these higher-level design decisions to be more important than specific values of hyperparameters.", "In parallel with the object classification subnet, we attach another small FCN to each pyramid level for the purpose of regressing the offset from each anchor box to a nearby ground-truth object, if one exists. The design of the box regression subnet is identical to the classification subnet except that it terminates in 4A4\ud835\udc344A4 italic_A linear outputs per spatial location, see Figure\u00a03 (d). For each of the A\ud835\udc34Aitalic_A anchors per spatial location, these 4444 outputs predict the relative offset between the anchor and the ground-truth box (we use the standard box parameterization from R-CNN [11]). We note that unlike most recent work, we use a class-agnostic bounding box regressor which uses fewer parameters and we found to be equally effective. The object classification subnet and the box regression subnet, though sharing a common structure, use separate parameters.", "RetinaNet forms a single FCN comprised of a ResNet-FPN backbone, a classification subnet, and a box regression subnet, see Figure\u00a03. As such, inference involves simply forwarding an image through the network. To improve speed, we only decode box predictions from at most 1k top-scoring predictions per FPN level, after thresholding detector confidence at 0.05. The top predictions from all levels are merged and non-maximum suppression with a threshold of 0.5 is applied to yield the final detections.", "We use the focal loss introduced in this work as the loss on the output of the classification subnet. As we will show in \u00a75, we find that \u03b3=2\ud835\udefe2\\gamma=2italic_\u03b3 = 2 works well in practice and the RetinaNet is relatively robust to \u03b3\u2208[0.5,5]\ud835\udefe0.55\\gamma\\in[0.5,5]italic_\u03b3 \u2208 [ 0.5 , 5 ]. We emphasize that when training RetinaNet, the focal loss is applied to all \u223csimilar-to\\scriptstyle\\sim\u223c100k anchors in each sampled image. This stands in contrast to common practice of using heuristic sampling (RPN) or hard example mining (OHEM, SSD) to select a small set of anchors (e.g., 256) for each minibatch. The total focal loss of an image is computed as the sum of the focal loss over all \u223csimilar-to\\scriptstyle\\sim\u223c100k anchors, normalized by the number of anchors assigned to a ground-truth box. We perform the normalization by the number of assigned anchors, not total anchors, since the vast majority of anchors are easy negatives and receive negligible loss values under the focal loss. Finally we note that \u03b1\ud835\udefc\\alphaitalic_\u03b1, the weight assigned to the rare class, also has a stable range, but it interacts with \u03b3\ud835\udefe\\gammaitalic_\u03b3 making it necessary to select the two together (see Tables\u00a01a and 1b). In general \u03b1\ud835\udefc\\alphaitalic_\u03b1 should be decreased slightly as \u03b3\ud835\udefe\\gammaitalic_\u03b3 is increased (for \u03b3=2\ud835\udefe2\\gamma=2italic_\u03b3 = 2, \u03b1=0.25\ud835\udefc0.25\\alpha=0.25italic_\u03b1 = 0.25 works best).", "We experiment with ResNet-50-FPN and ResNet-101-FPN backbones [20]. The base ResNet-50 and ResNet-101 models are pre-trained on ImageNet1k; we use the models released by [16]. New layers added for FPN are initialized as in [20]. All new conv layers except the final one in the RetinaNet subnets are initialized with bias b=0\ud835\udc4f0b=0italic_b = 0 and a Gaussian weight fill with \u03c3=0.01\ud835\udf0e0.01\\sigma=0.01italic_\u03c3 = 0.01. For the final conv layer of the classification subnet, we set the bias initialization to b=\u2212log\u2061((1\u2212\u03c0)/\u03c0)\ud835\udc4f1\ud835\udf0b\ud835\udf0bb=-\\log((1-\\pi)/\\pi)italic_b = - roman_log ( ( 1 - italic_\u03c0 ) / italic_\u03c0 ), where \u03c0\ud835\udf0b\\piitalic_\u03c0 specifies that at the start of training every anchor should be labeled as foreground with confidence of \u223csimilar-to\\scriptstyle\\sim\u223c\u03c0\ud835\udf0b\\piitalic_\u03c0. We use \u03c0=.01\ud835\udf0b.01\\pi=.01italic_\u03c0 = .01 in all experiments, although results are robust to the exact value. As explained in \u00a73.3, this initialization prevents the large number of background anchors from generating a large, destabilizing loss value in the first iteration of training.", "RetinaNet is trained with stochastic gradient descent (SGD). We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table\u00a01e.", "We present experimental results on the bounding box detection track of the challenging COCO benchmark [21]. For training, we follow common practice [1, 20] and use the COCO trainval35k split (union of 80k images from train and a random 35k subset of images from the 40k image val split). We report lesion and sensitivity studies by evaluating on the minival split (the remaining 5k images from val). For our main results, we report COCO AP on the test-dev split, which has no public labels and requires use of the evaluation server.", "We run numerous experiments to analyze the behavior of the loss function for dense detection along with various optimization strategies. For all experiments we use depth 50 or 101 ResNets [16] with a Feature Pyramid Network (FPN)\u00a0[20] constructed on top. For all ablation studies we use an image scale of 600 pixels for training and testing.", "Our first attempt to train RetinaNet uses standard cross entropy (CE) loss without any modifications to the initialization or learning strategy. This fails quickly, with the network diverging during training. However, simply initializing the last layer of our model such that the prior probability of detecting an object is \u03c0=.01\ud835\udf0b.01\\pi=.01italic_\u03c0 = .01 (see \u00a74.1) enables effective learning. Training RetinaNet with ResNet-50 and this initialization already yields a respectable AP of 30.2 on COCO. Results are insensitive to the exact value of \u03c0\ud835\udf0b\\piitalic_\u03c0 so we use \u03c0=.01\ud835\udf0b.01\\pi=.01italic_\u03c0 = .01 for all experiments.", "Our next attempt to improve learning involved using the \u03b1\ud835\udefc\\alphaitalic_\u03b1-balanced CE loss described in \u00a73.1. Results for various \u03b1\ud835\udefc\\alphaitalic_\u03b1 are shown in Table\u00a01a. Setting \u03b1=.75\ud835\udefc.75\\alpha=.75italic_\u03b1 = .75 gives a gain of 0.9 points AP.", "Results using our proposed focal loss are shown in Table\u00a01b. The focal loss introduces one new hyperparameter, the focusing parameter \u03b3\ud835\udefe\\gammaitalic_\u03b3, that controls the strength of the modulating term. When \u03b3=0\ud835\udefe0\\gamma=0italic_\u03b3 = 0, our loss is equivalent to the CE loss. As \u03b3\ud835\udefe\\gammaitalic_\u03b3 increases, the shape of the loss changes so that \u201ceasy\u201d examples with low loss get further discounted, see Figure\u00a01. FL shows large gains over CE as \u03b3\ud835\udefe\\gammaitalic_\u03b3 is increased. With \u03b3=2\ud835\udefe2\\gamma=2italic_\u03b3 = 2, FL yields a 2.9 AP improvement over the \u03b1\ud835\udefc\\alphaitalic_\u03b1-balanced CE loss.", "For the experiments in Table\u00a01b, for a fair comparison we find the best \u03b1\ud835\udefc\\alphaitalic_\u03b1 for each \u03b3\ud835\udefe\\gammaitalic_\u03b3. We observe that lower \u03b1\ud835\udefc\\alphaitalic_\u03b1\u2019s are selected for higher \u03b3\ud835\udefe\\gammaitalic_\u03b3\u2019s (as easy negatives are down-weighted, less emphasis needs to be placed on the positives). Overall, however, the benefit of changing \u03b3\ud835\udefe\\gammaitalic_\u03b3 is much larger, and indeed the best \u03b1\ud835\udefc\\alphaitalic_\u03b1\u2019s ranged in just [.25,.75] (we tested \u03b1\u2208[.01,.999]\ud835\udefc.01.999\\alpha\\in[.01,.999]italic_\u03b1 \u2208 [ .01 , .999 ]). We use \u03b3=2.0\ud835\udefe2.0\\gamma=2.0italic_\u03b3 = 2.0 with \u03b1=.25\ud835\udefc.25\\alpha=.25italic_\u03b1 = .25 for all experiments but \u03b1=.5\ud835\udefc.5\\alpha=.5italic_\u03b1 = .5 works nearly as well (.4 AP lower).", "To understand the focal loss better, we analyze the empirical distribution of the loss of a converged model. For this, we take take our default ResNet-101 600-pixel model trained with \u03b3=2\ud835\udefe2\\gamma=2italic_\u03b3 = 2 (which has 36.0 AP). We apply this model to a large number of random images and sample the predicted probability for \u223csimilar-to\\scriptstyle\\sim\u223c107superscript10710^{7}10 start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT negative windows and \u223csimilar-to\\scriptstyle\\sim\u223c105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT positive windows. Next, separately for positives and negatives, we compute FL for these samples, and normalize the loss such that it sums to one. Given the normalized loss, we can sort the loss from lowest to highest and plot its cumulative distribution function (CDF) for both positive and negative samples and for different settings for \u03b3\ud835\udefe\\gammaitalic_\u03b3 (even though model was trained with \u03b3=2\ud835\udefe2\\gamma=2italic_\u03b3 = 2).", "Cumulative distribution functions for positive and negative samples are shown in Figure\u00a04. If we observe the positive samples, we see that the CDF looks fairly similar for different values of \u03b3\ud835\udefe\\gammaitalic_\u03b3. For example, approximately 20% of the hardest positive samples account for roughly half of the positive loss, as \u03b3\ud835\udefe\\gammaitalic_\u03b3 increases more of the loss gets concentrated in the top 20% of examples, but the effect is minor.", "The effect of \u03b3\ud835\udefe\\gammaitalic_\u03b3 on negative samples is dramatically different. For \u03b3=0\ud835\udefe0\\gamma=0italic_\u03b3 = 0, the positive and negative CDFs are quite similar. However, as \u03b3\ud835\udefe\\gammaitalic_\u03b3 increases, substantially more weight becomes concentrated on the hard negative examples. In fact, with \u03b3=2\ud835\udefe2\\gamma=2italic_\u03b3 = 2 (our default setting), the vast majority of the loss comes from a small fraction of samples. As can be seen, FL can effectively discount the effect of easy negatives, focusing all attention on the hard negative examples.", "[31] proposed to improve training of two-stage detectors by constructing minibatches using high-loss examples. Specifically, in OHEM each example is scored by its loss, non-maximum suppression (nms) is then applied, and a minibatch is constructed with the highest-loss examples. The nms threshold and batch size are tunable parameters. Like the focal loss, OHEM puts more emphasis on misclassified examples, but unlike FL, OHEM completely discards easy examples. We also implement a variant of OHEM used in SSD [22]: after applying nms to all examples, the minibatch is constructed to enforce a 1:3 ratio between positives and negatives to help ensure each minibatch has enough positives.", "We test both OHEM variants in our setting of one-stage detection which has large class imbalance. Results for the original OHEM strategy and the \u2018OHEM 1:3\u2019 strategy for selected batch sizes and nms thresholds are shown in Table\u00a01d. These results use ResNet-101, our baseline trained with FL achieves 36.0 AP for this setting. In contrast, the best setting for OHEM (no 1:3 ratio, batch size 128, nms of .5) achieves 32.8 AP. This is a gap of 3.2 AP, showing FL is more effective than OHEM for training dense detectors. We note that we tried other parameter setting and variants for OHEM but did not achieve better results.", "Finally, in early experiments, we attempted to train with the hinge loss [13] on ptsubscript\ud835\udc5dtp_{\\textrm{t}}italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT, which sets loss to 0 above a certain value of ptsubscript\ud835\udc5dtp_{\\textrm{t}}italic_p start_POSTSUBSCRIPT t end_POSTSUBSCRIPT. However, this was unstable and we did not manage to obtain meaningful results. Results exploring alternate loss functions are in the appendix.", "One of the most important design factors in a one-stage detection system is how densely it covers the space of possible image boxes. Two-stage detectors can classify boxes at any position, scale, and aspect ratio using a region pooling operation [10]. In contrast, as one-stage detectors use a fixed sampling grid, a popular approach for achieving high coverage of boxes in these approaches is to use multiple \u2018anchors\u2019 [28] at each spatial position to cover boxes of various scales and aspect ratios.", "We sweep over the number of scale and aspect ratio anchors used at each spatial position and each pyramid level in FPN. We consider cases from a single square anchor at each location to 12 anchors per location spanning 4 sub-octave scales (2k/4superscript2\ud835\udc5842^{k/4}2 start_POSTSUPERSCRIPT italic_k / 4 end_POSTSUPERSCRIPT, for k\u22643\ud835\udc583k\\leq 3italic_k \u2264 3) and 3 aspect ratios [0.5, 1, 2]. Results using ResNet-50 are shown in Table\u00a01c. A surprisingly good AP (30.3) is achieved using just one square anchor. However, the AP can be improved by nearly 4 points (to 34.0) when using 3 scales and 3 aspect ratios per location. We used this setting for all other experiments in this work.", "Finally, we note that increasing beyond 6-9 anchors did not shown further gains. Thus while two-stage systems can classify arbitrary boxes in an image, the saturation of performance w.r.t. density implies the higher potential density of two-stage systems may not offer an advantage.", "Larger backbone networks yield higher accuracy, but also slower inference speeds. Likewise for input image scale (defined by the shorter image side). We show the impact of these two factors in Table\u00a01e. In Figure\u00a02 we plot the speed/accuracy trade-off curve for RetinaNet and compare it to recent methods using public numbers on COCO test-dev. The plot reveals that RetinaNet, enabled by our focal loss, forms an upper envelope over all existing methods, discounting the low-accuracy regime. RetinaNet with ResNet-101-FPN and a 600 pixel image scale (which we denote by RetinaNet-101-600 for simplicity) matches the accuracy of the recently published ResNet-101-FPN Faster R-CNN [20], while running in 122 ms per image compared to 172 ms (both measured on an Nvidia M40 GPU). Using larger scales allows RetinaNet to surpass the accuracy of all two-stage approaches, while still being faster. For faster runtimes, there is only one operating point (500 pixel input) at which using ResNet-50-FPN improves over ResNet-101-FPN. Addressing the high frame rate regime will likely require special network design, as in [27], and is beyond the scope of this work. We note that after publication, faster and more accurate results can now be obtained by a variant of Faster R-CNN from [12].", "We evaluate RetinaNet on the challenging COCO dataset and compare test-dev results to recent state-of-the-art methods including both one-stage and two-stage models. Results are presented in Table\u00a02 for our RetinaNet-101-800 model trained using scale jitter and for 1.5\u00d7\\times\u00d7 longer than the models in Table\u00a01e (giving a 1.3 AP gain). Compared to existing one-stage methods, our approach achieves a healthy 5.9 point AP gap (39.1 vs. 33.2) with the closest competitor, DSSD [9], while also being faster, see Figure\u00a02. Compared to recent two-stage methods, RetinaNet achieves a 2.3 point gap above the top-performing Faster R-CNN model based on Inception-ResNet-v2-TDM [32]. Plugging in ResNeXt-32x8d-101-FPN [38] as the RetinaNet backbone further improves results another 1.7 AP, surpassing 40 AP on COCO.", "In this work, we identify class imbalance as the primary obstacle preventing one-stage object detectors from surpassing top-performing, two-stage methods. To address this, we propose the focal loss which applies a modulating term to the cross entropy loss in order to focus learning on hard negative examples. Our approach is simple and highly effective. We demonstrate its efficacy by designing a fully convolutional one-stage detector and report extensive experimental analysis showing that it achieves state-of-the-art accuracy and speed. Source code is available at https://github.com/facebookresearch/Detectron [12]."], "figure_types": {"1a857da1a8ce47b2aa185b91b5cb215ddef24de7/5-Figure3-1.png": "schematic", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/6-Table1-1.png": "table", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/7-Figure4-1.png": "plot", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/8-Table2-1.png": "table", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/9-Figure5-1.png": "plot", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/9-Figure6-1.png": "plot", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/9-Figure7-1.png": "plot", "1a857da1a8ce47b2aa185b91b5cb215ddef24de7/9-Table3-1.png": "table"}}, "1312.6114": {"paper_id": "paper_96", "title": "Auto-Encoding Variational Bayes", "arxiv_url": "https://arxiv.org/abs/1312.6114", "s2orc_url": "https://www.semanticscholar.org/paper/5f5dc5b9a2ba710937e2c413b37b053cd673df02", "all_figures_tables": {"5f5dc5b9a2ba710937e2c413b37b053cd673df02/10-Figure4-1.png": "Figure 4: Visualisations of learned data manifold for generative models with two-dimensional latent space, learned with AEVB. Since the prior of the latent space is Gaussian, linearly spaced coordinates on the unit square were transformed through the inverse CDF of the Gaussian to produce values of the latent variables z. For each of these values z, we plotted the corresponding generative p\u03b8(x|z) with the learned parameters \u03b8.", "5f5dc5b9a2ba710937e2c413b37b053cd673df02/2-Figure1-1.png": "Figure 1: The type of directed graphical model under consideration. Solid lines denote the generative model p\u03b8(z)p\u03b8(x|z), dashed lines denote the variational approximation q\u03c6(z|x) to the intractable posterior p\u03b8(z|x). The variational parameters \u03c6 are learned jointly with the generative model parameters \u03b8.", "5f5dc5b9a2ba710937e2c413b37b053cd673df02/7-Figure2-1.png": "Figure 2: Comparison of our AEVB method to the wake-sleep algorithm, in terms of optimizing the lower bound, for different dimensionality of latent space (Nz). Our method converged considerably faster and reached a better solution in all experiments. Interestingly enough, more latent variables does not result in more overfitting, which is explained by the regularizing effect of the lower bound. Vertical axis: the estimated average variational lower bound per datapoint. The estimator variance was small (&lt; 1) and omitted. Horizontal axis: amount of training points evaluated. Computation took around 20-40 minutes per million training samples with a Intel Xeon CPU running at an effective 40 GFLOPS.", "5f5dc5b9a2ba710937e2c413b37b053cd673df02/8-Figure3-1.png": "Figure 3: Comparison of AEVB to the wake-sleep algorithm and Monte Carlo EM, in terms of the estimated marginal likelihood, for a different number of training points. Monte Carlo EM is not an on-line algorithm, and (unlike AEVB and the wake-sleep method) can\u2019t be applied efficiently for the full MNIST dataset."}, "referred_figures_tables": [["5f5dc5b9a2ba710937e2c413b37b053cd673df02/8-Figure3-1.png"]], "question_id": [6], "question": ["Why can\u2019t we use sampling based solutions instead of this algorithm in case of large datasets?"], "question_section": ["Method"], "question_trigger_sentence": ["A large dataset: we have so much data that batch optimization is too costly; we would like\nto make parameter updates using small minibatches or even single datapoints."], "question_type": ["Deep/complex question"], "evidential_info": [[{"context": "How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.", "rationale": "A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Samplingbased solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint."}, {"context": "For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder.", "rationale": "We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques."}, {"context": "For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used.The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC)\u00a0[DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure\u00a03.", "rationale": "For very low-dimensional latent space it is possible to estimate the marginal"}, {"context": "Very importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works efficiently in the case of:1.Intractability: the case where the integral of the marginal likelihood p_{\\boldsymbol{\\theta}}(\\mathbf{x})=\\int p_{\\boldsymbol{\\theta}}(\\mathbf{z})p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z})\\,d\\mathbf{z} is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density p_{\\boldsymbol{\\theta}}(\\mathbf{z}|\\mathbf{x})=p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z})p_{\\boldsymbol{\\theta}}(\\mathbf{z})/p_{\\boldsymbol{\\theta}}(\\mathbf{x}) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}), e.g. a neural network with a nonlinear hidden layer.2.A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling-based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint.", "rationale": "In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint."}]], "composition": ["It is hard to use sampling based solutions because batch optimization with so much data is too expensive. If you want to inference in almost any model with continuous latent variables and/or parameters, sampling based solution is not applicable. For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator which is one of sampling based solution. But we need to deal with high dimensional data and the AEVB algorithm is useful."], "Is_figure_in_evidence": [false], "Is_table_in_evidence": [true], "question_key": ["1487"], "passages": ["How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.", "For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder.", "The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity.", "Let us consider some dataset \ud835\udc17={\ud835\udc31(i)}i=1N\ud835\udc17superscriptsubscriptsuperscript\ud835\udc31\ud835\udc56\ud835\udc561\ud835\udc41\\mathbf{X}=\\{\\mathbf{x}^{(i)}\\}_{i=1}^{N}bold_X = { bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT consisting of N\ud835\udc41Nitalic_N i.i.d. samples of some continuous or discrete variable \ud835\udc31\ud835\udc31\\mathbf{x}bold_x. We assume that the data are generated by some random process, involving an unobserved continuous random variable \ud835\udc33\ud835\udc33\\mathbf{z}bold_z. The process consists of two steps: (1) a value \ud835\udc33(i)superscript\ud835\udc33\ud835\udc56\\mathbf{z}^{(i)}bold_z start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT is generated from some prior distribution p\ud835\udf3d*(\ud835\udc33)subscript\ud835\udc5dsuperscript\ud835\udf3d\ud835\udc33p_{\\boldsymbol{\\theta}^{*}}(\\mathbf{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_z ); (2) a value \ud835\udc31(i)superscript\ud835\udc31\ud835\udc56\\mathbf{x}^{(i)}bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT is generated from some conditional distribution p\ud835\udf3d*(\ud835\udc31|\ud835\udc33)subscript\ud835\udc5dsuperscript\ud835\udf3dconditional\ud835\udc31\ud835\udc33p_{\\boldsymbol{\\theta}^{*}}(\\mathbf{x}|\\mathbf{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_x | bold_z ). We assume that the prior p\ud835\udf3d*(\ud835\udc33)subscript\ud835\udc5dsuperscript\ud835\udf3d\ud835\udc33p_{\\boldsymbol{\\theta}^{*}}(\\mathbf{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_z ) and likelihood p\ud835\udf3d*(\ud835\udc31|\ud835\udc33)subscript\ud835\udc5dsuperscript\ud835\udf3dconditional\ud835\udc31\ud835\udc33p_{\\boldsymbol{\\theta}^{*}}(\\mathbf{x}|\\mathbf{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_x | bold_z ) come from parametric families of distributions p\ud835\udf3d(\ud835\udc33)subscript\ud835\udc5d\ud835\udf3d\ud835\udc33p_{\\boldsymbol{\\theta}}(\\mathbf{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z ) and p\ud835\udf3d(\ud835\udc31|\ud835\udc33)subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc31\ud835\udc33p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x | bold_z ), and that their PDFs are differentiable almost everywhere w.r.t. both \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8 and \ud835\udc33\ud835\udc33\\mathbf{z}bold_z. Unfortunately, a lot of this process is hidden from our view: the true parameters \ud835\udf3d*superscript\ud835\udf3d\\boldsymbol{\\theta}^{*}bold_italic_\u03b8 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT as well as the values of the latent variables \ud835\udc33(i)superscript\ud835\udc33\ud835\udc56\\mathbf{z}^{(i)}bold_z start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT are unknown to us.", "Very importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works efficiently in the case of:1.Intractability: the case where the integral of the marginal likelihood p\ud835\udf3d(\ud835\udc31)=\u222bp\ud835\udf3d(\ud835\udc33)p\ud835\udf3d(\ud835\udc31|\ud835\udc33)\ud835\udc51\ud835\udc33subscript\ud835\udc5d\ud835\udf3d\ud835\udc31subscript\ud835\udc5d\ud835\udf3d\ud835\udc33subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc31\ud835\udc33differential-d\ud835\udc33p_{\\boldsymbol{\\theta}}(\\mathbf{x})=\\int p_{\\boldsymbol{\\theta}}(\\mathbf{z})p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z})\\,d\\mathbf{z}italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x ) = \u222b italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z ) italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x | bold_z ) italic_d bold_z is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density p\ud835\udf3d(\ud835\udc33|\ud835\udc31)=p\ud835\udf3d(\ud835\udc31|\ud835\udc33)p\ud835\udf3d(\ud835\udc33)/p\ud835\udf3d(\ud835\udc31)subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc33\ud835\udc31subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc31\ud835\udc33subscript\ud835\udc5d\ud835\udf3d\ud835\udc33subscript\ud835\udc5d\ud835\udf3d\ud835\udc31p_{\\boldsymbol{\\theta}}(\\mathbf{z}|\\mathbf{x})=p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z})p_{\\boldsymbol{\\theta}}(\\mathbf{z})/p_{\\boldsymbol{\\theta}}(\\mathbf{x})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z | bold_x ) = italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x | bold_z ) italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z ) / italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x ) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions p\ud835\udf3d(\ud835\udc31|\ud835\udc33)subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc31\ud835\udc33p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x | bold_z ), e.g. a neural network with a nonlinear hidden layer.2.A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling-based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint.", "We are interested in, and propose a solution to, three related problems in the above scenario:1.Efficient approximate ML or MAP estimation for the parameters \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artificial data that resembles the real data.2.Efficient approximate posterior inference of the latent variable \ud835\udc33\ud835\udc33\\mathbf{z}bold_z given an observed value \ud835\udc31\ud835\udc31\\mathbf{x}bold_x for a choice of parameters \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8. This is useful for coding or data representation tasks.3.Efficient approximate marginal inference of the variable \ud835\udc31\ud835\udc31\\mathbf{x}bold_x. This allows us to perform all kinds of inference tasks where a prior over \ud835\udc31\ud835\udc31\\mathbf{x}bold_x is required. Common applications in computer vision include image denoising, inpainting and super-resolution.", "For the purpose of solving the above problems, let us introduce a recognition model q\u03d5(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ): an approximation to the intractable true posterior p\ud835\udf3d(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc33\ud835\udc31p_{\\boldsymbol{\\theta}}(\\mathbf{z}|\\mathbf{x})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z | bold_x ). Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters \u03d5bold-italic-\u03d5\\boldsymbol{\\phi}bold_italic_\u03d5 are not computed from some closed-form expectation. Instead, we\u2019ll introduce a method for learning the recognition model parameters \u03d5bold-italic-\u03d5\\boldsymbol{\\phi}bold_italic_\u03d5 jointly with the generative model parameters \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8.", "From a coding theory perspective, the unobserved variables \ud835\udc33\ud835\udc33\\mathbf{z}bold_z have an interpretation as a latent representation or code. In this paper we will therefore also refer to the recognition model q\u03d5(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) as a probabilistic encoder, since given a datapoint \ud835\udc31\ud835\udc31\\mathbf{x}bold_x it produces a distribution (e.g. a Gaussian) over the possible values of the code \ud835\udc33\ud835\udc33\\mathbf{z}bold_z from which the datapoint \ud835\udc31\ud835\udc31\\mathbf{x}bold_x could have been generated. In a similar vein we will refer to p\ud835\udf3d(\ud835\udc31|\ud835\udc33)subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc31\ud835\udc33p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x | bold_z ) as a probabilistic decoder, since given a code \ud835\udc33\ud835\udc33\\mathbf{z}bold_z it produces a distribution over the possible corresponding values of \ud835\udc31\ud835\udc31\\mathbf{x}bold_x.", "The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints log\u2061p\ud835\udf3d(\ud835\udc31(1),\u22ef,\ud835\udc31(N))=\u2211i=1Nlog\u2061p\ud835\udf3d(\ud835\udc31(i))subscript\ud835\udc5d\ud835\udf3dsuperscript\ud835\udc311\u22efsuperscript\ud835\udc31\ud835\udc41superscriptsubscript\ud835\udc561\ud835\udc41subscript\ud835\udc5d\ud835\udf3dsuperscript\ud835\udc31\ud835\udc56\\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(1)},\\cdots,\\mathbf{x}^{(N)})=\\sum_{i=1}^{N}\\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)})roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , \u22ef , bold_x start_POSTSUPERSCRIPT ( italic_N ) end_POSTSUPERSCRIPT ) = \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ), which can each be rewritten as:logp\ud835\udf3d(\ud835\udc31(i))=DKL(q\u03d5(\ud835\udc33|\ud835\udc31(i))||p\ud835\udf3d(\ud835\udc33|\ud835\udc31(i)))+\u2112(\ud835\udf3d,\u03d5;\ud835\udc31(i))\\displaystyle\\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)})=D_{KL}(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}^{(i)})||p_{\\boldsymbol{\\theta}}(\\mathbf{z}|\\mathbf{x}^{(i)}))+\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) = italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) ) + caligraphic_L ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT )(1)The first RHS term is the KL divergence of the approximate from the true posterior. Since this KL-divergence is non-negative, the second RHS term \u2112(\ud835\udf3d,\u03d5;\ud835\udc31(i))\u2112\ud835\udf3dbold-italic-\u03d5superscript\ud835\udc31\ud835\udc56\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})caligraphic_L ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) is called the (variational) lower bound on the marginal likelihood of datapoint i\ud835\udc56iitalic_i, and can be written as:log\u2061p\ud835\udf3d(\ud835\udc31(i))\u2265\u2112(\ud835\udf3d,\u03d5;\ud835\udc31(i))subscript\ud835\udc5d\ud835\udf3dsuperscript\ud835\udc31\ud835\udc56\u2112\ud835\udf3dbold-italic-\u03d5superscript\ud835\udc31\ud835\udc56\\displaystyle\\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)})\\geq\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) \u2265 caligraphic_L ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT )=\ud835\udd3cq\u03d5(\ud835\udc33|\ud835\udc31)[\u2212log\u2061q\u03d5(\ud835\udc33|\ud835\udc31)+log\u2061p\ud835\udf3d(\ud835\udc31,\ud835\udc33)]absentsubscript\ud835\udd3csubscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31delimited-[]subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31subscript\ud835\udc5d\ud835\udf3d\ud835\udc31\ud835\udc33\\displaystyle=\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})}\\left[-\\log q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})+\\log p_{\\boldsymbol{\\theta}}(\\mathbf{x},\\mathbf{z})\\right]= blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) end_POSTSUBSCRIPT [ - roman_log italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) + roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x , bold_z ) ](2)which can also be written as:\u2112(\ud835\udf3d,\u03d5;\ud835\udc31(i))=\u2212DKL(q\u03d5(\ud835\udc33|\ud835\udc31(i))||p\ud835\udf3d(\ud835\udc33))+\ud835\udd3cq\u03d5(\ud835\udc33|\ud835\udc31(i))[logp\ud835\udf3d(\ud835\udc31(i)|\ud835\udc33)]\\displaystyle\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})=-D_{KL}(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}^{(i)})||p_{\\boldsymbol{\\theta}}(\\mathbf{z}))+\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}^{(i)})}\\left[\\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}|\\mathbf{z})\\right]caligraphic_L ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) = - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z ) ) + blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z ) ](3)We want to differentiate and optimize the lower bound \u2112(\ud835\udf3d,\u03d5;\ud835\udc31(i))\u2112\ud835\udf3dbold-italic-\u03d5superscript\ud835\udc31\ud835\udc56\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})caligraphic_L ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) w.r.t. both the variational parameters \u03d5bold-italic-\u03d5\\boldsymbol{\\phi}bold_italic_\u03d5 and generative parameters \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8. However, the gradient of the lower bound w.r.t. \u03d5bold-italic-\u03d5\\boldsymbol{\\phi}bold_italic_\u03d5 is a bit problematic. The usual (na\u00efve) Monte Carlo gradient estimator for this type of problem is:\u2207\u03d5\ud835\udd3cq\u03d5(\ud835\udc33)[f(\ud835\udc33)]=\ud835\udd3cq\u03d5(\ud835\udc33)[f(\ud835\udc33)\u2207q\u03d5(\ud835\udc33)log\u2061q\u03d5(\ud835\udc33)]\u22431L\u2211l=1Lf(\ud835\udc33)\u2207q\u03d5(\ud835\udc33(l))log\u2061q\u03d5(\ud835\udc33(l))subscript\u2207bold-italic-\u03d5subscript\ud835\udd3csubscript\ud835\udc5ebold-italic-\u03d5\ud835\udc33delimited-[]\ud835\udc53\ud835\udc33subscript\ud835\udd3csubscript\ud835\udc5ebold-italic-\u03d5\ud835\udc33delimited-[]\ud835\udc53\ud835\udc33subscript\u2207subscript\ud835\udc5ebold-italic-\u03d5\ud835\udc33subscript\ud835\udc5ebold-italic-\u03d5\ud835\udc33similar-to-or-equals1\ud835\udc3fsuperscriptsubscript\ud835\udc591\ud835\udc3f\ud835\udc53\ud835\udc33subscript\u2207subscript\ud835\udc5ebold-italic-\u03d5superscript\ud835\udc33\ud835\udc59subscript\ud835\udc5ebold-italic-\u03d5superscript\ud835\udc33\ud835\udc59\\nabla_{\\boldsymbol{\\phi}}\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z})}\\left[f(\\mathbf{z})\\right]=\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z})}\\left[f(\\mathbf{z})\\nabla_{q_{\\boldsymbol{\\phi}}(\\mathbf{z})}\\log q_{\\boldsymbol{\\phi}}(\\mathbf{z})\\right]\\simeq\\frac{1}{L}\\sum_{l=1}^{L}f(\\mathbf{z})\\nabla_{q_{\\boldsymbol{\\phi}}(\\mathbf{z}^{(l)})}\\log q_{\\boldsymbol{\\phi}}(\\mathbf{z}^{(l)})\u2207 start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z ) end_POSTSUBSCRIPT [ italic_f ( bold_z ) ] = blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z ) end_POSTSUBSCRIPT [ italic_f ( bold_z ) \u2207 start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z ) end_POSTSUBSCRIPT roman_log italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z ) ] \u2243 divide start_ARG 1 end_ARG start_ARG italic_L end_ARG \u2211 start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_f ( bold_z ) \u2207 start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT roman_log italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ) where \ud835\udc33(l)\u223cq\u03d5(\ud835\udc33|\ud835\udc31(i))similar-tosuperscript\ud835\udc33\ud835\udc59subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33superscript\ud835\udc31\ud835\udc56\\mathbf{z}^{(l)}\\sim q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}^{(i)})bold_z start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT \u223c italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ). This gradient estimator exhibits exhibits very high variance (see e.g. \u00a0[BJP12]) and is impractical for our purposes.", "In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. We assume an approximate posterior in the form q\u03d5(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ), but please note that the technique can be applied to the case q\u03d5(\ud835\udc33)subscript\ud835\udc5ebold-italic-\u03d5\ud835\udc33q_{\\boldsymbol{\\phi}}(\\mathbf{z})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z ), i.e. where we do not condition on \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, as well. The fully variational Bayesian method for inferring a posterior over the parameters is given in the appendix.", "Under certain mild conditions outlined in section\u00a02.4 for a chosen approximate posterior q\u03d5(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) we can reparameterize the random variable \ud835\udc33~\u223cq\u03d5(\ud835\udc33|\ud835\udc31)similar-to~\ud835\udc33subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31\\widetilde{\\mathbf{z}}\\sim q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})over~ start_ARG bold_z end_ARG \u223c italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) using a differentiable transformation g\u03d5(\u03f5,\ud835\udc31)subscript\ud835\udc54bold-italic-\u03d5bold-italic-\u03f5\ud835\udc31g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon},\\mathbf{x})italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_\u03f5 , bold_x ) of an (auxiliary) noise variable \u03f5bold-italic-\u03f5\\boldsymbol{\\epsilon}bold_italic_\u03f5:\ud835\udc33~=g\u03d5(\u03f5,\ud835\udc31)\u00a0with\u00a0\u03f5\u223cp(\u03f5)~\ud835\udc33subscript\ud835\udc54bold-italic-\u03d5bold-italic-\u03f5\ud835\udc31\u00a0with\u00a0bold-italic-\u03f5similar-to\ud835\udc5dbold-italic-\u03f5\\displaystyle\\widetilde{\\mathbf{z}}=g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon},\\mathbf{x})\\text{\\quad with \\quad}\\boldsymbol{\\epsilon}\\sim p(\\boldsymbol{\\epsilon})over~ start_ARG bold_z end_ARG = italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_\u03f5 , bold_x ) with bold_italic_\u03f5 \u223c italic_p ( bold_italic_\u03f5 )(4)See section\u00a02.4 for general strategies for chosing such an approriate distribution p(\u03f5)\ud835\udc5dbold-italic-\u03f5p(\\boldsymbol{\\epsilon})italic_p ( bold_italic_\u03f5 ) and function g\u03d5(\u03f5,\ud835\udc31)subscript\ud835\udc54bold-italic-\u03d5bold-italic-\u03f5\ud835\udc31g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon},\\mathbf{x})italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_\u03f5 , bold_x ).We can now form Monte Carlo estimates of expectations of some function f(\ud835\udc33)\ud835\udc53\ud835\udc33f(\\mathbf{z})italic_f ( bold_z ) w.r.t. q\u03d5(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) as follows:\ud835\udd3cq\u03d5(\ud835\udc33|\ud835\udc31(i))[f(\ud835\udc33)]=\ud835\udd3cp(\u03f5)[f(g\u03d5(\u03f5,\ud835\udc31(i)))]subscript\ud835\udd3csubscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33superscript\ud835\udc31\ud835\udc56delimited-[]\ud835\udc53\ud835\udc33subscript\ud835\udd3c\ud835\udc5dbold-italic-\u03f5delimited-[]\ud835\udc53subscript\ud835\udc54bold-italic-\u03d5bold-italic-\u03f5superscript\ud835\udc31\ud835\udc56\\displaystyle\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}^{(i)})}\\left[f(\\mathbf{z})\\right]=\\mathbb{E}_{p(\\boldsymbol{\\epsilon})}\\left[f(g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon},\\mathbf{x}^{(i)}))\\right]blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ italic_f ( bold_z ) ] = blackboard_E start_POSTSUBSCRIPT italic_p ( bold_italic_\u03f5 ) end_POSTSUBSCRIPT [ italic_f ( italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_\u03f5 , bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) ) ]\u22431L\u2211l=1Lf(g\u03d5(\u03f5(l),\ud835\udc31(i)))\u00a0where\u00a0\u03f5(l)\u223cp(\u03f5)similar-to-or-equalsabsent1\ud835\udc3fsuperscriptsubscript\ud835\udc591\ud835\udc3f\ud835\udc53subscript\ud835\udc54bold-italic-\u03d5superscriptbold-italic-\u03f5\ud835\udc59superscript\ud835\udc31\ud835\udc56\u00a0where\u00a0superscriptbold-italic-\u03f5\ud835\udc59similar-to\ud835\udc5dbold-italic-\u03f5\\displaystyle\\simeq\\frac{1}{L}\\sum_{l=1}^{L}{f(g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon}^{(l)},\\mathbf{x}^{(i)}))}\\text{\\quad where \\quad}\\boldsymbol{\\epsilon}^{(l)}\\sim p(\\boldsymbol{\\epsilon})\u2243 divide start_ARG 1 end_ARG start_ARG italic_L end_ARG \u2211 start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_f ( italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT , bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) ) where bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT \u223c italic_p ( bold_italic_\u03f5 )(5)We apply this technique to the variational lower bound (eq.\u00a0(2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator \u2112~A(\ud835\udf3d,\u03d5;\ud835\udc31(i))\u2243\u2112(\ud835\udf3d,\u03d5;\ud835\udc31(i))similar-to-or-equalssuperscript~\u2112\ud835\udc34\ud835\udf3dbold-italic-\u03d5superscript\ud835\udc31\ud835\udc56\u2112\ud835\udf3dbold-italic-\u03d5superscript\ud835\udc31\ud835\udc56\\widetilde{\\mathcal{L}}^{A}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})\\simeq\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})over~ start_ARG caligraphic_L end_ARG start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) \u2243 caligraphic_L ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ):\u2112~A(\ud835\udf3d,\u03d5;\ud835\udc31(i))superscript~\u2112\ud835\udc34\ud835\udf3dbold-italic-\u03d5superscript\ud835\udc31\ud835\udc56\\displaystyle\\widetilde{\\mathcal{L}}^{A}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})over~ start_ARG caligraphic_L end_ARG start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT )=1L\u2211l=1Llog\u2061p\ud835\udf3d(\ud835\udc31(i),\ud835\udc33(i,l))\u2212log\u2061q\u03d5(\ud835\udc33(i,l)|\ud835\udc31(i))absent1\ud835\udc3fsuperscriptsubscript\ud835\udc591\ud835\udc3fsubscript\ud835\udc5d\ud835\udf3dsuperscript\ud835\udc31\ud835\udc56superscript\ud835\udc33\ud835\udc56\ud835\udc59subscript\ud835\udc5ebold-italic-\u03d5conditionalsuperscript\ud835\udc33\ud835\udc56\ud835\udc59superscript\ud835\udc31\ud835\udc56\\displaystyle=\\frac{1}{L}\\sum_{l=1}^{L}\\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)},\\mathbf{z}^{(i,l)})-\\log q_{\\boldsymbol{\\phi}}(\\mathbf{z}^{(i,l)}|\\mathbf{x}^{(i)})= divide start_ARG 1 end_ARG start_ARG italic_L end_ARG \u2211 start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) - roman_log italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT )where\u00a0\ud835\udc33(i,l)where\u00a0superscript\ud835\udc33\ud835\udc56\ud835\udc59\\displaystyle\\text{where \\quad}\\mathbf{z}^{(i,l)}where bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT=g\u03d5(\u03f5(i,l),\ud835\udc31(i))\u00a0and\u00a0\u03f5(l)\u223cp(\u03f5)absentsubscript\ud835\udc54bold-italic-\u03d5superscriptbold-italic-\u03f5\ud835\udc56\ud835\udc59superscript\ud835\udc31\ud835\udc56\u00a0and\u00a0superscriptbold-italic-\u03f5\ud835\udc59similar-to\ud835\udc5dbold-italic-\u03f5\\displaystyle=g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon}^{(i,l)},\\mathbf{x}^{(i)})\\text{\\quad and \\quad}\\boldsymbol{\\epsilon}^{(l)}\\sim p(\\boldsymbol{\\epsilon})= italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT , bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) and bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT \u223c italic_p ( bold_italic_\u03f5 )(6)Often, the KL-divergence DKL(q\u03d5(\ud835\udc33|\ud835\udc31(i))||p\ud835\udf3d(\ud835\udc33))D_{KL}(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}^{(i)})||p_{\\boldsymbol{\\theta}}(\\mathbf{z}))italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z ) ) of eq.\u00a0(3) can be integrated analytically (see appendix\u00a0B), such that only the expected reconstruction error \ud835\udd3cq\u03d5(\ud835\udc33|\ud835\udc31(i))[log\u2061p\ud835\udf3d(\ud835\udc31(i)|\ud835\udc33)]subscript\ud835\udd3csubscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33superscript\ud835\udc31\ud835\udc56delimited-[]subscript\ud835\udc5d\ud835\udf3dconditionalsuperscript\ud835\udc31\ud835\udc56\ud835\udc33\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}^{(i)})}\\left[\\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}|\\mathbf{z})\\right]blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z ) ] requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing \u03d5bold-italic-\u03d5\\boldsymbol{\\phi}bold_italic_\u03d5, encouraging the approximate posterior to be close to the prior p\ud835\udf3d(\ud835\udc33)subscript\ud835\udc5d\ud835\udf3d\ud835\udc33p_{\\boldsymbol{\\theta}}(\\mathbf{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z ).This yields a second version of the SGVB estimator \u2112~B(\ud835\udf3d,\u03d5;\ud835\udc31(i))\u2243\u2112(\ud835\udf3d,\u03d5;\ud835\udc31(i))similar-to-or-equalssuperscript~\u2112\ud835\udc35\ud835\udf3dbold-italic-\u03d5superscript\ud835\udc31\ud835\udc56\u2112\ud835\udf3dbold-italic-\u03d5superscript\ud835\udc31\ud835\udc56\\widetilde{\\mathcal{L}}^{B}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})\\simeq\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})over~ start_ARG caligraphic_L end_ARG start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) \u2243 caligraphic_L ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ), corresponding to eq.\u00a0(3), which typically has less variance than the generic estimator:\u2112~B(\ud835\udf3d,\u03d5;\ud835\udc31(i))superscript~\u2112\ud835\udc35\ud835\udf3dbold-italic-\u03d5superscript\ud835\udc31\ud835\udc56\\displaystyle\\widetilde{\\mathcal{L}}^{B}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})over~ start_ARG caligraphic_L end_ARG start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT )=\u2212DKL(q\u03d5(\ud835\udc33|\ud835\udc31(i))||p\ud835\udf3d(\ud835\udc33))+1L\u2211l=1L(logp\ud835\udf3d(\ud835\udc31(i)|\ud835\udc33(i,l)))\\displaystyle=-D_{KL}(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}^{(i)})||p_{\\boldsymbol{\\theta}}(\\mathbf{z}))+\\frac{1}{L}\\sum_{l=1}^{L}(\\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}|\\mathbf{z}^{(i,l)}))= - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z ) ) + divide start_ARG 1 end_ARG start_ARG italic_L end_ARG \u2211 start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) )where\u00a0\ud835\udc33(i,l)where\u00a0superscript\ud835\udc33\ud835\udc56\ud835\udc59\\displaystyle\\text{where \\quad}\\mathbf{z}^{(i,l)}where bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT=g\u03d5(\u03f5(i,l),\ud835\udc31(i))\u00a0and\u00a0\u03f5(l)\u223cp(\u03f5)absentsubscript\ud835\udc54bold-italic-\u03d5superscriptbold-italic-\u03f5\ud835\udc56\ud835\udc59superscript\ud835\udc31\ud835\udc56\u00a0and\u00a0superscriptbold-italic-\u03f5\ud835\udc59similar-to\ud835\udc5dbold-italic-\u03f5\\displaystyle=g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon}^{(i,l)},\\mathbf{x}^{(i)})\\text{\\quad and \\quad}\\boldsymbol{\\epsilon}^{(l)}\\sim p(\\boldsymbol{\\epsilon})= italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT , bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) and bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT \u223c italic_p ( bold_italic_\u03f5 )(7)Given multiple datapoints from a dataset \ud835\udc17\ud835\udc17\\mathbf{X}bold_X with N\ud835\udc41Nitalic_N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\u2112(\ud835\udf3d,\u03d5;\ud835\udc17)\u2243\u2112~M(\ud835\udf3d,\u03d5;\ud835\udc17M)=NM\u2211i=1M\u2112~(\ud835\udf3d,\u03d5;\ud835\udc31(i))similar-to-or-equals\u2112\ud835\udf3dbold-italic-\u03d5\ud835\udc17superscript~\u2112\ud835\udc40\ud835\udf3dbold-italic-\u03d5superscript\ud835\udc17\ud835\udc40\ud835\udc41\ud835\udc40superscriptsubscript\ud835\udc561\ud835\udc40~\u2112\ud835\udf3dbold-italic-\u03d5superscript\ud835\udc31\ud835\udc56\\displaystyle\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{X})\\simeq\\widetilde{\\mathcal{L}}^{M}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{X}^{M})=\\frac{N}{M}\\sum_{i=1}^{M}\\widetilde{\\mathcal{L}}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})caligraphic_L ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_X ) \u2243 over~ start_ARG caligraphic_L end_ARG start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_X start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT ) = divide start_ARG italic_N end_ARG start_ARG italic_M end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT over~ start_ARG caligraphic_L end_ARG ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT )(8)where the minibatch \ud835\udc17M={\ud835\udc31(i)}i=1Msuperscript\ud835\udc17\ud835\udc40superscriptsubscriptsuperscript\ud835\udc31\ud835\udc56\ud835\udc561\ud835\udc40\\mathbf{X}^{M}=\\{\\mathbf{x}^{(i)}\\}_{i=1}^{M}bold_X start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT = { bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT is a randomly drawn sample of M\ud835\udc40Mitalic_M datapoints from the full dataset \ud835\udc17\ud835\udc17\\mathbf{X}bold_X with N\ud835\udc41Nitalic_N datapoints. In our experiments we found that the number of samples L\ud835\udc3fLitalic_L per datapoint can be set to 1111 as long as the minibatch size M\ud835\udc40Mitalic_M was large enough, e.g. M=100\ud835\udc40100M=100italic_M = 100. Derivatives \u2207\ud835\udf3d,\u03d5\u2112~(\ud835\udf3d;\ud835\udc17M)subscript\u2207\ud835\udf3dbold-italic-\u03d5~\u2112\ud835\udf3dsuperscript\ud835\udc17\ud835\udc40\\nabla_{\\boldsymbol{\\theta},\\boldsymbol{\\phi}}\\widetilde{\\mathcal{L}}(\\boldsymbol{\\theta};\\mathbf{X}^{M})\u2207 start_POSTSUBSCRIPT bold_italic_\u03b8 , bold_italic_\u03d5 end_POSTSUBSCRIPT over~ start_ARG caligraphic_L end_ARG ( bold_italic_\u03b8 ; bold_X start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT ) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad\u00a0[DHS10]. See algorithm\u00a01 for a basic approach to compute the stochastic gradients.", "A connection with auto-encoders becomes clear when looking at the objective function given at eq.\u00a0(7). The first term is (the KL divergence of the approximate posterior from the prior) acts as a regularizer, while the second term is a an expected negative reconstruction error.The function g\u03d5(.)g_{\\boldsymbol{\\phi}}(.)italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( . ) is chosen such that it maps a datapoint \ud835\udc31(i)superscript\ud835\udc31\ud835\udc56\\mathbf{x}^{(i)}bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT and a random noise vector \u03f5(l)superscriptbold-italic-\u03f5\ud835\udc59\\boldsymbol{\\epsilon}^{(l)}bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT to a sample from the approximate posterior for that datapoint: \ud835\udc33(i,l)=g\u03d5(\u03f5(l),\ud835\udc31(i))superscript\ud835\udc33\ud835\udc56\ud835\udc59subscript\ud835\udc54bold-italic-\u03d5superscriptbold-italic-\u03f5\ud835\udc59superscript\ud835\udc31\ud835\udc56\\mathbf{z}^{(i,l)}=g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon}^{(l)},\\mathbf{x}^{(i)})bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT = italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT , bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) where \ud835\udc33(i,l)\u223cq\u03d5(\ud835\udc33|\ud835\udc31(i))similar-tosuperscript\ud835\udc33\ud835\udc56\ud835\udc59subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33superscript\ud835\udc31\ud835\udc56\\mathbf{z}^{(i,l)}\\sim q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}^{(i)})bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT \u223c italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ).Subsequently, the sample \ud835\udc33(i,l)superscript\ud835\udc33\ud835\udc56\ud835\udc59\\mathbf{z}^{(i,l)}bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT is then input to function log\u2061p\ud835\udf3d(\ud835\udc31(i)|\ud835\udc33(i,l))subscript\ud835\udc5d\ud835\udf3dconditionalsuperscript\ud835\udc31\ud835\udc56superscript\ud835\udc33\ud835\udc56\ud835\udc59\\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}|\\mathbf{z}^{(i,l)})roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ), which equals the probability density (or mass) of datapoint \ud835\udc31(i)superscript\ud835\udc31\ud835\udc56\\mathbf{x}^{(i)}bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT under the generative model, given \ud835\udc33(i,l)superscript\ud835\udc33\ud835\udc56\ud835\udc59\\mathbf{z}^{(i,l)}bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT. This term is a negative reconstruction error in auto-encoder parlance.", "In order to solve our problem we invoked an alternative method for generating samples from q\u03d5(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ). The essential parameterization trick is quite simple. Let \ud835\udc33\ud835\udc33\\mathbf{z}bold_z be a continuous random variable, and \ud835\udc33\u223cq\u03d5(\ud835\udc33|\ud835\udc31)similar-to\ud835\udc33subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31\\mathbf{z}\\sim q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})bold_z \u223c italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) be some conditional distribution. It is then often possible to express the random variable \ud835\udc33\ud835\udc33\\mathbf{z}bold_z as a deterministic variable \ud835\udc33=g\u03d5(\u03f5,\ud835\udc31)\ud835\udc33subscript\ud835\udc54bold-italic-\u03d5bold-italic-\u03f5\ud835\udc31\\mathbf{z}=g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon},\\mathbf{x})bold_z = italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_\u03f5 , bold_x ), where \u03f5bold-italic-\u03f5\\boldsymbol{\\epsilon}bold_italic_\u03f5 is an auxiliary variable with independent marginal p(\u03f5)\ud835\udc5dbold-italic-\u03f5p(\\boldsymbol{\\epsilon})italic_p ( bold_italic_\u03f5 ), and g\u03d5(.)g_{\\boldsymbol{\\phi}}(.)italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( . ) is some vector-valued function parameterized by \u03d5bold-italic-\u03d5\\boldsymbol{\\phi}bold_italic_\u03d5.", "This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t q\u03d5(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. \u03d5bold-italic-\u03d5\\boldsymbol{\\phi}bold_italic_\u03d5. A proof is as follows. Given the deterministic mapping \ud835\udc33=g\u03d5(\u03f5,\ud835\udc31)\ud835\udc33subscript\ud835\udc54bold-italic-\u03d5bold-italic-\u03f5\ud835\udc31\\mathbf{z}=g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon},\\mathbf{x})bold_z = italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_\u03f5 , bold_x ) we know that q\u03d5(\ud835\udc33|\ud835\udc31)\u220fidzi=p(\u03f5)\u220fid\u03f5isubscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31subscriptproduct\ud835\udc56\ud835\udc51subscript\ud835\udc67\ud835\udc56\ud835\udc5dbold-italic-\u03f5subscriptproduct\ud835\udc56\ud835\udc51subscriptitalic-\u03f5\ud835\udc56q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})\\prod_{i}dz_{i}=p(\\boldsymbol{\\epsilon})\\prod_{i}d\\epsilon_{i}italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) \u220f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_d italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_p ( bold_italic_\u03f5 ) \u220f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_d italic_\u03f5 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Therefore111Note that for infinitesimals we use the notational convention d\ud835\udc33=\u220fidzi\ud835\udc51\ud835\udc33subscriptproduct\ud835\udc56\ud835\udc51subscript\ud835\udc67\ud835\udc56d\\mathbf{z}=\\prod_{i}dz_{i}italic_d bold_z = \u220f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_d italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, \u222bq\u03d5(\ud835\udc33|\ud835\udc31)f(\ud835\udc33)\ud835\udc51\ud835\udc33=\u222bp(\u03f5)f(\ud835\udc33)\ud835\udc51\u03f5=\u222bp(\u03f5)f(g\u03d5(\u03f5,\ud835\udc31))\ud835\udc51\u03f5subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31\ud835\udc53\ud835\udc33differential-d\ud835\udc33\ud835\udc5dbold-italic-\u03f5\ud835\udc53\ud835\udc33differential-dbold-italic-\u03f5\ud835\udc5dbold-italic-\u03f5\ud835\udc53subscript\ud835\udc54bold-italic-\u03d5bold-italic-\u03f5\ud835\udc31differential-dbold-italic-\u03f5\\int q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})f(\\mathbf{z})\\,d\\mathbf{z}=\\int p(\\boldsymbol{\\epsilon})f(\\mathbf{z})\\,d\\boldsymbol{\\epsilon}=\\int p(\\boldsymbol{\\epsilon})f(g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon},\\mathbf{x}))\\,d\\boldsymbol{\\epsilon}\u222b italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) italic_f ( bold_z ) italic_d bold_z = \u222b italic_p ( bold_italic_\u03f5 ) italic_f ( bold_z ) italic_d bold_italic_\u03f5 = \u222b italic_p ( bold_italic_\u03f5 ) italic_f ( italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_\u03f5 , bold_x ) ) italic_d bold_italic_\u03f5. It follows that a differentiable estimator can be constructed: \u222bq\u03d5(\ud835\udc33|\ud835\udc31)f(\ud835\udc33)\ud835\udc51\ud835\udc33\u22431L\u2211l=1Lf(g\u03d5(\ud835\udc31,\u03f5(l)))similar-to-or-equalssubscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31\ud835\udc53\ud835\udc33differential-d\ud835\udc331\ud835\udc3fsuperscriptsubscript\ud835\udc591\ud835\udc3f\ud835\udc53subscript\ud835\udc54bold-italic-\u03d5\ud835\udc31superscriptbold-italic-\u03f5\ud835\udc59\\int q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})f(\\mathbf{z})\\,d\\mathbf{z}\\simeq\\frac{1}{L}\\sum_{l=1}^{L}f(g_{\\boldsymbol{\\phi}}(\\mathbf{x},\\boldsymbol{\\epsilon}^{(l)}))\u222b italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) italic_f ( bold_z ) italic_d bold_z \u2243 divide start_ARG 1 end_ARG start_ARG italic_L end_ARG \u2211 start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_f ( italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_x , bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ) ) where \u03f5(l)\u223cp(\u03f5)similar-tosuperscriptbold-italic-\u03f5\ud835\udc59\ud835\udc5dbold-italic-\u03f5\\boldsymbol{\\epsilon}^{(l)}\\sim p(\\boldsymbol{\\epsilon})bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT \u223c italic_p ( bold_italic_\u03f5 ). In section\u00a02.3 we applied this trick to obtain a differentiable estimator of the variational lower bound.", "Take, for example, the univariate Gaussian case: let z\u223cp(z|x)=\ud835\udca9(\u03bc,\u03c32)similar-to\ud835\udc67\ud835\udc5dconditional\ud835\udc67\ud835\udc65\ud835\udca9\ud835\udf07superscript\ud835\udf0e2z\\sim p(z|x)=\\mathcal{N}(\\mu,\\sigma^{2})italic_z \u223c italic_p ( italic_z | italic_x ) = caligraphic_N ( italic_\u03bc , italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). In this case, a valid reparameterization is z=\u03bc+\u03c3\u03f5\ud835\udc67\ud835\udf07\ud835\udf0eitalic-\u03f5z=\\mu+\\sigma\\epsilonitalic_z = italic_\u03bc + italic_\u03c3 italic_\u03f5, where \u03f5italic-\u03f5\\epsilonitalic_\u03f5 is an auxiliary noise variable \u03f5\u223c\ud835\udca9(0,1)similar-toitalic-\u03f5\ud835\udca901\\epsilon\\sim\\mathcal{N}(0,1)italic_\u03f5 \u223c caligraphic_N ( 0 , 1 ). Therefore, \ud835\udd3c\ud835\udca9(z;\u03bc,\u03c32)[f(z)]=\ud835\udd3c\ud835\udca9(\u03f5;0,1)[f(\u03bc+\u03c3\u03f5)]\u22431L\u2211l=1Lf(\u03bc+\u03c3\u03f5(l))subscript\ud835\udd3c\ud835\udca9\ud835\udc67\ud835\udf07superscript\ud835\udf0e2delimited-[]\ud835\udc53\ud835\udc67subscript\ud835\udd3c\ud835\udca9italic-\u03f501delimited-[]\ud835\udc53\ud835\udf07\ud835\udf0eitalic-\u03f5similar-to-or-equals1\ud835\udc3fsuperscriptsubscript\ud835\udc591\ud835\udc3f\ud835\udc53\ud835\udf07\ud835\udf0esuperscriptitalic-\u03f5\ud835\udc59\\mathbb{E}_{\\mathcal{N}(z;\\mu,\\sigma^{2})}\\left[f(z)\\right]=\\mathbb{E}_{\\mathcal{N}(\\epsilon;0,1)}\\left[f(\\mu+\\sigma\\epsilon)\\right]\\simeq\\frac{1}{L}\\sum_{l=1}^{L}f(\\mu+\\sigma\\epsilon^{(l)})blackboard_E start_POSTSUBSCRIPT caligraphic_N ( italic_z ; italic_\u03bc , italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ italic_f ( italic_z ) ] = blackboard_E start_POSTSUBSCRIPT caligraphic_N ( italic_\u03f5 ; 0 , 1 ) end_POSTSUBSCRIPT [ italic_f ( italic_\u03bc + italic_\u03c3 italic_\u03f5 ) ] \u2243 divide start_ARG 1 end_ARG start_ARG italic_L end_ARG \u2211 start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_f ( italic_\u03bc + italic_\u03c3 italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ) where \u03f5(l)\u223c\ud835\udca9(0,1)similar-tosuperscriptitalic-\u03f5\ud835\udc59\ud835\udca901\\epsilon^{(l)}\\sim\\mathcal{N}(0,1)italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT \u223c caligraphic_N ( 0 , 1 ).", "For which q\u03d5(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) can we choose such a differentiable transformation g\u03d5(.)g_{\\boldsymbol{\\phi}}(.)italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( . ) and auxiliary variable \u03f5\u223cp(\u03f5)similar-tobold-italic-\u03f5\ud835\udc5dbold-italic-\u03f5\\boldsymbol{\\epsilon}\\sim p(\\boldsymbol{\\epsilon})bold_italic_\u03f5 \u223c italic_p ( bold_italic_\u03f5 )? Three basic approaches are:1.Tractable inverse CDF. In this case, let \u03f5\u223c\ud835\udcb0(\ud835\udfce,\ud835\udc08)similar-tobold-italic-\u03f5\ud835\udcb00\ud835\udc08\\boldsymbol{\\epsilon}\\sim\\mathcal{U}(\\mathbf{0},\\mathbf{I})bold_italic_\u03f5 \u223c caligraphic_U ( bold_0 , bold_I ), and let g\u03d5(\u03f5,\ud835\udc31)subscript\ud835\udc54bold-italic-\u03d5bold-italic-\u03f5\ud835\udc31g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon},\\mathbf{x})italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_italic_\u03f5 , bold_x ) be the inverse CDF of q\u03d5(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ). Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal, Gompertz, Gumbel and Erlang distributions.2.Analogous to the Gaussian example, for any \u201dlocation-scale\u201d family of distributions we can choose the standard distribution (with location=0location0\\text{location}=0location = 0, scale=1scale1\\text{scale}=1scale = 1) as the auxiliary variable \u03f5bold-italic-\u03f5\\boldsymbol{\\epsilon}bold_italic_\u03f5, and let g(.)=location+scale\u22c5\u03f5g(.)=\\text{location}+\\text{scale}\\cdot\\boldsymbol{\\epsilon}italic_g ( . ) = location + scale \u22c5 bold_italic_\u03f5. Examples: Laplace, Elliptical, Student\u2019s t, Logistic, Uniform, Triangular and Gaussian distributions.3.Composition: It is often possible to express random variables as different transformations of auxiliary variables. Examples: Log-Normal (exponentiation of normally distributed variable), Gamma (a sum over exponentially distributed variables), Dirichlet (weighted sum of Gamma variates), Beta, Chi-Squared, and F distributions.", "When all three approaches fail, good approximations to the inverse CDF exist requiring computations with time complexity comparable to the PDF (see e.g. \u00a0[Dev86] for some methods).", "In this section we\u2019ll give an example where we use a neural network for the probabilistic encoder q\u03d5(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) (the approximation to the posterior of the generative model p\ud835\udf3d(\ud835\udc31,\ud835\udc33)subscript\ud835\udc5d\ud835\udf3d\ud835\udc31\ud835\udc33p_{\\boldsymbol{\\theta}}(\\mathbf{x},\\mathbf{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x , bold_z )) and where the parameters \u03d5bold-italic-\u03d5\\boldsymbol{\\phi}bold_italic_\u03d5 and \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8 are optimized jointly with the AEVB algorithm.", "Let the prior over the latent variables be the centered isotropic multivariate Gaussian p\ud835\udf3d(\ud835\udc33)=\ud835\udca9(\ud835\udc33;\ud835\udfce,\ud835\udc08)subscript\ud835\udc5d\ud835\udf3d\ud835\udc33\ud835\udca9\ud835\udc330\ud835\udc08p_{\\boldsymbol{\\theta}}(\\mathbf{z})=\\mathcal{N}(\\mathbf{z};\\mathbf{0},\\mathbf{I})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z ) = caligraphic_N ( bold_z ; bold_0 , bold_I ). Note that in this case, the prior lacks parameters. We let p\ud835\udf3d(\ud835\udc31|\ud835\udc33)subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc31\ud835\udc33p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x | bold_z ) be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from \ud835\udc33\ud835\udc33\\mathbf{z}bold_z with a MLP (a fully-connected neural network with a single hidden layer, see appendix\u00a0C). Note the true posterior p\ud835\udf3d(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5d\ud835\udf3dconditional\ud835\udc33\ud835\udc31p_{\\boldsymbol{\\theta}}(\\mathbf{z}|\\mathbf{x})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z | bold_x ) is in this case intractable.While there is much freedom in the form q\u03d5(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ), we\u2019ll assume the true (but intractable) posterior takes on a approximate Gaussian form with an approximately diagonal covariance. In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure222Note that this is just a (simplifying) choice, and not a limitation of our method.:log\u2061q\u03d5(\ud835\udc33|\ud835\udc31(i))subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33superscript\ud835\udc31\ud835\udc56\\displaystyle\\log q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}^{(i)})roman_log italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT )=log\u2061\ud835\udca9(\ud835\udc33;\ud835\udf41(i),\ud835\udf482(i)\ud835\udc08)absent\ud835\udca9\ud835\udc33superscript\ud835\udf41\ud835\udc56superscript\ud835\udf482\ud835\udc56\ud835\udc08\\displaystyle=\\log\\mathcal{N}(\\mathbf{z};\\boldsymbol{\\mu}^{(i)},\\boldsymbol{\\sigma}^{2(i)}\\mathbf{I})= roman_log caligraphic_N ( bold_z ; bold_italic_\u03bc start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , bold_italic_\u03c3 start_POSTSUPERSCRIPT 2 ( italic_i ) end_POSTSUPERSCRIPT bold_I )(9)where the mean and s.d. of the approximate posterior, \ud835\udf41(i)superscript\ud835\udf41\ud835\udc56\\boldsymbol{\\mu}^{(i)}bold_italic_\u03bc start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT and \ud835\udf48(i)superscript\ud835\udf48\ud835\udc56\\boldsymbol{\\sigma}^{(i)}bold_italic_\u03c3 start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT, are outputs of the encoding MLP, i.e. nonlinear functions of datapoint \ud835\udc31(i)superscript\ud835\udc31\ud835\udc56\\mathbf{x}^{(i)}bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT and the variational parameters \u03d5bold-italic-\u03d5\\boldsymbol{\\phi}bold_italic_\u03d5 (see appendix\u00a0C).", "As explained in section\u00a02.4, we sample from the posterior \ud835\udc33(i,l)\u223cq\u03d5(\ud835\udc33|\ud835\udc31(i))similar-tosuperscript\ud835\udc33\ud835\udc56\ud835\udc59subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33superscript\ud835\udc31\ud835\udc56\\mathbf{z}^{(i,l)}\\sim q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}^{(i)})bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT \u223c italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) using \ud835\udc33(i,l)=g\u03d5(\ud835\udc31(i),\u03f5(l))=\ud835\udf41(i)+\ud835\udf48(i)\u2299\u03f5(l)superscript\ud835\udc33\ud835\udc56\ud835\udc59subscript\ud835\udc54bold-italic-\u03d5superscript\ud835\udc31\ud835\udc56superscriptbold-italic-\u03f5\ud835\udc59superscript\ud835\udf41\ud835\udc56direct-productsuperscript\ud835\udf48\ud835\udc56superscriptbold-italic-\u03f5\ud835\udc59\\mathbf{z}^{(i,l)}=g_{\\boldsymbol{\\phi}}(\\mathbf{x}^{(i)},\\boldsymbol{\\epsilon}^{(l)})=\\boldsymbol{\\mu}^{(i)}+\\boldsymbol{\\sigma}^{(i)}\\odot\\boldsymbol{\\epsilon}^{(l)}bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT = italic_g start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ) = bold_italic_\u03bc start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT + bold_italic_\u03c3 start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT \u2299 bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT where \u03f5(l)\u223c\ud835\udca9(\ud835\udfce,\ud835\udc08)similar-tosuperscriptbold-italic-\u03f5\ud835\udc59\ud835\udca90\ud835\udc08\\boldsymbol{\\epsilon}^{(l)}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT \u223c caligraphic_N ( bold_0 , bold_I ). With \u2299direct-product\\odot\u2299 we signify an element-wise product.In this model both p\ud835\udf3d(\ud835\udc33)subscript\ud835\udc5d\ud835\udf3d\ud835\udc33p_{\\boldsymbol{\\theta}}(\\mathbf{z})italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_z ) (the prior) and q\u03d5(\ud835\udc33|\ud835\udc31)subscript\ud835\udc5ebold-italic-\u03d5conditional\ud835\udc33\ud835\udc31q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x})italic_q start_POSTSUBSCRIPT bold_italic_\u03d5 end_POSTSUBSCRIPT ( bold_z | bold_x ) are Gaussian; in this case, we can use the estimator of eq.\u00a0(7) where the KL divergence can be computed and differentiated without estimation (see appendix\u00a0B). The resulting estimator for this model and datapoint \ud835\udc31(i)superscript\ud835\udc31\ud835\udc56\\mathbf{x}^{(i)}bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT is:\u2112(\ud835\udf3d,\u03d5;\ud835\udc31(i))\u2112\ud835\udf3dbold-italic-\u03d5superscript\ud835\udc31\ud835\udc56\\displaystyle\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{x}^{(i)})caligraphic_L ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT )\u224312\u2211j=1J(1+log\u2061((\u03c3j(i))2)\u2212(\u03bcj(i))2\u2212(\u03c3j(i))2)+1L\u2211l=1Llog\u2061p\ud835\udf3d(\ud835\udc31(i)|\ud835\udc33(i,l))similar-to-or-equalsabsent12superscriptsubscript\ud835\udc571\ud835\udc3d1superscriptsuperscriptsubscript\ud835\udf0e\ud835\udc57\ud835\udc562superscriptsuperscriptsubscript\ud835\udf07\ud835\udc57\ud835\udc562superscriptsuperscriptsubscript\ud835\udf0e\ud835\udc57\ud835\udc5621\ud835\udc3fsuperscriptsubscript\ud835\udc591\ud835\udc3fsubscript\ud835\udc5d\ud835\udf3dconditionalsuperscript\ud835\udc31\ud835\udc56superscript\ud835\udc33\ud835\udc56\ud835\udc59\\displaystyle\\simeq\\frac{1}{2}\\sum_{j=1}^{J}\\left(1+\\log((\\sigma_{j}^{(i)})^{2})-(\\mu_{j}^{(i)})^{2}-(\\sigma_{j}^{(i)})^{2}\\right)+\\frac{1}{L}\\sum_{l=1}^{L}\\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}|\\mathbf{z}^{(i,l)})\u2243 divide start_ARG 1 end_ARG start_ARG 2 end_ARG \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_J end_POSTSUPERSCRIPT ( 1 + roman_log ( ( italic_\u03c3 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) - ( italic_\u03bc start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - ( italic_\u03c3 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) + divide start_ARG 1 end_ARG start_ARG italic_L end_ARG \u2211 start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT )where\u00a0\ud835\udc33(i,l)where\u00a0superscript\ud835\udc33\ud835\udc56\ud835\udc59\\displaystyle\\text{where\\quad}\\mathbf{z}^{(i,l)}where bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT=\ud835\udf41(i)+\ud835\udf48(i)\u2299\u03f5(l)\u00a0and\u00a0\u03f5(l)\u223c\ud835\udca9(0,\ud835\udc08)absentsuperscript\ud835\udf41\ud835\udc56direct-productsuperscript\ud835\udf48\ud835\udc56superscriptbold-italic-\u03f5\ud835\udc59\u00a0and\u00a0superscriptbold-italic-\u03f5\ud835\udc59similar-to\ud835\udca90\ud835\udc08\\displaystyle=\\boldsymbol{\\mu}^{(i)}+\\boldsymbol{\\sigma}^{(i)}\\odot\\boldsymbol{\\epsilon}^{(l)}\\text{\\quad and \\quad}\\boldsymbol{\\epsilon}^{(l)}\\sim\\mathcal{N}(0,\\mathbf{I})= bold_italic_\u03bc start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT + bold_italic_\u03c3 start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT \u2299 bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT and bold_italic_\u03f5 start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT \u223c caligraphic_N ( 0 , bold_I )(10)As explained above and in appendix\u00a0C, the decoding term log\u2061p\ud835\udf3d(\ud835\udc31(i)|\ud835\udc33(i,l))subscript\ud835\udc5d\ud835\udf3dconditionalsuperscript\ud835\udc31\ud835\udc56superscript\ud835\udc33\ud835\udc56\ud835\udc59\\log p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}|\\mathbf{z}^{(i,l)})roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) is a Bernoulli or Gaussian MLP, depending on the type of data we are modelling.", "The wake-sleep algorithm\u00a0[HDFN95] is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models. Like our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood.An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint.", "Stochastic variational inference\u00a0[HBWP13] has recently received increasing interest. Recently, [BJP12] introduced a control variate schemes to reduce the high variance of the na\u00efve gradient estimator discussed in section\u00a02.1, and applied to exponential family approximations of the posterior. In\u00a0[RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing the variance of the original gradient estimator. In\u00a0[SK13], a similar reparameterization as in this paper was used in an efficient version of a stochastic variational inference algorithm for learning the natural parameters of exponential-family approximating distributions.", "The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. In \u00a0[Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(\ud835\udc33)=\ud835\udca9(0,\ud835\udc08)\ud835\udc5d\ud835\udc33\ud835\udca90\ud835\udc08p(\\mathbf{z})=\\mathcal{N}(0,\\mathbf{I})italic_p ( bold_z ) = caligraphic_N ( 0 , bold_I ) and a conditional distribution p(\ud835\udc31|\ud835\udc33)=\ud835\udca9(\ud835\udc31;\ud835\udc16\ud835\udc33,\u03f5\ud835\udc08)\ud835\udc5dconditional\ud835\udc31\ud835\udc33\ud835\udca9\ud835\udc31\ud835\udc16\ud835\udc33italic-\u03f5\ud835\udc08p(\\mathbf{x}|\\mathbf{z})=\\mathcal{N}(\\mathbf{x};\\mathbf{W}\\mathbf{z},\\epsilon\\mathbf{I})italic_p ( bold_x | bold_z ) = caligraphic_N ( bold_x ; bold_Wz , italic_\u03f5 bold_I ), specifically the case with infinitesimally small \u03f5italic-\u03f5\\epsilonitalic_\u03f5.", "In relevant recent work on autoencoders\u00a0[VLL+{}^{+}start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT10] it was shown that the training criterion of unregularized autoencoders corresponds to maximization of a lower bound (see the infomax principle\u00a0[Lin89]) of the mutual information between input X\ud835\udc4bXitalic_X and latent representation Z\ud835\udc4dZitalic_Z. Maximizing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional entropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding model\u00a0[VLL+{}^{+}start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT10], i.e. the negative reconstrution error.However, it is well known that this reconstruction criterion is in itself not sufficient for learning useful representations\u00a0[BCV13].Regularization techniques have been proposed to make autoencoders learn useful representations, such as denoising, contractive and sparse autoencoder variants\u00a0 [BCV13]. The SGVB objective contains a regularization term dictated by the variational bound (e.g. eq.\u00a0(10)), lacking the usual nuisance regularization hyperparameter required to learn useful representations.Related are also encoder-decoder architectures such as the predictive sparse decomposition (PSD)\u00a0[KRL08], from which we drew some inspiration. Also relevant are the recently introduced Generative Stochastic Networks\u00a0[BTL13] where noisy auto-encoders learn the transition operator of a Markov chain that samples from the data distribution. In\u00a0[SL10] a recognition model was employed for efficient learning with Deep Boltzmann Machines.These methods are targeted at either unnormalized models (i.e. undirected models like Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm for learning a general class of directed probabilistic models.", "The recently proposed DARN method \u00a0[GMW13], also learns a directed probabilistic model using an auto-encoding structure, however their method applies to binary latent variables.Even more recently, \u00a0[RMW14] also make the connection between auto-encoders, directed proabilistic models and stochastic variational inference using the reparameterization trick we describe in this paper. Their work was developed independently of ours and provides an additional perspective on AEVB.", "We trained generative models of images from the MNIST and Frey Face datasets333Available at http://www.cs.nyu.edu/~roweis/data.html and compared learning algorithms in terms of the variational lower bound, and the estimated marginal likelihood.", "The generative model (encoder) and variational approximation (decoder) from section\u00a03 were used, where the described encoder and decoder have an equal number of hidden units. Since the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval (0,1)01(0,1)( 0 , 1 ) using a sigmoidal activation function at the decoder output.Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder.", "Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator \u2207\ud835\udf3d,\u03d5\u2112(\ud835\udf3d,\u03d5;\ud835\udc17)subscript\u2207\ud835\udf3dbold-italic-\u03d5\u2112\ud835\udf3dbold-italic-\u03d5\ud835\udc17\\nabla_{\\boldsymbol{\\theta},\\boldsymbol{\\phi}}\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\phi};\\mathbf{X})\u2207 start_POSTSUBSCRIPT bold_italic_\u03b8 , bold_italic_\u03d5 end_POSTSUBSCRIPT caligraphic_L ( bold_italic_\u03b8 , bold_italic_\u03d5 ; bold_X ) (see algorithm \u00a01), plus a small weight decay term corresponding to a prior p(\ud835\udf3d)=\ud835\udca9(0,\ud835\udc08)\ud835\udc5d\ud835\udf3d\ud835\udca90\ud835\udc08p(\\boldsymbol{\\theta})=\\mathcal{N}(0,\\mathbf{I})italic_p ( bold_italic_\u03b8 ) = caligraphic_N ( 0 , bold_I ). Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.", "We compared performance of AEVB to the wake-sleep algorithm\u00a0[HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-encoder. All parameters, both variational and generative, were initialized by random sampling from \ud835\udca9(0,0.01)\ud835\udca900.01\\mathcal{N}(0,0.01)caligraphic_N ( 0 , 0.01 ), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad\u00a0[DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size M=100\ud835\udc40100M=100italic_M = 100 were used, with L=1\ud835\udc3f1L=1italic_L = 1 samples per datapoint.", "We trained generative models (decoders) and corresponding encoders (a.k.a. recognition models) having 500500500500 hidden units in case of MNIST, and 200200200200 hidden units in case of the Frey Face dataset (to prevent overfitting, since it is a considerably smaller dataset). The chosen number of hidden units is based on prior literature on auto-encoders, and the relative performance of different algorithms was not very sensitive to these choices. Figure\u00a02 shows the results when comparing the lower bounds. Interestingly, superfluous latent variables did not result in overfitting, which is explained by the regularizing nature of the variational bound.", "For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used.The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC)\u00a0[DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure\u00a03.", "If we choose a low-dimensional latent space (e.g. 2D), we can use the learned encoders (recognition model) to project high-dimensional data to a low-dimensional manifold. See appendix\u00a0A for visualisations of the 2D latent manifolds for the MNIST and Frey Face datasets.", "We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reflected in experimental results.", "Since the SGVB estimator and the AEVB algorithm can be applied to almost any inference and learning problem with continuous latent variables, there are plenty of future directions: (i) learning hierarchical generative architectures with deep neural networks (e.g. convolutional networks) used for the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic Bayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models with latent variables, useful for learning complicated noise distributions."], "figure_types": {"5f5dc5b9a2ba710937e2c413b37b053cd673df02/10-Figure4-1.png": "photograph(s)", "5f5dc5b9a2ba710937e2c413b37b053cd673df02/2-Figure1-1.png": "schematic", "5f5dc5b9a2ba710937e2c413b37b053cd673df02/7-Figure2-1.png": "plot", "5f5dc5b9a2ba710937e2c413b37b053cd673df02/8-Figure3-1.png": "plot"}}, "2105.05233": {"paper_id": "paper_98", "title": "Diffusion Models Beat GANs on Image Synthesis", "arxiv_url": "https://arxiv.org/abs/2105.05233", "s2orc_url": "https://www.semanticscholar.org/paper/64ea8f180d0682e6c18d1eb688afdb2027c02794", "all_figures_tables": {"64ea8f180d0682e6c18d1eb688afdb2027c02794/1-Figure1-1.png": "Figure 1: Selected samples from our best ImageNet 512\u00d7512 model (FID 3.85)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/10-Table5-1.png": "Table 5: Sample quality comparison with state-of-the-art generative models for each task. ADM refers to our ablated diffusion model, and ADM-G additionally uses classifier guidance. LSUN diffusion models are sampled using 1000 steps (see Appendix J). ImageNet diffusion models are sampled using 250 steps, except when we use the DDIM sampler with 25 steps. *No BigGAN-deep model was available at this resolution, so we trained our own. \u2020Values are taken from a previous paper, due to lack of public models or samples. \u2021Results use two-resolution stacks.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/11-Figure6-1.png": "Figure 6: Samples from BigGAN-deep with truncation 1.0 (FID 6.95, left) vs samples from our diffusion model with guidance (FID 4.59, middle) and samples from the training set (right).", "64ea8f180d0682e6c18d1eb688afdb2027c02794/11-Table6-1.png": "Table 6: Comparing our single, upsampling and classifier guided models. For upsampling, we use the upsampling stack from Nichol and Dhariwal [43] combined with our architecture improvements, which we refer to as ADM-U. The base resolution for the two-stage upsampling models is 64 and 128 for the 256 and 512 models, respectively. When combining classifier guidance with upsampling, we only guide the lower resolution model.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/17-Table7-1.png": "Table 7: Throughput of our ImageNet models, measured in Images per V100-sec.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/17-Table8-1.png": "Table 8: Evaluating an ImageNet 128\u00d7128 model throughout training (classifier scale 1.0).", "64ea8f180d0682e6c18d1eb688afdb2027c02794/18-Table10-1.png": "Table 10: Training compute requirements for our diffusion models compared to StyleGAN2 and BigGAN-deep. Training iterations for each diffusion model are mentioned in parenthesis. Compute is measured in V100-days. \u2020ImageNet 256\u00d7256 classifier with 150K iterations (instead of 500K). \u2021ImageNet 64\u00d764 classifier with batch size 256 (instead of 1024). *ImageNet 128\u00d7128 classifier with batch size 256 (instead of 1024).", "64ea8f180d0682e6c18d1eb688afdb2027c02794/18-Table9-1.png": "Table 9: Evaluating an ImageNet 256\u00d7256 model throughout training (classifier scale 1.0).", "64ea8f180d0682e6c18d1eb688afdb2027c02794/20-Figure7-1.png": "Figure 7: Nearest neighbors for samples from a classifier guided model on ImageNet 256\u00d7256. For each image, the top row is a sample, and the remaining rows are the top 3 nearest neighbors from the dataset. The top samples were generated with classifier scale 1 and 250 diffusion sampling steps (FID 4.59). The bottom samples were generated with classifier scale 2.5 and 25 DDIM steps (FID 5.44).", "64ea8f180d0682e6c18d1eb688afdb2027c02794/20-Figure8-1.png": "Figure 8: Samples when increasing the classifier scale from 0.0 (left) to 5.5 (right). Each row corresponds to a fixed noise seed. We observe that the classifier drastically changes some images, while leaving others relatively unaffected.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/21-Figure9-1.png": "Figure 9: Samples from StyleGAN2 (or StyleGAN for bedrooms) with truncation 1.0 (left) vs samples from our diffusion models (middle) and samples from the training set (right).", "64ea8f180d0682e6c18d1eb688afdb2027c02794/24-Figure11-1.png": "Figure 11: The effect of changing temperature for an ImageNet 128\u00d7128 model.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/24-Figure12-1.png": "Figure 12: Samples at temperature 0.98 with epsilon scaling (left) and noise scaling (right).", "64ea8f180d0682e6c18d1eb688afdb2027c02794/27-Table11-1.png": "Table 11: Hyperparameters for diffusion models. *We used 200K iterations for LSUN cat, 250K for LSUN horse, and 500K for LSUN bedroom.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/27-Table12-1.png": "Table 12: Hyperparameters for classification models. *For our ImageNet 128\u00d7128\u2192 512\u00d7512 upsamples, we used a different classifier for the base model, with batch size 1024 and learning rate 6e-5.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/28-Table13-1.png": "Table 13: Hyperparameters for upsampling diffusion models. *We chose this as an optimization, with the intuition that a lower-resolution path should be unnecessary for upsampling 128x128 images.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/28-Table14-1.png": "Table 14: Hyperparameters for classifier-guided sampling.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/29-Table15-1.png": "Table 15: Results of sweeping over 250 step sampling schedules on LSUN bedrooms. The schedule is expressed as a sequence of five integers, where each integer is the number of steps allocated to one fifth of the diffusion process. The first integer corresponding to t \u2208 [0, 199] and the last to t \u2208 [T \u2212 200, T \u2212 1]. Thus, 50, 50, 50, 50, 50 is a uniform schedule, and 250, 0, 0, 0, 0 is a schedule where all timesteps are spent near t = 0.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/30-Figure13-1.png": "Figure 13: Samples from our best 512\u00d7512 model (FID: 3.85). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/31-Figure14-1.png": "Figure 14: Samples from our best 512\u00d7512 model (FID: 3.85). Classes are 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/32-Figure15-1.png": "Figure 15: Difficult class samples from our best 512\u00d7512 model (FID: 3.85). Classes are 432: bassoon, 468: cab, 424: barbershop, 444: bicycle-built-for-two, 981: ballplayer, 550: espresso maker.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/33-Figure16-1.png": "Figure 16: Samples from our guided 512\u00d7512 model using 250 steps with classifier scale 4.0 (FID 7.72). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/34-Figure17-1.png": "Figure 17: Samples from our guided 512\u00d7512 model using 250 steps with classifier scale 4.0 (FID 7.72). Classes are 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/35-Figure18-1.png": "Figure 18: Random samples from our best ImageNet 512\u00d7512 model (FID 3.85).", "64ea8f180d0682e6c18d1eb688afdb2027c02794/36-Figure19-1.png": "Figure 19: Random samples from our guided 512\u00d7512 model using 250 steps with classifier scale 4.0 (FID 7.72).", "64ea8f180d0682e6c18d1eb688afdb2027c02794/37-Figure20-1.png": "Figure 20: Samples using our best 256\u00d7256 model (FID 3.94). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric", "64ea8f180d0682e6c18d1eb688afdb2027c02794/38-Figure21-1.png": "Figure 21: Samples from our guided 256\u00d7256 model using 250 steps with classifier scale 1.0 (FID 4.59). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric", "64ea8f180d0682e6c18d1eb688afdb2027c02794/39-Figure22-1.png": "Figure 22: Samples from our guided 256\u00d7256 model using 25 DDIM steps with classifier scale 2.5 (FID 5.44). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric", "64ea8f180d0682e6c18d1eb688afdb2027c02794/4-Table1-1.png": "Table 1: Ablation of various architecture changes, evaluated at 700K and 1200K iterations", "64ea8f180d0682e6c18d1eb688afdb2027c02794/40-Figure23-1.png": "Figure 23: Random samples from our best 256\u00d7256 model (FID 3.94).", "64ea8f180d0682e6c18d1eb688afdb2027c02794/41-Figure24-1.png": "Figure 24: Random samples from our guided 256\u00d7256 model using 250 steps with classifier scale 1.0 (FID 4.59).", "64ea8f180d0682e6c18d1eb688afdb2027c02794/42-Figure25-1.png": "Figure 25: Random samples from our LSUN bedroom model using 1000 sampling steps. (FID 1.90)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/43-Figure26-1.png": "Figure 26: Random samples from our LSUN horse model using 1000 sampling steps. (FID 2.57)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/44-Figure27-1.png": "Figure 27: Random samples from our LSUN cat model using 1000 sampling steps. (FID 5.57)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/5-Figure2-1.png": "Figure 2: Ablation of various architecture changes, showing FID as a function of wall-clock time. FID evaluated over 10k samples instead of 50k for efficiency.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/5-Table2-1.png": "Table 2: Ablation of various attention configurations. More heads or lower channels per heads both lead to improved FID.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/5-Table3-1.png": "Table 3: Ablating the element-wise operation used when projecting timestep and class embeddings into each residual block. Replacing AdaGN with the Addition + GroupNorm layer from Ho et al. [25] makes FID worse.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/8-Figure3-1.png": "Figure 3: Samples from an unconditional diffusion model with classifier guidance to condition on the class \"Pembroke Welsh corgi\". Using classifier scale 1.0 (left; FID: 33.0) does not produce convincing samples in this class, whereas classifier scale 10.0 (right; FID: 12.0) produces much more class-consistent images.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Figure4-1.png": "Figure 4: Change in sample quality as we vary scale of the classifier gradients for a class-conditional ImageNet 128\u00d7128 model.", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Figure5-1.png": "Figure 5: Trade-offs when varying truncation for BigGAN-deep and gradient scale for classifier guidance. Models are evaluated on ImageNet 128\u00d7128. The BigGAN-deep results were produced using the TFHub model [12] at truncation levels [0.1, 0.2, 0.3, ..., 1.0].", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Table4-1.png": "Table 4: Effect of classifier guidance on sample quality. Both conditional and unconditional models were trained for 2M iterations on ImageNet 256\u00d7256 with batch size 256."}, "referred_figures_tables": [["64ea8f180d0682e6c18d1eb688afdb2027c02794/5-Figure2-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Figure4-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Figure5-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/4-Table1-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Table4-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/10-Table5-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/10-Table5-1.png"], ["64ea8f180d0682e6c18d1eb688afdb2027c02794/5-Figure2-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/4-Table1-1.png"], ["64ea8f180d0682e6c18d1eb688afdb2027c02794/5-Figure2-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/4-Table1-1.png"], ["64ea8f180d0682e6c18d1eb688afdb2027c02794/8-Figure3-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Figure4-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Figure5-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Table4-1.png"], ["64ea8f180d0682e6c18d1eb688afdb2027c02794/8-Figure3-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Figure4-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Figure5-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Table4-1.png"], ["64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Figure4-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Figure5-1.png", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Table4-1.png"], ["64ea8f180d0682e6c18d1eb688afdb2027c02794/10-Table5-1.png"]], "question_id": [0, 4, 6, 8, 9, 10, 11], "question": ["What are the FID values achieved by authors using Diffusion Model on ImageNet?", "Which are the metrics used by authors to compare the performance of the models?", "What is the final improved architecture used by authors for experiments in this paper?", "Why did the authors have to scale the classifier gradients by a constant factor larger than 1?", "How does the trade-off between fidelity and diversity vary with the Gradient Scale?", "What is IS as a measure of fidelity?", "In terms of image synthesis, do the GANs perform better than VQ-VAE or not?"], "question_section": ["Abstract", "Background", "Architecture Improvements", "Classifier Guidance", "Classifier Guidance", "Related Work", "Abstract"], "question_trigger_sentence": ["We show that diffusion models can achieve image sample quality superior to the\ncurrent state-of-the-art generative models. ", "For comparing sample quality across models, we perform quantitative evaluations using the following metrics. While these metrics are often used in practice and correspond well with human judgement, they are not a perfect proxy, and finding better metrics for sample quality evaluation is still an open problem.", "In this section we conduct several architecture ablations to find the model architecture that provides the best sample quality for diffusion models.", "In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1.", "We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4.", "We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity).", "VQ-VAE [65] and VQ-VAE-2 [51] are autoregressive models trained on top of quantized latent codes, greatly reducing the computational resources required to train these models on large images."], "question_type": ["Testing question", "Shallow question", "Deep/complex question", "Deep/complex question", "Deep/complex question", "Testing question", "Shallow question"], "evidential_info": [[{"context": "For all comparisons in this section, we train models on ImageNet 128\\times128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments.", "rationale": "They obtain state-of-the-art image generation on LSUN and ImageNet 64\u00d764. For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs. Table 5 shows the performance of ADM."}, {"context": "Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision.", "rationale": "We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\u00d7256 and 512\u00d7512."}, {"context": "Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art image generation on LSUN and ImageNet 64\\times64. For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs. These models obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as measured by recall, and can even do so using only 25 diffusion steps.", "rationale": "For all comparisons in this section, we train models on ImageNet 128\u00d7128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1."}, {"context": "We also compare guidance to using a two-stage upsampling stack. Nichol and Dhariwal [43] and Saharia et\u00a0al. [53] train two-stage diffusion models by combining a low-resolution diffusion model with a corresponding upsampling diffusion model. In this approach, the upsampling model is trained to upsample images from the training set, and conditions on low-resolution images that are concatenated channel-wise to the model input using a simple interpolation (e.g. bilinear). During sampling, the low-resolution model produces a sample, and then the upsampling model is conditioned on this sample. This greatly improves FID on ImageNet 256\\times256, but does not reach the same performance as state-of-the-art models like BigGAN-deep Nichol and Dhariwal (2021); Saharia et\u00a0al. (2021), as seen in Table 5.", "rationale": "During sampling, the low-resolution model produces a sample, and then the upsampling model is conditioned on this sample. This greatly improves FID on ImageNet 256\\times256, but does not reach the same performance as state-of-the-art models like BigGAN-deep Nichol and Dhariwal (2021); Saharia et al. (2021), as seen in Table 5. In Table 5, we can see the results of ImageNet."}, {"context": "The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et\u00a0al. [25] and the improvements from Nichol and Dhariwal [43] and Song et\u00a0al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et\u00a0al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\\times256 and 512\\times512.", "rationale": "Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. Table 4 shows the results on ImageNet 256x256."}], [{"context": "Inception Score (IS) was proposed by Salimans et\u00a0al. [54], and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS Barratt and Sharma (2018). To better capture diversity than IS, Fr\u00e9chet Inception Distance (FID) was proposed by Heusel et\u00a0al. [23], who argued that it is more consistent with human judgement than Inception Score. FID provides a symmetric measure of the distance between two image distributions in the Inception-V3 Szegedy et\u00a0al. (2015) latent space. Recently, sFID was proposed by Nash et\u00a0al. [42] as a version of FID that uses spatial features rather than the standard pooled features. They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure. Finally, Kynk\u00e4\u00e4nniemi et\u00a0al. [32] proposed Improved Precision and Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into the data manifold (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall).", "rationale": "We use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work [27, 28, 5, 25]. We use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage."}, {"context": "We use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work Karras et\u00a0al. (2019a, b); Brock et\u00a0al. (2018); Ho et\u00a0al. (2020). We use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage. When comparing against other methods, we re-compute these metrics using public samples or models whenever possible. This is for two reasons: first, some papers Karras et\u00a0al. (2019a, b); Ho et\u00a0al. (2020) compare against arbitrary subsets of the training set which are not readily available; and second, subtle implementation differences can affect the resulting FID values Parmar et\u00a0al. (2021). To ensure consistent comparisons, we use the entire training set as the reference batch Heusel et\u00a0al. (2017); Brock et\u00a0al. (2018), and evaluate metrics for all models using the same codebase.", "rationale": "Before this paragraph, authors write that \u201cFor comparing sample quality across models, we perform quantitative evaluations using the following metrics.\u201d"}, {"context": "For all comparisons in this section, we train models on ImageNet 128\\times128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments.", "rationale": "In Table 1, authors report FID 700K and 1200K. P7 indicates FID as one of metrics. Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task."}], [{"context": "In this section we conduct several architecture ablations to find the model architecture that provides the best sample quality for diffusion models.", "rationale": "P0 introduces the several architectural changes."}, {"context": "We explore the following architectural changes:", "rationale": "This paragraph summarizes this chapter as a whole. To answer this question, we can concentrate on this section."}, {"context": "For all comparisons in this section, we train models on ImageNet 128\\times128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments.", "rationale": "P2 summarizes the entire paragraph, so you can refer to it to see sections 3 and 5."}, {"context": "In the rest of the paper, we use this final improved model architecture as our default: variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.", "rationale": "In the rest of the paper, we use this final improved model architecture as our default: variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks."}, {"context": "The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et\u00a0al. [25] and the improvements from Nichol and Dhariwal [43] and Song et\u00a0al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et\u00a0al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\\times256 and 512\\times512.", "rationale": "We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments."}], [{"context": "We can safely ignore the constant term C_{4}, since it corresponds to the normalizing coefficient Z in Equation 2. We have thus found that the conditional transition operator can be approximated by a Gaussian similar to the unconditional transition operator, but with its mean shifted by \\Sigma g. Algorithm 1 summaries the corresponding sampling algorithm. We include an optional scale factor s for the gradients, which we describe in more detail in Section 4.3.", "rationale": "In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1. When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection. Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Figure 3 shows an example of this effect."}, {"context": "In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1. When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection.Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Figure 3 shows an example of this effect.", "rationale": "When using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples."}, {"context": "To understand the effect of scaling classifier gradients, note that s\\cdot\\mathop{}\\!\\nabla_{\\!x}\\log p(y|x)=\\mathop{}\\!\\nabla_{\\!x}\\log\\frac{1}{Z}p(y|x)^{s}, where Z is an arbitrary constant. As a result, the conditioning process is still theoretically grounded in a re-normalized classifier distribution proportional to p(y|x)^{s}. When s>1, this distribution becomes sharper than p(y|x), since larger values are amplified by the exponent. In other words, using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.", "rationale": "Figure 4 shows the effect of the varying scale of the classifier gradients for a class-conditional ImageNet 128\u00d7128 model. Figure 4 demonstrates that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity)."}, {"context": "Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision.", "rationale": "In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples [61]."}, {"context": "The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et\u00a0al. [25] and the improvements from Nichol and Dhariwal [43] and Song et\u00a0al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et\u00a0al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\\times256 and 512\\times512.", "rationale": "We include an optional scale factor s for the gradients, which we describe in more detail in Section 4.3."}], [{"context": "We hypothesize that the gap between diffusion models and GANs stems from at least two factors: first, that the model architectures used by recent GAN literature have been heavily explored and refined; second, that GANs are able to trade off diversity for fidelity, producing high quality samples but not covering the whole distribution. We aim to bring these benefits to diffusion models, first by improving model architecture and then by devising a scheme for trading off diversity for fidelity. With these improvements, we achieve a new state-of-the-art, surpassing GANs on several different metrics and datasets.", "rationale": "Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point."}, {"context": "In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1. When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection.Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Figure 3 shows an example of this effect.", "rationale": "Using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples."}, {"context": "To understand the effect of scaling classifier gradients, note that s\\cdot\\mathop{}\\!\\nabla_{\\!x}\\log p(y|x)=\\mathop{}\\!\\nabla_{\\!x}\\log\\frac{1}{Z}p(y|x)^{s}, where Z is an arbitrary constant. As a result, the conditioning process is still theoretically grounded in a re-normalized classifier distribution proportional to p(y|x)^{s}. When s>1, this distribution becomes sharper than p(y|x), since larger values are amplified by the exponent. In other words, using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.", "rationale": "In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity."}, {"context": "Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision.", "rationale": "In P4, they state that devising a scheme for trading off diversity for fidelity, they acheive a new state-of-the-art. Next paragraph of P4, they write that the trading off is described in section 4."}, {"context": "The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et\u00a0al. [25] and the improvements from Nichol and Dhariwal [43] and Song et\u00a0al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et\u00a0al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\\times256 and 512\\times512.", "rationale": "When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection. Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Figure 3 shows an example of this effect."}, {"context": "We have shown that diffusion models, a class of likelihood-based models with a stationary training objective, can obtain better sample quality than state-of-the-art GANs. Our improved architecture is sufficient to achieve this on unconditional image generation tasks, and our classifier guidance technique allows us to do so on class-conditional tasks. In the latter case, we find that the scale of the classifier gradients can be adjusted to trade off diversity for fidelity. These guided diffusion models can reduce the sampling time gap between GANs and diffusion models, although diffusion models still require multiple forward passes during sampling. Finally, by combining guidance with upsampling, we can further improve sample quality on high-resolution conditional image synthesis.", "rationale": "We find that the scale of the classifier gradients can be adjusted to trade off diversity for fidelity."}], [{"context": "Inception Score (IS) was proposed by Salimans et\u00a0al. [54], and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS Barratt and Sharma (2018). To better capture diversity than IS, Fr\u00e9chet Inception Distance (FID) was proposed by Heusel et\u00a0al. [23], who argued that it is more consistent with human judgement than Inception Score. FID provides a symmetric measure of the distance between two image distributions in the Inception-V3 Szegedy et\u00a0al. (2015) latent space. Recently, sFID was proposed by Nash et\u00a0al. [42] as a version of FID that uses spatial features rather than the standard pooled features. They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure. Finally, Kynk\u00e4\u00e4nniemi et\u00a0al. [32] proposed Improved Precision and Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into the data manifold (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall).", "rationale": "It measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS."}, {"context": "Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision.", "rationale": "We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity)."}], [{"context": "GANs Goodfellow et\u00a0al. (2014) currently hold the state-of-the-art on most image generation tasks Brock et\u00a0al. (2018); Wu et\u00a0al. (2019); Karras et\u00a0al. (2019b) as measured by sample quality metrics such as FID Heusel et\u00a0al. (2017), Inception Score Salimans et\u00a0al. (2016) and Precision Kynk\u00e4\u00e4nniemi et\u00a0al. (2019). However, some of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity than state-of-the-art likelihood-based models Razavi et\u00a0al. (2019); Nichol and Dhariwal (2021); Nash et\u00a0al. (2021). Furthermore, GANs are often difficult to train, collapsing without carefully selected hyperparameters and regularizers Brock et\u00a0al. (2018); Miyato et\u00a0al. (2018); Brock et\u00a0al. (2016).", "rationale": "GANs [19] currently hold the state-of-the-art on most image generation tasks [5, 68, 28] as measured by sample quality metrics such as FID [23], Inception Score [54] and Precision [32]. However, some of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity than state-of-the-art likelihood-based like VQ-VAE-2 Razavi et al. (2019)."}, {"context": "While GANs hold the state-of-the-art, their drawbacks make them difficult to scale and apply to new domains. As a result, much work has been done to achieve GAN-like sample quality with likelihood-based models Razavi et\u00a0al. (2019); Ho et\u00a0al. (2020); Nash et\u00a0al. (2021); Child (2021). While these models capture more diversity and are typically easier to scale and train than GANs, they still fall short in terms of visual sample quality. Furthermore, except for VAEs, sampling from these models is slower than GANs in terms of wall-clock time.", "rationale": "Much work has been done to achieve GAN-like sample quality with likelihood-based models [51, 25, 42, 9]. While these models capture more diversity and are typically easier to scale and train than GANs, they still fall short in terms of visual sample quality."}, {"context": "Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art image generation on LSUN and ImageNet 64\\times64. For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs. These models obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as measured by recall, and can even do so using only 25 diffusion steps.", "rationale": "Other likelihood-based models have been shown to produce high-fidelity image samples. VQ-VAE [65] and VQ-VAE-2 [51] are autoregressive models trained on top of quantized latent codes, greatly reducing the computational resources required to train these models on large images. These models produce diverse and high quality images, but still fall short of GANs without expensive rejection sampling and special metrics to compensate for blurriness."}, {"context": "Other likelihood-based models have been shown to produce high-fidelity image samples. VQ-VAE van\u00a0den Oord et\u00a0al. (2017) and VQ-VAE-2 Razavi et\u00a0al. (2019) are autoregressive models trained on top of quantized latent codes, greatly reducing the computational resources required to train these models on large images. These models produce diverse and high quality images, but still fall short of GANs without expensive rejection sampling and special metrics to compensate for blurriness. DCTransformer Nash et\u00a0al. (2021) is a related method which relies on a more intelligent compression scheme. VAEs are another promising class of likelihood-based models, and recent methods such as NVAE Vahdat and Kautz (2020) and VDVAE Child (2021) have successfully been applied to difficult image generation domains. Energy-based models are another class of likelihood-based models with a rich history Ackley et\u00a0al. (1985); Dayan et\u00a0al. (1995); Hinton (2002). Sampling from the EBM distribution is challenging, and Xie et\u00a0al. [70] demonstrate that Langevin dynamics can be used to sample coherent images from these models. Du and Mordatch [15] further improve upon this approach, obtaining high quality images. More recently, Gao et\u00a0al. [18] incorporate diffusion steps into an energy-based model, and find that doing so improves image samples from these models.", "rationale": "Table 5 shows that in ImageNet 256x256 experiment, BigGAN-deep beats VQ-VAE2."}]], "composition": ["They obtain state-of-the-art image generation on ImageNet 64\u00d764. For higher resolution ImageNet. Table 5 shows the performance of ADM. Metrics include FID, sFID, Prec, Rec.", "They use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work. Moreover, they use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage. In Table 4, they report FID, sFID, IS, Precision, and Recall as metrics.", "They use variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.", "When using a scale of 1, they observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection. Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. When using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.", "When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection. Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.", "IS measures of fidelity but it has a drawback that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS.", "Fidelity can be higher, but GANs are not always better in terms of low diversity. In table 5 ImageNet256x256 experiment, BigGAN-deep beats VA-VAE2 about FID, sFID, Precision but lose about Recall."], "Is_figure_in_evidence": [true, true, true, true, true, true, false], "Is_table_in_evidence": [true, false, false, false, false, false, true], "question_key": ["1524", "1527", "1528", "1529", "1530", "1531", "1532"], "passages": ["Over the past few years, generative models have gained the ability to generate human-like natural language Brown et\u00a0al. (2020), infinite high-quality synthetic images Brock et\u00a0al. (2018); Karras et\u00a0al. (2019b); Razavi et\u00a0al. (2019) and highly diverse human speech and music van\u00a0den Oord et\u00a0al. (2016); Dhariwal et\u00a0al. (2020). These models can be used in a variety of ways, such as generating images from text prompts Zhang et\u00a0al. (2016); Ramesh et\u00a0al. (2021) or learning useful feature representations Donahue and Simonyan (2019); Chen et\u00a0al. (2020a). While these models are already capable of producing realistic images and sound, there is still much room for improvement beyond the current state-of-the-art, and better generative models could have wide-ranging impacts on graphic design, games, music production, and countless other fields.", "GANs Goodfellow et\u00a0al. (2014) currently hold the state-of-the-art on most image generation tasks Brock et\u00a0al. (2018); Wu et\u00a0al. (2019); Karras et\u00a0al. (2019b) as measured by sample quality metrics such as FID Heusel et\u00a0al. (2017), Inception Score Salimans et\u00a0al. (2016) and Precision Kynk\u00e4\u00e4nniemi et\u00a0al. (2019). However, some of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity than state-of-the-art likelihood-based models Razavi et\u00a0al. (2019); Nichol and Dhariwal (2021); Nash et\u00a0al. (2021). Furthermore, GANs are often difficult to train, collapsing without carefully selected hyperparameters and regularizers Brock et\u00a0al. (2018); Miyato et\u00a0al. (2018); Brock et\u00a0al. (2016).", "While GANs hold the state-of-the-art, their drawbacks make them difficult to scale and apply to new domains. As a result, much work has been done to achieve GAN-like sample quality with likelihood-based models Razavi et\u00a0al. (2019); Ho et\u00a0al. (2020); Nash et\u00a0al. (2021); Child (2021). While these models capture more diversity and are typically easier to scale and train than GANs, they still fall short in terms of visual sample quality. Furthermore, except for VAEs, sampling from these models is slower than GANs in terms of wall-clock time.", "Diffusion models are a class of likelihood-based models which have recently been shown to produce high-quality images Sohl-Dickstein et\u00a0al. (2015); Song and Ermon (2020b); Ho et\u00a0al. (2020) while offering desirable properties such as distribution coverage, a stationary training objective, and easy scalability. These models generate samples by gradually removing noise from a signal, and their training objective can be expressed as a reweighted variational lower-bound Ho et\u00a0al. (2020). This class of models already holds the state-of-the-art Song et\u00a0al. (2020b) on CIFAR-10 Krizhevsky et\u00a0al. (2009), but still lags behind GANs on difficult generation datasets like LSUN and ImageNet. Nichol and Dhariwal [43] found that these models improve reliably with increased compute, and can produce high-quality samples even on the difficult ImageNet 256\u00d7\\times\u00d7256 dataset using an upsampling stack. However, the FID of this model is still not competitive with BigGAN-deep Brock et\u00a0al. (2018), the current state-of-the-art on this dataset.", "We hypothesize that the gap between diffusion models and GANs stems from at least two factors: first, that the model architectures used by recent GAN literature have been heavily explored and refined; second, that GANs are able to trade off diversity for fidelity, producing high quality samples but not covering the whole distribution. We aim to bring these benefits to diffusion models, first by improving model architecture and then by devising a scheme for trading off diversity for fidelity. With these improvements, we achieve a new state-of-the-art, surpassing GANs on several different metrics and datasets.", "The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et\u00a0al. [25] and the improvements from Nichol and Dhariwal [43] and Song et\u00a0al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et\u00a0al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\u00d7\\times\u00d7256 and 512\u00d7\\times\u00d7512.", "In this section, we provide a brief overview of diffusion models. For a more detailed mathematical description, we refer the reader to Appendix B.", "On a high level, diffusion models sample from a distribution by reversing a gradual noising process. In particular, sampling starts with noise xTsubscript\ud835\udc65\ud835\udc47x_{T}italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT and produces gradually less-noisy samples xT\u22121,xT\u22122,\u2026subscript\ud835\udc65\ud835\udc471subscript\ud835\udc65\ud835\udc472\u2026x_{T-1},x_{T-2},...italic_x start_POSTSUBSCRIPT italic_T - 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_T - 2 end_POSTSUBSCRIPT , \u2026 until reaching a final sample x0subscript\ud835\udc650x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Each timestep t\ud835\udc61titalic_t corresponds to a certain noise level, and xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT can be thought of as a mixture of a signal x0subscript\ud835\udc650x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT with some noise \u03f5italic-\u03f5\\epsilonitalic_\u03f5 where the signal to noise ratio is determined by the timestep t\ud835\udc61titalic_t. For the remainder of this paper, we assume that the noise \u03f5italic-\u03f5\\epsilonitalic_\u03f5 is drawn from a diagonal Gaussian distribution, which works well for natural images and simplifies various derivations.", "A diffusion model learns to produce a slightly more \u201cdenoised\u201d xt\u22121subscript\ud835\udc65\ud835\udc611x_{t-1}italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT from xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Ho et\u00a0al. [25] parameterize this model as a function \u03f5\u03b8(xt,t)subscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61\\epsilon_{\\theta}(x_{t},t)italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) which predicts the noise component of a noisy sample xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. To train these models, each sample in a minibatch is produced by randomly drawing a data sample x0subscript\ud835\udc650x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, a timestep t\ud835\udc61titalic_t, and noise \u03f5italic-\u03f5\\epsilonitalic_\u03f5, which together give rise to a noised sample xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT (Equation 17).The training objective is then \u2016\u03f5\u03b8(xt,t)\u2212\u03f5\u20162superscriptnormsubscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61italic-\u03f52||\\epsilon_{\\theta}(x_{t},t)-\\epsilon||^{2}| | italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) - italic_\u03f5 | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, i.e. a simple mean-squared error loss between the true noise and the predicted noise (Equation 26).", "It is not immediately obvious how to sample from a noise predictor \u03f5\u03b8(xt,t)subscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61\\epsilon_{\\theta}(x_{t},t)italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ). Recall that diffusion sampling proceeds by repeatedly predicting xt\u22121subscript\ud835\udc65\ud835\udc611x_{t-1}italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT from xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, starting from xTsubscript\ud835\udc65\ud835\udc47x_{T}italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT. Ho et\u00a0al. [25] show that, under reasonable assumptions, we can model the distribution p\u03b8(xt\u22121|xt)subscript\ud835\udc5d\ud835\udf03conditionalsubscript\ud835\udc65\ud835\udc611subscript\ud835\udc65\ud835\udc61p_{\\theta}(x_{t-1}|x_{t})italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) of xt\u22121subscript\ud835\udc65\ud835\udc611x_{t-1}italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT given xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as a diagonal Gaussian \ud835\udca9(xt\u22121;\u03bc\u03b8(xt,t),\u03a3\u03b8(xt,t))\ud835\udca9subscript\ud835\udc65\ud835\udc611subscript\ud835\udf07\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61subscript\u03a3\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61\\mathcal{N}(x_{t-1};\\mu_{\\theta}(x_{t},t),\\Sigma_{\\theta}(x_{t},t))caligraphic_N ( italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; italic_\u03bc start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) , roman_\u03a3 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) ),where the mean \u03bc\u03b8(xt,t)subscript\ud835\udf07\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61\\mu_{\\theta}(x_{t},t)italic_\u03bc start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) can be calculated as a function of \u03f5\u03b8(xt,t)subscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61\\epsilon_{\\theta}(x_{t},t)italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) (Equation 27).The variance \u03a3\u03b8(xt,t)subscript\u03a3\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61\\Sigma_{\\theta}(x_{t},t)roman_\u03a3 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) of this Gaussian distribution can be fixed to a known constant Ho et\u00a0al. (2020) or learned with a separate neural network head Nichol and Dhariwal (2021),and both approaches yield high-quality samples when the total number of diffusion steps T\ud835\udc47Titalic_T is large enough.", "Ho et\u00a0al. [25] observe that the simple mean-sqaured error objective, Lsimplesubscript\ud835\udc3fsimpleL_{\\text{simple}}italic_L start_POSTSUBSCRIPT simple end_POSTSUBSCRIPT, works better in practice than the actual variational lower bound Lvlbsubscript\ud835\udc3fvlbL_{\\text{vlb}}italic_L start_POSTSUBSCRIPT vlb end_POSTSUBSCRIPT that can be derived from interpreting the denoising diffusion model as a VAE. They also note that training with this objective and using their corresponding sampling procedure is equivalent to the denoising score matching model from Song and Ermon [58], who use Langevin dynamics to sample from a denoising model trained with multiple noise levels to produce high quality image samples. We often use \u201cdiffusion models\u201d as shorthand to refer to both classes of models.", "Following the breakthrough work of Song and Ermon [58] and Ho et\u00a0al. [25], several recent papers have proposed improvements to diffusion models. Here we describe a few of these improvements, which we employ for our models.", "Nichol and Dhariwal [43] find that fixing the variance \u03a3\u03b8(xt,t)subscript\u03a3\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61\\Sigma_{\\theta}(x_{t},t)roman_\u03a3 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) to a constant as done in Ho et\u00a0al. [25] is sub-optimal for sampling with fewer diffusion steps, and propose to parameterize \u03a3\u03b8(xt,t)subscript\u03a3\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61\\Sigma_{\\theta}(x_{t},t)roman_\u03a3 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) as a neural network whose output v\ud835\udc63vitalic_v is interpolated as:\u03a3\u03b8(xt,t)subscript\u03a3\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61\\displaystyle\\Sigma_{\\theta}(x_{t},t)roman_\u03a3 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t )=exp\u2061(vlog\u2061\u03b2t+(1\u2212v)log\u2061\u03b2~t)absent\ud835\udc63subscript\ud835\udefd\ud835\udc611\ud835\udc63subscript~\ud835\udefd\ud835\udc61\\displaystyle=\\exp(v\\log\\beta_{t}+(1-v)\\log\\tilde{\\beta}_{t})= roman_exp ( italic_v roman_log italic_\u03b2 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + ( 1 - italic_v ) roman_log over~ start_ARG italic_\u03b2 end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(1)", "Here, \u03b2tsubscript\ud835\udefd\ud835\udc61\\beta_{t}italic_\u03b2 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and \u03b2~tsubscript~\ud835\udefd\ud835\udc61\\tilde{\\beta}_{t}over~ start_ARG italic_\u03b2 end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT (Equation 19) are the variances in Ho et\u00a0al. [25] corresponding to upper and lower bounds for the reverse process variances. Additionally, Nichol and Dhariwal [43] propose a hybrid objective for training both \u03f5\u03b8(xt,t)subscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61\\epsilon_{\\theta}(x_{t},t)italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) and \u03a3\u03b8(xt,t)subscript\u03a3\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61\\Sigma_{\\theta}(x_{t},t)roman_\u03a3 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) using the weighted sum Lsimple+\u03bbLvlbsubscript\ud835\udc3fsimple\ud835\udf06subscript\ud835\udc3fvlbL_{\\text{simple}}+\\lambda L_{\\text{vlb}}italic_L start_POSTSUBSCRIPT simple end_POSTSUBSCRIPT + italic_\u03bb italic_L start_POSTSUBSCRIPT vlb end_POSTSUBSCRIPT. Learning the reverse process variances with their hybrid objective allows sampling with fewer steps without much drop in sample quality. We adopt this objective and parameterization, and use it throughout our experiments.", "Song et\u00a0al. [57] propose DDIM, which formulates an alternative non-Markovian noising process that has the same forward marginals as DDPM, but allows producing different reverse samplers by changing the variance of the reverse noise. By setting this noise to 0, they provide a way to turn any model \u03f5\u03b8(xt,t)subscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61\\epsilon_{\\theta}(x_{t},t)italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) into a deterministic mapping from latents to images, and find that this provides an alternative way to sample with fewer steps. We adopt this sampling approach when using fewer than 50 sampling steps, since Nichol and Dhariwal [43] found it to be beneficial in this regime.", "For comparing sample quality across models, we perform quantitative evaluations using the following metrics. While these metrics are often used in practice and correspond well with human judgement, they are not a perfect proxy, and finding better metrics for sample quality evaluation is still an open problem.", "Inception Score (IS) was proposed by Salimans et\u00a0al. [54], and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS Barratt and Sharma (2018). To better capture diversity than IS, Fr\u00e9chet Inception Distance (FID) was proposed by Heusel et\u00a0al. [23], who argued that it is more consistent with human judgement than Inception Score. FID provides a symmetric measure of the distance between two image distributions in the Inception-V3 Szegedy et\u00a0al. (2015) latent space. Recently, sFID was proposed by Nash et\u00a0al. [42] as a version of FID that uses spatial features rather than the standard pooled features. They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure. Finally, Kynk\u00e4\u00e4nniemi et\u00a0al. [32] proposed Improved Precision and Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into the data manifold (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall).", "We use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work Karras et\u00a0al. (2019a, b); Brock et\u00a0al. (2018); Ho et\u00a0al. (2020). We use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage. When comparing against other methods, we re-compute these metrics using public samples or models whenever possible. This is for two reasons: first, some papers Karras et\u00a0al. (2019a, b); Ho et\u00a0al. (2020) compare against arbitrary subsets of the training set which are not readily available; and second, subtle implementation differences can affect the resulting FID values Parmar et\u00a0al. (2021). To ensure consistent comparisons, we use the entire training set as the reference batch Heusel et\u00a0al. (2017); Brock et\u00a0al. (2018), and evaluate metrics for all models using the same codebase.", "In this section we conduct several architecture ablations to find the model architecture that provides the best sample quality for diffusion models.", "Ho et\u00a0al. [25] introduced the UNet architecture for diffusion models, which Jolicoeur-Martineau et\u00a0al. [26] found to substantially improve sample quality over the previous architectures Song and Ermon (2020a); Lin et\u00a0al. (2016) used for denoising score matching. The UNet model uses a stack of residual layers and downsampling convolutions, followed by a stack of residual layers with upsampling colvolutions, with skip connections connecting the layers with the same spatial size. In addition, they use a global attention layer at the 16\u00d7\\times\u00d716 resolution with a single head, and add a projection of the timestep embedding into each residual block. Song et\u00a0al. [60] found that further changes to the UNet architecture improved performance on the CIFAR-10 Krizhevsky et\u00a0al. (2009) and CelebA-64 Liu et\u00a0al. (2015) datasets. We show the same result on ImageNet 128\u00d7\\times\u00d7128, finding that architecture can indeed give a substantial boost to sample quality on much larger and more diverse datasets at a higher resolution.", "We explore the following architectural changes:", "\u2022Increasing depth versus width, holding model size relatively constant.\u2022Increasing the number of attention heads.\u2022Using attention at 32\u00d7\\times\u00d732, 16\u00d7\\times\u00d716, and 8\u00d7\\times\u00d78 resolutions rather than only at 16\u00d7\\times\u00d716.\u2022Using the BigGAN Brock et\u00a0al. (2018) residual block for upsampling and downsampling the activations, following Song et\u00a0al. (2020b).\u2022Rescaling residual connections with 1212\\frac{1}{\\sqrt{2}}divide start_ARG 1 end_ARG start_ARG square-root start_ARG 2 end_ARG end_ARG, following Song et\u00a0al. (2020b); Karras et\u00a0al. (2019a, b).", "For all comparisons in this section, we train models on ImageNet 128\u00d7\\times\u00d7128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments.", "We also study other attention configurations that better match the Transformer architecture Vaswani et\u00a0al. (2017). To this end, we experimented with either fixing attention heads to a constant, or fixing the number of channels per head. For the rest of the architecture, we use 128 base channels, 2 residual blocks per resolution, multi-resolution attention, and BigGAN up/downsampling, and we train the models for 700K iterations. Table 2 shows our results, indicating that more heads or fewer channels per head improves FID. In Figure 2, we see 64 channels is best for wall-clock time, so we opt to use 64 channels per head as our default. We note that this choice also better matches modern transformer architectures, and is on par with our other configurations in terms of final FID.", "We also experiment with a layer Nichol and Dhariwal (2021) that we refer to as adaptive group normalization (AdaGN), which incorporates the timestep and class embedding into each residual block after a group normalization operation Wu and He (2018), similar to adaptive instance norm Karras et\u00a0al. (2019a) and FiLM Perez et\u00a0al. (2017). We define this layer as AdaGN(h,y)=ys\u00a0GroupNorm(h)+ybAdaGN\u210e\ud835\udc66subscript\ud835\udc66\ud835\udc60\u00a0GroupNorm\u210esubscript\ud835\udc66\ud835\udc4f\\text{AdaGN}(h,y)=y_{s}\\text{ GroupNorm}(h)+y_{b}AdaGN ( italic_h , italic_y ) = italic_y start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT GroupNorm ( italic_h ) + italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT, where h\u210ehitalic_h is the intermediate activations of the residual block following the first convolution, and y=[ys,yb]\ud835\udc66subscript\ud835\udc66\ud835\udc60subscript\ud835\udc66\ud835\udc4fy=[y_{s},y_{b}]italic_y = [ italic_y start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ] is obtained from a linear projection of the timestep and class embedding.", "We had already seen AdaGN improve our earliest diffusion models, and so had it included by default in all our runs. In Table 3, we explicitly ablate this choice, and find that the adaptive group normalization layer indeed improved FID. Both models use 128 base channels and 2 residual blocks per resolution, multi-resolution attention with 64 channels per head, and BigGAN up/downsampling, and were trained for 700K iterations.", "In the rest of the paper, we use this final improved model architecture as our default: variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.", "In addition to employing well designed architectures, GANs for conditional image synthesis Mirza and Osindero (2014); Brock et\u00a0al. (2018) make heavy use of class labels. This often takes the form of class-conditional normalization statistics Dumoulin et\u00a0al. (2017); de\u00a0Vries et\u00a0al. (2017) as well as discriminators with heads that are explicitly designed to behave like classifiers p(y|x)\ud835\udc5dconditional\ud835\udc66\ud835\udc65p(y|x)italic_p ( italic_y | italic_x ) Miyato and Koyama (2018). As further evidence that class information is crucial to the success of these models, Lucic et\u00a0al. [36] find that it is helpful to generate synthetic labels when working in a label-limited regime.", "Given this observation for GANs, it makes sense to explore different ways to condition diffusion models on class labels. We already incorporate class information into normalization layers (Section 3.1). Here, we explore a different approach: exploiting a classifier p(y|x)\ud835\udc5dconditional\ud835\udc66\ud835\udc65p(y|x)italic_p ( italic_y | italic_x ) to improve a diffusion generator. Sohl-Dickstein et\u00a0al. [56] and Song et\u00a0al. [60] show one way to achieve this, wherein a pre-trained diffusion model can be conditioned using the gradients of a classifier. In particular, we can train a classifier p\u03d5(y|xt,t)subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\ud835\udc61p_{\\phi}(y|x_{t},t)italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) on noisy images xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and then use gradients \u2207xtlog\u2061p\u03d5(y|xt,t)subscript\u2207subscript\ud835\udc65\ud835\udc61subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\ud835\udc61\\mathop{}\\!\\nabla_{\\!x_{t}}\\log p_{\\phi}(y|x_{t},t)\u2207 start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) to guide the diffusion sampling process towards an arbitrary class label y\ud835\udc66yitalic_y.", "In this section, we first review two ways of deriving conditional sampling processes using classifiers. We then describe how we use such classifiers in practice to improve sample quality. We choose the notation p\u03d5(y|xt,t)=p\u03d5(y|xt)subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\ud835\udc61subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61p_{\\phi}(y|x_{t},t)=p_{\\phi}(y|x_{t})italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) = italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) and \u03f5\u03b8(xt,t)=\u03f5\u03b8(xt)subscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61\ud835\udc61subscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61\\epsilon_{\\theta}(x_{t},t)=\\epsilon_{\\theta}(x_{t})italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) = italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) for brevity, noting that they refer to separate functions for each timestep t\ud835\udc61titalic_t and at training time the models must be conditioned on the input t\ud835\udc61titalic_t.", "We start with a diffusion model with an unconditional reverse noising process p\u03b8(xt|xt+1)subscript\ud835\udc5d\ud835\udf03conditionalsubscript\ud835\udc65\ud835\udc61subscript\ud835\udc65\ud835\udc611p_{\\theta}(x_{t}|x_{t+1})italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ). To condition this on a label y\ud835\udc66yitalic_y, it suffices to sample each transition111We must also sample xTsubscript\ud835\udc65\ud835\udc47x_{T}italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT conditioned on y\ud835\udc66yitalic_y, but a noisy enough diffusion process causes xTsubscript\ud835\udc65\ud835\udc47x_{T}italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT to be nearly Gaussian even in the conditional case. according top\u03b8,\u03d5(xt|xt+1,y)=Zp\u03b8(xt|xt+1)p\u03d5(y|xt)subscript\ud835\udc5d\ud835\udf03italic-\u03d5conditionalsubscript\ud835\udc65\ud835\udc61subscript\ud835\udc65\ud835\udc611\ud835\udc66\ud835\udc4dsubscript\ud835\udc5d\ud835\udf03conditionalsubscript\ud835\udc65\ud835\udc61subscript\ud835\udc65\ud835\udc611subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\\displaystyle p_{\\theta,\\phi}(x_{t}|x_{t+1},y)=Zp_{\\theta}(x_{t}|x_{t+1})p_{\\phi}(y|x_{t})italic_p start_POSTSUBSCRIPT italic_\u03b8 , italic_\u03d5 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT , italic_y ) = italic_Z italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(2)", "where Z\ud835\udc4dZitalic_Z is a normalizing constant (proof in Appendix H). It is typically intractable to sample from this distribution exactly, but Sohl-Dickstein et\u00a0al. [56] show that it can be approximated as a perturbed Gaussian distribution. Here, we review this derivation.", "Recall that our diffusion model predicts the previous timestep xtsubscript\ud835\udc65\ud835\udc61x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT from timestep xt+1subscript\ud835\udc65\ud835\udc611x_{t+1}italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT using a Gaussian distribution:p\u03b8(xt|xt+1)subscript\ud835\udc5d\ud835\udf03conditionalsubscript\ud835\udc65\ud835\udc61subscript\ud835\udc65\ud835\udc611\\displaystyle p_{\\theta}(x_{t}|x_{t+1})italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT )=\ud835\udca9(\u03bc,\u03a3)absent\ud835\udca9\ud835\udf07\u03a3\\displaystyle=\\mathcal{N}(\\mu,\\Sigma)= caligraphic_N ( italic_\u03bc , roman_\u03a3 )(3)log\u2061p\u03b8(xt|xt+1)subscript\ud835\udc5d\ud835\udf03conditionalsubscript\ud835\udc65\ud835\udc61subscript\ud835\udc65\ud835\udc611\\displaystyle\\log p_{\\theta}(x_{t}|x_{t+1})roman_log italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT )=\u221212(xt\u2212\u03bc)T\u03a3\u22121(xt\u2212\u03bc)+Cabsent12superscriptsubscript\ud835\udc65\ud835\udc61\ud835\udf07\ud835\udc47superscript\u03a31subscript\ud835\udc65\ud835\udc61\ud835\udf07\ud835\udc36\\displaystyle=-\\frac{1}{2}(x_{t}-\\mu)^{T}\\Sigma^{-1}(x_{t}-\\mu)+C= - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bc ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_\u03a3 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bc ) + italic_C(4)", "We can assume that log\u03d5\u2061p(y|xt)subscriptitalic-\u03d5\ud835\udc5dconditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\\log_{\\phi}p(y|x_{t})roman_log start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT italic_p ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) has low curvature compared to \u03a3\u22121superscript\u03a31\\Sigma^{-1}roman_\u03a3 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT. This assumption is reasonable in the limit of infinite diffusion steps, where \u2016\u03a3\u2016\u21920\u2192norm\u03a30||\\Sigma||\\to 0| | roman_\u03a3 | | \u2192 0. In this case, we can approximate log\u2061p\u03d5(y|xt)subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\\log p_{\\phi}(y|x_{t})roman_log italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) using a Taylor expansion around xt=\u03bcsubscript\ud835\udc65\ud835\udc61\ud835\udf07x_{t}=\\muitalic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_\u03bc aslog\u2061p\u03d5(y|xt)subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\\displaystyle\\log p_{\\phi}(y|x_{t})roman_log italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )\u2248log\u2061p\u03d5(y|xt)|xt=\u03bc+(xt\u2212\u03bc)\u2207xtlog\u2061p\u03d5(y|xt)|xt=\u03bcabsentevaluated-atsubscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61subscript\ud835\udc65\ud835\udc61\ud835\udf07evaluated-atsubscript\ud835\udc65\ud835\udc61\ud835\udf07subscript\u2207subscript\ud835\udc65\ud835\udc61subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61subscript\ud835\udc65\ud835\udc61\ud835\udf07\\displaystyle\\approx\\log p_{\\phi}(y|x_{t})|_{x_{t}=\\mu}+(x_{t}-\\mu)\\mathop{}\\!\\nabla_{\\!x_{t}}\\log p_{\\phi}(y|x_{t})|_{x_{t}=\\mu}\u2248 roman_log italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) | start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_\u03bc end_POSTSUBSCRIPT + ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bc ) \u2207 start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) | start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_\u03bc end_POSTSUBSCRIPT(5)=(xt\u2212\u03bc)g+C1absentsubscript\ud835\udc65\ud835\udc61\ud835\udf07\ud835\udc54subscript\ud835\udc361\\displaystyle=(x_{t}-\\mu)g+C_{1}= ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bc ) italic_g + italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(6)", "Here, g=\u2207xtlog\u2061p\u03d5(y|xt)|xt=\u03bc\ud835\udc54evaluated-atsubscript\u2207subscript\ud835\udc65\ud835\udc61subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61subscript\ud835\udc65\ud835\udc61\ud835\udf07g=\\mathop{}\\!\\nabla_{\\!x_{t}}\\log p_{\\phi}(y|x_{t})|_{x_{t}=\\mu}italic_g = \u2207 start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) | start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_\u03bc end_POSTSUBSCRIPT, and C1subscript\ud835\udc361C_{1}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is a constant. This giveslog\u2061(p\u03b8(xt|xt+1)p\u03d5(y|xt))subscript\ud835\udc5d\ud835\udf03conditionalsubscript\ud835\udc65\ud835\udc61subscript\ud835\udc65\ud835\udc611subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\\displaystyle\\log(p_{\\theta}(x_{t}|x_{t+1})p_{\\phi}(y|x_{t}))roman_log ( italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) )\u2248\u221212(xt\u2212\u03bc)T\u03a3\u22121(xt\u2212\u03bc)+(xt\u2212\u03bc)g+C2absent12superscriptsubscript\ud835\udc65\ud835\udc61\ud835\udf07\ud835\udc47superscript\u03a31subscript\ud835\udc65\ud835\udc61\ud835\udf07subscript\ud835\udc65\ud835\udc61\ud835\udf07\ud835\udc54subscript\ud835\udc362\\displaystyle\\approx-\\frac{1}{2}(x_{t}-\\mu)^{T}\\Sigma^{-1}(x_{t}-\\mu)+(x_{t}-\\mu)g+C_{2}\u2248 - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bc ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_\u03a3 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bc ) + ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bc ) italic_g + italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT(7)=\u221212(xt\u2212\u03bc\u2212\u03a3g)T\u03a3\u22121(xt\u2212\u03bc\u2212\u03a3g)+12gT\u03a3g+C2absent12superscriptsubscript\ud835\udc65\ud835\udc61\ud835\udf07\u03a3\ud835\udc54\ud835\udc47superscript\u03a31subscript\ud835\udc65\ud835\udc61\ud835\udf07\u03a3\ud835\udc5412superscript\ud835\udc54\ud835\udc47\u03a3\ud835\udc54subscript\ud835\udc362\\displaystyle=-\\frac{1}{2}(x_{t}-\\mu-\\Sigma g)^{T}\\Sigma^{-1}(x_{t}-\\mu-\\Sigma g)+\\frac{1}{2}g^{T}\\Sigma g+C_{2}= - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bc - roman_\u03a3 italic_g ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_\u03a3 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bc - roman_\u03a3 italic_g ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_g start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_\u03a3 italic_g + italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT(8)=\u221212(xt\u2212\u03bc\u2212\u03a3g)T\u03a3\u22121(xt\u2212\u03bc\u2212\u03a3g)+C3absent12superscriptsubscript\ud835\udc65\ud835\udc61\ud835\udf07\u03a3\ud835\udc54\ud835\udc47superscript\u03a31subscript\ud835\udc65\ud835\udc61\ud835\udf07\u03a3\ud835\udc54subscript\ud835\udc363\\displaystyle=-\\frac{1}{2}(x_{t}-\\mu-\\Sigma g)^{T}\\Sigma^{-1}(x_{t}-\\mu-\\Sigma g)+C_{3}= - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bc - roman_\u03a3 italic_g ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_\u03a3 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_\u03bc - roman_\u03a3 italic_g ) + italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT(9)=log\u2061p(z)+C4,z\u223c\ud835\udca9(\u03bc+\u03a3g,\u03a3)formulae-sequenceabsent\ud835\udc5d\ud835\udc67subscript\ud835\udc364similar-to\ud835\udc67\ud835\udca9\ud835\udf07\u03a3\ud835\udc54\u03a3\\displaystyle=\\log p(z)+C_{4},z\\sim\\mathcal{N}(\\mu+\\Sigma g,\\Sigma)= roman_log italic_p ( italic_z ) + italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , italic_z \u223c caligraphic_N ( italic_\u03bc + roman_\u03a3 italic_g , roman_\u03a3 )(10)", "We can safely ignore the constant term C4subscript\ud835\udc364C_{4}italic_C start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, since it corresponds to the normalizing coefficient Z\ud835\udc4dZitalic_Z in Equation 2. We have thus found that the conditional transition operator can be approximated by a Gaussian similar to the unconditional transition operator, but with its mean shifted by \u03a3g\u03a3\ud835\udc54\\Sigma groman_\u03a3 italic_g. Algorithm 1 summaries the corresponding sampling algorithm. We include an optional scale factor s\ud835\udc60sitalic_s for the gradients, which we describe in more detail in Section 4.3.", "The above derivation for conditional sampling is only valid for the stochastic diffusion sampling process, and cannot be applied to deterministic sampling methods like DDIM Song et\u00a0al. (2020a). To this end, we use a score-based conditioning trick adapted from Song et\u00a0al. [60], which leverages the connection between diffusion models and score matching Song and Ermon (2020b). In particular, if we have a model \u03f5\u03b8(xt)subscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61\\epsilon_{\\theta}(x_{t})italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) that predicts the noise added to a sample, then this can be used to derive a score function:\u2207xtlog\u2061p\u03b8(xt)=\u221211\u2212\u03b1\u00aft\u03f5\u03b8(xt)subscript\u2207subscript\ud835\udc65\ud835\udc61subscript\ud835\udc5d\ud835\udf03subscript\ud835\udc65\ud835\udc6111subscript\u00af\ud835\udefc\ud835\udc61subscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61\\displaystyle\\mathop{}\\!\\nabla_{\\!x_{t}}\\log p_{\\theta}(x_{t})=-\\frac{1}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(x_{t})\u2207 start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = - divide start_ARG 1 end_ARG start_ARG square-root start_ARG 1 - over\u00af start_ARG italic_\u03b1 end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG end_ARG italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(11)", "We can now substitute this into the score function for p(xt)p(y|xt)\ud835\udc5dsubscript\ud835\udc65\ud835\udc61\ud835\udc5dconditional\ud835\udc66subscript\ud835\udc65\ud835\udc61p(x_{t})p(y|x_{t})italic_p ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) italic_p ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ):\u2207xtlog\u2061(p\u03b8(xt)p\u03d5(y|xt))subscript\u2207subscript\ud835\udc65\ud835\udc61subscript\ud835\udc5d\ud835\udf03subscript\ud835\udc65\ud835\udc61subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\\displaystyle\\mathop{}\\!\\nabla_{\\!x_{t}}\\log(p_{\\theta}(x_{t})p_{\\phi}(y|x_{t}))\u2207 start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) )=\u2207xtlog\u2061p\u03b8(xt)+\u2207xtlog\u2061p\u03d5(y|xt)absentsubscript\u2207subscript\ud835\udc65\ud835\udc61subscript\ud835\udc5d\ud835\udf03subscript\ud835\udc65\ud835\udc61subscript\u2207subscript\ud835\udc65\ud835\udc61subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\\displaystyle=\\mathop{}\\!\\nabla_{\\!x_{t}}\\log p_{\\theta}(x_{t})+\\mathop{}\\!\\nabla_{\\!x_{t}}\\log p_{\\phi}(y|x_{t})= \u2207 start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + \u2207 start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(12)=\u221211\u2212\u03b1\u00aft\u03f5\u03b8(xt)+\u2207xtlog\u2061p\u03d5(y|xt)absent11subscript\u00af\ud835\udefc\ud835\udc61subscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61subscript\u2207subscript\ud835\udc65\ud835\udc61subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\\displaystyle=-\\frac{1}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(x_{t})+\\mathop{}\\!\\nabla_{\\!x_{t}}\\log p_{\\phi}(y|x_{t})= - divide start_ARG 1 end_ARG start_ARG square-root start_ARG 1 - over\u00af start_ARG italic_\u03b1 end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG end_ARG italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + \u2207 start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(13)", "Finally, we can define a new epsilon prediction \u03f5^(xt)^italic-\u03f5subscript\ud835\udc65\ud835\udc61\\hat{\\epsilon}(x_{t})over^ start_ARG italic_\u03f5 end_ARG ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) which corresponds to the score of the joint distribution:\u03f5^(xt)\u2254\u03f5\u03b8(xt)\u22121\u2212\u03b1\u00aft\u2207xtlog\u2061p\u03d5(y|xt)\u2254^italic-\u03f5subscript\ud835\udc65\ud835\udc61subscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc611subscript\u00af\ud835\udefc\ud835\udc61subscript\u2207subscript\ud835\udc65\ud835\udc61subscript\ud835\udc5ditalic-\u03d5conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\\displaystyle\\hat{\\epsilon}(x_{t})\\coloneqq\\epsilon_{\\theta}(x_{t})-\\sqrt{1-\\bar{\\alpha}_{t}}\\mathop{}\\!\\nabla_{\\!x_{t}}\\log p_{\\phi}(y|x_{t})over^ start_ARG italic_\u03f5 end_ARG ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2254 italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - square-root start_ARG 1 - over\u00af start_ARG italic_\u03b1 end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG \u2207 start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_y | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )(14)", "We can then use the exact same sampling procedure as used for regular DDIM, but with the modified noise predictions \u03f5^\u03b8(xt)subscript^italic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61\\hat{\\epsilon}_{\\theta}(x_{t})over^ start_ARG italic_\u03f5 end_ARG start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) instead of \u03f5\u03b8(xt)subscriptitalic-\u03f5\ud835\udf03subscript\ud835\udc65\ud835\udc61\\epsilon_{\\theta}(x_{t})italic_\u03f5 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ). Algorithm 2 summaries the corresponding sampling algorithm.", "To apply classifier guidance to a large scale generative task, we train classification models on ImageNet. Our classifier architecture is simply the downsampling trunk of the UNet model with an attention pool Radford et\u00a0al. (2021) at the 8x8 layer to produce the final output. We train these classifiers on the same noising distribution as the corresponding diffusion model, and also add random crops to reduce overfitting. After training, we incorporate the classifier into the sampling process of the diffusion model using Equation 10, as outlined by Algorithm 1.", "In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1. When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection.Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Figure 3 shows an example of this effect.", "To understand the effect of scaling classifier gradients, note that s\u22c5\u2207xlog\u2061p(y|x)=\u2207xlog\u20611Zp(y|x)s\u22c5\ud835\udc60subscript\u2207\ud835\udc65\ud835\udc5dconditional\ud835\udc66\ud835\udc65subscript\u2207\ud835\udc651\ud835\udc4d\ud835\udc5dsuperscriptconditional\ud835\udc66\ud835\udc65\ud835\udc60s\\cdot\\mathop{}\\!\\nabla_{\\!x}\\log p(y|x)=\\mathop{}\\!\\nabla_{\\!x}\\log\\frac{1}{Z}p(y|x)^{s}italic_s \u22c5 \u2207 start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT roman_log italic_p ( italic_y | italic_x ) = \u2207 start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT roman_log divide start_ARG 1 end_ARG start_ARG italic_Z end_ARG italic_p ( italic_y | italic_x ) start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT, where Z\ud835\udc4dZitalic_Z is an arbitrary constant. As a result, the conditioning process is still theoretically grounded in a re-normalized classifier distribution proportional to p(y|x)s\ud835\udc5dsuperscriptconditional\ud835\udc66\ud835\udc65\ud835\udc60p(y|x)^{s}italic_p ( italic_y | italic_x ) start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT. When s>1\ud835\udc601s>1italic_s > 1, this distribution becomes sharper than p(y|x)\ud835\udc5dconditional\ud835\udc66\ud835\udc65p(y|x)italic_p ( italic_y | italic_x ), since larger values are amplified by the exponent. In other words, using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.", "In the above derivations, we assumed that the underlying diffusion model was unconditional, modeling p(x)\ud835\udc5d\ud835\udc65p(x)italic_p ( italic_x ). It is also possible to train conditional diffusion models, p(x|y)\ud835\udc5dconditional\ud835\udc65\ud835\udc66p(x|y)italic_p ( italic_x | italic_y ), and use classifier guidance in the exact same way.Table 4 shows that the sample quality of both unconditional and conditional models can be greatly improved by classifier guidance. We see that, with a high enough scale, the guided unconditional model can get quite close to the FID of an unguided conditional model, although training directly with the class labels still helps. Guiding a conditional model further improves FID.", "Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision.", "To evaluate our improved model architecture on unconditional image generation, we train separate diffusion models on three LSUN Yu et\u00a0al. (2015) classes: bedroom, horse, and cat. To evaluate classifier guidance, we train conditional diffusion models on the ImageNet Russakovsky et\u00a0al. (2014) dataset at 128\u00d7\\times\u00d7128, 256\u00d7\\times\u00d7256, and 512\u00d7\\times\u00d7512 resolution.", "Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art image generation on LSUN and ImageNet 64\u00d7\\times\u00d764. For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs. These models obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as measured by recall, and can even do so using only 25 diffusion steps.", "Figure 6 compares random samples from the best BigGAN-deep model to our best diffusion model. While the samples are of similar perceptual quality, the diffusion model contains more modes than the GAN, such as zoomed ostrich heads, single flamingos, different orientations of cheeseburgers, and a tinca fish with no human holding it. We also check our generated samples for nearest neighbors in the Inception-V3 feature space in Appendix C, and we show additional samples in Appendices K-M.", "We also compare guidance to using a two-stage upsampling stack. Nichol and Dhariwal [43] and Saharia et\u00a0al. [53] train two-stage diffusion models by combining a low-resolution diffusion model with a corresponding upsampling diffusion model. In this approach, the upsampling model is trained to upsample images from the training set, and conditions on low-resolution images that are concatenated channel-wise to the model input using a simple interpolation (e.g. bilinear). During sampling, the low-resolution model produces a sample, and then the upsampling model is conditioned on this sample. This greatly improves FID on ImageNet 256\u00d7\\times\u00d7256, but does not reach the same performance as state-of-the-art models like BigGAN-deep Nichol and Dhariwal (2021); Saharia et\u00a0al. (2021), as seen in Table 5.", "In Table 6, we show that guidance and upsampling improve sample quality along different axes. While upsampling improves precision while keeping a high recall, guidance provides a knob to trade off diversity for much higher precision. We achieve the best FIDs by using guidance at a lower resolution before upsampling to a higher resolution, indicating that these approaches complement one another.", "Score based generative models were introduced by Song and Ermon [59] as a way of modeling a data distribution using its gradients, and then sampling using Langevin dynamics Welling and Teh (2011). Ho et\u00a0al. [25] found a connection between this method and diffusion models Sohl-Dickstein et\u00a0al. (2015), and achieved excellent sample quality by leveraging this connection. After this breakthrough work, many works followed up with more promising results: Kong et\u00a0al. [30] and Chen et\u00a0al. [8] demonstrated that diffusion models work well for audio; Jolicoeur-Martineau et\u00a0al. [26] found that a GAN-like setup could improve samples from these models; Song et\u00a0al. [60] explored ways to leverage techniques from stochastic differential equations to improve the sample quality obtained by score-based models; Song et\u00a0al. [57] and Nichol and Dhariwal [43] proposed methods to improve sampling speed; Nichol and Dhariwal [43] and Saharia et\u00a0al. [53] demonstrated promising results on the difficult ImageNet generation task using upsampling diffusion models. Also related to diffusion models, and following the work of Sohl-Dickstein et\u00a0al. [56], Goyal et\u00a0al. [21] described a technique for learning a model with learned iterative generation steps, and found that it could achieve good image samples when trained with a likelihood objective.", "One missing element from previous work on diffusion models is a way to trade off diversity for fidelity. Other generative techniques provide natural levers for this trade-off. Brock et\u00a0al. [5] introduced the truncation trick for GANs, wherein the latent vector is sampled from a truncated normal distribution. They found that increasing truncation naturally led to a decrease in diversity but an increase in fidelity. More recently, Razavi et\u00a0al. [51] proposed to use classifier rejection sampling to filter out bad samples from an autoregressive likelihood-based model, and found that this technique improved FID. Most likelihood-based models also allow for low-temperature sampling Ackley et\u00a0al. (1985), which provides a natural way to emphasize modes of the data distribution (see Appendix G).", "Other likelihood-based models have been shown to produce high-fidelity image samples. VQ-VAE van\u00a0den Oord et\u00a0al. (2017) and VQ-VAE-2 Razavi et\u00a0al. (2019) are autoregressive models trained on top of quantized latent codes, greatly reducing the computational resources required to train these models on large images. These models produce diverse and high quality images, but still fall short of GANs without expensive rejection sampling and special metrics to compensate for blurriness. DCTransformer Nash et\u00a0al. (2021) is a related method which relies on a more intelligent compression scheme. VAEs are another promising class of likelihood-based models, and recent methods such as NVAE Vahdat and Kautz (2020) and VDVAE Child (2021) have successfully been applied to difficult image generation domains. Energy-based models are another class of likelihood-based models with a rich history Ackley et\u00a0al. (1985); Dayan et\u00a0al. (1995); Hinton (2002). Sampling from the EBM distribution is challenging, and Xie et\u00a0al. [70] demonstrate that Langevin dynamics can be used to sample coherent images from these models. Du and Mordatch [15] further improve upon this approach, obtaining high quality images. More recently, Gao et\u00a0al. [18] incorporate diffusion steps into an energy-based model, and find that doing so improves image samples from these models.", "Other works have controlled generative models with a pre-trained classifier. For example, an emerging body of work Galatolo et\u00a0al. (2021); Patashnik et\u00a0al. (2021); Adverb (2021) aims to optimize GAN latent spaces for text prompts using pre-trained CLIP Radford et\u00a0al. (2021) models. More similar to our work, Song et\u00a0al. [60] uses a classifier to generate class-conditional CIFAR-10 images with a diffusion model. In some cases, classifiers can act as stand-alone generative models. For example, Santurkar et\u00a0al. [55] demonstrate that a robust image classifier can be used as a stand-alone generative model, and Grathwohl et\u00a0al. [22] train a model which is jointly a classifier and an energy-based model.", "While we believe diffusion models are an extremely promising direction for generative modeling, they are still slower than GANs at sampling time due to the use of multiple denoising steps (and therefore forward passes). One promising work in this direction is from Luhman and Luhman [37], who explore a way to distill the DDIM sampling process into a single step model. The samples from the single step model are not yet competitive with GANs, but are much better than previous single-step likelihood-based models. Future work in this direction might be able to completely close the sampling speed gap between diffusion models and GANs without sacrificing image quality.", "Our proposed classifier guidance technique is currently limited to labeled datasets, and we have provided no effective strategy for trading off diversity for fidelity on unlabeled datasets. In the future, our method could be extended to unlabeled data by clustering samples to produce synthetic labels Lucic et\u00a0al. (2019) or by training discriminative models to predict when samples are in the true data distribution or from the sampling distribution.", "The effectiveness of classifier guidance demonstrates that we can obtain powerful generative models from the gradients of a classification function. This could be used to condition pre-trained models in a plethora of ways, for example by conditioning an image generator with a text caption using a noisy version of CLIP Radford et\u00a0al. (2021), similar to recent methods that guide GANs using text prompts Galatolo et\u00a0al. (2021); Patashnik et\u00a0al. (2021); Adverb (2021). It also suggests that large unlabeled datasets could be leveraged in the future to pre-train powerful diffusion models that can later be improved by using a classifier with desirable properties.", "We have shown that diffusion models, a class of likelihood-based models with a stationary training objective, can obtain better sample quality than state-of-the-art GANs. Our improved architecture is sufficient to achieve this on unconditional image generation tasks, and our classifier guidance technique allows us to do so on class-conditional tasks. In the latter case, we find that the scale of the classifier gradients can be adjusted to trade off diversity for fidelity. These guided diffusion models can reduce the sampling time gap between GANs and diffusion models, although diffusion models still require multiple forward passes during sampling. Finally, by combining guidance with upsampling, we can further improve sample quality on high-resolution conditional image synthesis.", "We thank Alec Radford, Mark Chen, Pranav Shyam and Raul Puri for providing feedback on this work."], "figure_types": {"64ea8f180d0682e6c18d1eb688afdb2027c02794/1-Figure1-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/10-Table5-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/11-Figure6-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/11-Table6-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/17-Table7-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/17-Table8-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/18-Table10-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/18-Table9-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/20-Figure7-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/20-Figure8-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/21-Figure9-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/24-Figure11-1.png": "plot", "64ea8f180d0682e6c18d1eb688afdb2027c02794/24-Figure12-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/27-Table11-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/27-Table12-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/28-Table13-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/28-Table14-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/29-Table15-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/30-Figure13-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/31-Figure14-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/32-Figure15-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/33-Figure16-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/34-Figure17-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/35-Figure18-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/36-Figure19-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/37-Figure20-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/38-Figure21-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/39-Figure22-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/4-Table1-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/40-Figure23-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/41-Figure24-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/42-Figure25-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/43-Figure26-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/44-Figure27-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/5-Figure2-1.png": "plot", "64ea8f180d0682e6c18d1eb688afdb2027c02794/5-Table2-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/5-Table3-1.png": "table", "64ea8f180d0682e6c18d1eb688afdb2027c02794/8-Figure3-1.png": "photograph(s)", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Figure4-1.png": "plot", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Figure5-1.png": "plot", "64ea8f180d0682e6c18d1eb688afdb2027c02794/9-Table4-1.png": "table"}}}